Directory structure:
└── 2noise-chattts/
    ├── README.md
    ├── LICENSE
    ├── requirements.txt
    ├── setup.py
    ├── ChatTTS/
    │   ├── __init__.py
    │   ├── core.py
    │   ├── norm.py
    │   ├── config/
    │   │   ├── __init__.py
    │   │   └── config.py
    │   ├── model/
    │   │   ├── __init__.py
    │   │   ├── dvae.py
    │   │   ├── embed.py
    │   │   ├── gpt.py
    │   │   ├── processors.py
    │   │   ├── speaker.py
    │   │   ├── tokenizer.py
    │   │   ├── cuda/
    │   │   │   ├── __init__.py
    │   │   │   ├── patch.py
    │   │   │   └── te_llama.py
    │   │   └── velocity/
    │   │       ├── __init__.py
    │   │       ├── block_manager.py
    │   │       ├── configs.py
    │   │       ├── llama.py
    │   │       ├── llm.py
    │   │       ├── llm_engine.py
    │   │       ├── model_loader.py
    │   │       ├── model_runner.py
    │   │       ├── output.py
    │   │       ├── sampler.py
    │   │       ├── sampling_params.py
    │   │       ├── scheduler.py
    │   │       ├── sequence.py
    │   │       └── worker.py
    │   ├── res/
    │   │   ├── __init__.py
    │   │   ├── homophones_map.json
    │   │   └── sha256_map.json
    │   └── utils/
    │       ├── __init__.py
    │       ├── dl.py
    │       ├── gpu.py
    │       ├── io.py
    │       └── log.py
    ├── docs/
    │   ├── cn/
    │   │   └── README.md
    │   ├── es/
    │   │   └── README.md
    │   ├── fr/
    │   │   └── README.md
    │   ├── jp/
    │   │   └── README.md
    │   ├── kr/
    │   │   └── README.md
    │   └── ru/
    │       └── README.md
    ├── examples/
    │   ├── __init__.py
    │   ├── api/
    │   │   ├── README.md
    │   │   ├── client.py
    │   │   ├── main.py
    │   │   ├── postScript.py
    │   │   └── requirements.txt
    │   ├── cmd/
    │   │   ├── run.py
    │   │   └── stream.py
    │   ├── ipynb/
    │   │   ├── colab.ipynb
    │   │   └── example.ipynb
    │   ├── onnx/
    │   │   ├── README.md
    │   │   ├── exporter.py
    │   │   ├── gpt.py
    │   │   └── modeling_llama.py
    │   └── web/
    │       ├── __init__.py
    │       ├── ex.py
    │       ├── funcs.py
    │       └── webui.py
    ├── tests/
    │   ├── #511.py
    │   ├── #588.py
    │   ├── #655.py
    │   └── testall.sh
    ├── tools/
    │   ├── __init__.py
    │   ├── audio/
    │   │   ├── __init__.py
    │   │   ├── av.py
    │   │   ├── ffmpeg.py
    │   │   ├── np.py
    │   │   └── pcm.py
    │   ├── checksum/
    │   │   ├── main.go
    │   │   └── tmpl.go
    │   ├── llm/
    │   │   ├── __init__.py
    │   │   └── llm.py
    │   ├── logger/
    │   │   ├── __init__.py
    │   │   └── log.py
    │   ├── normalizer/
    │   │   ├── __init__.py
    │   │   ├── en.py
    │   │   └── zh.py
    │   └── seeder/
    │       ├── __init__.py
    │       └── ctx.py
    └── .github/
        └── workflows/
            ├── checksum.yml
            ├── close-issue.yml
            ├── pull-format.yml
            ├── push-format.yml
            ├── unitest.yml
            └── upload-pypi.yml

================================================
FILE: README.md
================================================
<div align="center">

<a href="https://trendshift.io/repositories/10489" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10489" alt="2noise%2FChatTTS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

# ChatTTS
A generative speech model for daily dialogue.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)

[![Huggingface](https://img.shields.io/badge/🤗%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)

**English** | [**简体中文**](docs/cn/README.md) | [**日本語**](docs/jp/README.md) | [**Русский**](docs/ru/README.md) | [**Español**](docs/es/README.md) | [**Français**](docs/fr/README.md) | [**한국어**](docs/kr/README.md)

</div>

## Introduction
> [!Note]
> This repo contains the algorithm infrastructure and some simple examples.

> [!Tip]
> For the extended end-user products, please refer to the index repo [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS/tree/en) maintained by the community.

ChatTTS is a text-to-speech model designed specifically for dialogue scenarios such as LLM assistant.

### Supported Languages
- [x] English
- [x] Chinese
- [ ] Coming Soon...

### Highlights
> You can refer to **[this video on Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)** for the detailed description.

1. **Conversational TTS**: ChatTTS is optimized for dialogue-based tasks, enabling natural and expressive speech synthesis. It supports multiple speakers, facilitating interactive conversations.
2. **Fine-grained Control**: The model could predict and control fine-grained prosodic features, including laughter, pauses, and interjections. 
3. **Better Prosody**: ChatTTS surpasses most of open-source TTS models in terms of prosody. We provide pretrained models to support further research and development.

### Dataset & Model
> [!Important]
> The released model is for academic purposes only.

- The main model is trained with Chinese and English audio data of 100,000+ hours.
- The open-source version on **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** is a 40,000 hours pre-trained model without SFT.

### Roadmap
- [x] Open-source the 40k-hours-base model and spk_stats file.
- [x] Streaming audio generation.
- [x] Open-source DVAE encoder and zero shot inferring code.
- [ ] Multi-emotion controlling.
- [ ] ChatTTS.cpp (new repo in `2noise` org is welcomed)

### Licenses

#### The Code

The code is published under `AGPLv3+` license.

#### The model

The model is published under `CC BY-NC 4.0` license. It is intended for educational and research use, and should not be used for any commercial or illegal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information. The information and data used in this repo, are for academic and research purposes only. The data obtained from publicly available sources, and the authors do not claim any ownership or copyright over the data.

### Disclaimer

ChatTTS is a powerful text-to-speech system. However, it is very important to utilize this technology responsibly and ethically. To limit the use of ChatTTS, we added a small amount of high-frequency noise during the training of the 40,000-hour model, and compressed the audio quality as much as possible using MP3 format, to prevent malicious actors from potentially using it for criminal purposes. At the same time, we have internally trained a detection model and plan to open-source it in the future.

### Contact
> GitHub issues/PRs are always welcomed.

#### Formal Inquiries
For formal inquiries about the model and roadmap, please contact us at **open-source@2noise.com**.

#### Online Chat
##### 1. QQ Group (Chinese Social APP)
- **Group 1**, 808364215
- **Group 2**, 230696694
- **Group 3**, 933639842
- **Group 4**, 608667975

##### 2. Discord Server
Join by clicking [here](https://discord.gg/Ud5Jxgx5yD).

## Get Started
### Clone Repo
```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### Install requirements
#### 1. Install Directly
```bash
pip install --upgrade -r requirements.txt
```

#### 2. Install from conda
```bash
conda create -n chattts python=3.11
conda activate chattts
pip install -r requirements.txt
```

#### Optional: Install vLLM (Linux only)
```bash
pip install safetensors vllm==0.2.7 torchaudio
```

#### Unrecommended Optional: Install TransformerEngine if using NVIDIA GPU (Linux only)
> [!Warning]
> DO NOT INSTALL! 
> The adaptation of TransformerEngine is currently under development and CANNOT run properly now. 
> Only install it on developing purpose. See more details on at #672 #676

> [!Note]
> The installation process is very slow.

```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable
```

#### Unrecommended Optional: Install FlashAttention-2 (mainly NVIDIA GPU)
> [!Warning]
> DO NOT INSTALL! 
> Currently the FlashAttention-2 will slow down the generating speed according to [this issue](https://github.com/huggingface/transformers/issues/26990). 
> Only install it on developing purpose.

> [!Note]
> See supported devices at the [Hugging Face Doc](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2).


```bash
pip install flash-attn --no-build-isolation
```

### Quick Start
> Make sure you are under the project root directory when you execute these commands below.

#### 1. Launch WebUI
```bash
python examples/web/webui.py
```

#### 2. Infer by Command Line
> It will save audio to `./output_audio_n.mp3`

```bash
python examples/cmd/run.py "Your text 1." "Your text 2."
```

## Installation

1. Install the stable version from PyPI
```bash
pip install ChatTTS
```

2. Install the latest version from GitHub
```bash
pip install git+https://github.com/2noise/ChatTTS
```

3. Install from local directory in dev mode
```bash
pip install -e .
```

### Basic Usage

```python
import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) # Set to True for better performance

texts = ["PUT YOUR 1st TEXT HERE", "PUT YOUR 2nd TEXT HERE"]

wavs = chat.infer(texts)

for i in range(len(wavs)):
    """
    In some versions of torchaudio, the first line works but in other versions, so does the second line.
    """
    try:
        torchaudio.save(f"basic_output{i}.wav", torch.from_numpy(wavs[i]).unsqueeze(0), 24000)
    except:
        torchaudio.save(f"basic_output{i}.wav", torch.from_numpy(wavs[i]), 24000)
```

### Advanced Usage

```python
###################################
# Sample a speaker from Gaussian.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # add sampled speaker 
    temperature = .3,   # using custom temperature
    top_P = 0.7,        # top P decode
    top_K = 20,         # top K decode
)

###################################
# For sentence level manual control.

# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# For word level manual control.

text = 'What is [uv_break]your favorite english food?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
"""
In some versions of torchaudio, the first line works but in other versions, so does the second line.
"""
try:
    torchaudio.save("word_level_output.wav", torch.from_numpy(wavs[0]).unsqueeze(0), 24000)
except:
    torchaudio.save("word_level_output.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Example: self introduction</h4></summary>

```python
inputs_en = """
chat T T S is a text to speech model designed for dialogue applications. 
[uv_break]it supports mixed language input [uv_break]and offers multi speaker 
capabilities with precise control over prosodic elements like 
[uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation. 
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
""".replace('\n', '') # English is still experimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save("self_introduction_output.wav", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align="center">

**male speaker**

</td>
<td align="center">

**female speaker**

</td>
</tr>
<tr>
<td align="center">

[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align="center">

[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>


</details>

## FAQ

#### 1. How much VRAM do I need? How about infer speed?
For a 30-second audio clip, at least 4GB of GPU memory is required. For the 4090 GPU, it can generate audio corresponding to approximately 7 semantic tokens per second. The Real-Time Factor (RTF) is around 0.3.

#### 2. Model stability is not good enough, with issues such as multi speakers or poor audio quality.

This is a problem that typically occurs with autoregressive models (for bark and valle). It's generally difficult to avoid. One can try multiple samples to find a suitable result.

#### 3. Besides laughter, can we control anything else? Can we control other emotions?

In the current released model, the only token-level control units are `[laugh]`, `[uv_break]`, and `[lbreak]`. In future versions, we may open-source models with additional emotional control capabilities.

## Acknowledgements
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) and [valle](https://arxiv.org/abs/2301.02111) demonstrate a remarkable TTS result by an autoregressive-style system.
- [fish-speech](https://github.com/fishaudio/fish-speech) reveals capability of GVQ as audio tokenizer for LLM modeling.
- [vocos](https://github.com/gemelo-ai/vocos) which is used as a pretrained vocoder.

## Special Appreciation
- [wlu-audio lab](https://audio.westlake.edu.cn/) for early algorithm experiments.

## Thanks to all contributors for their efforts
[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

<div align="center">

  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>



================================================
FILE: LICENSE
================================================
                    GNU AFFERO GENERAL PUBLIC LICENSE
                       Version 3, 19 November 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU Affero General Public License is a free, copyleft license for
software and other kinds of works, specifically designed to ensure
cooperation with the community in the case of network server software.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
our General Public Licenses are intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  Developers that use our General Public Licenses protect your rights
with two steps: (1) assert copyright on the software, and (2) offer
you this License which gives you legal permission to copy, distribute
and/or modify the software.

  A secondary benefit of defending all users' freedom is that
improvements made in alternate versions of the program, if they
receive widespread use, become available for other developers to
incorporate.  Many developers of free software are heartened and
encouraged by the resulting cooperation.  However, in the case of
software used on network servers, this result may fail to come about.
The GNU General Public License permits making a modified version and
letting the public access it on a server without ever releasing its
source code to the public.

  The GNU Affero General Public License is designed specifically to
ensure that, in such cases, the modified source code becomes available
to the community.  It requires the operator of a network server to
provide the source code of the modified version running there to the
users of that server.  Therefore, public use of a modified version, on
a publicly accessible server, gives the public access to the source
code of the modified version.

  An older license, called the Affero General Public License and
published by Affero, was designed to accomplish similar goals.  This is
a different license, not a version of the Affero GPL, but Affero has
released a new version of the Affero GPL which permits relicensing under
this license.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU Affero General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Remote Network Interaction; Use with the GNU General Public License.

  Notwithstanding any other provision of this License, if you modify the
Program, your modified version must prominently offer all users
interacting with it remotely through a computer network (if your version
supports such interaction) an opportunity to receive the Corresponding
Source of your version by providing access to the Corresponding Source
from a network server at no charge, through some standard or customary
means of facilitating copying of software.  This Corresponding Source
shall include the Corresponding Source for any work covered by version 3
of the GNU General Public License that is incorporated pursuant to the
following paragraph.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the work with which it is combined will remain governed by version
3 of the GNU General Public License.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU Affero General Public License from time to time.  Such new versions
will be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU Affero General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU Affero General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU Affero General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If your software can interact with users remotely through a computer
network, you should also make sure that it provides a way for users to
get its source.  For example, if your program is a web application, its
interface could display a "Source" link that leads users to an archive
of the code.  There are many ways you could offer source, and different
solutions will be better for different programs; see section 13 for the
specific requirements.

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU AGPL, see
<https://www.gnu.org/licenses/>.



================================================
FILE: requirements.txt
================================================
numpy<2.0.0
numba
torch>=2.1.0
torchaudio
tqdm
vector_quantize_pytorch
transformers>=4.41.1
vocos
IPython
gradio
pybase16384
pynini==2.1.5; sys_platform == 'linux'
WeTextProcessing; sys_platform == 'linux'
nemo_text_processing; sys_platform == 'linux'
av
pydub



================================================
FILE: setup.py
================================================
import os
from setuptools import setup, find_packages

version = "v0.0.0"

setup(
    name="chattts",
    version=os.environ.get("CHTTS_VER", version).lstrip("v"),
    description="A generative speech model for daily dialogue",
    long_description=open("README.md", encoding="utf8").read(),
    long_description_content_type="text/markdown",
    author="2noise",
    author_email="open-source@2noise.com",
    maintainer="fumiama",
    url="https://github.com/2noise/ChatTTS",
    packages=find_packages(include=["ChatTTS", "ChatTTS.*"]),
    package_data={
        "ChatTTS.res": ["homophones_map.json", "sha256_map.json"],
    },
    license="AGPLv3+",
    install_requires=[
        "numba",
        "numpy<2.0.0",
        "pybase16384",
        "torch>=2.1.0",
        "torchaudio",
        "tqdm",
        "transformers>=4.41.1",
        "vector_quantize_pytorch",
        "vocos",
    ],
    platforms="any",
    classifiers=[
        "Programming Language :: Python :: 3",
        "Operating System :: OS Independent",
        "License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)",
    ],
)



================================================
FILE: ChatTTS/__init__.py
================================================
from .core import Chat



================================================
FILE: ChatTTS/core.py
================================================
import os
import re
import logging
import tempfile
from dataclasses import dataclass, asdict
from typing import Literal, Optional, List, Tuple, Dict, Union
from json import load
from pathlib import Path

import numpy as np
import torch
from vocos import Vocos
from vocos.pretrained import instantiate_class
from huggingface_hub import snapshot_download

from .config import Config
from .model import DVAE, Embed, GPT, gen_logits, Tokenizer, Speaker
from .utils import (
    load_safetensors,
    check_all_assets,
    download_all_assets,
    select_device,
    get_latest_modified_file,
    del_all,
)
from .utils import logger as utils_logger

from .norm import Normalizer


class Chat:
    def __init__(self, logger=logging.getLogger(__name__)):
        self.logger = logger
        utils_logger.set_logger(logger)

        self.config = Config()

        self.normalizer = Normalizer(
            os.path.join(os.path.dirname(__file__), "res", "homophones_map.json"),
            logger,
        )
        with open(
            os.path.join(os.path.dirname(__file__), "res", "sha256_map.json")
        ) as f:
            self.sha256_map: Dict[str, str] = load(f)

        self.context = GPT.Context()

    def has_loaded(self, use_decoder=False):
        not_finish = False
        check_list = ["vocos", "gpt", "tokenizer", "embed"]

        if use_decoder:
            check_list.append("decoder")
        else:
            check_list.append("dvae")

        for module in check_list:
            if not hasattr(self, module):
                self.logger.warning(f"{module} not initialized.")
                not_finish = True

        return not not_finish

    def download_models(
        self,
        source: Literal["huggingface", "local", "custom"] = "local",
        force_redownload=False,
        custom_path: Optional[torch.serialization.FILE_LIKE] = None,
    ) -> Optional[str]:
        if source == "local":
            download_path = custom_path if custom_path is not None else os.getcwd()
            if (
                not check_all_assets(Path(download_path), self.sha256_map, update=True)
                or force_redownload
            ):
                with tempfile.TemporaryDirectory() as tmp:
                    download_all_assets(tmpdir=tmp, homedir=download_path)
                if not check_all_assets(
                    Path(download_path), self.sha256_map, update=False
                ):
                    self.logger.error(
                        "download to local path %s failed.", download_path
                    )
                    return None
        elif source == "huggingface":
            try:
                download_path = (
                    get_latest_modified_file(
                        os.path.join(
                            os.getenv(
                                "HF_HOME", os.path.expanduser("~/.cache/huggingface")
                            ),
                            "hub/models--2Noise--ChatTTS/snapshots",
                        )
                    )
                    if custom_path is None
                    else get_latest_modified_file(
                        os.path.join(custom_path, "models--2Noise--ChatTTS/snapshots")
                    )
                )
            except:
                download_path = None
            if download_path is None or force_redownload:
                self.logger.log(
                    logging.INFO,
                    f"download from HF: https://huggingface.co/2Noise/ChatTTS",
                )
                try:
                    download_path = snapshot_download(
                        repo_id="2Noise/ChatTTS",
                        allow_patterns=["*.yaml", "*.json", "*.safetensors"],
                        cache_dir=custom_path,
                        force_download=force_redownload,
                    )
                except:
                    download_path = None
                else:
                    self.logger.log(
                        logging.INFO,
                        f"load latest snapshot from cache: {download_path}",
                    )
        elif source == "custom":
            self.logger.log(logging.INFO, f"try to load from local: {custom_path}")
            if not check_all_assets(Path(custom_path), self.sha256_map, update=False):
                self.logger.error("check models in custom path %s failed.", custom_path)
                return None
            download_path = custom_path

        if download_path is None:
            self.logger.error("Model download failed")
            return None

        return download_path

    def load(
        self,
        source: Literal["huggingface", "local", "custom"] = "local",
        force_redownload=False,
        compile: bool = False,
        custom_path: Optional[torch.serialization.FILE_LIKE] = None,
        device: Optional[torch.device] = None,
        coef: Optional[torch.Tensor] = None,
        use_flash_attn=False,
        use_vllm=False,
        experimental: bool = False,
    ) -> bool:
        download_path = self.download_models(source, force_redownload, custom_path)
        if download_path is None:
            return False
        return self._load(
            device=device,
            compile=compile,
            coef=coef,
            use_flash_attn=use_flash_attn,
            use_vllm=use_vllm,
            experimental=experimental,
            **{
                k: os.path.join(download_path, v)
                for k, v in asdict(self.config.path).items()
            },
        )

    def unload(self):
        logger = self.logger
        self.normalizer.destroy()
        del self.normalizer
        del self.sha256_map
        del_list = ["vocos", "gpt", "decoder", "dvae", "tokenizer", "embed"]
        for module in del_list:
            if hasattr(self, module):
                delattr(self, module)
        self.__init__(logger)

    def sample_random_speaker(self) -> str:
        return self.speaker.sample_random()

    def sample_audio_speaker(self, wav: Union[np.ndarray, torch.Tensor]) -> str:
        return self.speaker.encode_prompt(self.dvae.sample_audio(wav))

    @dataclass(repr=False, eq=False)
    class RefineTextParams:
        prompt: str = ""
        top_P: float = 0.7
        top_K: int = 20
        temperature: float = 0.7
        repetition_penalty: float = 1.0
        max_new_token: int = 384
        min_new_token: int = 0
        show_tqdm: bool = True
        ensure_non_empty: bool = True
        manual_seed: Optional[int] = None

    @dataclass(repr=False, eq=False)
    class InferCodeParams(RefineTextParams):
        prompt: str = "[speed_5]"
        spk_emb: Optional[str] = None
        spk_smp: Optional[str] = None
        txt_smp: Optional[str] = None
        temperature: float = 0.3
        repetition_penalty: float = 1.05
        max_new_token: int = 2048
        stream_batch: int = 24
        stream_speed: int = 12000
        pass_first_n_batches: int = 2

    def infer(
        self,
        text,
        stream=False,
        lang=None,
        skip_refine_text=False,
        refine_text_only=False,
        use_decoder=True,
        do_text_normalization=True,
        do_homophone_replacement=True,
        split_text=True,
        max_split_batch=4,
        params_refine_text=RefineTextParams(),
        params_infer_code=InferCodeParams(),
    ):
        self.context.set(False)

        if split_text and isinstance(text, str):
            if "\n" in text:
                text = text.split("\n")
            else:
                text = re.split(r"(?<=。)|(?<=\.\s)", text)
                nt = []
                if isinstance(text, list):
                    for t in text:
                        if t:
                            nt.append(t)
                    text = nt
                else:
                    text = [text]
            self.logger.info("split text into %d parts", len(text))
            self.logger.debug("%s", str(text))

        if len(text) == 0:
            return []

        res_gen = self._infer(
            text,
            stream,
            lang,
            skip_refine_text,
            refine_text_only,
            use_decoder,
            do_text_normalization,
            do_homophone_replacement,
            split_text,
            max_split_batch,
            params_refine_text,
            params_infer_code,
        )
        if stream:
            return res_gen
        elif not refine_text_only:
            stripped_wavs = []
            for wavs in res_gen:
                for wav in wavs:
                    stripped_wavs.append(wav[np.abs(wav) > 1e-5])
            if split_text:
                return [np.concatenate(stripped_wavs)]
            return stripped_wavs
        else:
            return next(res_gen)

    def interrupt(self):
        self.context.set(True)

    @torch.no_grad()
    def _load(
        self,
        vocos_ckpt_path: str = None,
        dvae_ckpt_path: str = None,
        gpt_ckpt_path: str = None,
        embed_path: str = None,
        decoder_ckpt_path: str = None,
        tokenizer_path: str = None,
        device: Optional[torch.device] = None,
        compile: bool = False,
        coef: Optional[str] = None,
        use_flash_attn=False,
        use_vllm=False,
        experimental: bool = False,
    ):
        if device is None:
            device = select_device(experimental=experimental)
            self.logger.info("use device %s", str(device))
        self.device = device
        self.device_gpt = device if "mps" not in str(device) else torch.device("cpu")
        self.compile = compile

        feature_extractor = instantiate_class(
            args=(), init=asdict(self.config.vocos.feature_extractor)
        )
        backbone = instantiate_class(args=(), init=asdict(self.config.vocos.backbone))
        head = instantiate_class(args=(), init=asdict(self.config.vocos.head))
        vocos = (
            Vocos(feature_extractor=feature_extractor, backbone=backbone, head=head)
            .to(
                # Vocos on mps will crash, use cpu fallback.
                # Plus, complex dtype used in the decode process of Vocos is not supported in torch_npu now,
                # so we put this calculation of data on CPU instead of NPU.
                "cpu"
                if "mps" in str(device) or "npu" in str(device)
                else device
            )
            .eval()
        )
        assert vocos_ckpt_path, "vocos_ckpt_path should not be None"
        vocos.load_state_dict(load_safetensors(vocos_ckpt_path))
        self.vocos = vocos
        self.logger.log(logging.INFO, "vocos loaded.")

        # computation of MelSpectrogram on npu is not support now, use cpu fallback.
        dvae_device = torch.device("cpu") if "npu" in str(self.device) else device
        dvae = DVAE(
            decoder_config=asdict(self.config.dvae.decoder),
            encoder_config=asdict(self.config.dvae.encoder),
            vq_config=asdict(self.config.dvae.vq),
            dim=self.config.dvae.decoder.idim,
            coef=coef,
            device=dvae_device,
        )
        coef = str(dvae)
        assert dvae_ckpt_path, "dvae_ckpt_path should not be None"
        dvae.load_pretrained(dvae_ckpt_path, dvae_device)
        self.dvae = dvae.eval()
        self.logger.log(logging.INFO, "dvae loaded.")

        embed = Embed(
            self.config.embed.hidden_size,
            self.config.embed.num_audio_tokens,
            self.config.embed.num_text_tokens,
            self.config.embed.num_vq,
        )
        embed.load_pretrained(embed_path, device=device)
        self.embed = embed.to(device)
        self.logger.log(logging.INFO, "embed loaded.")

        gpt = GPT(
            gpt_config=asdict(self.config.gpt),
            embed=self.embed,
            use_flash_attn=use_flash_attn,
            use_vllm=use_vllm,
            device=device,
            device_gpt=self.device_gpt,
            logger=self.logger,
        ).eval()
        assert gpt_ckpt_path, "gpt_ckpt_path should not be None"
        gpt.load_pretrained(gpt_ckpt_path, embed_path, experimental=experimental)
        gpt.prepare(compile=compile and "cuda" in str(device))
        self.gpt = gpt
        self.logger.log(logging.INFO, "gpt loaded.")

        self.speaker = Speaker(
            self.config.gpt.hidden_size, self.config.spk_stat, device
        )
        self.logger.log(logging.INFO, "speaker loaded.")

        decoder = DVAE(
            decoder_config=asdict(self.config.decoder),
            dim=self.config.decoder.idim,
            coef=coef,
            device=device,
        )
        coef = str(decoder)
        assert decoder_ckpt_path, "decoder_ckpt_path should not be None"
        decoder.load_pretrained(decoder_ckpt_path, device)
        self.decoder = decoder.eval()
        self.logger.log(logging.INFO, "decoder loaded.")

        if tokenizer_path:
            self.tokenizer = Tokenizer(tokenizer_path)
            self.logger.log(logging.INFO, "tokenizer loaded.")

        self.coef = coef

        return self.has_loaded()

    def _infer(
        self,
        text: Union[List[str], str],
        stream=False,
        lang=None,
        skip_refine_text=False,
        refine_text_only=False,
        use_decoder=True,
        do_text_normalization=True,
        do_homophone_replacement=True,
        split_text=True,
        max_split_batch=4,
        params_refine_text=RefineTextParams(),
        params_infer_code=InferCodeParams(),
    ):

        assert self.has_loaded(use_decoder=use_decoder)

        if not isinstance(text, list):
            text = [text]

        text = [
            self.normalizer(
                t,
                do_text_normalization,
                do_homophone_replacement,
                lang,
            )
            for t in text
        ]

        self.logger.debug("normed texts %s", str(text))

        if not skip_refine_text:
            refined = self._refine_text(
                text,
                self.device,
                params_refine_text,
            )
            text_tokens = refined.ids
            text_tokens = [i[i.less(self.tokenizer.break_0_ids)] for i in text_tokens]
            text = self.tokenizer.decode(text_tokens)
            refined.destroy()
            if refine_text_only:
                if split_text and isinstance(text, list):
                    text = "\n".join(text)
                yield text
                return

        if split_text and len(text) > 1 and params_infer_code.spk_smp is None:
            refer_text = text[0]
            result = next(
                self._infer_code(
                    refer_text,
                    False,
                    self.device,
                    use_decoder,
                    params_infer_code,
                )
            )
            wavs = self._decode_to_wavs(
                result.hiddens if use_decoder else result.ids,
                use_decoder,
            )
            result.destroy()
            assert len(wavs), 1
            params_infer_code.spk_smp = self.sample_audio_speaker(wavs[0])
            params_infer_code.txt_smp = refer_text

        if stream:
            length = 0
            pass_batch_count = 0
        if split_text:
            n = len(text) // max_split_batch
            if len(text) % max_split_batch:
                n += 1
        else:
            n = 1
            max_split_batch = len(text)
        for i in range(n):
            text_remain = text[i * max_split_batch :]
            if len(text_remain) > max_split_batch:
                text_remain = text_remain[:max_split_batch]
            if split_text:
                self.logger.info(
                    "infer split %d~%d",
                    i * max_split_batch,
                    i * max_split_batch + len(text_remain),
                )
            for result in self._infer_code(
                text_remain,
                stream,
                self.device,
                use_decoder,
                params_infer_code,
            ):
                wavs = self._decode_to_wavs(
                    result.hiddens if use_decoder else result.ids,
                    use_decoder,
                )
                result.destroy()
                if stream:
                    pass_batch_count += 1
                    if pass_batch_count <= params_infer_code.pass_first_n_batches:
                        continue
                    a = length
                    b = a + params_infer_code.stream_speed
                    if b > wavs.shape[1]:
                        b = wavs.shape[1]
                    new_wavs = wavs[:, a:b]
                    length = b
                    yield new_wavs
                else:
                    yield wavs
            if stream:
                new_wavs = wavs[:, length:]
                keep_cols = np.sum(np.abs(new_wavs) > 1e-5, axis=0) > 0
                yield new_wavs[:][:, keep_cols]

    @torch.inference_mode()
    def _vocos_decode(self, spec: torch.Tensor) -> np.ndarray:
        if "mps" in str(self.device) or "npu" in str(self.device):
            return self.vocos.decode(spec.cpu()).cpu().numpy()
        else:
            return self.vocos.decode(spec).cpu().numpy()

    @torch.inference_mode()
    def _decode_to_wavs(
        self,
        result_list: List[torch.Tensor],
        use_decoder: bool,
    ):
        decoder = self.decoder if use_decoder else self.dvae
        max_x_len = -1
        if len(result_list) == 0:
            return np.array([], dtype=np.float32)
        for result in result_list:
            if result.size(0) > max_x_len:
                max_x_len = result.size(0)
        batch_result = torch.zeros(
            (len(result_list), result_list[0].size(1), max_x_len),
            dtype=result_list[0].dtype,
            device=result_list[0].device,
        )
        for i in range(len(result_list)):
            src = result_list[i]
            batch_result[i].narrow(1, 0, src.size(0)).copy_(src.permute(1, 0))
            del src
        del_all(result_list)
        mel_specs = decoder(batch_result)
        del batch_result
        wavs = self._vocos_decode(mel_specs)
        del mel_specs
        return wavs

    @torch.no_grad()
    def _infer_code(
        self,
        text: Tuple[List[str], str],
        stream: bool,
        device: torch.device,
        return_hidden: bool,
        params: InferCodeParams,
    ):

        gpt = self.gpt

        if not isinstance(text, list):
            text = [text]

        assert len(text), "text should not be empty"

        if not isinstance(params.temperature, list):
            temperature = [params.temperature] * self.config.gpt.num_vq
        else:
            temperature = params.temperature

        input_ids, attention_mask, text_mask = self.tokenizer.encode(
            self.speaker.decorate_code_prompts(
                text,
                params.prompt,
                params.txt_smp,
                params.spk_emb,
            ),
            self.config.gpt.num_vq,
            prompt=(
                self.speaker.decode_prompt(params.spk_smp)
                if params.spk_smp is not None
                else None
            ),
            device=self.device_gpt,
        )
        start_idx = input_ids.shape[-2]

        num_code = self.config.gpt.num_audio_tokens - 1

        logits_warpers, logits_processors = gen_logits(
            num_code=num_code,
            top_P=params.top_P,
            top_K=params.top_K,
            repetition_penalty=params.repetition_penalty,
        )

        if gpt.is_vllm:
            from .model.velocity import SamplingParams

            sample_params = SamplingParams(
                temperature=temperature,
                max_new_token=params.max_new_token,
                max_tokens=8192,
                min_new_token=params.min_new_token,
                logits_processors=(logits_processors, logits_warpers),
                eos_token=num_code,
                infer_text=False,
                start_idx=start_idx,
            )
            input_ids = [i.tolist() for i in input_ids]

            result = gpt.llm.generate(
                None,
                sample_params,
                input_ids,
            )

            token_ids = []
            hidden_states = []
            for i in result:
                token_ids.append(torch.tensor(i.outputs[0].token_ids))
                hidden_states.append(
                    i.outputs[0].hidden_states.to(torch.float32).to(self.device)
                )

            del text_mask, input_ids

            return [
                GPT.GenerationOutputs(
                    ids=token_ids,
                    hiddens=hidden_states,
                    attentions=[],
                ),
            ]

        emb = self.embed(input_ids, text_mask)

        del text_mask

        if params.spk_emb is not None:
            self.speaker.apply(
                emb,
                params.spk_emb,
                input_ids,
                self.tokenizer.spk_emb_ids,
                self.gpt.device_gpt,
            )

        result = gpt.generate(
            emb,
            input_ids,
            temperature=torch.tensor(temperature, device=device),
            eos_token=num_code,
            attention_mask=attention_mask,
            max_new_token=params.max_new_token,
            min_new_token=params.min_new_token,
            logits_processors=(*logits_processors, *logits_warpers),
            infer_text=False,
            return_hidden=return_hidden,
            stream=stream,
            show_tqdm=params.show_tqdm,
            ensure_non_empty=params.ensure_non_empty,
            stream_batch=params.stream_batch,
            manual_seed=params.manual_seed,
            context=self.context,
        )

        del emb, input_ids

        return result

    @torch.no_grad()
    def _refine_text(
        self,
        text: str,
        device: torch.device,
        params: RefineTextParams,
    ):

        gpt = self.gpt

        if not isinstance(text, list):
            text = [text]

        input_ids, attention_mask, text_mask = self.tokenizer.encode(
            self.speaker.decorate_text_prompts(text, params.prompt),
            self.config.gpt.num_vq,
            device=self.device_gpt,
        )

        logits_warpers, logits_processors = gen_logits(
            num_code=self.tokenizer.len,
            top_P=params.top_P,
            top_K=params.top_K,
            repetition_penalty=params.repetition_penalty,
        )

        if gpt.is_vllm:
            from .model.velocity import SamplingParams

            sample_params = SamplingParams(
                repetition_penalty=params.repetition_penalty,
                temperature=params.temperature,
                top_p=params.top_P,
                top_k=params.top_K,
                max_new_token=params.max_new_token,
                max_tokens=8192,
                min_new_token=params.min_new_token,
                logits_processors=(logits_processors, logits_warpers),
                eos_token=self.tokenizer.eos_token,
                infer_text=True,
                start_idx=input_ids.shape[-2],
            )
            input_ids_list = [i.tolist() for i in input_ids]
            del input_ids

            result = gpt.llm.generate(
                None, sample_params, input_ids_list, params.show_tqdm
            )
            token_ids = []
            hidden_states = []
            for i in result:
                token_ids.append(torch.tensor(i.outputs[0].token_ids))
                hidden_states.append(i.outputs[0].hidden_states)

            del text_mask, input_ids_list, result

            return GPT.GenerationOutputs(
                ids=token_ids,
                hiddens=hidden_states,
                attentions=[],
            )

        emb = self.embed(input_ids, text_mask)

        del text_mask

        result = next(
            gpt.generate(
                emb,
                input_ids,
                temperature=torch.tensor([params.temperature], device=device),
                eos_token=self.tokenizer.eos_token,
                attention_mask=attention_mask,
                max_new_token=params.max_new_token,
                min_new_token=params.min_new_token,
                logits_processors=(*logits_processors, *logits_warpers),
                infer_text=True,
                stream=False,
                show_tqdm=params.show_tqdm,
                ensure_non_empty=params.ensure_non_empty,
                manual_seed=params.manual_seed,
                context=self.context,
            )
        )

        del emb, input_ids

        return result



================================================
FILE: ChatTTS/norm.py
================================================
import json
import logging
import re
from typing import Dict, Tuple, List, Literal, Callable, Optional
import sys

from numba import jit
import numpy as np

from .utils import del_all


@jit(nopython=True)
def _find_index(table: np.ndarray, val: np.uint16):
    for i in range(table.size):
        if table[i] == val:
            return i
    return -1


@jit(nopython=True)
def _fast_replace(
    table: np.ndarray, text: bytes
) -> Tuple[np.ndarray, List[Tuple[str, str]]]:
    result = np.frombuffer(text, dtype=np.uint16).copy()
    replaced_words = []
    for i in range(result.size):
        ch = result[i]
        p = _find_index(table[0], ch)
        if p >= 0:
            repl_char = table[1][p]
            result[i] = repl_char
            replaced_words.append((chr(ch), chr(repl_char)))
    return result, replaced_words


@jit(nopython=True)
def _split_tags(text: str) -> Tuple[List[str], List[str]]:
    texts: List[str] = []
    tags: List[str] = []
    current_text = ""
    current_tag = ""
    for c in text:
        if c == "[":
            texts.append(current_text)
            current_text = ""
            current_tag = c
        elif current_tag != "":
            current_tag += c
        else:
            current_text += c
        if c == "]":
            tags.append(current_tag)
            current_tag = ""
    if current_text != "":
        texts.append(current_text)
    return texts, tags


@jit(nopython=True)
def _combine_tags(texts: List[str], tags: List[str]) -> str:
    text = ""
    for t in texts:
        tg = ""
        if len(tags) > 0:
            tg = tags.pop(0)
        text += t + tg
    return text


class Normalizer:
    def __init__(self, map_file_path: str, logger=logging.getLogger(__name__)):
        self.logger = logger
        self.normalizers: Dict[str, Callable[[str], str]] = {}
        self.homophones_map = self._load_homophones_map(map_file_path)
        """
        homophones_map

        Replace the mispronounced characters with correctly pronounced ones.

        Creation process of homophones_map.json:

        1. Establish a word corpus using the [Tencent AI Lab Embedding Corpora v0.2.0 large] with 12 million entries. After cleaning, approximately 1.8 million entries remain. Use ChatTTS to infer the text.
        2. Record discrepancies between the inferred and input text, identifying about 180,000 misread words.
        3. Create a pinyin to common characters mapping using correctly read characters by ChatTTS.
        4. For each discrepancy, extract the correct pinyin using [python-pinyin] and find homophones with the correct pronunciation from the mapping.

        Thanks to:
        [Tencent AI Lab Embedding Corpora for Chinese and English Words and Phrases](https://ai.tencent.com/ailab/nlp/en/embedding.html)
        [python-pinyin](https://github.com/mozillazg/python-pinyin)

        """
        self.coding = "utf-16-le" if sys.byteorder == "little" else "utf-16-be"
        self.reject_pattern = re.compile(r"[^\u4e00-\u9fffA-Za-z，。、,\. ]")
        self.sub_pattern = re.compile(r"\[[\w_]+\]")
        self.chinese_char_pattern = re.compile(r"[\u4e00-\u9fff]")
        self.english_word_pattern = re.compile(r"\b[A-Za-z]+\b")
        self.character_simplifier = str.maketrans(
            {
                "：": "，",
                "；": "，",
                "！": "。",
                "（": "，",
                "）": "，",
                "【": "，",
                "】": "，",
                "『": "，",
                "』": "，",
                "「": "，",
                "」": "，",
                "《": "，",
                "》": "，",
                "－": "，",
                ":": ",",
                ";": ",",
                "!": ".",
                "(": ",",
                ")": ",",
                # "[": ",",
                # "]": ",",
                ">": ",",
                "<": ",",
                "-": ",",
            }
        )
        self.halfwidth_2_fullwidth = str.maketrans(
            {
                "!": "！",
                '"': "“",
                "'": "‘",
                "#": "＃",
                "$": "＄",
                "%": "％",
                "&": "＆",
                "(": "（",
                ")": "）",
                ",": "，",
                "-": "－",
                "*": "＊",
                "+": "＋",
                ".": "。",
                "/": "／",
                ":": "：",
                ";": "；",
                "<": "＜",
                "=": "＝",
                ">": "＞",
                "?": "？",
                "@": "＠",
                # '[': '［',
                "\\": "＼",
                # ']': '］',
                "^": "＾",
                # '_': '＿',
                "`": "｀",
                "{": "｛",
                "|": "｜",
                "}": "｝",
                "~": "～",
            }
        )

    def __call__(
        self,
        text: str,
        do_text_normalization=True,
        do_homophone_replacement=True,
        lang: Optional[Literal["zh", "en"]] = None,
    ) -> str:
        if do_text_normalization:
            _lang = self._detect_language(text) if lang is None else lang
            if _lang in self.normalizers:
                texts, tags = _split_tags(text)
                self.logger.debug("split texts %s, tags %s", str(texts), str(tags))
                texts = [self.normalizers[_lang](t) for t in texts]
                self.logger.debug("normed texts %s", str(texts))
                text = _combine_tags(texts, tags) if len(tags) > 0 else texts[0]
                self.logger.debug("combined text %s", text)
            if _lang == "zh":
                text = self._apply_half2full_map(text)
        invalid_characters = self._count_invalid_characters(text)
        if len(invalid_characters):
            self.logger.warning(f"found invalid characters: {invalid_characters}")
            text = self._apply_character_map(text)
        if do_homophone_replacement:
            arr, replaced_words = _fast_replace(
                self.homophones_map,
                text.encode(self.coding),
            )
            if replaced_words:
                text = arr.tobytes().decode(self.coding)
                repl_res = ", ".join([f"{_[0]}->{_[1]}" for _ in replaced_words])
                self.logger.info(f"replace homophones: {repl_res}")
        if len(invalid_characters):
            texts, tags = _split_tags(text)
            self.logger.debug("split texts %s, tags %s", str(texts), str(tags))
            texts = [self.reject_pattern.sub("", t) for t in texts]
            self.logger.debug("normed texts %s", str(texts))
            text = _combine_tags(texts, tags) if len(tags) > 0 else texts[0]
            self.logger.debug("combined text %s", text)
        return text

    def register(self, name: str, normalizer: Callable[[str], str]) -> bool:
        if name in self.normalizers:
            self.logger.warning(f"name {name} has been registered")
            return False
        try:
            val = normalizer("test string 测试字符串")
            if not isinstance(val, str):
                self.logger.warning("normalizer must have caller type (str) -> str")
                return False
        except Exception as e:
            self.logger.warning(e)
            return False
        self.normalizers[name] = normalizer
        return True

    def unregister(self, name: str):
        if name in self.normalizers:
            del self.normalizers[name]

    def destroy(self):
        del_all(self.normalizers)
        del self.homophones_map

    def _load_homophones_map(self, map_file_path: str) -> np.ndarray:
        with open(map_file_path, "r", encoding="utf-8") as f:
            homophones_map: Dict[str, str] = json.load(f)
        map = np.empty((2, len(homophones_map)), dtype=np.uint32)
        for i, k in enumerate(homophones_map.keys()):
            map[:, i] = (ord(k), ord(homophones_map[k]))
        del homophones_map
        return map

    def _count_invalid_characters(self, s: str):
        s = self.sub_pattern.sub("", s)
        non_alphabetic_chinese_chars = self.reject_pattern.findall(s)
        return set(non_alphabetic_chinese_chars)

    def _apply_half2full_map(self, text: str) -> str:
        return text.translate(self.halfwidth_2_fullwidth)

    def _apply_character_map(self, text: str) -> str:
        return text.translate(self.character_simplifier)

    def _detect_language(self, sentence: str) -> Literal["zh", "en"]:
        chinese_chars = self.chinese_char_pattern.findall(sentence)
        english_words = self.english_word_pattern.findall(sentence)

        if len(chinese_chars) > len(english_words):
            return "zh"
        else:
            return "en"



================================================
FILE: ChatTTS/config/__init__.py
================================================
from .config import Config



================================================
FILE: ChatTTS/config/config.py
================================================
from dataclasses import dataclass


@dataclass(repr=False, eq=False)
class Path:
    vocos_ckpt_path: str = "asset/Vocos.safetensors"
    dvae_ckpt_path: str = "asset/DVAE.safetensors"
    gpt_ckpt_path: str = "asset/gpt"
    decoder_ckpt_path: str = "asset/Decoder.safetensors"
    tokenizer_path: str = "asset/tokenizer"
    embed_path: str = "asset/Embed.safetensors"


@dataclass(repr=False, eq=False)
class Decoder:
    idim: int = 384
    odim: int = 384
    hidden: int = 512
    n_layer: int = 12
    bn_dim: int = 128


@dataclass(repr=False, eq=False)
class VQ:
    dim: int = 1024
    levels: tuple = (5, 5, 5, 5)
    G: int = 2
    R: int = 2


@dataclass(repr=False, eq=False)
class DVAE:
    encoder: Decoder = Decoder(
        idim=512,
        odim=1024,
        hidden=256,
        n_layer=12,
        bn_dim=128,
    )
    decoder: Decoder = Decoder(
        idim=512,
        odim=512,
        hidden=256,
        n_layer=12,
        bn_dim=128,
    )
    vq: VQ = VQ()


@dataclass(repr=False, eq=False)
class GPT:
    hidden_size: int = 768
    intermediate_size: int = 3072
    num_attention_heads: int = 12
    num_hidden_layers: int = 20
    use_cache: bool = False
    max_position_embeddings: int = 4096

    spk_emb_dim: int = 192
    spk_KL: bool = False
    num_audio_tokens: int = 626
    num_text_tokens: int = 21178
    num_vq: int = 4


@dataclass(repr=False, eq=False)
class Embed:
    hidden_size: int = 768
    num_audio_tokens: int = 626
    num_text_tokens: int = 21178
    num_vq: int = 4


@dataclass(repr=False, eq=False)
class FeatureExtractorInitArgs:
    sample_rate: int = 24000
    n_fft: int = 1024
    hop_length: int = 256
    n_mels: int = 100
    padding: str = "center"


@dataclass(repr=False, eq=False)
class FeatureExtractor:
    class_path: str = "vocos.feature_extractors.MelSpectrogramFeatures"
    init_args: FeatureExtractorInitArgs = FeatureExtractorInitArgs()


@dataclass(repr=False, eq=False)
class BackboneInitArgs:
    input_channels: int = 100
    dim: int = 512
    intermediate_dim: int = 1536
    num_layers: int = 8


@dataclass(repr=False, eq=False)
class Backbone:
    class_path: str = "vocos.models.VocosBackbone"
    init_args: BackboneInitArgs = BackboneInitArgs()


@dataclass(repr=False, eq=False)
class FourierHeadInitArgs:
    dim: int = 512
    n_fft: int = 1024
    hop_length: int = 256
    padding: str = "center"


@dataclass(repr=False, eq=False)
class FourierHead:
    class_path: str = "vocos.heads.ISTFTHead"
    init_args: FourierHeadInitArgs = FourierHeadInitArgs()


@dataclass(repr=False, eq=False)
class Vocos:
    feature_extractor: FeatureExtractor = FeatureExtractor()
    backbone: Backbone = Backbone()
    head: FourierHead = FourierHead()


@dataclass(repr=False, eq=False)
class Config:
    path: Path = Path()
    decoder: Decoder = Decoder()
    dvae: DVAE = DVAE()
    gpt: GPT = GPT()
    embed: Embed = Embed()
    vocos: Vocos = Vocos()
    spk_stat: str = (
        "愐穤巩噅廷戇笉屈癐媄垹垧帶爲漈塀殐慄亅倴庲舴猂瑈圐狴夥圓帍戛挠腉耐劤坽喳幾战謇聀崒栄呥倸庭燡欈杁襐褄乭埗幺爃弔摁斐捔兕佖廐舏竾豃磐姓趡佄幒爚欄豄讐皳訵仩帆投謌荃蝐叄圝伆幦抂茁呄掑斃讹傮庞爣蜀橁偐祄亥兡常爂欍扉丐浔佱僈強払伅扂蛐徴憍傞巀戺欀艂琐嗴啥値彷刂權穈扒卤俔贲庛初笂卄贐枴仭亁庛剎猢扃缐趤刁偵幪舏伌煁婐潤晍位弾舙茥穁葏蠣訑企庤刊笍橁溑僔云偁庯戚伍潉膐脴僵噔廃艅匊祂唐憴壝嗙席爥欁虁谐牴帽势弿牳蜁兀蛐傄喩丿帔刔圆衁廐罤庁促帙劢伈汄樐檄勵伴弝舑欍罅虐昴劭勅帜刼朊蕁虐蓴樑伫幨扑謪剀堐稴丵伱弐舮諸赁習俔容厱幫牶謃孄糐答嗝僊帜燲笄終瀒判久僤帘爴茇千孑冄凕佳引扐蜁歁缏裄剽儺恘爋朏眿廐呄塍嘇幻爱茠詁訐剴唭俐幾戊欀硁菐贄楕偒巡爀弎屄莐睳賙凶彎刅漄區唐溴剑劋庽舽猄煃跐夔惥伾庮舎伈罁垑坄怅业怯刁朇獁嶏覔坩俳巶爜朐潁崐萄俹凛常爺笌穀聐此夡倛帡刀匉終窏舣販侽怿扉伥贿憐忓謩姆幌犊漂慆癒却甝兎帼戏欅詂浐朔仹壭帰臷弎恇菐獤帡偖帘爞伅腂皐纤囅充幓戠伥灂丐訤戱倱弋爮嬌癁恐孄侥劬忶刓國詀桒古偩嘄庬戚茝赂监燤嘑勌幦舽持呂諐棤姑再底舡笍艃瀐孴倉傔弋爔猠乁濑塄偽嘧恂舛缇襃厐窴仡刱忕別漇穁岏缴廽价庌爊謈硄讑惤倁儂庭爋伇蝂嶐莔摝傠库刞茄歃戏薤伍伯廮创笠塄熐兴勽俄帅剉最腀砐敤卝侍弆戺朒虃旐蚄梕亖幔牻朣扅贐玔堝噅帡剌圅摀崐彤流僳庙爖嬇啁渐悤堁丛幆刧挜彃悐幤刹嚟恕芁看聀摐焔向乁帖爭欁癃糒圄弙佱廜戤謍婀咐昴焍亩廦艏拼謿芐癤怹兽幸舳朇畁喐稔毝丼弈懲挀譂勑哴啁伎常舭笯晁堑俄叩剔廟爍欦絁夒伤休傑廳戌蜅潆癐彴摑勯床刽欅艁砐忄搉从廡舊猥潂唐委仱僜廼爤朄呃弐礔滵垓幩爄挂筁乐籤刕凟幵爠弉癅乑吴勥伖帪舩茆婁碐幤叭乢巜艳猁桀桐啄唩俊幍舮猀艅焐螔琽亀帋爜缅噃咐斤喩予幩爛笆摀浐猴依侹幃刕園慄蛐栤澹仑座爼謉桃慐浔斕偻幛懰嬓衁愐氄悅仿应芔漄衃敐謤傁匩幹抃圉癄廐裄屵噉幍利謍聂搐蛔嚙坍怗舁圐畃膐栄刵东巆戤諾呃偑媤嗨跞忶爝眄祂朒嶔僭劉忾刐匋癄袐翴珅僷廲芄茈恈皐擄崑伄廉牍匃剃犏澤唑丄庺戃伃煀某杄偙亽帴切缌罄挐尴噙倰带舞漄橄塐糴俩僯帀般漀坂栐更両俇廱舌猁慂拐偤嶱卶应刪眉獁茐伔嘅偺帟舊漂恀栐暄喡乞庙舆匂敀潑恔劑侖延戦盽怶唯慳蝘蟃孫娎益袰玍屃痶翮笪儚裀倹椌玻翀詵筽舘惯堿某侰晈藏缮詗廦夸妎瑻瀒裔媀憞唃冶璭狻渠荑奬熹茅愺氰菣滠翦岓褌泣崲嚭欓湒聙宺爄蛅愸庍匃帆誔穮懌蓪玷澌氋抌訙屌臞廛玸听屺希疭孝凂紋新煎彃膲跱尪懁眆窴珏卓揨菸紭概囥显壌榄垫嘮嬭覤媸侵佮烒耸觌婀秋狃帹葯訤桜糨笾腢伀肶悍炂艤禖岅臺惘梷瞍友盁佨岧憳瓧嘴汬藊愌蘤嶠硴绤蜲襏括勾谂縨妥蓪澭竭萢藜纞糲煮愆瀯孯琓罂諺塿燗狟弙衯揻縷丱糅臄梱瀮杰巳猙亊符胠匃泀廏圃膂蒃籏礩岈簹缌劺燲褡孓膜拔蠿觮呋煣厌尷熜論弲牭紫寊誃紀橴賬傸箍弚窃侫簲慯烣渽祌壓媥噜夽夛諛玹疮禄冪謇媽衤盰缺繑薫兾萧嵱打滽箺嚯凣狢蠜崼覽烸簶盯籓摀苶峸懗泲涻凮愳緗剋笔懆廡瞿椏礤惐藥崍腈烄伹亯昣翬褍絋桫僨吨莌丛矄蜞娈憊苆塁蓏嚢嫼绻崱婋囱蠸篯晣芀繼索兓僖誹岯圪褰蠇唓妷胅巁渮砛傈蝷嵚冃購赁峍裋荂舾符熻岳墩寮粃凲袑彚太绲头摯繳狁俥籌冝諝註坎幫擤詒宒凕賐唶梎噔弼課屿覍囨焬櫱撪蝮蝬簸懰櫫涺嵍睻屪翔峞慘滟熲昱军烊舿尦舄糖奁溏凂彆蝲糴禍困皻灏牋睒诙嶱臀开蓈眎腼丢纻廏憤嫖暭袭崲肸螛妒榗紉谨窮袃瑠聍绊腆亿冲葐喋縔詖岑兾给堸赏旻桀蛨媆訂峦紷敯囬偐筨岸焸拭笵殒哜墒萍屓娓諙械臮望摰芑寭准僞谹氍旋憢菮屃划欣瘫谎蘻哐繁籥禦僿誵皯墓燀縿笞熦绗稹榎矻綞蓓帡戓沺区才畃洊詪糐裶盰窶耎偌劂誐庩惝滜沺哮呃煐譠崄槀猄肼蔐擋湌蠺篃恥諌瞦宍堫挪裕崑慩狲悠煋仛愞砈粵八棁害楐妋萔貨尵奂苰怫誎傫岆蕯屇脉夈仆茎刓繸芺壸碗曛汁戭炻獻凉媁兎狜爴怰賃纎袏娷禃蓥膹薪渻罸窿粫凾褄舺窮墫干苊繁冏僮訸夯绛蓪虛羽慲烏憷趎睊蠰莍塞成廎盁欏喓蜮譤崆楁囘矇薭伣艘虝帴奮苢渶虎暣翐蝃尾稈糶瀴罐嵚氮葯笫慐棌悶炯竻爅们媡姢嫺窷刮歫劈裩屬椕賑蜹薊刲義哯尗褦瓀稾礋揣窼舫尋姁椄侸嗫珺修纘媃腽蛛稹梭呛瀈蘟縀礉論夵售主梮蠉娅娭裀誼嶭観枳倊簈褃擞綿催瞃溶苊笛襹櫲盅六囫獩佃粨慯瓢眸旱荃婨蔞岋祗墼焻网牻琖詆峋秉胳媴袭澓賢経稟壩胫碯偏囫嶎纆窈槊賐撹璬莃缘誾宭愊眗喷监劋萘訯總槿棭戾墮犄恌縈簍樥蛔杁袭嫛憫倆篏墵賈羯茎觳蒜致娢慄勒覸蘍曲栂葭宆妋皽缽免盳猼蔂糥觧烳檸佯憓煶蔐筼种繷琲膌塄剰讎対腕棥渽忲俛浪譬秛惛壒嘸淫冻曄睻砃奫貯庴爅粓脮脡娎妖峵蘲討惋泊蠀㴆"
    )



================================================
FILE: ChatTTS/model/__init__.py
================================================
from .dvae import DVAE
from .embed import Embed
from .gpt import GPT
from .processors import gen_logits
from .speaker import Speaker
from .tokenizer import Tokenizer



================================================
FILE: ChatTTS/model/dvae.py
================================================
import math
from typing import List, Optional, Literal, Union

import numpy as np
import pybase16384 as b14
import torch
import torch.nn as nn
import torchaudio
from vector_quantize_pytorch import GroupedResidualFSQ

from ..utils import load_safetensors


class ConvNeXtBlock(nn.Module):
    def __init__(
        self,
        dim: int,
        intermediate_dim: int,
        kernel: int,
        dilation: int,
        layer_scale_init_value: float = 1e-6,
    ):
        # ConvNeXt Block copied from Vocos.
        super().__init__()
        self.dwconv = nn.Conv1d(
            dim,
            dim,
            kernel_size=kernel,
            padding=dilation * (kernel // 2),
            dilation=dilation,
            groups=dim,
        )  # depthwise conv

        self.norm = nn.LayerNorm(dim, eps=1e-6)
        self.pwconv1 = nn.Linear(
            dim, intermediate_dim
        )  # pointwise/1x1 convs, implemented with linear layers
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(intermediate_dim, dim)
        self.weight = (
            nn.Parameter(layer_scale_init_value * torch.ones(dim), requires_grad=True)
            if layer_scale_init_value > 0
            else None
        )

    def forward(self, x: torch.Tensor, cond=None) -> torch.Tensor:
        residual = x

        y = self.dwconv(x)
        y.transpose_(1, 2)  # (B, C, T) -> (B, T, C)
        x = self.norm(y)
        del y
        y = self.pwconv1(x)
        del x
        x = self.act(y)
        del y
        y = self.pwconv2(x)
        del x
        if self.weight is not None:
            y *= self.weight
        y.transpose_(1, 2)  # (B, T, C) -> (B, C, T)

        x = y + residual
        del y

        return x


class GFSQ(nn.Module):

    def __init__(
        self, dim: int, levels: List[int], G: int, R: int, eps=1e-5, transpose=True
    ):
        super(GFSQ, self).__init__()
        self.quantizer = GroupedResidualFSQ(
            dim=dim,
            levels=list(levels),
            num_quantizers=R,
            groups=G,
        )
        self.n_ind = math.prod(levels)
        self.eps = eps
        self.transpose = transpose
        self.G = G
        self.R = R

    def _embed(self, x: torch.Tensor):
        if self.transpose:
            x = x.transpose(1, 2)
        """
        x = rearrange(
            x, "b t (g r) -> g b t r", g = self.G, r = self.R,
        )
        """
        x = x.view(x.size(0), x.size(1), self.G, self.R).permute(2, 0, 1, 3)
        feat = self.quantizer.get_output_from_indices(x)
        return feat.transpose_(1, 2) if self.transpose else feat

    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        return super().__call__(x)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.transpose:
            x.transpose_(1, 2)
        # feat, ind = self.quantizer(x)
        _, ind = self.quantizer(x)
        """
        ind = rearrange(
            ind, "g b t r ->b t (g r)",
        )
        """
        ind = ind.permute(1, 2, 0, 3).contiguous()
        ind = ind.view(ind.size(0), ind.size(1), -1)
        """
        embed_onehot_tmp = F.one_hot(ind.long(), self.n_ind)
        embed_onehot = embed_onehot_tmp.to(x.dtype)
        del embed_onehot_tmp
        e_mean = torch.mean(embed_onehot, dim=[0, 1])
        # e_mean = e_mean / (e_mean.sum(dim=1) + self.eps).unsqueeze(1)
        torch.div(e_mean, (e_mean.sum(dim=1) + self.eps).unsqueeze(1), out=e_mean)
        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + self.eps), dim=1))

        return 
            torch.zeros(perplexity.shape, dtype=x.dtype, device=x.device),
            feat.transpose_(1, 2) if self.transpose else feat,
            perplexity,
        """
        return ind.transpose_(1, 2) if self.transpose else ind


class DVAEDecoder(nn.Module):
    def __init__(
        self,
        idim: int,
        odim: int,
        n_layer=12,
        bn_dim=64,
        hidden=256,
        kernel=7,
        dilation=2,
        up=False,
    ):
        super().__init__()
        self.up = up
        self.conv_in = nn.Sequential(
            nn.Conv1d(idim, bn_dim, 3, 1, 1),
            nn.GELU(),
            nn.Conv1d(bn_dim, hidden, 3, 1, 1),
        )
        self.decoder_block = nn.ModuleList(
            [
                ConvNeXtBlock(
                    hidden,
                    hidden * 4,
                    kernel,
                    dilation,
                )
                for _ in range(n_layer)
            ]
        )
        self.conv_out = nn.Conv1d(hidden, odim, kernel_size=1, bias=False)

    def forward(self, x: torch.Tensor, conditioning=None) -> torch.Tensor:
        # B, C, T
        y = self.conv_in(x)
        del x
        for f in self.decoder_block:
            y = f(y, conditioning)

        x = self.conv_out(y)
        del y
        return x


class MelSpectrogramFeatures(torch.nn.Module):
    def __init__(
        self,
        sample_rate=24000,
        n_fft=1024,
        hop_length=256,
        n_mels=100,
        padding: Literal["center", "same"] = "center",
        device: torch.device = torch.device("cpu"),
    ):
        super().__init__()
        self.device = device
        if padding not in ["center", "same"]:
            raise ValueError("Padding must be 'center' or 'same'.")
        self.padding = padding
        self.mel_spec = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            n_mels=n_mels,
            center=padding == "center",
            power=1,
        )

    def __call__(self, audio: torch.Tensor) -> torch.Tensor:
        return super().__call__(audio)

    def forward(self, audio: torch.Tensor) -> torch.Tensor:
        audio = audio.to(self.device)
        mel: torch.Tensor = self.mel_spec(audio)
        features = torch.log(torch.clip(mel, min=1e-5))
        return features


class DVAE(nn.Module):
    def __init__(
        self,
        decoder_config: dict,
        encoder_config: Optional[dict] = None,
        vq_config: Optional[dict] = None,
        dim=512,
        coef: Optional[str] = None,
        device: torch.device = torch.device("cpu"),
    ):
        super().__init__()
        if coef is None:
            coef = torch.rand(100)
        else:
            coef = torch.from_numpy(
                np.frombuffer(b14.decode_from_string(coef), dtype=np.float32).copy()
            )
        self.register_buffer("coef", coef.unsqueeze(0).unsqueeze_(2))

        if encoder_config is not None:
            self.downsample_conv = nn.Sequential(
                nn.Conv1d(100, dim, 3, 1, 1),
                nn.GELU(),
                nn.Conv1d(dim, dim, 4, 2, 1),
                nn.GELU(),
            )
            self.preprocessor_mel = MelSpectrogramFeatures(device=device)
            self.encoder: Optional[DVAEDecoder] = DVAEDecoder(**encoder_config)

        self.decoder = DVAEDecoder(**decoder_config)
        self.out_conv = nn.Conv1d(dim, 100, 3, 1, 1, bias=False)
        if vq_config is not None:
            self.vq_layer = GFSQ(**vq_config)
        else:
            self.vq_layer = None

    def __repr__(self) -> str:
        return b14.encode_to_string(
            self.coef.cpu().numpy().astype(np.float32).tobytes()
        )

    def __call__(
        self, inp: torch.Tensor, mode: Literal["encode", "decode"] = "decode"
    ) -> torch.Tensor:
        return super().__call__(inp, mode)

    @torch.inference_mode()
    def load_pretrained(self, filename: str, device: torch.device):
        state_dict_tensors = load_safetensors(filename)
        self.load_state_dict(state_dict_tensors)
        self.to(device)

    @torch.inference_mode()
    def forward(
        self, inp: torch.Tensor, mode: Literal["encode", "decode"] = "decode"
    ) -> torch.Tensor:
        if mode == "encode" and hasattr(self, "encoder") and self.vq_layer is not None:
            mel = self.preprocessor_mel(inp)
            x: torch.Tensor = self.downsample_conv(
                torch.div(mel, self.coef.view(100, 1).expand(mel.shape), out=mel),
            ).unsqueeze_(0)
            del mel
            x = self.encoder(x)
            ind = self.vq_layer(x)
            del x
            return ind

        if self.vq_layer is not None:
            vq_feats = self.vq_layer._embed(inp)
        else:
            vq_feats = inp

        vq_feats = (
            vq_feats.view(
                (vq_feats.size(0), 2, vq_feats.size(1) // 2, vq_feats.size(2)),
            )
            .permute(0, 2, 3, 1)
            .flatten(2)
        )

        dec_out = self.out_conv(
            self.decoder(
                x=vq_feats,
            ),
        )

        del vq_feats

        return torch.mul(dec_out, self.coef, out=dec_out)

    @torch.inference_mode()
    def sample_audio(self, wav: Union[np.ndarray, torch.Tensor]) -> torch.Tensor:
        if isinstance(wav, np.ndarray):
            wav = torch.from_numpy(wav)
        return self(wav, "encode").squeeze_(0)



================================================
FILE: ChatTTS/model/embed.py
================================================
import torch
import torch.nn as nn
from torch.nn.utils.parametrizations import weight_norm

from ..utils import load_safetensors


class Embed(nn.Module):
    def __init__(
        self, hidden_size: int, num_audio_tokens: int, num_text_tokens: int, num_vq=4
    ):
        super().__init__()

        self.num_vq = num_vq
        self.num_audio_tokens = num_audio_tokens

        self.model_dim = hidden_size
        self.emb_code = nn.ModuleList(
            [nn.Embedding(num_audio_tokens, self.model_dim) for _ in range(num_vq)],
        )
        self.emb_text = nn.Embedding(num_text_tokens, self.model_dim)

        self.head_text = weight_norm(
            nn.Linear(self.model_dim, num_text_tokens, bias=False),
            name="weight",
        )
        self.head_code = nn.ModuleList(
            [
                weight_norm(
                    nn.Linear(self.model_dim, num_audio_tokens, bias=False),
                    name="weight",
                )
                for _ in range(self.num_vq)
            ],
        )

    @torch.inference_mode()
    def load_pretrained(self, filename: str, device: torch.device):
        state_dict_tensors = load_safetensors(filename)
        self.load_state_dict(state_dict_tensors)
        self.to(device)

    def __call__(
        self, input_ids: torch.Tensor, text_mask: torch.Tensor
    ) -> torch.Tensor:
        """
        get_emb
        """
        return super().__call__(input_ids, text_mask)

    @torch.inference_mode()
    def forward(self, input_ids: torch.Tensor, text_mask: torch.Tensor) -> torch.Tensor:
        """
        get_emb
        """
        device = next(self.parameters()).device
        emb_text: torch.Tensor = self.emb_text(
            input_ids[text_mask].narrow(1, 0, 1).squeeze_(1).to(device)
        )

        text_mask_inv = text_mask.logical_not().to(device)
        masked_input_ids: torch.Tensor = input_ids[text_mask_inv].to(device)

        emb_code = [
            self.emb_code[i](masked_input_ids[:, i]) for i in range(self.num_vq)
        ]
        emb_code = torch.stack(emb_code, 2).sum(2)

        emb = torch.zeros(
            (input_ids.shape[:-1]) + (emb_text.shape[-1],),
            device=emb_text.device,
            dtype=emb_text.dtype,
        )
        emb[text_mask] = emb_text
        emb[text_mask_inv] = emb_code.to(emb.dtype)

        del emb_text, emb_code, text_mask_inv

        return emb



================================================
FILE: ChatTTS/model/gpt.py
================================================
import platform
from dataclasses import dataclass
import logging
from typing import Union, List, Optional, Tuple, Callable
import gc

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.parametrize as P
from tqdm import tqdm
from transformers import LlamaModel, LlamaConfig
from transformers.cache_utils import Cache
from transformers.modeling_outputs import BaseModelOutputWithPast
from transformers.utils import is_flash_attn_2_available

from ..utils import del_all
from .embed import Embed


class GPT(nn.Module):
    def __init__(
        self,
        gpt_config: dict,
        embed: Embed,
        use_flash_attn=False,
        use_vllm=False,
        device=torch.device("cpu"),
        device_gpt=torch.device("cpu"),
        logger=logging.getLogger(__name__),
    ):
        super().__init__()

        self.logger = logger

        self.device = device
        self.device_gpt = device_gpt

        self.generator = torch.Generator(device=device)

        self.num_vq = int(gpt_config["num_vq"])
        self.num_audio_tokens = int(gpt_config["num_audio_tokens"])
        self.num_text_tokens = int(gpt_config["num_text_tokens"])

        self.use_flash_attn = use_flash_attn
        self.is_te_llama = False
        self.is_vllm = use_vllm

        if self.is_vllm:
            return

        self.llama_config = self._build_llama_config(gpt_config)

        self.emb_code = [ec.__call__ for ec in embed.emb_code]
        self.emb_text = embed.emb_text.__call__
        self.head_text = embed.head_text.__call__
        self.head_code = [hc.__call__ for hc in embed.head_code]

    def load_pretrained(
        self, gpt_folder: str, embed_file_path: str, experimental=False
    ):
        if self.is_vllm and platform.system().lower() == "linux":

            from .velocity import LLM

            self.llm = LLM(
                model=gpt_folder,
                num_audio_tokens=self.num_audio_tokens,
                num_text_tokens=self.num_text_tokens,
                post_model_path=embed_file_path,
            )
            self.logger.info("vLLM model loaded")
            return

        self.gpt: LlamaModel = LlamaModel.from_pretrained(gpt_folder).to(
            self.device_gpt
        )
        del self.gpt.embed_tokens

        if (
            experimental
            and "cuda" in str(self.device_gpt)
            and platform.system().lower() == "linux"
        ):  # is TELlamaModel
            try:
                from .cuda import TELlamaModel

                self.logger.warning(
                    "Linux with CUDA, try NVIDIA accelerated TELlamaModel because experimental is enabled"
                )
                state_dict = self.gpt.state_dict()
                vanilla = TELlamaModel.from_state_dict(state_dict, self.llama_config)
                # Force mem release. Taken from huggingface code
                del state_dict, self.gpt
                gc.collect()
                self.gpt = vanilla
                self.is_te_llama = True
            except Exception as e:
                self.logger.warning(
                    f"use default LlamaModel for importing TELlamaModel error: {e}"
                )

    class Context:
        def __init__(self):
            self._interrupt = False

        def set(self, v: bool):
            self._interrupt = v

        def get(self) -> bool:
            return self._interrupt

    def _build_llama_config(
        self,
        config: dict,
    ) -> Tuple[LlamaModel, LlamaConfig]:

        if self.use_flash_attn and is_flash_attn_2_available():
            llama_config = LlamaConfig(
                **config,
                attn_implementation="flash_attention_2",
            )
            self.logger.warning(
                "enabling flash_attention_2 may make gpt be even slower"
            )
        else:
            llama_config = LlamaConfig(**config)

        return llama_config

    def prepare(self, compile=False):
        if self.use_flash_attn and is_flash_attn_2_available():
            self.gpt = self.gpt.to(dtype=torch.float16)
        if compile and not self.is_te_llama and not self.is_vllm:
            try:
                self.compile(backend="inductor", dynamic=True)
                self.gpt.compile(backend="inductor", dynamic=True)
            except RuntimeError as e:
                self.logger.warning(f"compile failed: {e}. fallback to normal mode.")

    @dataclass(repr=False, eq=False)
    class _GenerationInputs:
        position_ids: torch.Tensor
        cache_position: torch.Tensor
        use_cache: bool
        input_ids: Optional[torch.Tensor] = None
        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
        attention_mask: Optional[torch.Tensor] = None
        inputs_embeds: Optional[torch.Tensor] = None

        def to(self, device: torch.device, dtype: torch.dtype):
            if self.attention_mask is not None:
                self.attention_mask = self.attention_mask.to(device, dtype=dtype)
            if self.position_ids is not None:
                self.position_ids = self.position_ids.to(device, dtype=dtype)
            if self.inputs_embeds is not None:
                self.inputs_embeds = self.inputs_embeds.to(device, dtype=dtype)
            if self.cache_position is not None:
                self.cache_position = self.cache_position.to(device, dtype=dtype)

    @torch.no_grad()
    def _prepare_generation_inputs(
        self,
        input_ids: torch.Tensor,
        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
        cache_position: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        use_cache=True,
    ) -> _GenerationInputs:
        # With static cache, the `past_key_values` is None
        # TODO joao: standardize interface for the different Cache classes and remove of this if
        has_static_cache = False
        if past_key_values is None:
            if hasattr(self.gpt.layers[0], "self_attn"):
                past_key_values = getattr(
                    self.gpt.layers[0].self_attn, "past_key_value", None
                )
            has_static_cache = past_key_values is not None

        past_length = 0
        if past_key_values is not None:
            if isinstance(past_key_values, Cache):
                past_length = (
                    int(cache_position[0])
                    if cache_position is not None
                    else past_key_values.get_seq_length()
                )
                try:
                    max_cache_length = past_key_values.get_max_cache_shape()
                except:
                    max_cache_length = (
                        past_key_values.get_max_length()
                    )  # deprecated in transformers 4.48
                cache_length = (
                    past_length
                    if max_cache_length is None
                    else min(max_cache_length, past_length)
                )
            # TODO joao: remove this `else` after `generate` prioritizes `Cache` objects
            else:
                cache_length = past_length = past_key_values[0][0].shape[2]
                max_cache_length = None

            # Keep only the unprocessed tokens:
            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where
            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as
            # input)
            if (
                attention_mask is not None
                and attention_mask.shape[1] > input_ids.shape[1]
            ):
                start = attention_mask.shape[1] - past_length
                input_ids = input_ids.narrow(1, -start, start)
            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard
            # input_ids based on the past_length.
            elif past_length < input_ids.shape[1]:
                input_ids = input_ids.narrow(
                    1, past_length, input_ids.size(1) - past_length
                )
            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.

            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.
            if (
                max_cache_length is not None
                and attention_mask is not None
                and cache_length + input_ids.shape[1] > max_cache_length
            ):
                attention_mask = attention_mask.narrow(
                    1, -max_cache_length, max_cache_length
                )

        if attention_mask is not None and position_ids is None:
            # create position_ids on the fly for batch generation
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask.eq(0), 1)
            if past_key_values:
                position_ids = position_ids.narrow(
                    1, -input_ids.shape[1], input_ids.shape[1]
                )

        input_length = (
            position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]
        )
        if cache_position is None:
            cache_position = torch.arange(
                past_length, past_length + input_length, device=input_ids.device
            )
        else:
            cache_position = cache_position.narrow(0, -input_length, input_length)

        if has_static_cache:
            past_key_values = None

        model_inputs = self._GenerationInputs(
            position_ids=position_ids,
            cache_position=cache_position,
            use_cache=use_cache,
        )

        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step
        if inputs_embeds is not None and past_key_values is None:
            model_inputs.inputs_embeds = inputs_embeds
        else:
            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise
            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114
            # TODO: use `next_tokens` directly instead.
            model_inputs.input_ids = input_ids.contiguous()

        model_inputs.past_key_values = past_key_values
        model_inputs.attention_mask = attention_mask

        return model_inputs

    @dataclass(repr=False, eq=False)
    class GenerationOutputs:
        ids: List[torch.Tensor]
        attentions: List[Optional[Tuple[torch.FloatTensor, ...]]]
        hiddens: List[torch.Tensor]

        def destroy(self):
            del_all(self.ids)
            del_all(self.attentions)
            del_all(self.hiddens)

    @torch.no_grad()
    def _prepare_generation_outputs(
        self,
        inputs_ids: torch.Tensor,
        start_idx: int,
        end_idx: torch.Tensor,
        attentions: List[Optional[Tuple[torch.FloatTensor, ...]]],
        hiddens: List[torch.Tensor],
        infer_text: bool,
    ) -> GenerationOutputs:
        inputs_ids = [
            inputs_ids[idx].narrow(0, start_idx, i) for idx, i in enumerate(end_idx)
        ]
        if infer_text:
            inputs_ids = [i.narrow(1, 0, 1).squeeze_(1) for i in inputs_ids]

        if len(hiddens) > 0:
            hiddens = torch.stack(hiddens, 1)
            hiddens = [
                hiddens[idx].narrow(0, 0, i) for idx, i in enumerate(end_idx.int())
            ]

        return self.GenerationOutputs(
            ids=inputs_ids,
            attentions=attentions,
            hiddens=hiddens,
        )

    @torch.no_grad()
    def generate(
        self,
        emb: torch.Tensor,
        inputs_ids: torch.Tensor,
        temperature: torch.Tensor,
        eos_token: Union[int, torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        max_new_token=2048,
        min_new_token=0,
        logits_processors: Tuple[
            Callable[[torch.LongTensor, torch.FloatTensor], torch.FloatTensor]
        ] = (),
        infer_text=False,
        return_attn=False,
        return_hidden=False,
        stream=False,
        show_tqdm=True,
        ensure_non_empty=True,
        stream_batch=24,
        manual_seed: Optional[int] = None,
        context=Context(),
    ):

        attentions: List[Optional[Tuple[torch.FloatTensor, ...]]] = []
        hiddens = []
        stream_iter = 0

        start_idx, end_idx = inputs_ids.shape[1], torch.zeros(
            inputs_ids.shape[0], device=inputs_ids.device, dtype=torch.long
        )
        finish = torch.zeros(inputs_ids.shape[0], device=inputs_ids.device).bool()

        old_temperature = temperature

        temperature = (
            temperature.unsqueeze(0)
            .expand(inputs_ids.shape[0], -1)
            .contiguous()
            .view(-1, 1)
        )

        attention_mask_cache = torch.ones(
            (
                inputs_ids.shape[0],
                inputs_ids.shape[1] + max_new_token,
            ),
            dtype=torch.bool,
            device=inputs_ids.device,
        )
        if attention_mask is not None:
            attention_mask_cache.narrow(1, 0, attention_mask.shape[1]).copy_(
                attention_mask
            )

        progress = inputs_ids.size(1)
        # pre-allocate inputs_ids
        inputs_ids_buf = torch.zeros(
            inputs_ids.size(0),
            progress + max_new_token,
            inputs_ids.size(2),
            dtype=inputs_ids.dtype,
            device=inputs_ids.device,
        )
        inputs_ids_buf.narrow(1, 0, progress).copy_(inputs_ids)
        del inputs_ids
        inputs_ids = inputs_ids_buf.narrow(1, 0, progress)

        pbar: Optional[tqdm] = None

        if show_tqdm:
            pbar = tqdm(
                total=max_new_token,
                desc="text" if infer_text else "code",
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}(max) [{elapsed}, {rate_fmt}{postfix}]",
            )

        past_key_values = None

        for i in range(max_new_token):

            model_input = self._prepare_generation_inputs(
                inputs_ids,
                past_key_values,
                attention_mask_cache.narrow(1, 0, inputs_ids.shape[1]),
                use_cache=not self.is_te_llama,
            )

            if i > 0:
                del emb
                inputs_ids_emb = model_input.input_ids.to(self.device_gpt)
                if infer_text:
                    emb: torch.Tensor = self.emb_text(inputs_ids_emb[:, :, 0])
                else:
                    code_emb = [
                        self.emb_code[i](inputs_ids_emb[:, :, i])
                        for i in range(self.num_vq)
                    ]
                    emb = torch.stack(code_emb, 3).sum(3)
                del inputs_ids_emb, model_input.input_ids
            model_input.inputs_embeds = emb

            model_input.to(self.device_gpt, self.gpt.dtype)

            outputs: BaseModelOutputWithPast = self.gpt(
                attention_mask=model_input.attention_mask,
                position_ids=model_input.position_ids,
                past_key_values=model_input.past_key_values,
                inputs_embeds=model_input.inputs_embeds,
                use_cache=model_input.use_cache,
                output_attentions=return_attn,
                cache_position=model_input.cache_position,
            )
            del_all(model_input)
            attentions.append(outputs.attentions)
            hidden_states = outputs.last_hidden_state.to(
                self.device, dtype=torch.float
            )  # 🐻
            past_key_values = outputs.past_key_values
            del_all(outputs)
            if return_hidden:
                hiddens.append(hidden_states.narrow(1, -1, 1).squeeze_(1))

            with P.cached():
                if infer_text:
                    logits: torch.Tensor = self.head_text(hidden_states)
                else:
                    # logits = torch.stack([self.head_code[i](hidden_states) for i in range(self.num_vq)], 3)
                    logits = torch.empty(
                        hidden_states.size(0),
                        hidden_states.size(1),
                        self.num_audio_tokens,
                        self.num_vq,
                        dtype=torch.float,
                        device=self.device,
                    )
                    for num_vq_iter in range(self.num_vq):
                        x: torch.Tensor = self.head_code[num_vq_iter](hidden_states)
                        logits[..., num_vq_iter] = x
                        del x

            del hidden_states

            # logits = logits[:, -1].float()
            logits = logits.narrow(1, -1, 1).squeeze_(1).float()

            if not infer_text:
                # logits = rearrange(logits, "b c n -> (b n) c")
                logits = logits.permute(0, 2, 1)
                logits = logits.reshape(-1, logits.size(2))
                # logits_token = rearrange(inputs_ids[:, start_idx:], "b c n -> (b n) c")
                inputs_ids_sliced = inputs_ids.narrow(
                    1,
                    start_idx,
                    inputs_ids.size(1) - start_idx,
                ).permute(0, 2, 1)
                logits_token = inputs_ids_sliced.reshape(
                    inputs_ids_sliced.size(0) * inputs_ids_sliced.size(1),
                    -1,
                ).to(self.device)
                del inputs_ids_sliced
            else:
                logits_token = (
                    inputs_ids.narrow(
                        1,
                        start_idx,
                        inputs_ids.size(1) - start_idx,
                    )
                    .narrow(2, 0, 1)
                    .to(self.device)
                )

            logits /= temperature

            for logitsProcessors in logits_processors:
                logits = logitsProcessors(logits_token, logits)

            del logits_token

            if i < min_new_token:
                logits[:, eos_token] = -torch.inf

            scores = F.softmax(logits, dim=-1)

            del logits

            if manual_seed is None:
                idx_next = torch.multinomial(scores, num_samples=1).to(finish.device)
            else:
                idx_next = torch.multinomial(
                    scores,
                    num_samples=1,
                    generator=self.generator.manual_seed(manual_seed),
                ).to(finish.device)

            del scores

            if not infer_text:
                # idx_next = rearrange(idx_next, "(b n) 1 -> b n", n=self.num_vq)
                idx_next = idx_next.view(-1, self.num_vq)
                finish_or = idx_next.eq(eos_token).any(1)
                finish.logical_or_(finish_or)
                del finish_or
                inputs_ids_buf.narrow(1, progress, 1).copy_(idx_next.unsqueeze_(1))
            else:
                finish_or = idx_next.eq(eos_token).any(1)
                finish.logical_or_(finish_or)
                del finish_or
                inputs_ids_buf.narrow(1, progress, 1).copy_(
                    idx_next.unsqueeze_(-1).expand(-1, -1, self.num_vq),
                )

            if i == 0 and finish.any():
                self.logger.warning(
                    "unexpected end at index %s",
                    str([unexpected_idx.item() for unexpected_idx in finish.nonzero()]),
                )
                if ensure_non_empty and manual_seed is None:
                    if show_tqdm:
                        pbar.close()
                    self.logger.warning("regenerate in order to ensure non-empty")
                    del_all(attentions)
                    del_all(hiddens)
                    del (
                        start_idx,
                        end_idx,
                        finish,
                        temperature,
                        attention_mask_cache,
                        past_key_values,
                        idx_next,
                        inputs_ids_buf,
                    )
                    new_gen = self.generate(
                        emb,
                        inputs_ids,
                        old_temperature,
                        eos_token,
                        attention_mask,
                        max_new_token,
                        min_new_token,
                        logits_processors,
                        infer_text,
                        return_attn,
                        return_hidden,
                        stream,
                        show_tqdm,
                        ensure_non_empty,
                        stream_batch,
                        manual_seed,
                        context,
                    )
                    for result in new_gen:
                        yield result
                    del inputs_ids
                return

            del idx_next
            progress += 1
            inputs_ids = inputs_ids_buf.narrow(1, 0, progress)

            not_finished = finish.logical_not().to(end_idx.device)
            end_idx.add_(not_finished.int())
            stream_iter += not_finished.any().int()
            if stream:
                if stream_iter > 0 and stream_iter % stream_batch == 0:
                    self.logger.debug("yield stream result, end: %d", end_idx)
                    yield self._prepare_generation_outputs(
                        inputs_ids,
                        start_idx,
                        end_idx,
                        attentions,
                        hiddens,
                        infer_text,
                    )
            del not_finished

            if finish.all() or context.get():
                break

            if pbar is not None:
                pbar.update(1)

        if pbar is not None:
            pbar.close()

        if not finish.all():
            if context.get():
                self.logger.warning("generation is interrupted")
            else:
                self.logger.warning(
                    f"incomplete result. hit max_new_token: {max_new_token}"
                )

        del finish, inputs_ids_buf

        yield self._prepare_generation_outputs(
            inputs_ids,
            start_idx,
            end_idx,
            attentions,
            hiddens,
            infer_text,
        )



================================================
FILE: ChatTTS/model/processors.py
================================================
import torch
import torch.nn.functional as F
from transformers.generation import TopKLogitsWarper, TopPLogitsWarper


class CustomRepetitionPenaltyLogitsProcessorRepeat:

    def __init__(self, penalty: float, max_input_ids: int, past_window: int):
        if not isinstance(penalty, float) or not (penalty > 0):
            raise ValueError(
                f"`penalty` has to be a strictly positive float, but is {penalty}"
            )

        self.penalty = penalty
        self.max_input_ids = max_input_ids
        self.past_window = past_window

    def __call__(
        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
    ) -> torch.FloatTensor:
        if input_ids.size(1) > self.past_window:
            input_ids = input_ids.narrow(1, -self.past_window, self.past_window)
        freq = F.one_hot(input_ids, scores.size(1)).sum(1)
        if freq.size(0) > self.max_input_ids:
            freq.narrow(
                0, self.max_input_ids, freq.size(0) - self.max_input_ids
            ).zero_()
        alpha = torch.pow(self.penalty, freq)
        scores = scores.contiguous()
        inp = scores.multiply(alpha)
        oth = scores.divide(alpha)
        con = scores < 0
        out = torch.where(con, inp, oth)
        del inp, oth, scores, con, alpha
        return out


def gen_logits(
    num_code: int,
    top_P=0.7,
    top_K=20,
    repetition_penalty=1.0,
):
    logits_warpers = []
    if top_P is not None:
        logits_warpers.append(TopPLogitsWarper(top_P, min_tokens_to_keep=3))
    if top_K is not None:
        logits_warpers.append(TopKLogitsWarper(top_K, min_tokens_to_keep=3))

    logits_processors = []
    if repetition_penalty is not None and repetition_penalty != 1:
        logits_processors.append(
            CustomRepetitionPenaltyLogitsProcessorRepeat(
                repetition_penalty, num_code, 16
            )
        )

    return logits_warpers, logits_processors



================================================
FILE: ChatTTS/model/speaker.py
================================================
import lzma
from typing import List, Optional, Union

import pybase16384 as b14
import numpy as np
import torch
import torch.nn.functional as F


class Speaker:
    def __init__(self, dim: int, spk_cfg: str, device=torch.device("cpu")) -> None:
        spk_stat = torch.from_numpy(
            np.frombuffer(b14.decode_from_string(spk_cfg), dtype=np.float16).copy()
        ).to(device=device)
        self.std, self.mean = spk_stat.requires_grad_(False).chunk(2)
        self.dim = dim

    def sample_random(self) -> str:
        return self._encode(self._sample_random())

    @torch.inference_mode()
    def apply(
        self,
        emb: torch.Tensor,
        spk_emb: Union[str, torch.Tensor],
        input_ids: torch.Tensor,
        spk_emb_ids: int,
        device: torch.device,
        inplace: bool = True,
    ) -> torch.Tensor:
        if isinstance(spk_emb, str):
            spk_emb_tensor = torch.from_numpy(self._decode(spk_emb))
        else:
            spk_emb_tensor = spk_emb
        n = (
            F.normalize(
                spk_emb_tensor,
                p=2.0,
                dim=0,
                eps=1e-12,
            )
            .to(device)
            .unsqueeze_(0)
            .expand(emb.size(0), -1)
            .unsqueeze_(1)
            .expand(emb.shape)
        )
        cond = input_ids.narrow(-1, 0, 1).eq(spk_emb_ids).expand(emb.shape)
        out = torch.where(cond, n, emb, out=emb if inplace else None)
        if inplace:
            del cond, n
        return out

    @staticmethod
    @torch.no_grad()
    def decorate_code_prompts(
        text: List[str],
        prompt: str,
        txt_smp: Optional[str],
        spk_emb: Optional[str],
    ) -> List[str]:
        for i, t in enumerate(text):
            text[i] = (
                t.replace("[Stts]", "")
                .replace("[spk_emb]", "")
                .replace("[empty_spk]", "")
                .strip()
            )
            """
            see https://github.com/2noise/ChatTTS/issues/459
            """

        if prompt:
            text = [prompt + i for i in text]

        txt_smp = "" if txt_smp is None else txt_smp
        if spk_emb is not None:
            text = [f"[Stts][spk_emb]{txt_smp}{i}[Ptts]" for i in text]
        else:
            text = [f"[Stts][empty_spk]{txt_smp}{i}[Ptts]" for i in text]

        return text

    @staticmethod
    @torch.no_grad()
    def decorate_text_prompts(text: List[str], prompt: str) -> List[str]:
        return [f"[Sbreak]{i}[Pbreak]{prompt}" for i in text]

    @staticmethod
    @torch.no_grad()
    def encode_prompt(prompt: torch.Tensor) -> str:
        arr: np.ndarray = prompt.cpu().numpy().astype(np.uint16)
        shp = arr.shape
        assert len(shp) == 2, "prompt must be a 2D tensor"
        s = b14.encode_to_string(
            np.array(shp, dtype="<u2").tobytes()
            + lzma.compress(
                arr.astype("<u2").tobytes(),
                format=lzma.FORMAT_RAW,
                filters=[{"id": lzma.FILTER_LZMA2, "preset": 9 | lzma.PRESET_EXTREME}],
            ),
        )
        del arr
        return s

    @staticmethod
    @torch.no_grad()
    def decode_prompt(prompt: str) -> torch.Tensor:
        dec = b14.decode_from_string(prompt)
        shp = np.frombuffer(dec[:4], dtype="<u2")
        p = np.frombuffer(
            lzma.decompress(
                dec[4:],
                format=lzma.FORMAT_RAW,
                filters=[{"id": lzma.FILTER_LZMA2, "preset": 9 | lzma.PRESET_EXTREME}],
            ),
            dtype="<u2",
        ).copy()
        del dec
        return torch.from_numpy(p.astype(np.int32)).view(*shp)

    @torch.no_grad()
    def _sample_random(self) -> torch.Tensor:
        spk = (
            torch.randn(self.dim, device=self.std.device, dtype=self.std.dtype)
            .mul_(self.std)
            .add_(self.mean)
        )
        return spk

    @staticmethod
    @torch.no_grad()
    def _encode(spk_emb: torch.Tensor) -> str:
        arr: np.ndarray = spk_emb.to(dtype=torch.float16, device="cpu").numpy()
        s = b14.encode_to_string(
            lzma.compress(
                arr.tobytes(),
                format=lzma.FORMAT_RAW,
                filters=[{"id": lzma.FILTER_LZMA2, "preset": 9 | lzma.PRESET_EXTREME}],
            ),
        )
        del arr
        return s

    @staticmethod
    def _decode(spk_emb: str) -> np.ndarray:
        return np.frombuffer(
            lzma.decompress(
                b14.decode_from_string(spk_emb),
                format=lzma.FORMAT_RAW,
                filters=[{"id": lzma.FILTER_LZMA2, "preset": 9 | lzma.PRESET_EXTREME}],
            ),
            dtype=np.float16,
        ).copy()



================================================
FILE: ChatTTS/model/tokenizer.py
================================================
import os

os.environ["TOKENIZERS_PARALLELISM"] = "false"
"""
https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning
"""

from typing import List, Tuple, Optional, Union

import torch
from transformers import BertTokenizerFast

from ..utils import del_all


class Tokenizer:
    def __init__(
        self,
        tokenizer_path: torch.serialization.FILE_LIKE,
    ):
        """
        tokenizer: BertTokenizerFast = torch.load(
            tokenizer_path, map_location=device, mmap=True
        )
        # tokenizer.save_pretrained("asset/tokenizer", legacy_format=False)
        """
        tokenizer: BertTokenizerFast = BertTokenizerFast.from_pretrained(tokenizer_path)
        self._tokenizer = tokenizer

        self.len = len(tokenizer)
        self.spk_emb_ids = tokenizer.convert_tokens_to_ids("[spk_emb]")
        self.break_0_ids = tokenizer.convert_tokens_to_ids("[break_0]")
        self.eos_token = tokenizer.convert_tokens_to_ids("[Ebreak]")

    @torch.inference_mode()
    def encode(
        self,
        text: List[str],
        num_vq: int,
        prompt: Optional[torch.Tensor] = None,
        device="cpu",
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:

        input_ids_lst = []
        attention_mask_lst = []
        max_input_ids_len = -1
        max_attention_mask_len = -1
        prompt_size = 0

        if prompt is not None:
            assert prompt.size(0) == num_vq, "prompt dim 0 must equal to num_vq"
            prompt_size = prompt.size(1)

        # avoid random speaker embedding of tokenizer in the other dims
        for t in text:
            x = self._tokenizer.encode_plus(
                t, return_tensors="pt", add_special_tokens=False, padding=True
            )
            input_ids_lst.append(x["input_ids"].squeeze_(0))
            attention_mask_lst.append(x["attention_mask"].squeeze_(0))
            del_all(x)
            ids_sz = input_ids_lst[-1].size(0)
            if ids_sz > max_input_ids_len:
                max_input_ids_len = ids_sz
            attn_sz = attention_mask_lst[-1].size(0)
            if attn_sz > max_attention_mask_len:
                max_attention_mask_len = attn_sz

        if prompt is not None:
            max_input_ids_len += prompt_size
            max_attention_mask_len += prompt_size

        input_ids = torch.zeros(
            len(input_ids_lst),
            max_input_ids_len,
            device=device,
            dtype=input_ids_lst[0].dtype,
        )
        for i in range(len(input_ids_lst)):
            input_ids.narrow(0, i, 1).narrow(
                1,
                max_input_ids_len - prompt_size - input_ids_lst[i].size(0),
                input_ids_lst[i].size(0),
            ).copy_(
                input_ids_lst[i]
            )  # left padding
        del_all(input_ids_lst)

        attention_mask = torch.zeros(
            len(attention_mask_lst),
            max_attention_mask_len,
            device=device,
            dtype=attention_mask_lst[0].dtype,
        )
        for i in range(len(attention_mask_lst)):
            attn = attention_mask.narrow(0, i, 1)
            attn.narrow(
                1,
                max_attention_mask_len - prompt_size - attention_mask_lst[i].size(0),
                attention_mask_lst[i].size(0),
            ).copy_(
                attention_mask_lst[i]
            )  # left padding
            if prompt_size > 0:
                attn.narrow(
                    1,
                    max_attention_mask_len - prompt_size,
                    prompt_size,
                ).fill_(1)
        del_all(attention_mask_lst)

        text_mask = attention_mask.bool()
        new_input_ids = input_ids.unsqueeze_(-1).expand(-1, -1, num_vq).clone()
        del input_ids

        if prompt_size > 0:
            text_mask.narrow(1, max_input_ids_len - prompt_size, prompt_size).fill_(0)
            prompt_t = prompt.t().unsqueeze_(0).expand(new_input_ids.size(0), -1, -1)
            new_input_ids.narrow(
                1,
                max_input_ids_len - prompt_size,
                prompt_size,
            ).copy_(prompt_t)
            del prompt_t

        return new_input_ids, attention_mask, text_mask

    @torch.inference_mode
    def decode(
        self,
        sequences: Union[List[int], List[List[int]]],
        skip_special_tokens: bool = False,
        clean_up_tokenization_spaces: bool = None,
        **kwargs,
    ):
        return self._tokenizer.batch_decode(
            sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs
        )



================================================
FILE: ChatTTS/model/cuda/__init__.py
================================================
from .te_llama import TELlamaModel



================================================
FILE: ChatTTS/model/cuda/patch.py
================================================
import torch


class LlamaRMSNorm(torch.nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        LlamaRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = torch.nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states: torch.Tensor):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight.to(hidden_states.device) * hidden_states.to(input_dtype)



================================================
FILE: ChatTTS/model/cuda/te_llama.py
================================================
# Copyright (c) 2022-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# See LICENSE for license information.
#
# From https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/te_llama/te_llama.py
#
# Edited by fumiama.

import re
from contextlib import contextmanager
from typing import Dict

import transformer_engine as te
from transformer_engine.pytorch.attention import RotaryPositionEmbedding

import torch

import transformers
from transformers.models.llama.modeling_llama import (
    LlamaModel,
    LlamaConfig,
)
from transformers.modeling_utils import _load_state_dict_into_model

from .patch import LlamaRMSNorm


@contextmanager
def replace_decoder(te_decoder_cls, llama_rms_norm_cls):
    """
    Replace `LlamaDecoderLayer` with custom `TELlamaDecoderLayer`.
    """
    original_llama_decoder_cls = (
        transformers.models.llama.modeling_llama.LlamaDecoderLayer
    )
    transformers.models.llama.modeling_llama.LlamaDecoderLayer = te_decoder_cls
    original_llama_rms_norm_cls = transformers.models.llama.modeling_llama.LlamaRMSNorm
    transformers.models.llama.modeling_llama.LlamaRMSNorm = llama_rms_norm_cls
    try:
        yield
    finally:
        transformers.models.llama.modeling_llama.LlamaDecoderLayer = (
            original_llama_decoder_cls
        )
        transformers.models.llama.modeling_llama.LlamaRMSNorm = (
            original_llama_rms_norm_cls
        )


class TELlamaDecoderLayer(te.pytorch.TransformerLayer):
    """
    Wrapper class over TE's `TransformerLayer`. This makes the wrapper very
    similar to HF's `LlamaDecoderLayer` and easier to replace it in the code.

    Args:
        config: LlamaConfig
        args: positional args (for compatibility with `LlamaDecoderLayer`)
        kwargs: keyword args (for compatibility with `LlamaDecoderLayer`)
    """

    def __init__(self, config, *args, **kwargs):
        super().__init__(
            hidden_size=config.hidden_size,
            ffn_hidden_size=config.intermediate_size,
            num_attention_heads=config.num_attention_heads,
            bias=False,
            layernorm_epsilon=config.rms_norm_eps,
            hidden_dropout=0,
            attention_dropout=0,
            fuse_qkv_params=False,
            normalization="RMSNorm",
            activation="swiglu",
            attn_input_format="bshd",
            num_gqa_groups=config.num_key_value_heads,
        )
        te_rope = RotaryPositionEmbedding(
            config.hidden_size // config.num_attention_heads
        )
        self.te_rope_emb = te_rope(max_seq_len=config.max_position_embeddings).cuda()

    def forward(self, hidden_states, *args, attention_mask, **kwargs):
        """
        Custom forward to make sure we only pass relevant arguments to the
        forward pass of the `TransformerLayer`. Also, make sure the output
        format matches the output of the HF's `LlamaDecoderLayer`.
        """
        return (
            super().forward(
                hidden_states,
                attention_mask=attention_mask,
                rotary_pos_emb=self.te_rope_emb,
            ),
        )


class TELlamaModel:
    """
    LM created with `LlamaModel`. The underlying `LlamaDecoderLayer`
    class is monkey-patched with `TELlamaDecoderLayer` class before
    initializing the causal LM with `LlamaModel`.

    Args:
        config: LlamaConfig
    """

    def __new__(cls, config: LlamaConfig):
        with replace_decoder(
            te_decoder_cls=TELlamaDecoderLayer, llama_rms_norm_cls=LlamaRMSNorm
        ):
            model = LlamaModel(config)
        return model

    @classmethod
    def from_state_dict(
        cls,
        state_dict: Dict[str, torch.Tensor],
        config: LlamaConfig,
    ):
        """
        Custom method adapted from `from_pretrained` method in HuggingFace
        Transformers repo: https://github.com/huggingface/transformers/blob/f497f564bb76697edab09184a252fc1b1a326d1e/src/transformers/modeling_utils.py#L2579
        """

        vanilla_model = cls(config)

        # replace_params copies parameters relevant only to TransformerEngine
        _replace_params(state_dict, vanilla_model.state_dict(), config)
        # _load_state_dict_into_model copies parameters other than those in TransformerEngine
        _load_state_dict_into_model(vanilla_model, state_dict, start_prefix="")

        return vanilla_model


def _replace_params(hf_state_dict, te_state_dict, config):
    # collect all layer prefixes to update
    all_layer_prefixes = set()
    for param_key in hf_state_dict.keys():
        layer_prefix_pat = "model.layers.\d+."
        m = re.match(layer_prefix_pat, param_key)
        if m is not None:
            all_layer_prefixes.add(m.group())

    for layer_prefix in all_layer_prefixes:
        # When loading weights into models with less number of layers, skip the
        # copy if the corresponding layer doesn't exist in HF model
        if layer_prefix + "input_layernorm.weight" in hf_state_dict:
            te_state_dict[
                layer_prefix + "self_attention.layernorm_qkv.layer_norm_weight"
            ].data[:] = hf_state_dict[layer_prefix + "input_layernorm.weight"].data[:]

        if layer_prefix + "self_attn.q_proj.weight" in hf_state_dict:
            te_state_dict[
                layer_prefix + "self_attention.layernorm_qkv.query_weight"
            ].data[:] = hf_state_dict[layer_prefix + "self_attn.q_proj.weight"].data[:]

        if layer_prefix + "self_attn.k_proj.weight" in hf_state_dict:
            te_state_dict[
                layer_prefix + "self_attention.layernorm_qkv.key_weight"
            ].data[:] = hf_state_dict[layer_prefix + "self_attn.k_proj.weight"].data[:]

        if layer_prefix + "self_attn.v_proj.weight" in hf_state_dict:
            te_state_dict[
                layer_prefix + "self_attention.layernorm_qkv.value_weight"
            ].data[:] = hf_state_dict[layer_prefix + "self_attn.v_proj.weight"].data[:]

        if layer_prefix + "self_attn.o_proj.weight" in hf_state_dict:
            te_state_dict[layer_prefix + "self_attention.proj.weight"].data[:] = (
                hf_state_dict[layer_prefix + "self_attn.o_proj.weight"].data[:]
            )

        if layer_prefix + "post_attention_layernorm.weight" in hf_state_dict:
            te_state_dict[layer_prefix + "layernorm_mlp.layer_norm_weight"].data[:] = (
                hf_state_dict[layer_prefix + "post_attention_layernorm.weight"].data[:]
            )

        # It may happen that gate_proj.weight and up_proj.weight will be in the different files, so we need to
        # load them separately.
        if layer_prefix + "mlp.gate_proj.weight" in hf_state_dict:
            te_state_dict[layer_prefix + "layernorm_mlp.fc1_weight"].data[
                : config.intermediate_size
            ] = hf_state_dict[layer_prefix + "mlp.gate_proj.weight"].data

        if layer_prefix + "mlp.up_proj.weight" in hf_state_dict:
            te_state_dict[layer_prefix + "layernorm_mlp.fc1_weight"].data[
                config.intermediate_size :
            ] = hf_state_dict[layer_prefix + "mlp.up_proj.weight"].data

        if layer_prefix + "mlp.down_proj.weight" in hf_state_dict:
            te_state_dict[layer_prefix + "layernorm_mlp.fc2_weight"].data[:] = (
                hf_state_dict[layer_prefix + "mlp.down_proj.weight"].data[:]
            )
    return all_layer_prefixes



================================================
FILE: ChatTTS/model/velocity/__init__.py
================================================
from .llm import LLM
from .sampling_params import SamplingParams



================================================
FILE: ChatTTS/model/velocity/block_manager.py
================================================
"""A block manager that manages token blocks."""

import enum
from typing import Dict, List, Optional, Set, Tuple

from vllm.block import PhysicalTokenBlock
from .sequence import Sequence, SequenceGroup, SequenceStatus
from vllm.utils import Device

# Mapping: logical block number -> physical block.
BlockTable = List[PhysicalTokenBlock]


class BlockAllocator:
    """Manages free physical token blocks for a device.

    The allocator maintains a list of free blocks and allocates a block when
    requested. When a block is freed, its reference count is decremented. If
    the reference count becomes zero, the block is added back to the free list.
    """

    def __init__(
        self,
        device: Device,
        block_size: int,
        num_blocks: int,
    ) -> None:
        self.device = device
        self.block_size = block_size
        self.num_blocks = num_blocks

        # Initialize the free blocks.
        self.free_blocks: BlockTable = []
        for i in range(num_blocks):
            block = PhysicalTokenBlock(
                device=device, block_number=i, block_size=block_size
            )
            self.free_blocks.append(block)

    def allocate(self) -> PhysicalTokenBlock:
        if not self.free_blocks:
            raise ValueError("Out of memory! No free blocks are available.")
        block = self.free_blocks.pop()
        block.ref_count = 1
        return block

    def free(self, block: PhysicalTokenBlock) -> None:
        if block.ref_count == 0:
            raise ValueError(f"Double free! {block} is already freed.")
        block.ref_count -= 1
        if block.ref_count == 0:
            self.free_blocks.append(block)

    def get_num_free_blocks(self) -> int:
        return len(self.free_blocks)


class AllocStatus(enum.Enum):
    """Result for BlockSpaceManager.can_allocate

    1. Ok: seq_group can be allocated now.
    2. Later: seq_group cannot be allocated.
      The capacity of allocator is larger than seq_group required.
    3. Never: seq_group can never be allocated.
      The seq_group is too large to allocated in GPU.
    """

    OK = enum.auto()
    LATER = enum.auto()
    NEVER = enum.auto()


class BlockSpaceManager:
    """Manages the mapping between logical and physical token blocks."""

    def __init__(
        self,
        block_size: int,
        num_gpu_blocks: int,
        num_cpu_blocks: int,
        watermark: float = 0.01,
        sliding_window: Optional[int] = None,
    ) -> None:
        self.block_size = block_size
        self.num_total_gpu_blocks = num_gpu_blocks
        self.num_total_cpu_blocks = num_cpu_blocks

        self.block_sliding_window = None
        if sliding_window is not None:
            assert sliding_window % block_size == 0, (sliding_window, block_size)
            self.block_sliding_window = sliding_window // block_size

        self.watermark = watermark
        assert watermark >= 0.0

        self.watermark_blocks = int(watermark * num_gpu_blocks)
        self.gpu_allocator = BlockAllocator(Device.GPU, block_size, num_gpu_blocks)
        self.cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)
        # Mapping: seq_id -> BlockTable.
        self.block_tables: Dict[int, BlockTable] = {}

    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:
        # FIXME(woosuk): Here we assume that all sequences in the group share
        # the same prompt. This may not be true for preempted sequences.
        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]
        num_required_blocks = len(seq.logical_token_blocks)
        if self.block_sliding_window is not None:
            num_required_blocks = min(num_required_blocks, self.block_sliding_window)
        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()

        # Use watermark to avoid frequent cache eviction.
        if self.num_total_gpu_blocks - num_required_blocks < self.watermark_blocks:
            return AllocStatus.NEVER
        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:
            return AllocStatus.OK
        else:
            return AllocStatus.LATER

    def allocate(self, seq_group: SequenceGroup) -> None:
        # NOTE: Here we assume that all sequences in the group have the same
        # prompt.
        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]

        # Allocate new physical token blocks that will store the prompt tokens.
        block_table: BlockTable = []
        for logical_idx in range(len(seq.logical_token_blocks)):
            if (
                self.block_sliding_window is not None
                and logical_idx >= self.block_sliding_window
            ):
                block = block_table[logical_idx % self.block_sliding_window]
            else:
                block = self.gpu_allocator.allocate()
            # Set the reference counts of the token blocks.
            block.ref_count = seq_group.num_seqs()
            block_table.append(block)

        # Assign the block table for each sequence.
        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):
            self.block_tables[seq.seq_id] = block_table.copy()

    def can_append_slot(self, seq_group: SequenceGroup) -> bool:
        # Simple heuristic: If there is at least one free block
        # for each sequence, we can append.
        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()
        num_seqs = seq_group.num_seqs(status=SequenceStatus.RUNNING)
        return num_seqs <= num_free_gpu_blocks

    def append_slot(self, seq: Sequence) -> Optional[Tuple[int, int]]:
        """Allocate a physical slot for a new token."""
        logical_blocks = seq.logical_token_blocks
        block_table = self.block_tables[seq.seq_id]

        if len(block_table) < len(logical_blocks):
            if (
                self.block_sliding_window
                and len(block_table) >= self.block_sliding_window
            ):
                # re-use a block
                block_table.append(
                    block_table[len(block_table) % self.block_sliding_window]
                )
            else:
                # The sequence has a new logical block.
                # Allocate a new physical block.
                block = self.gpu_allocator.allocate()
                block_table.append(block)
                return None

        # We want to append the token to the last physical block.
        last_block = block_table[-1]
        assert last_block.device == Device.GPU
        if last_block.ref_count == 1:
            # Not shared with other sequences. Appendable.
            return None
        else:
            # The last block is shared with other sequences.
            # Copy on Write: Allocate a new block and copy the tokens.
            new_block = self.gpu_allocator.allocate()
            block_table[-1] = new_block
            self.gpu_allocator.free(last_block)
            return last_block.block_number, new_block.block_number

    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:
        # NOTE: fork does not allocate a new physical block.
        # Thus, it is always safe from OOM.
        src_block_table = self.block_tables[parent_seq.seq_id]
        self.block_tables[child_seq.seq_id] = src_block_table.copy()
        for block in src_block_table:
            block.ref_count += 1

    def _get_physical_blocks(
        self, seq_group: SequenceGroup
    ) -> List[PhysicalTokenBlock]:
        # NOTE: Here, we assume that the physical blocks are only shared by
        # the sequences in the same group.
        blocks: Set[PhysicalTokenBlock] = set()
        for seq in seq_group.get_seqs():
            if seq.is_finished():
                continue
            blocks.update(self.block_tables[seq.seq_id])
        return list(blocks)

    def can_swap_in(self, seq_group: SequenceGroup) -> bool:
        blocks = self._get_physical_blocks(seq_group)
        num_swapped_seqs = seq_group.num_seqs(status=SequenceStatus.SWAPPED)
        num_free_blocks = self.gpu_allocator.get_num_free_blocks()
        # NOTE: Conservatively, we assume that every sequence will allocate
        # at least one free block right after the swap-in.
        # NOTE: This should match the logic in can_append_slot().
        num_required_blocks = len(blocks) + num_swapped_seqs
        return num_free_blocks - num_required_blocks >= self.watermark_blocks

    def swap_in(self, seq_group: SequenceGroup) -> Dict[int, int]:
        # CPU block -> GPU block.
        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}
        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):
            new_block_table: BlockTable = []
            block_table = self.block_tables[seq.seq_id]

            for cpu_block in block_table:
                if cpu_block in mapping:
                    gpu_block = mapping[cpu_block]
                    gpu_block.ref_count += 1
                else:
                    gpu_block = self.gpu_allocator.allocate()
                    mapping[cpu_block] = gpu_block
                new_block_table.append(gpu_block)
                # Free the CPU block swapped in to GPU.
                self.cpu_allocator.free(cpu_block)
            self.block_tables[seq.seq_id] = new_block_table

        block_number_mapping = {
            cpu_block.block_number: gpu_block.block_number
            for cpu_block, gpu_block in mapping.items()
        }
        return block_number_mapping

    def can_swap_out(self, seq_group: SequenceGroup) -> bool:
        blocks = self._get_physical_blocks(seq_group)
        return len(blocks) <= self.cpu_allocator.get_num_free_blocks()

    def swap_out(self, seq_group: SequenceGroup) -> Dict[int, int]:
        # GPU block -> CPU block.
        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}
        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
            new_block_table: BlockTable = []
            block_table = self.block_tables[seq.seq_id]

            for gpu_block in block_table:
                if gpu_block in mapping:
                    cpu_block = mapping[gpu_block]
                    cpu_block.ref_count += 1
                else:
                    cpu_block = self.cpu_allocator.allocate()
                    mapping[gpu_block] = cpu_block
                new_block_table.append(cpu_block)
                # Free the GPU block swapped out to CPU.
                self.gpu_allocator.free(gpu_block)
            self.block_tables[seq.seq_id] = new_block_table

        block_number_mapping = {
            gpu_block.block_number: cpu_block.block_number
            for gpu_block, cpu_block in mapping.items()
        }
        return block_number_mapping

    def _free_block_table(self, block_table: BlockTable) -> None:
        for block in set(block_table):
            if block.device == Device.GPU:
                self.gpu_allocator.free(block)
            else:
                self.cpu_allocator.free(block)

    def free(self, seq: Sequence) -> None:
        if seq.seq_id not in self.block_tables:
            # Already freed or haven't been scheduled yet.
            return
        block_table = self.block_tables[seq.seq_id]
        self._free_block_table(block_table)
        del self.block_tables[seq.seq_id]

    def reset(self) -> None:
        for block_table in self.block_tables.values():
            self._free_block_table(block_table)
        self.block_tables.clear()

    def get_block_table(self, seq: Sequence) -> List[int]:
        block_table = self.block_tables[seq.seq_id]
        return [block.block_number for block in block_table]

    def get_num_free_gpu_blocks(self) -> int:
        return self.gpu_allocator.get_num_free_blocks()

    def get_num_free_cpu_blocks(self) -> int:
        return self.cpu_allocator.get_num_free_blocks()



================================================
FILE: ChatTTS/model/velocity/configs.py
================================================
from typing import Optional, Union, Tuple
import os

import torch
from transformers import PretrainedConfig

from vllm.logger import init_logger
from vllm.transformers_utils.config import get_config
from vllm.utils import get_cpu_memory, is_hip

import argparse
import dataclasses
from dataclasses import dataclass


logger = init_logger(__name__)

_GB = 1 << 30


class ModelConfig:
    """Configuration for the model.

    Args:
        model: Name or path of the huggingface model to use.
        tokenizer: Name or path of the huggingface tokenizer to use.
        tokenizer_mode: Tokenizer mode. "auto" will use the fast tokenizer if
            available, and "slow" will always use the slow tokenizer.
        trust_remote_code: Trust remote code (e.g., from HuggingFace) when
            downloading the model and tokenizer.
        download_dir: Directory to download and load the weights, default to the
            default cache directory of huggingface.
        load_format: The format of the model weights to load:
            "auto" will try to load the weights in the safetensors format and
                fall back to the pytorch bin format if safetensors format is
                not available.
            "pt" will load the weights in the pytorch bin format.
            "safetensors" will load the weights in the safetensors format.
            "npcache" will load the weights in pytorch format and store
                a numpy cache to speed up the loading.
            "dummy" will initialize the weights with random values, which is
                mainly for profiling.
        dtype: Data type for model weights and activations. The "auto" option
            will use FP16 precision for FP32 and FP16 models, and BF16 precision
            for BF16 models.
        seed: Random seed for reproducibility.
        revision: The specific model version to use. It can be a branch name,
            a tag name, or a commit id. If unspecified, will use the default
            version.
        tokenizer_revision: The specific tokenizer version to use. It can be a
            branch name, a tag name, or a commit id. If unspecified, will use
            the default version.
        max_model_len: Maximum length of a sequence (including prompt and
            output). If None, will be derived from the model.
        quantization: Quantization method that was used to quantize the model
            weights. If None, we assume the model weights are not quantized.
        enforce_eager: Whether to enforce eager execution. If True, we will
            disable CUDA graph and always execute the model in eager mode.
            If False, we will use CUDA graph and eager execution in hybrid.
        max_context_len_to_capture: Maximum context len covered by CUDA graphs.
            When a sequence has context length larger than this, we fall back
            to eager mode.
    """

    def __init__(
        self,
        model: str,
        tokenizer: str,
        tokenizer_mode: str,
        trust_remote_code: bool,
        download_dir: Optional[str],
        load_format: str,
        dtype: Union[str, torch.dtype],
        seed: int,
        revision: Optional[str] = None,
        tokenizer_revision: Optional[str] = None,
        max_model_len: Optional[int] = None,
        quantization: Optional[str] = None,
        enforce_eager: bool = False,
        max_context_len_to_capture: Optional[int] = None,
        num_audio_tokens: int = 1024,
        num_text_tokens: int = 80,
    ) -> None:
        self.model = model
        self.tokenizer = tokenizer
        self.tokenizer_mode = tokenizer_mode
        self.trust_remote_code = trust_remote_code
        self.download_dir = download_dir
        self.load_format = load_format
        self.seed = seed
        self.revision = revision
        self.tokenizer_revision = tokenizer_revision
        self.quantization = quantization
        self.enforce_eager = enforce_eager
        self.max_context_len_to_capture = max_context_len_to_capture
        self.num_audio_tokens = num_audio_tokens
        self.num_text_tokens = num_text_tokens

        if os.environ.get("VLLM_USE_MODELSCOPE", "False").lower() == "true":
            # download model from ModelScope hub,
            # lazy import so that modelscope is not required for normal use.
            from modelscope.hub.snapshot_download import (
                snapshot_download,
            )  # pylint: disable=C

            model_path = snapshot_download(
                model_id=model, cache_dir=download_dir, revision=revision
            )
            self.model = model_path
            self.download_dir = model_path
            self.tokenizer = model_path

        self.hf_config = get_config(self.model, trust_remote_code, revision)
        self.dtype = _get_and_verify_dtype(self.hf_config, dtype)
        self.max_model_len = _get_and_verify_max_len(self.hf_config, max_model_len)
        self._verify_load_format()
        self._verify_tokenizer_mode()
        self._verify_quantization()
        self._verify_cuda_graph()

    def _verify_load_format(self) -> None:
        load_format = self.load_format.lower()
        supported_load_format = ["auto", "pt", "safetensors", "npcache", "dummy"]
        rocm_not_supported_load_format = []
        if load_format not in supported_load_format:
            raise ValueError(
                f"Unknown load format: {self.load_format}. Must be one of "
                "'auto', 'pt', 'safetensors', 'npcache', or 'dummy'."
            )
        if is_hip() and load_format in rocm_not_supported_load_format:
            rocm_supported_load_format = [
                f
                for f in supported_load_format
                if (f not in rocm_not_supported_load_format)
            ]
            raise ValueError(
                f"load format '{load_format}' is not supported in ROCm. "
                f"Supported load format are "
                f"{rocm_supported_load_format}"
            )

        # TODO: Remove this check once HF updates the pt weights of Mixtral.
        architectures = getattr(self.hf_config, "architectures", [])
        if "MixtralForCausalLM" in architectures and load_format == "pt":
            raise ValueError(
                "Currently, the 'pt' format is not supported for Mixtral. "
                "Please use the 'safetensors' format instead. "
            )
        self.load_format = load_format

    def _verify_tokenizer_mode(self) -> None:
        tokenizer_mode = self.tokenizer_mode.lower()
        if tokenizer_mode not in ["auto", "slow"]:
            raise ValueError(
                f"Unknown tokenizer mode: {self.tokenizer_mode}. Must be "
                "either 'auto' or 'slow'."
            )
        self.tokenizer_mode = tokenizer_mode

    def _verify_quantization(self) -> None:
        supported_quantization = ["awq", "gptq", "squeezellm"]
        rocm_not_supported_quantization = ["awq"]
        if self.quantization is not None:
            self.quantization = self.quantization.lower()

        # Parse quantization method from the HF model config, if available.
        hf_quant_config = getattr(self.hf_config, "quantization_config", None)
        if hf_quant_config is not None:
            hf_quant_method = str(hf_quant_config["quant_method"]).lower()
            if self.quantization is None:
                self.quantization = hf_quant_method
            elif self.quantization != hf_quant_method:
                raise ValueError(
                    "Quantization method specified in the model config "
                    f"({hf_quant_method}) does not match the quantization "
                    f"method specified in the `quantization` argument "
                    f"({self.quantization})."
                )

        if self.quantization is not None:
            if self.quantization not in supported_quantization:
                raise ValueError(
                    f"Unknown quantization method: {self.quantization}. Must "
                    f"be one of {supported_quantization}."
                )
            if is_hip() and self.quantization in rocm_not_supported_quantization:
                raise ValueError(
                    f"{self.quantization} quantization is currently not supported "
                    f"in ROCm."
                )
            logger.warning(
                f"{self.quantization} quantization is not fully "
                "optimized yet. The speed can be slower than "
                "non-quantized models."
            )

    def _verify_cuda_graph(self) -> None:
        if self.max_context_len_to_capture is None:
            self.max_context_len_to_capture = self.max_model_len
        self.max_context_len_to_capture = min(
            self.max_context_len_to_capture, self.max_model_len
        )

    def verify_with_parallel_config(
        self,
        parallel_config: "ParallelConfig",
    ) -> None:
        total_num_attention_heads = self.hf_config.num_attention_heads
        tensor_parallel_size = parallel_config.tensor_parallel_size
        if total_num_attention_heads % tensor_parallel_size != 0:
            raise ValueError(
                f"Total number of attention heads ({total_num_attention_heads})"
                " must be divisible by tensor parallel size "
                f"({tensor_parallel_size})."
            )

        total_num_hidden_layers = self.hf_config.num_hidden_layers
        pipeline_parallel_size = parallel_config.pipeline_parallel_size
        if total_num_hidden_layers % pipeline_parallel_size != 0:
            raise ValueError(
                f"Total number of hidden layers ({total_num_hidden_layers}) "
                "must be divisible by pipeline parallel size "
                f"({pipeline_parallel_size})."
            )

    def get_sliding_window(self) -> Optional[int]:
        return getattr(self.hf_config, "sliding_window", None)

    def get_vocab_size(self) -> int:
        return self.hf_config.vocab_size

    def get_hidden_size(self) -> int:
        return self.hf_config.hidden_size

    def get_head_size(self) -> int:
        # FIXME(woosuk): This may not be true for all models.
        return self.hf_config.hidden_size // self.hf_config.num_attention_heads

    def get_total_num_kv_heads(self) -> int:
        """Returns the total number of KV heads."""
        # For GPTBigCode & Falcon:
        # NOTE: for falcon, when new_decoder_architecture is True, the
        # multi_query flag is ignored and we use n_head_kv for the number of
        # KV heads.
        falcon_model_types = ["falcon", "RefinedWeb", "RefinedWebModel"]
        new_decoder_arch_falcon = (
            self.hf_config.model_type in falcon_model_types
            and getattr(self.hf_config, "new_decoder_architecture", False)
        )
        if not new_decoder_arch_falcon and getattr(
            self.hf_config, "multi_query", False
        ):
            # Multi-query attention, only one KV head.
            # Currently, tensor parallelism is not supported in this case.
            return 1

        attributes = [
            # For Falcon:
            "n_head_kv",
            "num_kv_heads",
            # For LLaMA-2:
            "num_key_value_heads",
            # For ChatGLM:
            "multi_query_group_num",
        ]
        for attr in attributes:
            num_kv_heads = getattr(self.hf_config, attr, None)
            if num_kv_heads is not None:
                return num_kv_heads

        # For non-grouped-query attention models, the number of KV heads is
        # equal to the number of attention heads.
        return self.hf_config.num_attention_heads

    def get_num_kv_heads(self, parallel_config: "ParallelConfig") -> int:
        """Returns the number of KV heads per GPU."""
        total_num_kv_heads = self.get_total_num_kv_heads()
        # If tensor parallelism is used, we divide the number of KV heads by
        # the tensor parallel size. We will replicate the KV heads in the
        # case where the number of KV heads is smaller than the tensor
        # parallel size so each GPU has at least one KV head.
        return max(1, total_num_kv_heads // parallel_config.tensor_parallel_size)

    def get_num_layers(self, parallel_config: "ParallelConfig") -> int:
        total_num_hidden_layers = self.hf_config.num_hidden_layers
        return total_num_hidden_layers // parallel_config.pipeline_parallel_size


class CacheConfig:
    """Configuration for the KV cache.

    Args:
        block_size: Size of a cache block in number of tokens.
        gpu_memory_utilization: Fraction of GPU memory to use for the
            vLLM execution.
        swap_space: Size of the CPU swap space per GPU (in GiB).
    """

    def __init__(
        self,
        block_size: int,
        gpu_memory_utilization: float,
        swap_space: int,
        sliding_window: Optional[int] = None,
    ) -> None:
        self.block_size = block_size
        self.gpu_memory_utilization = gpu_memory_utilization
        self.swap_space_bytes = swap_space * _GB
        self.sliding_window = sliding_window
        self._verify_args()

        # Will be set after profiling.
        self.num_gpu_blocks = None
        self.num_cpu_blocks = None

    def _verify_args(self) -> None:
        if self.gpu_memory_utilization > 1.0:
            raise ValueError(
                "GPU memory utilization must be less than 1.0. Got "
                f"{self.gpu_memory_utilization}."
            )

    def verify_with_parallel_config(
        self,
        parallel_config: "ParallelConfig",
    ) -> None:
        total_cpu_memory = get_cpu_memory()
        # FIXME(woosuk): Here, it is assumed that the GPUs in a tensor parallel
        # group are in the same node. However, the GPUs may span multiple nodes.
        num_gpus_per_node = parallel_config.tensor_parallel_size
        cpu_memory_usage = self.swap_space_bytes * num_gpus_per_node

        msg = (
            f"{cpu_memory_usage / _GB:.2f} GiB out of "
            f"the {total_cpu_memory / _GB:.2f} GiB total CPU memory is "
            "allocated for the swap space."
        )
        if cpu_memory_usage > 0.7 * total_cpu_memory:
            raise ValueError("Too large swap space. " + msg)
        elif cpu_memory_usage > 0.4 * total_cpu_memory:
            logger.warning("Possibly too large swap space. " + msg)


class ParallelConfig:
    """Configuration for the distributed execution.

    Args:
        pipeline_parallel_size: Number of pipeline parallel groups.
        tensor_parallel_size: Number of tensor parallel groups.
        worker_use_ray: Whether to use Ray for model workers. Will be set to
            True if either pipeline_parallel_size or tensor_parallel_size is
            greater than 1.
    """

    def __init__(
        self,
        pipeline_parallel_size: int,
        tensor_parallel_size: int,
        worker_use_ray: bool,
        max_parallel_loading_workers: Optional[int] = None,
    ) -> None:
        self.pipeline_parallel_size = pipeline_parallel_size
        self.tensor_parallel_size = tensor_parallel_size
        self.worker_use_ray = worker_use_ray
        self.max_parallel_loading_workers = max_parallel_loading_workers

        self.world_size = pipeline_parallel_size * tensor_parallel_size
        if self.world_size > 1:
            self.worker_use_ray = True
        self._verify_args()

    def _verify_args(self) -> None:
        if self.pipeline_parallel_size > 1:
            raise NotImplementedError("Pipeline parallelism is not supported yet.")


class SchedulerConfig:
    """Scheduler configuration.

    Args:
        max_num_batched_tokens: Maximum number of tokens to be processed in
            a single iteration.
        max_num_seqs: Maximum number of sequences to be processed in a single
            iteration.
        max_model_len: Maximum length of a sequence (including prompt
            and generated text).
        max_paddings: Maximum number of paddings to be added to a batch.
    """

    def __init__(
        self,
        max_num_batched_tokens: Optional[int],
        max_num_seqs: int,
        max_model_len: int,
        max_paddings: int,
    ) -> None:
        if max_num_batched_tokens is not None:
            self.max_num_batched_tokens = max_num_batched_tokens
        else:
            # If max_model_len is too short, use 2048 as the default value for
            # higher throughput.
            self.max_num_batched_tokens = max(max_model_len, 2048)
        self.max_num_seqs = max_num_seqs
        self.max_model_len = max_model_len
        self.max_paddings = max_paddings
        self._verify_args()

    def _verify_args(self) -> None:
        if self.max_num_batched_tokens < self.max_model_len:
            raise ValueError(
                f"max_num_batched_tokens ({self.max_num_batched_tokens}) is "
                f"smaller than max_model_len ({self.max_model_len}). "
                "This effectively limits the maximum sequence length to "
                "max_num_batched_tokens and makes vLLM reject longer "
                "sequences. Please increase max_num_batched_tokens or "
                "decrease max_model_len."
            )
        if self.max_num_batched_tokens < self.max_num_seqs:
            raise ValueError(
                f"max_num_batched_tokens ({self.max_num_batched_tokens}) must "
                "be greater than or equal to max_num_seqs "
                f"({self.max_num_seqs})."
            )


_STR_DTYPE_TO_TORCH_DTYPE = {
    "half": torch.float16,
    "float16": torch.float16,
    "float": torch.float32,
    "float32": torch.float32,
    "bfloat16": torch.bfloat16,
}

_ROCM_NOT_SUPPORTED_DTYPE = ["float", "float32"]


def _get_and_verify_dtype(
    config: PretrainedConfig,
    dtype: Union[str, torch.dtype],
) -> torch.dtype:
    # NOTE: getattr(config, "torch_dtype", torch.float32) is not correct
    # because config.torch_dtype can be None.
    config_dtype = getattr(config, "torch_dtype", None)
    if config_dtype is None:
        config_dtype = torch.float32

    if isinstance(dtype, str):
        dtype = dtype.lower()
        if dtype == "auto":
            if config_dtype == torch.float32:
                # Following the common practice, we use float16 for float32
                # models.
                torch_dtype = torch.float16
            else:
                torch_dtype = config_dtype
        else:
            if dtype not in _STR_DTYPE_TO_TORCH_DTYPE:
                raise ValueError(f"Unknown dtype: {dtype}")
            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]
    elif isinstance(dtype, torch.dtype):
        torch_dtype = dtype
    else:
        raise ValueError(f"Unknown dtype: {dtype}")

    if is_hip() and torch_dtype == torch.float32:
        rocm_supported_dtypes = [
            k
            for k, v in _STR_DTYPE_TO_TORCH_DTYPE.items()
            if (k not in _ROCM_NOT_SUPPORTED_DTYPE)
        ]
        raise ValueError(
            f"dtype '{dtype}' is not supported in ROCm. "
            f"Supported dtypes are {rocm_supported_dtypes}"
        )

    # Verify the dtype.
    if torch_dtype != config_dtype:
        if torch_dtype == torch.float32:
            # Upcasting to float32 is allowed.
            pass
        elif config_dtype == torch.float32:
            # Downcasting from float32 to float16 or bfloat16 is allowed.
            pass
        else:
            # Casting between float16 and bfloat16 is allowed with a warning.
            logger.warning(f"Casting {config_dtype} to {torch_dtype}.")

    return torch_dtype


def _get_and_verify_max_len(
    hf_config: PretrainedConfig,
    max_model_len: Optional[int],
) -> int:
    """Get and verify the model's maximum length."""
    derived_max_model_len = float("inf")
    possible_keys = [
        # OPT
        "max_position_embeddings",
        # GPT-2
        "n_positions",
        # MPT
        "max_seq_len",
        # ChatGLM2
        "seq_length",
        # Others
        "max_sequence_length",
        "max_seq_length",
        "seq_len",
    ]
    for key in possible_keys:
        max_len_key = getattr(hf_config, key, None)
        if max_len_key is not None:
            derived_max_model_len = min(derived_max_model_len, max_len_key)
    if derived_max_model_len == float("inf"):
        if max_model_len is not None:
            # If max_model_len is specified, we use it.
            return max_model_len

        default_max_len = 2048
        logger.warning(
            "The model's config.json does not contain any of the following "
            "keys to determine the original maximum length of the model: "
            f"{possible_keys}. Assuming the model's maximum length is "
            f"{default_max_len}."
        )
        derived_max_model_len = default_max_len

    rope_scaling = getattr(hf_config, "rope_scaling", None)
    if rope_scaling is not None:
        assert "factor" in rope_scaling
        scaling_factor = rope_scaling["factor"]
        if rope_scaling["type"] == "yarn":
            derived_max_model_len = rope_scaling["original_max_position_embeddings"]
        derived_max_model_len *= scaling_factor

    if max_model_len is None:
        max_model_len = derived_max_model_len
    elif max_model_len > derived_max_model_len:
        raise ValueError(
            f"User-specified max_model_len ({max_model_len}) is greater than "
            f"the derived max_model_len ({max_len_key}={derived_max_model_len}"
            " in model's config.json). This may lead to incorrect model "
            "outputs or CUDA errors. Make sure the value is correct and "
            "within the model context size."
        )
    return int(max_model_len)


@dataclass
class EngineArgs:
    """Arguments for vLLM engine."""

    model: str
    tokenizer: Optional[str] = None
    tokenizer_mode: str = "auto"
    trust_remote_code: bool = False
    download_dir: Optional[str] = None
    load_format: str = "auto"
    dtype: str = "auto"
    seed: int = 0
    max_model_len: Optional[int] = None
    worker_use_ray: bool = False
    pipeline_parallel_size: int = 1
    tensor_parallel_size: int = 1
    max_parallel_loading_workers: Optional[int] = None
    block_size: int = 16
    swap_space: int = 4  # GiB
    gpu_memory_utilization: float = 0.90
    max_num_batched_tokens: Optional[int] = None
    max_num_seqs: int = 256
    max_paddings: int = 256
    disable_log_stats: bool = False
    revision: Optional[str] = None
    tokenizer_revision: Optional[str] = None
    quantization: Optional[str] = None
    enforce_eager: bool = False
    max_context_len_to_capture: int = 8192
    num_audio_tokens: int = 1024
    num_text_tokens: int = 80

    def __post_init__(self):
        if self.tokenizer is None:
            self.tokenizer = self.model

    @staticmethod
    def add_cli_args(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:
        """Shared CLI arguments for vLLM engine."""

        # NOTE: If you update any of the arguments below, please also
        # make sure to update docs/source/models/engine_args.rst

        # Model arguments
        parser.add_argument(
            "--model",
            type=str,
            default="facebook/opt-125m",
            help="name or path of the huggingface model to use",
        )
        parser.add_argument(
            "--tokenizer",
            type=str,
            default=EngineArgs.tokenizer,
            help="name or path of the huggingface tokenizer to use",
        )
        parser.add_argument(
            "--revision",
            type=str,
            default=None,
            help="the specific model version to use. It can be a branch "
            "name, a tag name, or a commit id. If unspecified, will use "
            "the default version.",
        )
        parser.add_argument(
            "--tokenizer-revision",
            type=str,
            default=None,
            help="the specific tokenizer version to use. It can be a branch "
            "name, a tag name, or a commit id. If unspecified, will use "
            "the default version.",
        )
        parser.add_argument(
            "--tokenizer-mode",
            type=str,
            default=EngineArgs.tokenizer_mode,
            choices=["auto", "slow"],
            help='tokenizer mode. "auto" will use the fast '
            'tokenizer if available, and "slow" will '
            "always use the slow tokenizer.",
        )
        parser.add_argument(
            "--trust-remote-code",
            action="store_true",
            help="trust remote code from huggingface",
        )
        parser.add_argument(
            "--download-dir",
            type=str,
            default=EngineArgs.download_dir,
            help="directory to download and load the weights, "
            "default to the default cache dir of "
            "huggingface",
        )
        parser.add_argument(
            "--load-format",
            type=str,
            default=EngineArgs.load_format,
            choices=["auto", "pt", "safetensors", "npcache", "dummy"],
            help="The format of the model weights to load. "
            '"auto" will try to load the weights in the safetensors format '
            "and fall back to the pytorch bin format if safetensors format "
            "is not available. "
            '"pt" will load the weights in the pytorch bin format. '
            '"safetensors" will load the weights in the safetensors format. '
            '"npcache" will load the weights in pytorch format and store '
            "a numpy cache to speed up the loading. "
            '"dummy" will initialize the weights with random values, '
            "which is mainly for profiling.",
        )
        parser.add_argument(
            "--dtype",
            type=str,
            default=EngineArgs.dtype,
            choices=["auto", "half", "float16", "bfloat16", "float", "float32"],
            help="data type for model weights and activations. "
            'The "auto" option will use FP16 precision '
            "for FP32 and FP16 models, and BF16 precision "
            "for BF16 models.",
        )
        parser.add_argument(
            "--max-model-len",
            type=int,
            default=None,
            help="model context length. If unspecified, "
            "will be automatically derived from the model.",
        )
        # Parallel arguments
        parser.add_argument(
            "--worker-use-ray",
            action="store_true",
            help="use Ray for distributed serving, will be "
            "automatically set when using more than 1 GPU",
        )
        parser.add_argument(
            "--pipeline-parallel-size",
            "-pp",
            type=int,
            default=EngineArgs.pipeline_parallel_size,
            help="number of pipeline stages",
        )
        parser.add_argument(
            "--tensor-parallel-size",
            "-tp",
            type=int,
            default=EngineArgs.tensor_parallel_size,
            help="number of tensor parallel replicas",
        )
        parser.add_argument(
            "--max-parallel-loading-workers",
            type=int,
            help="load model sequentially in multiple batches, "
            "to avoid RAM OOM when using tensor "
            "parallel and large models",
        )
        # KV cache arguments
        parser.add_argument(
            "--block-size",
            type=int,
            default=EngineArgs.block_size,
            choices=[8, 16, 32],
            help="token block size",
        )
        # TODO(woosuk): Support fine-grained seeds (e.g., seed per request).
        parser.add_argument(
            "--seed", type=int, default=EngineArgs.seed, help="random seed"
        )
        parser.add_argument(
            "--swap-space",
            type=int,
            default=EngineArgs.swap_space,
            help="CPU swap space size (GiB) per GPU",
        )
        parser.add_argument(
            "--gpu-memory-utilization",
            type=float,
            default=EngineArgs.gpu_memory_utilization,
            help="the fraction of GPU memory to be used for "
            "the model executor, which can range from 0 to 1."
            "If unspecified, will use the default value of 0.9.",
        )
        parser.add_argument(
            "--max-num-batched-tokens",
            type=int,
            default=EngineArgs.max_num_batched_tokens,
            help="maximum number of batched tokens per " "iteration",
        )
        parser.add_argument(
            "--max-num-seqs",
            type=int,
            default=EngineArgs.max_num_seqs,
            help="maximum number of sequences per iteration",
        )
        parser.add_argument(
            "--max-paddings",
            type=int,
            default=EngineArgs.max_paddings,
            help="maximum number of paddings in a batch",
        )
        parser.add_argument(
            "--disable-log-stats",
            action="store_true",
            help="disable logging statistics",
        )
        # Quantization settings.
        parser.add_argument(
            "--quantization",
            "-q",
            type=str,
            choices=["awq", "gptq", "squeezellm", None],
            default=None,
            help="Method used to quantize the weights. If "
            "None, we first check the `quantization_config` "
            "attribute in the model config file. If that is "
            "None, we assume the model weights are not "
            "quantized and use `dtype` to determine the data "
            "type of the weights.",
        )
        parser.add_argument(
            "--enforce-eager",
            action="store_true",
            help="Always use eager-mode PyTorch. If False, "
            "will use eager mode and CUDA graph in hybrid "
            "for maximal performance and flexibility.",
        )
        parser.add_argument(
            "--max-context-len-to-capture",
            type=int,
            default=EngineArgs.max_context_len_to_capture,
            help="maximum context length covered by CUDA "
            "graphs. When a sequence has context length "
            "larger than this, we fall back to eager mode.",
        )
        return parser

    @classmethod
    def from_cli_args(cls, args: argparse.Namespace) -> "EngineArgs":
        # Get the list of attributes of this dataclass.
        attrs = [attr.name for attr in dataclasses.fields(cls)]
        # Set the attributes from the parsed arguments.
        engine_args = cls(**{attr: getattr(args, attr) for attr in attrs})
        return engine_args

    def create_engine_configs(
        self,
    ) -> Tuple[ModelConfig, CacheConfig, ParallelConfig, SchedulerConfig]:
        model_config = ModelConfig(
            self.model,
            self.tokenizer,
            self.tokenizer_mode,
            self.trust_remote_code,
            self.download_dir,
            self.load_format,
            self.dtype,
            self.seed,
            self.revision,
            self.tokenizer_revision,
            self.max_model_len,
            self.quantization,
            self.enforce_eager,
            self.max_context_len_to_capture,
            self.num_audio_tokens,
            self.num_text_tokens,
        )
        cache_config = CacheConfig(
            self.block_size,
            self.gpu_memory_utilization,
            self.swap_space,
            model_config.get_sliding_window(),
        )
        parallel_config = ParallelConfig(
            self.pipeline_parallel_size,
            self.tensor_parallel_size,
            self.worker_use_ray,
            self.max_parallel_loading_workers,
        )
        scheduler_config = SchedulerConfig(
            self.max_num_batched_tokens,
            self.max_num_seqs,
            model_config.max_model_len,
            self.max_paddings,
        )
        return model_config, cache_config, parallel_config, scheduler_config


@dataclass
class AsyncEngineArgs(EngineArgs):
    """Arguments for asynchronous vLLM engine."""

    engine_use_ray: bool = False
    disable_log_requests: bool = False
    max_log_len: Optional[int] = None

    @staticmethod
    def add_cli_args(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:
        parser = EngineArgs.add_cli_args(parser)
        parser.add_argument(
            "--engine-use-ray",
            action="store_true",
            help="use Ray to start the LLM engine in a "
            "separate process as the server process.",
        )
        parser.add_argument(
            "--disable-log-requests",
            action="store_true",
            help="disable logging requests",
        )
        parser.add_argument(
            "--max-log-len",
            type=int,
            default=None,
            help="max number of prompt characters or prompt "
            "ID numbers being printed in log. "
            "Default: unlimited.",
        )
        return parser



================================================
FILE: ChatTTS/model/velocity/llama.py
================================================
# coding=utf-8
# Adapted from
# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py
# Copyright 2023 The vLLM team.
# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Inference-only LLaMA model compatible with HuggingFace weights."""
from typing import Any, Dict, List, Optional, Tuple

import torch
from torch import nn
from transformers import LlamaConfig

from vllm.model_executor.input_metadata import InputMetadata
from vllm.model_executor.layers.activation import SiluAndMul
from vllm.model_executor.layers.attention import PagedAttention
from vllm.model_executor.layers.layernorm import RMSNorm
from vllm.model_executor.layers.linear import (
    LinearMethodBase,
    MergedColumnParallelLinear,
    QKVParallelLinear,
    RowParallelLinear,
)
from vllm.model_executor.layers.rotary_embedding import get_rope
from vllm.model_executor.layers.sampler import Sampler
from vllm.model_executor.layers.vocab_parallel_embedding import (
    VocabParallelEmbedding,
    ParallelLMHead,
)
from vllm.model_executor.parallel_utils.parallel_state import (
    get_tensor_model_parallel_world_size,
)
from vllm.model_executor.sampling_metadata import SamplingMetadata
from vllm.model_executor.weight_utils import (
    default_weight_loader,
    hf_model_weights_iterator,
)
from vllm.sequence import SamplerOutput

KVCache = Tuple[torch.Tensor, torch.Tensor]


class LlamaMLP(nn.Module):

    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int,
        hidden_act: str,
        linear_method: Optional[LinearMethodBase] = None,
    ) -> None:
        super().__init__()
        self.gate_up_proj = MergedColumnParallelLinear(
            hidden_size,
            [intermediate_size] * 2,
            bias=False,
            linear_method=linear_method,
        )
        self.down_proj = RowParallelLinear(
            intermediate_size, hidden_size, bias=False, linear_method=linear_method
        )
        if hidden_act != "silu":
            raise ValueError(
                f"Unsupported activation: {hidden_act}. "
                "Only silu is supported for now."
            )
        self.act_fn = SiluAndMul()

    def forward(self, x):
        gate_up, _ = self.gate_up_proj(x)
        x = self.act_fn(gate_up)
        x, _ = self.down_proj(x)
        return x


class LlamaAttention(nn.Module):

    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        num_kv_heads: int,
        rope_theta: float = 10000,
        rope_scaling: Optional[Dict[str, Any]] = None,
        max_position_embeddings: int = 8192,
        linear_method: Optional[LinearMethodBase] = None,
    ) -> None:
        super().__init__()
        self.hidden_size = hidden_size
        tp_size = get_tensor_model_parallel_world_size()
        self.total_num_heads = num_heads
        assert self.total_num_heads % tp_size == 0
        self.num_heads = self.total_num_heads // tp_size
        self.total_num_kv_heads = num_kv_heads
        if self.total_num_kv_heads >= tp_size:
            # Number of KV heads is greater than TP size, so we partition
            # the KV heads across multiple tensor parallel GPUs.
            assert self.total_num_kv_heads % tp_size == 0
        else:
            # Number of KV heads is less than TP size, so we replicate
            # the KV heads across multiple tensor parallel GPUs.
            assert tp_size % self.total_num_kv_heads == 0
        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
        self.head_dim = hidden_size // self.total_num_heads
        self.q_size = self.num_heads * self.head_dim
        self.kv_size = self.num_kv_heads * self.head_dim
        self.scaling = self.head_dim**-0.5
        self.rope_theta = rope_theta
        self.max_position_embeddings = max_position_embeddings

        self.qkv_proj = QKVParallelLinear(
            hidden_size,
            self.head_dim,
            self.total_num_heads,
            self.total_num_kv_heads,
            bias=False,
            linear_method=linear_method,
        )
        self.o_proj = RowParallelLinear(
            self.total_num_heads * self.head_dim,
            hidden_size,
            bias=False,
            linear_method=linear_method,
        )

        self.rotary_emb = get_rope(
            self.head_dim,
            rotary_dim=self.head_dim,
            max_position=max_position_embeddings,
            base=rope_theta,
            rope_scaling=rope_scaling,
        )
        self.attn = PagedAttention(
            self.num_heads, self.head_dim, self.scaling, num_kv_heads=self.num_kv_heads
        )

    def forward(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        kv_cache: KVCache,
        input_metadata: InputMetadata,
    ) -> torch.Tensor:
        qkv, _ = self.qkv_proj(hidden_states)
        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
        q, k = self.rotary_emb(positions, q, k)
        k_cache, v_cache = kv_cache
        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)
        output, _ = self.o_proj(attn_output)
        return output


class LlamaDecoderLayer(nn.Module):

    def __init__(
        self,
        config: LlamaConfig,
        linear_method: Optional[LinearMethodBase] = None,
    ) -> None:
        super().__init__()
        self.hidden_size = config.hidden_size
        rope_theta = getattr(config, "rope_theta", 10000)
        rope_scaling = getattr(config, "rope_scaling", None)
        max_position_embeddings = getattr(config, "max_position_embeddings", 8192)
        self.self_attn = LlamaAttention(
            hidden_size=self.hidden_size,
            num_heads=config.num_attention_heads,
            num_kv_heads=config.num_key_value_heads,
            rope_theta=rope_theta,
            rope_scaling=rope_scaling,
            max_position_embeddings=max_position_embeddings,
            linear_method=linear_method,
        )
        self.mlp = LlamaMLP(
            hidden_size=self.hidden_size,
            intermediate_size=config.intermediate_size,
            hidden_act=config.hidden_act,
            linear_method=linear_method,
        )
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps
        )

    def forward(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        kv_cache: KVCache,
        input_metadata: InputMetadata,
        residual: Optional[torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        # Self Attention
        if residual is None:
            residual = hidden_states
            hidden_states = self.input_layernorm(hidden_states)
        else:
            hidden_states, residual = self.input_layernorm(hidden_states, residual)
        hidden_states = self.self_attn(
            positions=positions,
            hidden_states=hidden_states,
            kv_cache=kv_cache,
            input_metadata=input_metadata,
        )

        # Fully Connected
        hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)
        hidden_states = self.mlp(hidden_states)
        return hidden_states, residual


class LlamaModel(nn.Module):

    def __init__(
        self,
        config: LlamaConfig,
        linear_method: Optional[LinearMethodBase] = None,
    ) -> None:
        super().__init__()
        self.config = config
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size
        self.embed_tokens = VocabParallelEmbedding(
            config.vocab_size,
            config.hidden_size,
        )
        self.layers = nn.ModuleList(
            [
                LlamaDecoderLayer(config, linear_method)
                for _ in range(config.num_hidden_layers)
            ]
        )
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        input_emb: torch.Tensor,
        positions: torch.Tensor,
        kv_caches: List[KVCache],
        input_metadata: InputMetadata,
    ) -> torch.Tensor:
        hidden_states = input_emb
        residual = None
        for i in range(len(self.layers)):
            layer = self.layers[i]
            hidden_states, residual = layer(
                positions,
                hidden_states,
                kv_caches[i],
                input_metadata,
                residual,
            )
        hidden_states, _ = self.norm(hidden_states, residual)
        return hidden_states

    def load_weights(
        self,
        model_name_or_path: str,
        cache_dir: Optional[str] = None,
        load_format: str = "auto",
        revision: Optional[str] = None,
    ):
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            ("qkv_proj", "q_proj", "q"),
            ("qkv_proj", "k_proj", "k"),
            ("qkv_proj", "v_proj", "v"),
            ("gate_up_proj", "gate_proj", 0),
            ("gate_up_proj", "up_proj", 1),
        ]
        params_dict = dict(self.named_parameters())
        for name, loaded_weight in hf_model_weights_iterator(
            model_name_or_path, cache_dir, load_format, revision
        ):
            if "rotary_emb.inv_freq" in name:
                continue
            if "rotary_emb.cos_cached" in name or "rotary_emb.sin_cached" in name:
                # Models trained using ColossalAI may include these tensors in
                # the checkpoint. Skip them.
                continue
            for param_name, weight_name, shard_id in stacked_params_mapping:
                if weight_name not in name:
                    continue
                name = name.replace(weight_name, param_name)
                # Skip loading extra bias for GPTQ models.
                if name.endswith(".bias") and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                # Skip loading extra bias for GPTQ models.
                if name.endswith(".bias") and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = getattr(param, "weight_loader", default_weight_loader)
                weight_loader(param, loaded_weight)


class LlamaForCausalLM(nn.Module):

    def __init__(
        self,
        config: LlamaConfig,
        linear_method: Optional[LinearMethodBase] = None,
    ) -> None:
        super().__init__()
        self.config = config
        self.linear_method = linear_method
        self.model = LlamaModel(config, linear_method)
        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)
        self.sampler = Sampler(config.vocab_size)

    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        kv_caches: List[KVCache],
        input_metadata: InputMetadata,
    ) -> torch.Tensor:
        hidden_states = self.model(input_ids, positions, kv_caches, input_metadata)
        return hidden_states

    def sample(
        self,
        hidden_states: torch.Tensor,
        sampling_metadata: SamplingMetadata,
    ) -> Optional[SamplerOutput]:
        next_tokens = self.sampler(
            self.lm_head.weight, hidden_states, sampling_metadata
        )
        return next_tokens

    def load_weights(
        self,
        model_name_or_path: str,
        cache_dir: Optional[str] = None,
        load_format: str = "auto",
        revision: Optional[str] = None,
    ):
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            ("qkv_proj", "q_proj", "q"),
            ("qkv_proj", "k_proj", "k"),
            ("qkv_proj", "v_proj", "v"),
            ("gate_up_proj", "gate_proj", 0),
            ("gate_up_proj", "up_proj", 1),
        ]
        params_dict = dict(self.named_parameters())
        for name, loaded_weight in hf_model_weights_iterator(
            model_name_or_path, cache_dir, load_format, revision
        ):
            if "rotary_emb.inv_freq" in name:
                continue
            if "rotary_emb.cos_cached" in name or "rotary_emb.sin_cached" in name:
                # Models trained using ColossalAI may include these tensors in
                # the checkpoint. Skip them.
                continue
            for param_name, weight_name, shard_id in stacked_params_mapping:
                if weight_name not in name:
                    continue
                name = name.replace(weight_name, param_name)
                # Skip loading extra bias for GPTQ models.
                if name.endswith(".bias") and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                # Skip loading extra bias for GPTQ models.
                if name.endswith(".bias") and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = getattr(param, "weight_loader", default_weight_loader)
                weight_loader(param, loaded_weight)



================================================
FILE: ChatTTS/model/velocity/llm.py
================================================
from typing import List, Optional, Union

from tqdm import tqdm
from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast
from vllm.utils import Counter

from .configs import EngineArgs
from .llm_engine import LLMEngine
from .output import RequestOutput
from .sampling_params import SamplingParams


class LLM:
    """An LLM for generating texts from given prompts and sampling parameters.

    This class includes a tokenizer, a language model (possibly distributed
    across multiple GPUs), and GPU memory space allocated for intermediate
    states (aka KV cache). Given a batch of prompts and sampling parameters,
    this class generates texts from the model, using an intelligent batching
    mechanism and efficient memory management.

    NOTE: This class is intended to be used for offline inference. For online
    serving, use the `AsyncLLMEngine` class instead.
    NOTE: For the comprehensive list of arguments, see `EngineArgs`.

    Args:
        model: The name or path of a HuggingFace Transformers model.
        tokenizer: The name or path of a HuggingFace Transformers tokenizer.
        tokenizer_mode: The tokenizer mode. "auto" will use the fast tokenizer
            if available, and "slow" will always use the slow tokenizer.
        trust_remote_code: Trust remote code (e.g., from HuggingFace) when
            downloading the model and tokenizer.
        tensor_parallel_size: The number of GPUs to use for distributed
            execution with tensor parallelism.
        dtype: The data type for the model weights and activations. Currently,
            we support `float32`, `float16`, and `bfloat16`. If `auto`, we use
            the `torch_dtype` attribute specified in the model config file.
            However, if the `torch_dtype` in the config is `float32`, we will
            use `float16` instead.
        quantization: The method used to quantize the model weights. Currently,
            we support "awq", "gptq" and "squeezellm". If None, we first check
            the `quantization_config` attribute in the model config file. If
            that is None, we assume the model weights are not quantized and use
            `dtype` to determine the data type of the weights.
        revision: The specific model version to use. It can be a branch name,
            a tag name, or a commit id.
        tokenizer_revision: The specific tokenizer version to use. It can be a
            branch name, a tag name, or a commit id.
        seed: The seed to initialize the random number generator for sampling.
        gpu_memory_utilization: The ratio (between 0 and 1) of GPU memory to
            reserve for the model weights, activations, and KV cache. Higher
            values will increase the KV cache size and thus improve the model's
            throughput. However, if the value is too high, it may cause out-of-
            memory (OOM) errors.
        swap_space: The size (GiB) of CPU memory per GPU to use as swap space.
            This can be used for temporarily storing the states of the requests
            when their `best_of` sampling parameters are larger than 1. If all
            requests will have `best_of=1`, you can safely set this to 0.
            Otherwise, too small values may cause out-of-memory (OOM) errors.
        enforce_eager: Whether to enforce eager execution. If True, we will
            disable CUDA graph and always execute the model in eager mode.
            If False, we will use CUDA graph and eager execution in hybrid.
        max_context_len_to_capture: Maximum context len covered by CUDA graphs.
            When a sequence has context length larger than this, we fall back
            to eager mode.
    """

    def __init__(
        self,
        model: str,
        tokenizer: Optional[str] = None,
        tokenizer_mode: str = "auto",
        trust_remote_code: bool = False,
        tensor_parallel_size: int = 1,
        dtype: str = "auto",
        quantization: Optional[str] = None,
        revision: Optional[str] = None,
        tokenizer_revision: Optional[str] = None,
        seed: int = 0,
        gpu_memory_utilization: float = 0.9,
        swap_space: int = 4,
        enforce_eager: bool = False,
        max_context_len_to_capture: int = 8192,
        post_model_path: str = None,
        num_audio_tokens: int = 0,
        num_text_tokens: int = 0,
        **kwargs,
    ) -> None:
        if "disable_log_stats" not in kwargs:
            kwargs["disable_log_stats"] = True
        engine_args = EngineArgs(
            model=model,
            tokenizer=tokenizer,
            tokenizer_mode=tokenizer_mode,
            trust_remote_code=trust_remote_code,
            tensor_parallel_size=tensor_parallel_size,
            dtype=dtype,
            quantization=quantization,
            revision=revision,
            tokenizer_revision=tokenizer_revision,
            seed=seed,
            gpu_memory_utilization=gpu_memory_utilization,
            swap_space=swap_space,
            enforce_eager=enforce_eager,
            max_context_len_to_capture=max_context_len_to_capture,
            num_audio_tokens=num_audio_tokens,
            num_text_tokens=num_text_tokens,
            **kwargs,
        )
        self.llm_engine = LLMEngine.from_engine_args(engine_args, post_model_path)
        self.request_counter = Counter()

    def get_tokenizer(self) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]:
        return self.llm_engine.tokenizer

    def set_tokenizer(
        self,
        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
    ) -> None:
        self.llm_engine.tokenizer = tokenizer

    def generate(
        self,
        prompts: Optional[Union[str, List[str]]] = None,
        sampling_params: Optional[SamplingParams] = None,
        prompt_token_ids: Optional[List[List[int]]] = None,
        use_tqdm: bool = True,
    ) -> List[RequestOutput]:
        """Generates the completions for the input prompts.

        NOTE: This class automatically batches the given prompts, considering
        the memory constraint. For the best performance, put all of your prompts
        into a single list and pass it to this method.

        Args:
            prompts: A list of prompts to generate completions for.
            sampling_params: The sampling parameters for text generation. If
                None, we use the default sampling parameters.
            prompt_token_ids: A list of token IDs for the prompts. If None, we
                use the tokenizer to convert the prompts to token IDs.
            use_tqdm: Whether to use tqdm to display the progress bar.

        Returns:
            A list of `RequestOutput` objects containing the generated
            completions in the same order as the input prompts.
        """
        if prompts is None and prompt_token_ids is None:
            raise ValueError("Either prompts or prompt_token_ids must be " "provided.")
        if isinstance(prompts, str):
            # Convert a single prompt to a list.
            prompts = [prompts]
        if (
            prompts is not None
            and prompt_token_ids is not None
            and len(prompts) != len(prompt_token_ids)
        ):
            raise ValueError(
                "The lengths of prompts and prompt_token_ids " "must be the same."
            )
        if sampling_params is None:
            # Use default sampling params.
            sampling_params = SamplingParams()

        # Add requests to the engine.
        num_requests = len(prompts) if prompts is not None else len(prompt_token_ids)
        for i in range(num_requests):
            prompt = prompts[i] if prompts is not None else None
            token_ids = None if prompt_token_ids is None else prompt_token_ids[i]
            self._add_request(prompt, sampling_params, token_ids)

        rtns = self._run_engine(use_tqdm)
        for i, rtn in enumerate(rtns):
            token_ids = rtn.outputs[0].token_ids
            for j, token_id in enumerate(token_ids):
                if len(token_id) == 1:
                    token_ids[j] = token_id[0]
                else:
                    token_ids[j] = list(token_id)

        return rtns

    def _add_request(
        self,
        prompt: Optional[str],
        sampling_params: SamplingParams,
        prompt_token_ids: Optional[List[int]],
    ) -> None:
        request_id = str(next(self.request_counter))
        self.llm_engine.add_request(
            request_id, prompt, sampling_params, prompt_token_ids
        )

    def _run_engine(self, use_tqdm: bool) -> List[RequestOutput]:
        # Initialize tqdm.
        if use_tqdm:
            num_requests = self.llm_engine.get_num_unfinished_requests()
            pbar = tqdm(total=num_requests, desc="Processed prompts")
        # Run the engine.
        outputs: List[RequestOutput] = []
        while self.llm_engine.has_unfinished_requests():
            step_outputs = self.llm_engine.step()
            for output in step_outputs:
                if output.finished:
                    outputs.append(output)
                    if use_tqdm:
                        pbar.update(1)
        if use_tqdm:
            pbar.close()
        # Sort the outputs by request ID.
        # This is necessary because some requests may be finished earlier than
        # its previous requests.
        outputs = sorted(outputs, key=lambda x: int(x.request_id))
        return outputs



================================================
FILE: ChatTTS/model/velocity/llm_engine.py
================================================
import copy
from collections import defaultdict
import os
import time
from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Tuple, Union

from vllm.config import CacheConfig, ModelConfig, ParallelConfig, SchedulerConfig
from .scheduler import Scheduler, SchedulerOutputs
from .configs import EngineArgs
from vllm.engine.metrics import record_metrics
from vllm.engine.ray_utils import RayWorkerVllm, initialize_cluster, ray
from vllm.logger import init_logger
from .output import RequestOutput
from .sampling_params import SamplingParams
from .sequence import (
    SamplerOutput,
    Sequence,
    SequenceGroup,
    SequenceGroupOutput,
    SequenceOutput,
    SequenceStatus,
)
from vllm.transformers_utils.tokenizer import detokenize_incrementally, get_tokenizer
from vllm.utils import Counter, set_cuda_visible_devices, get_ip, get_open_port
import numpy as np

if ray:
    from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy

if TYPE_CHECKING:
    from ray.util.placement_group import PlacementGroup

logger = init_logger(__name__)

_LOGGING_INTERVAL_SEC = 5


class LLMEngine:
    """An LLM engine that receives requests and generates texts.

    This is the main class for the vLLM engine. It receives requests
    from clients and generates texts from the LLM. It includes a tokenizer, a
    language model (possibly distributed across multiple GPUs), and GPU memory
    space allocated for intermediate states (aka KV cache). This class utilizes
    iteration-level scheduling and efficient memory management to maximize the
    serving throughput.

    The `LLM` class wraps this class for offline batched inference and the
    `AsyncLLMEngine` class wraps this class for online serving.

    NOTE: The config arguments are derived from the `EngineArgs` class. For the
    comprehensive list of arguments, see `EngineArgs`.

    Args:
        model_config: The configuration related to the LLM model.
        cache_config: The configuration related to the KV cache memory
            management.
        parallel_config: The configuration related to distributed execution.
        scheduler_config: The configuration related to the request scheduler.
        placement_group: Ray placement group for distributed execution.
            Required for distributed execution.
        log_stats: Whether to log statistics.
    """

    def __init__(
        self,
        model_config: ModelConfig,
        cache_config: CacheConfig,
        parallel_config: ParallelConfig,
        scheduler_config: SchedulerConfig,
        placement_group: Optional["PlacementGroup"],
        post_model_path: str,
        log_stats: bool,
    ) -> None:
        logger.info(
            "Initializing an LLM engine with config: "
            f"model={model_config.model!r}, "
            f"tokenizer={model_config.tokenizer!r}, "
            f"tokenizer_mode={model_config.tokenizer_mode}, "
            f"revision={model_config.revision}, "
            f"tokenizer_revision={model_config.tokenizer_revision}, "
            f"trust_remote_code={model_config.trust_remote_code}, "
            f"dtype={model_config.dtype}, "
            f"max_seq_len={model_config.max_model_len}, "
            f"download_dir={model_config.download_dir!r}, "
            f"load_format={model_config.load_format}, "
            f"tensor_parallel_size={parallel_config.tensor_parallel_size}, "
            f"quantization={model_config.quantization}, "
            f"enforce_eager={model_config.enforce_eager}, "
            f"seed={model_config.seed}), "
            f"post_model_path={post_model_path!r}"
        )
        # TODO(woosuk): Print more configs in debug mode.

        self.model_config = model_config
        self.cache_config = cache_config
        self.parallel_config = parallel_config
        self.scheduler_config = scheduler_config
        self.log_stats = log_stats
        self._verify_args()
        self.post_model_path = post_model_path
        self.seq_counter = Counter()

        # Create the parallel GPU workers.
        if self.parallel_config.worker_use_ray:
            # Disable Ray usage stats collection.
            ray_usage = os.environ.get("RAY_USAGE_STATS_ENABLED", "0")
            if ray_usage != "1":
                os.environ["RAY_USAGE_STATS_ENABLED"] = "0"
            self._init_workers_ray(placement_group)
        else:
            self._init_workers()

        # Profile the memory usage and initialize the cache.
        self._init_cache()

        # Create the scheduler.
        self.scheduler = Scheduler(scheduler_config, cache_config)

        # Logging.
        self.last_logging_time = 0.0
        # List of (timestamp, num_tokens)
        self.num_prompt_tokens: List[Tuple[float, int]] = []
        # List of (timestamp, num_tokens)
        self.num_generation_tokens: List[Tuple[float, int]] = []

    def _init_workers(self):
        # Lazy import the Worker to avoid importing torch.cuda/xformers
        # before CUDA_VISIBLE_DEVICES is set in the Worker
        from .worker import Worker

        assert (
            self.parallel_config.world_size == 1
        ), "Ray is required if parallel_config.world_size > 1."

        self.workers: List[Worker] = []
        distributed_init_method = f"tcp://{get_ip()}:{get_open_port()}"
        self.driver_worker = Worker(
            self.model_config,
            self.parallel_config,
            self.scheduler_config,
            local_rank=0,
            rank=0,
            distributed_init_method=distributed_init_method,
            is_driver_worker=True,
            post_model_path=self.post_model_path,
        )
        self._run_workers("init_model")
        self._run_workers("load_model")

    def _init_workers_ray(self, placement_group: "PlacementGroup", **ray_remote_kwargs):
        if self.parallel_config.tensor_parallel_size == 1:
            num_gpus = self.cache_config.gpu_memory_utilization
        else:
            num_gpus = 1

        self.driver_dummy_worker: RayWorkerVllm = None
        self.workers: List[RayWorkerVllm] = []

        driver_ip = get_ip()
        for bundle_id, bundle in enumerate(placement_group.bundle_specs):
            if not bundle.get("GPU", 0):
                continue
            scheduling_strategy = PlacementGroupSchedulingStrategy(
                placement_group=placement_group,
                placement_group_capture_child_tasks=True,
                placement_group_bundle_index=bundle_id,
            )
            worker = ray.remote(
                num_cpus=0,
                num_gpus=num_gpus,
                scheduling_strategy=scheduling_strategy,
                **ray_remote_kwargs,
            )(RayWorkerVllm).remote(self.model_config.trust_remote_code)

            worker_ip = ray.get(worker.get_node_ip.remote())
            if worker_ip == driver_ip and self.driver_dummy_worker is None:
                # If the worker is on the same node as the driver, we use it
                # as the resource holder for the driver process.
                self.driver_dummy_worker = worker
            else:
                self.workers.append(worker)

        if self.driver_dummy_worker is None:
            raise ValueError(
                "Ray does not allocate any GPUs on the driver node. Consider "
                "adjusting the Ray placement group or running the driver on a "
                "GPU node."
            )

        driver_node_id, driver_gpu_ids = ray.get(
            self.driver_dummy_worker.get_node_and_gpu_ids.remote()
        )
        worker_node_and_gpu_ids = ray.get(
            [worker.get_node_and_gpu_ids.remote() for worker in self.workers]
        )

        node_workers = defaultdict(list)
        node_gpus = defaultdict(list)

        node_workers[driver_node_id].append(0)
        node_gpus[driver_node_id].extend(driver_gpu_ids)
        for i, (node_id, gpu_ids) in enumerate(worker_node_and_gpu_ids, start=1):
            node_workers[node_id].append(i)
            node_gpus[node_id].extend(gpu_ids)
        for node_id, gpu_ids in node_gpus.items():
            node_gpus[node_id] = sorted(gpu_ids)

        # Set CUDA_VISIBLE_DEVICES for the driver.
        set_cuda_visible_devices(node_gpus[driver_node_id])
        for worker, (node_id, _) in zip(self.workers, worker_node_and_gpu_ids):
            worker.set_cuda_visible_devices.remote(node_gpus[node_id])

        distributed_init_method = f"tcp://{driver_ip}:{get_open_port()}"

        # Lazy import the Worker to avoid importing torch.cuda/xformers
        # before CUDA_VISIBLE_DEVICES is set in the Worker
        from vllm.worker.worker import Worker

        # Initialize torch distributed process group for the workers.
        model_config = copy.deepcopy(self.model_config)
        parallel_config = copy.deepcopy(self.parallel_config)
        scheduler_config = copy.deepcopy(self.scheduler_config)

        for rank, (worker, (node_id, _)) in enumerate(
            zip(self.workers, worker_node_and_gpu_ids), start=1
        ):
            local_rank = node_workers[node_id].index(rank)
            worker.init_worker.remote(
                lambda rank=rank, local_rank=local_rank: Worker(
                    model_config,
                    parallel_config,
                    scheduler_config,
                    local_rank,
                    rank,
                    distributed_init_method,
                )
            )

        driver_rank = 0
        driver_local_rank = node_workers[driver_node_id].index(driver_rank)
        self.driver_worker = Worker(
            model_config,
            parallel_config,
            scheduler_config,
            driver_local_rank,
            driver_rank,
            distributed_init_method,
            is_driver_worker=True,
        )

        self._run_workers("init_model")
        self._run_workers(
            "load_model",
            max_concurrent_workers=self.parallel_config.max_parallel_loading_workers,
        )

    def _verify_args(self) -> None:
        self.model_config.verify_with_parallel_config(self.parallel_config)
        self.cache_config.verify_with_parallel_config(self.parallel_config)

    def _init_cache(self) -> None:
        """Profiles the memory usage and initializes the KV cache."""
        # Get the maximum number of blocks that can be allocated on GPU and CPU.
        num_blocks = self._run_workers(
            "profile_num_available_blocks",
            block_size=self.cache_config.block_size,
            gpu_memory_utilization=self.cache_config.gpu_memory_utilization,
            cpu_swap_space=self.cache_config.swap_space_bytes,
        )

        # Since we use a shared centralized controller, we take the minimum
        # number of blocks across all workers to make sure all the memory
        # operators can be applied to all workers.
        num_gpu_blocks = min(b[0] for b in num_blocks)
        num_cpu_blocks = min(b[1] for b in num_blocks)
        # FIXME(woosuk): Change to debug log.
        logger.info(
            f"# GPU blocks: {num_gpu_blocks}, " f"# CPU blocks: {num_cpu_blocks}"
        )

        if num_gpu_blocks <= 0:
            raise ValueError(
                "No available memory for the cache blocks. "
                "Try increasing `gpu_memory_utilization` when "
                "initializing the engine."
            )
        max_seq_len = self.cache_config.block_size * num_gpu_blocks
        if self.model_config.max_model_len > max_seq_len:
            raise ValueError(
                f"The model's max seq len ({self.model_config.max_model_len}) "
                "is larger than the maximum number of tokens that can be "
                f"stored in KV cache ({max_seq_len}). Try increasing "
                "`gpu_memory_utilization` or decreasing `max_model_len` when "
                "initializing the engine."
            )

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        # Initialize the cache.
        self._run_workers("init_cache_engine", cache_config=self.cache_config)
        # Warm up the model. This includes capturing the model into CUDA graph
        # if enforce_eager is False.
        self._run_workers("warm_up_model")

    @classmethod
    def from_engine_args(
        cls, engine_args: EngineArgs, post_model_path=None
    ) -> "LLMEngine":
        """Creates an LLM engine from the engine arguments."""
        # Create the engine configs.
        engine_configs = engine_args.create_engine_configs()
        parallel_config = engine_configs[2]
        # Initialize the cluster.
        placement_group = initialize_cluster(parallel_config)
        # Create the LLM engine.
        engine = cls(
            *engine_configs,
            placement_group,
            log_stats=not engine_args.disable_log_stats,
            post_model_path=post_model_path,
        )
        return engine

    def add_request(
        self,
        request_id: str,
        prompt: Optional[str],
        sampling_params: SamplingParams,
        prompt_token_ids: Optional[List[int]] = None,
        arrival_time: Optional[float] = None,
    ) -> None:
        """Add a request to the engine's request pool.

        The request is added to the request pool and will be processed by the
        scheduler as `engine.step()` is called. The exact scheduling policy is
        determined by the scheduler.

        Args:
            request_id: The unique ID of the request.
            prompt: The prompt string. Can be None if prompt_token_ids is
                provided.
            sampling_params: The sampling parameters for text generation.
            prompt_token_ids: The token IDs of the prompt. If None, we
                use the tokenizer to convert the prompts to token IDs.
            arrival_time: The arrival time of the request. If None, we use
                the current monotonic time.
        """
        if arrival_time is None:
            arrival_time = time.monotonic()

        assert prompt_token_ids is not None, "prompt_token_ids must be provided"
        # Create the sequences.
        block_size = self.cache_config.block_size
        seq_id = next(self.seq_counter)
        seq = Sequence(seq_id, prompt, prompt_token_ids, block_size)

        # Create the sequence group.
        seq_group = SequenceGroup(request_id, [seq], sampling_params, arrival_time)

        # Add the sequence group to the scheduler.
        self.scheduler.add_seq_group(seq_group)

    def abort_request(self, request_id: Union[str, Iterable[str]]) -> None:
        """Aborts a request(s) with the given ID.

        Args:
            request_id: The ID(s) of the request to abort.
        """
        self.scheduler.abort_seq_group(request_id)

    def get_model_config(self) -> ModelConfig:
        """Gets the model configuration."""
        return self.model_config

    def get_num_unfinished_requests(self) -> int:
        """Gets the number of unfinished requests."""
        return self.scheduler.get_num_unfinished_seq_groups()

    def has_unfinished_requests(self) -> bool:
        """Returns True if there are unfinished requests."""
        return self.scheduler.has_unfinished_seqs()

    def _check_beam_search_early_stopping(
        self,
        early_stopping: Union[bool, str],
        sampling_params: SamplingParams,
        best_running_seq: Sequence,
        current_worst_seq: Sequence,
    ) -> bool:
        assert sampling_params.use_beam_search
        length_penalty = sampling_params.length_penalty
        if early_stopping is True:
            return True

        current_worst_score = current_worst_seq.get_beam_search_score(
            length_penalty=length_penalty, eos_token_id=self.tokenizer.eos_token_id
        )
        if early_stopping is False:
            highest_attainable_score = best_running_seq.get_beam_search_score(
                length_penalty=length_penalty, eos_token_id=self.tokenizer.eos_token_id
            )
        else:
            assert early_stopping == "never"
            if length_penalty > 0.0:
                # If length_penalty > 0.0, beam search will prefer longer
                # sequences. The highest attainable score calculation is
                # based on the longest possible sequence length in this case.
                max_possible_length = max(
                    best_running_seq.get_prompt_len() + sampling_params.max_tokens,
                    self.scheduler_config.max_model_len,
                )
                highest_attainable_score = best_running_seq.get_beam_search_score(
                    length_penalty=length_penalty,
                    eos_token_id=self.tokenizer.eos_token_id,
                    seq_len=max_possible_length,
                )
            else:
                # Otherwise, beam search will prefer shorter sequences. The
                # highest attainable score calculation is based on the current
                # sequence length.
                highest_attainable_score = best_running_seq.get_beam_search_score(
                    length_penalty=length_penalty,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
        return current_worst_score >= highest_attainable_score

    def _process_sequence_group_outputs(
        self, seq_group: SequenceGroup, outputs: SequenceGroupOutput
    ) -> None:
        # Process prompt logprobs
        prompt_logprobs = outputs.prompt_logprobs
        if prompt_logprobs is not None:
            seq_group.prompt_logprobs = prompt_logprobs

        # Process samples
        samples = outputs.samples
        parent_seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)
        existing_finished_seqs = seq_group.get_finished_seqs()
        parent_child_dict = {parent_seq.seq_id: [] for parent_seq in parent_seqs}
        for sample in samples:
            parent_child_dict[sample.parent_seq_id].append(sample)
        # List of (child, parent)
        child_seqs: List[Tuple[Sequence, Sequence]] = []

        # Process the child samples for each parent sequence
        for parent in parent_seqs:
            child_samples: List[SequenceOutput] = parent_child_dict[parent.seq_id]
            if len(child_samples) == 0:
                # This parent sequence has no children samples. Remove
                # the parent sequence from the sequence group since it will
                # not be used in the future iterations.
                parent.status = SequenceStatus.FINISHED_ABORTED
                seq_group.remove(parent.seq_id)
                self.scheduler.free_seq(parent)
                continue
            # Fork the parent sequence if there are multiple child samples.
            for child_sample in child_samples[:-1]:
                new_child_seq_id = next(self.seq_counter)
                child = parent.fork(new_child_seq_id)
                child.append_token_id(
                    child_sample.output_token,
                    child_sample.logprobs,
                    child_sample.hidden_states,
                    child_sample.finished,
                )
                child_seqs.append((child, parent))
            # Continue the parent sequence for the last child sample.
            # We reuse the parent sequence here to reduce redundant memory
            # copies, especially when using non-beam search sampling methods.
            last_child_sample = child_samples[-1]
            parent.append_token_id(
                last_child_sample.output_token,
                last_child_sample.logprobs,
                last_child_sample.hidden_states,
                last_child_sample.finished,
            )
            child_seqs.append((parent, parent))

        for seq, _ in child_seqs:
            # self._decode_sequence(seq, seq_group.sampling_params)
            self._check_stop(seq, seq_group.sampling_params)

        # Non-beam search case
        if not seq_group.sampling_params.use_beam_search:
            # For newly created child sequences, add them to the sequence group
            # and fork them in block manager if they are not finished.
            for seq, parent in child_seqs:
                if seq is not parent:
                    seq_group.add(seq)
                    if not seq.is_finished():
                        self.scheduler.fork_seq(parent, seq)

            # Free the finished and selected parent sequences' memory in block
            # manager. Keep them in the sequence group as candidate output.
            # NOTE: we need to fork the new sequences before freeing the
            # old sequences.
            for seq, parent in child_seqs:
                if seq is parent and seq.is_finished():
                    self.scheduler.free_seq(seq)
            return

        # Beam search case
        # Select the child sequences to keep in the sequence group.
        selected_child_seqs = []
        unselected_child_seqs = []
        beam_width = seq_group.sampling_params.best_of
        length_penalty = seq_group.sampling_params.length_penalty

        # Select the newly finished sequences with the highest scores
        # to replace existing finished sequences.
        # Tuple of (seq, parent, is_new)
        existing_finished_seqs = [(seq, None, False) for seq in existing_finished_seqs]
        new_finished_seqs = [
            (seq, parent, True) for seq, parent in child_seqs if seq.is_finished()
        ]
        all_finished_seqs = existing_finished_seqs + new_finished_seqs
        # Sort the finished sequences by their scores.
        all_finished_seqs.sort(
            key=lambda x: x[0].get_beam_search_score(
                length_penalty=length_penalty, eos_token_id=self.tokenizer.eos_token_id
            ),
            reverse=True,
        )
        for seq, parent, is_new in all_finished_seqs[:beam_width]:
            if is_new:
                # A newly generated child sequence finishes and has a high
                # score, so we will add it into the sequence group.
                selected_child_seqs.append((seq, parent))
        for seq, parent, is_new in all_finished_seqs[beam_width:]:
            if is_new:
                # A newly generated child sequence finishes but has a low
                # score, so we will not add it into the sequence group.
                # Additionally, if this sequence is a continuation of a
                # parent sequence, we will need remove the parent sequence
                # from the sequence group.
                unselected_child_seqs.append((seq, parent))
            else:
                # An existing finished sequence has a low score, so we will
                # remove it from the sequence group.
                seq_group.remove(seq.seq_id)

        # select the top beam_width sequences from the running
        # sequences for the next iteration to continue the beam
        # search.
        running_child_seqs = [
            (seq, parent) for seq, parent in child_seqs if not seq.is_finished()
        ]
        # Sort the running sequences by their scores.
        running_child_seqs.sort(
            key=lambda x: x[0].get_beam_search_score(
                length_penalty=length_penalty, eos_token_id=self.tokenizer.eos_token_id
            ),
            reverse=True,
        )

        # Check if we can stop the beam search.
        if len(running_child_seqs) == 0:
            # No running sequences, stop the beam search.
            stop_beam_search = True
        elif len(all_finished_seqs) < beam_width:
            # Not enough finished sequences, continue the beam search.
            stop_beam_search = False
        else:
            # Check the early stopping criteria
            best_running_seq = running_child_seqs[0][0]
            current_worst_seq = all_finished_seqs[beam_width - 1][0]
            stop_beam_search = self._check_beam_search_early_stopping(
                seq_group.sampling_params.early_stopping,
                seq_group.sampling_params,
                best_running_seq,
                current_worst_seq,
            )

        if stop_beam_search:
            # Stop the beam search and remove all the running sequences from
            # the sequence group.
            unselected_child_seqs.extend(running_child_seqs)
        else:
            # Continue the beam search and select the top beam_width sequences
            # to continue the beam search.
            selected_child_seqs.extend(running_child_seqs[:beam_width])
            # The remaining running sequences will not be used in the next
            # iteration. Again, if these sequences are continuations of
            # parent sequences, we will need to remove the parent sequences
            # from the sequence group.
            unselected_child_seqs.extend(running_child_seqs[beam_width:])

        # For newly created child sequences, add them to the sequence group
        # and fork them in block manager if they are not finished.
        for seq, parent in selected_child_seqs:
            if seq is not parent:
                seq_group.add(seq)
                if not seq.is_finished():
                    self.scheduler.fork_seq(parent, seq)

        # Free the finished and selected parent sequences' memory in block
        # manager. Keep them in the sequence group as candidate output.
        for seq, parent in selected_child_seqs:
            if seq is parent and seq.is_finished():
                self.scheduler.free_seq(seq)

        # Remove the unselected parent sequences from the sequence group and
        # free their memory in block manager.
        for seq, parent in unselected_child_seqs:
            if seq is parent:
                # Remove the parent sequence if it is not selected for next
                # iteration
                seq_group.remove(seq.seq_id)
                self.scheduler.free_seq(seq)

    def _process_model_outputs(
        self, output: SamplerOutput, scheduler_outputs: SchedulerOutputs
    ) -> List[RequestOutput]:
        # Update the scheduled sequence groups with the model outputs.
        scheduled_seq_groups = scheduler_outputs.scheduled_seq_groups
        for seq_group, outputs in zip(scheduled_seq_groups, output):
            self._process_sequence_group_outputs(seq_group, outputs)

        # Free the finished sequence groups.
        self.scheduler.free_finished_seq_groups()

        # Create the outputs.
        request_outputs: List[RequestOutput] = []
        for seq_group in scheduled_seq_groups + scheduler_outputs.ignored_seq_groups:
            request_output = RequestOutput.from_seq_group(seq_group)
            request_outputs.append(request_output)

        if self.log_stats:
            # Log the system stats.
            self._log_system_stats(
                scheduler_outputs.prompt_run, scheduler_outputs.num_batched_tokens
            )
        return request_outputs

    def step(self) -> List[RequestOutput]:
        """Performs one decoding iteration and returns newly generated results.

        This function performs one decoding iteration of the engine. It first
        schedules the sequences to be executed in the next iteration and the
        token blocks to be swapped in/out/copy. Then, it executes the model
        and updates the scheduler with the model outputs. Finally, it decodes
        the sequences and returns the newly generated results.
        """
        seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()

        if not scheduler_outputs.is_empty():
            # Execute the model.
            all_outputs = self._run_workers(
                "execute_model",
                driver_kwargs={
                    "seq_group_metadata_list": seq_group_metadata_list,
                    "blocks_to_swap_in": scheduler_outputs.blocks_to_swap_in,
                    "blocks_to_swap_out": scheduler_outputs.blocks_to_swap_out,
                    "blocks_to_copy": scheduler_outputs.blocks_to_copy,
                },
            )

            # Only the driver worker returns the sampling results.
            output = all_outputs[0]
        else:
            output = []

        return self._process_model_outputs(output, scheduler_outputs)

    def _log_system_stats(
        self,
        prompt_run: bool,
        num_batched_tokens: int,
    ) -> None:
        now = time.monotonic()
        # Log the number of batched input tokens.
        if prompt_run:
            self.num_prompt_tokens.append((now, num_batched_tokens))
        else:
            self.num_generation_tokens.append((now, num_batched_tokens))

        should_log = now - self.last_logging_time >= _LOGGING_INTERVAL_SEC
        if not should_log:
            return

        # Discard the old stats.
        self.num_prompt_tokens = [
            (t, n) for t, n in self.num_prompt_tokens if now - t < _LOGGING_INTERVAL_SEC
        ]
        self.num_generation_tokens = [
            (t, n)
            for t, n in self.num_generation_tokens
            if now - t < _LOGGING_INTERVAL_SEC
        ]

        if len(self.num_prompt_tokens) > 1:
            total_num_tokens = sum(n for _, n in self.num_prompt_tokens[:-1])
            window = now - self.num_prompt_tokens[0][0]
            avg_prompt_throughput = total_num_tokens / window
        else:
            avg_prompt_throughput = 0.0
        if len(self.num_generation_tokens) > 1:
            total_num_tokens = sum(n for _, n in self.num_generation_tokens[:-1])
            window = now - self.num_generation_tokens[0][0]
            avg_generation_throughput = total_num_tokens / window
        else:
            avg_generation_throughput = 0.0

        total_num_gpu_blocks = self.cache_config.num_gpu_blocks
        num_free_gpu_blocks = self.scheduler.block_manager.get_num_free_gpu_blocks()
        num_used_gpu_blocks = total_num_gpu_blocks - num_free_gpu_blocks
        gpu_cache_usage = num_used_gpu_blocks / total_num_gpu_blocks

        total_num_cpu_blocks = self.cache_config.num_cpu_blocks
        if total_num_cpu_blocks > 0:
            num_free_cpu_blocks = self.scheduler.block_manager.get_num_free_cpu_blocks()
            num_used_cpu_blocks = total_num_cpu_blocks - num_free_cpu_blocks
            cpu_cache_usage = num_used_cpu_blocks / total_num_cpu_blocks
        else:
            cpu_cache_usage = 0.0

        record_metrics(
            avg_prompt_throughput=avg_prompt_throughput,
            avg_generation_throughput=avg_generation_throughput,
            scheduler_running=len(self.scheduler.running),
            scheduler_swapped=len(self.scheduler.swapped),
            scheduler_waiting=len(self.scheduler.waiting),
            gpu_cache_usage=gpu_cache_usage,
            cpu_cache_usage=cpu_cache_usage,
        )

        logger.info(
            "Avg prompt throughput: "
            f"{avg_prompt_throughput:.1f} tokens/s, "
            "Avg generation throughput: "
            f"{avg_generation_throughput:.1f} tokens/s, "
            f"Running: {len(self.scheduler.running)} reqs, "
            f"Swapped: {len(self.scheduler.swapped)} reqs, "
            f"Pending: {len(self.scheduler.waiting)} reqs, "
            f"GPU KV cache usage: {gpu_cache_usage * 100:.1f}%, "
            f"CPU KV cache usage: {cpu_cache_usage * 100:.1f}%"
        )
        self.last_logging_time = now

    def _decode_sequence(self, seq: Sequence, prms: SamplingParams) -> None:
        """Decodes the new token for a sequence."""
        (new_tokens, new_output_text, prefix_offset, read_offset) = (
            detokenize_incrementally(
                self.tokenizer,
                all_input_ids=seq.get_token_ids(),
                prev_tokens=seq.tokens,
                prefix_offset=seq.prefix_offset,
                read_offset=seq.read_offset,
                skip_special_tokens=prms.skip_special_tokens,
                spaces_between_special_tokens=prms.spaces_between_special_tokens,
            )
        )
        if seq.tokens is None:
            seq.tokens = new_tokens
        else:
            seq.tokens.extend(new_tokens)
        seq.prefix_offset = prefix_offset
        seq.read_offset = read_offset
        seq.output_text += new_output_text

    def _check_stop(self, seq: Sequence, sampling_params: SamplingParams) -> None:
        """Stop the finished sequences."""
        for stop_str in sampling_params.stop:
            if seq.output_text.endswith(stop_str):
                if not sampling_params.include_stop_str_in_output:
                    # Truncate the output text so that the stop string is
                    # not included in the output.
                    seq.output_text = seq.output_text[: -len(stop_str)]
                seq.status = SequenceStatus.FINISHED_STOPPED
                return
        if seq.data.finished:
            seq.status = SequenceStatus.FINISHED_STOPPED
            return

        for token_id in seq.get_last_token_id():
            if token_id == sampling_params.eos_token:
                seq.status = SequenceStatus.FINISHED_STOPPED
                return

        # Check if the sequence has reached max_model_len.
        if seq.get_len() > self.scheduler_config.max_model_len:
            seq.status = SequenceStatus.FINISHED_LENGTH_CAPPED
            return

        # Check if the sequence has reached max_tokens.
        if seq.get_output_len() == sampling_params.max_tokens:
            seq.status = SequenceStatus.FINISHED_LENGTH_CAPPED
            return

        # Check if the sequence has generated the EOS token.
        if (not sampling_params.ignore_eos) and seq.get_last_token_id()[
            0
        ] == sampling_params.eos_token:
            seq.status = SequenceStatus.FINISHED_STOPPED
            return

    def _run_workers(
        self,
        method: str,
        *args,
        driver_args: Optional[List[Any]] = None,
        driver_kwargs: Optional[Dict[str, Any]] = None,
        max_concurrent_workers: Optional[int] = None,
        **kwargs,
    ) -> Any:
        """Runs the given method on all workers."""

        if max_concurrent_workers:
            raise NotImplementedError("max_concurrent_workers is not supported yet.")

        # Start the ray workers first.
        ray_worker_outputs = [
            worker.execute_method.remote(method, *args, **kwargs)
            for worker in self.workers
        ]

        if driver_args is None:
            driver_args = args
        if driver_kwargs is None:
            driver_kwargs = kwargs

        # Start the driver worker after all the ray workers.
        driver_worker_output = getattr(self.driver_worker, method)(
            *driver_args, **driver_kwargs
        )

        # Get the results of the ray workers.
        if self.workers:
            ray_worker_outputs = ray.get(ray_worker_outputs)

        return [driver_worker_output] + ray_worker_outputs



================================================
FILE: ChatTTS/model/velocity/model_loader.py
================================================
"""Utilities for selecting and loading models."""

import contextlib

import torch
import torch.nn as nn

from vllm.config import ModelConfig
from vllm.model_executor.models import ModelRegistry
from vllm.model_executor.weight_utils import get_quant_config, initialize_dummy_weights

from .llama import LlamaModel


@contextlib.contextmanager
def _set_default_torch_dtype(dtype: torch.dtype):
    """Sets the default torch dtype to the given dtype."""
    old_dtype = torch.get_default_dtype()
    torch.set_default_dtype(dtype)
    yield
    torch.set_default_dtype(old_dtype)


def get_model(model_config: ModelConfig) -> nn.Module:
    # Get the (maybe quantized) linear method.
    linear_method = None
    if model_config.quantization is not None:
        quant_config = get_quant_config(
            model_config.quantization,
            model_config.model,
            model_config.hf_config,
            model_config.download_dir,
        )
        capability = torch.cuda.get_device_capability()
        capability = capability[0] * 10 + capability[1]
        if capability < quant_config.get_min_capability():
            raise ValueError(
                f"The quantization method {model_config.quantization} is not "
                "supported for the current GPU. "
                f"Minimum capability: {quant_config.get_min_capability()}. "
                f"Current capability: {capability}."
            )
        supported_dtypes = quant_config.get_supported_act_dtypes()
        if model_config.dtype not in supported_dtypes:
            raise ValueError(
                f"{model_config.dtype} is not supported for quantization "
                f"method {model_config.quantization}. Supported dtypes: "
                f"{supported_dtypes}"
            )
        linear_method = quant_config.get_linear_method()

    with _set_default_torch_dtype(model_config.dtype):
        # Create a model instance.
        # The weights will be initialized as empty tensors.
        with torch.device("cuda"):
            model = LlamaModel(model_config.hf_config, linear_method)
        if model_config.load_format == "dummy":
            # NOTE(woosuk): For accurate performance evaluation, we assign
            # random values to the weights.
            initialize_dummy_weights(model)
        else:
            # Load the weights from the cached or downloaded files.
            model.load_weights(
                model_config.model,
                model_config.download_dir,
                model_config.load_format,
                model_config.revision,
            )
    return model.eval()



================================================
FILE: ChatTTS/model/velocity/model_runner.py
================================================
import time
from typing import Dict, List, Optional, Tuple, Union

import numpy as np
import torch
import torch.nn as nn

from .configs import ModelConfig, ParallelConfig, SchedulerConfig
from vllm.logger import init_logger
from .model_loader import get_model
from vllm.model_executor import InputMetadata, SamplingMetadata
from vllm.model_executor.parallel_utils.communication_op import (
    broadcast,
    broadcast_object_list,
)
from .sampling_params import SamplingParams, SamplingType
from .sequence import (
    SamplerOutput,
    SequenceData,
    SequenceGroupMetadata,
    SequenceGroupOutput,
    SequenceOutput,
)
from vllm.utils import in_wsl
from ..embed import Embed
from .sampler import Sampler
from safetensors.torch import safe_open

logger = init_logger(__name__)

KVCache = Tuple[torch.Tensor, torch.Tensor]
_PAD_SLOT_ID = -1
# Capture graphs for batch size 1, 2, 4, 8, 16, 24, 32, 40, ..., 256.
# NOTE: _get_graph_batch_size needs to be updated if this list is changed.
_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [8 * i for i in range(1, 33)]


class ModelRunner:

    def __init__(
        self,
        model_config: ModelConfig,
        parallel_config: ParallelConfig,
        scheduler_config: SchedulerConfig,
        is_driver_worker: bool = False,
        post_model_path: str = None,
    ):
        self.model_config = model_config
        self.parallel_config = parallel_config
        self.scheduler_config = scheduler_config
        self.is_driver_worker = is_driver_worker
        self.post_model_path = post_model_path

        # model_config can be None in tests/samplers/test_sampler.py.
        # FIXME(woosuk): This is a hack to make the tests work. Refactor this.
        self.sliding_window = (
            model_config.get_sliding_window() if model_config is not None else None
        )
        self.model = None
        self.block_size = None  # Set after initial profiling.

        self.graph_runners: Dict[int, CUDAGraphRunner] = {}
        self.graph_memory_pool = None  # Set during graph capture.

        self.max_context_len_to_capture = (
            self.model_config.max_context_len_to_capture
            if self.model_config is not None
            else 0
        )
        # When using CUDA graph, the input block tables must be padded to
        # max_context_len_to_capture. However, creating the block table in
        # Python can be expensive. To optimize this, we cache the block table
        # in numpy and only copy the actual input content at every iteration.
        # The shape of the cached block table will be
        # (max batch size to capture, max context len to capture / block size).
        self.graph_block_tables = None  # Set after initial profiling.
        # cache in_wsl result
        self.in_wsl = in_wsl()

    def load_model(self) -> None:
        self.model = get_model(self.model_config)
        self.post_model = Embed(
            self.model_config.get_hidden_size(),
            self.model_config.num_audio_tokens,
            self.model_config.num_text_tokens,
        )
        state_dict_tensors = {}
        with safe_open(self.post_model_path, framework="pt", device=0) as f:
            for k in f.keys():
                state_dict_tensors[k] = f.get_tensor(k)
        self.post_model.load_state_dict(state_dict_tensors)
        self.post_model.to(next(self.model.parameters())).eval()
        self.sampler = Sampler(self.post_model, self.model_config.num_audio_tokens, 4)

    def set_block_size(self, block_size: int) -> None:
        self.block_size = block_size

        max_num_blocks = (
            self.max_context_len_to_capture + block_size - 1
        ) // block_size
        self.graph_block_tables = np.zeros(
            (max(_BATCH_SIZES_TO_CAPTURE), max_num_blocks), dtype=np.int32
        )

    def _prepare_prompt(
        self,
        seq_group_metadata_list: List[SequenceGroupMetadata],
    ) -> Tuple[torch.Tensor, torch.Tensor, InputMetadata, List[int]]:
        assert len(seq_group_metadata_list) > 0
        input_tokens: List[List[int]] = []
        input_positions: List[List[int]] = []
        slot_mapping: List[List[int]] = []

        prompt_lens: List[int] = []
        for seq_group_metadata in seq_group_metadata_list:
            assert seq_group_metadata.is_prompt
            seq_ids = list(seq_group_metadata.seq_data.keys())
            assert len(seq_ids) == 1
            seq_id = seq_ids[0]

            seq_data = seq_group_metadata.seq_data[seq_id]
            prompt_tokens = seq_data.get_token_ids()
            prompt_len = len(prompt_tokens)
            prompt_lens.append(prompt_len)

            input_tokens.append(prompt_tokens)
            # NOTE(woosuk): Here we assume that the first token in the prompt
            # is always the first token in the sequence.
            input_positions.append(list(range(prompt_len)))

            if seq_group_metadata.block_tables is None:
                # During memory profiling, the block tables are not initialized
                # yet. In this case, we just use a dummy slot mapping.
                slot_mapping.append([_PAD_SLOT_ID] * prompt_len)
                continue

            # Compute the slot mapping.
            slot_mapping.append([])
            block_table = seq_group_metadata.block_tables[seq_id]
            # Mask the [0, start_idx) tokens of the prompt with _PAD_SLOT_ID,
            # where start_idx is max(0, prompt_len - sliding_window).
            # For example, if the prompt len is 10, sliding window is 8, and
            # block size is 4, the first two tokens are masked and the slot
            # mapping will be [-1, -1, 2, 3, 4, 5, 6, 7, 0, 1].
            start_idx = 0
            if self.sliding_window is not None:
                start_idx = max(0, prompt_len - self.sliding_window)
            for i in range(prompt_len):
                if i < start_idx:
                    slot_mapping[-1].append(_PAD_SLOT_ID)
                    continue

                block_number = block_table[i // self.block_size]
                block_offset = i % self.block_size
                slot = block_number * self.block_size + block_offset
                slot_mapping[-1].append(slot)

        max_prompt_len = max(prompt_lens)
        input_tokens = _make_tensor_with_pad(
            input_tokens, max_prompt_len, pad=0, dtype=torch.long
        )
        input_positions = _make_tensor_with_pad(
            input_positions, max_prompt_len, pad=0, dtype=torch.long
        )
        slot_mapping = _make_tensor_with_pad(
            slot_mapping, max_prompt_len, pad=_PAD_SLOT_ID, dtype=torch.long
        )

        input_metadata = InputMetadata(
            is_prompt=True,
            slot_mapping=slot_mapping,
            max_context_len=None,
            context_lens=None,
            block_tables=None,
            use_cuda_graph=False,
        )
        return input_tokens, input_positions, input_metadata, prompt_lens

    def _prepare_decode(
        self,
        seq_group_metadata_list: List[SequenceGroupMetadata],
    ) -> Tuple[torch.Tensor, torch.Tensor, InputMetadata]:
        assert len(seq_group_metadata_list) > 0
        input_tokens: List[List[int]] = []
        input_positions: List[List[int]] = []
        slot_mapping: List[List[int]] = []
        context_lens: List[int] = []
        block_tables: List[List[int]] = []

        for seq_group_metadata in seq_group_metadata_list:
            assert not seq_group_metadata.is_prompt

            seq_ids = list(seq_group_metadata.seq_data.keys())
            for seq_id in seq_ids:
                seq_data = seq_group_metadata.seq_data[seq_id]
                generation_token = seq_data.get_last_token_id()
                input_tokens.append([generation_token])

                seq_len = seq_data.get_len()
                position = seq_len - 1
                input_positions.append([position])

                context_len = (
                    seq_len
                    if self.sliding_window is None
                    else min(seq_len, self.sliding_window)
                )
                context_lens.append(context_len)

                block_table = seq_group_metadata.block_tables[seq_id]
                block_number = block_table[position // self.block_size]
                block_offset = position % self.block_size
                slot = block_number * self.block_size + block_offset
                slot_mapping.append([slot])

                if self.sliding_window is not None:
                    sliding_window_blocks = self.sliding_window // self.block_size
                    block_table = block_table[-sliding_window_blocks:]
                block_tables.append(block_table)

        batch_size = len(input_tokens)
        max_context_len = max(context_lens)
        use_captured_graph = (
            not self.model_config.enforce_eager
            and batch_size <= _BATCH_SIZES_TO_CAPTURE[-1]
            and max_context_len <= self.max_context_len_to_capture
        )
        if use_captured_graph:
            # Pad the input tokens, positions, and slot mapping to match the
            # batch size of the captured graph.
            graph_batch_size = _get_graph_batch_size(batch_size)
            assert graph_batch_size >= batch_size
            for _ in range(graph_batch_size - batch_size):
                input_tokens.append([])
                input_positions.append([])
                slot_mapping.append([])
                context_lens.append(1)
                block_tables.append([])
            batch_size = graph_batch_size

        input_tokens = _make_tensor_with_pad(
            input_tokens, max_len=1, pad=0, dtype=torch.long, device="cuda"
        )
        input_positions = _make_tensor_with_pad(
            input_positions, max_len=1, pad=0, dtype=torch.long, device="cuda"
        )
        slot_mapping = _make_tensor_with_pad(
            slot_mapping, max_len=1, pad=_PAD_SLOT_ID, dtype=torch.long, device="cuda"
        )
        context_lens = torch.tensor(context_lens, dtype=torch.int, device="cuda")

        if use_captured_graph:
            # The shape of graph_block_tables is
            # [max batch size, max context len // block size].
            input_block_tables = self.graph_block_tables[:batch_size]
            for i, block_table in enumerate(block_tables):
                if block_table:
                    input_block_tables[i, : len(block_table)] = block_table
            block_tables = torch.tensor(input_block_tables, device="cuda")
        else:
            block_tables = _make_tensor_with_pad(
                block_tables,
                max_len=max_context_len,
                pad=0,
                dtype=torch.int,
                device="cuda",
            )

        input_metadata = InputMetadata(
            is_prompt=False,
            slot_mapping=slot_mapping,
            max_context_len=max_context_len,
            context_lens=context_lens,
            block_tables=block_tables,
            use_cuda_graph=use_captured_graph,
        )
        return input_tokens, input_positions, input_metadata

    def _prepare_sample(
        self,
        seq_group_metadata_list: List[SequenceGroupMetadata],
        prompt_lens: List[int],
    ) -> SamplingMetadata:
        seq_groups: List[Tuple[List[int], SamplingParams]] = []
        selected_token_indices: List[int] = []
        selected_token_start_idx = 0
        categorized_sample_indices = {t: [] for t in SamplingType}
        categorized_sample_indices_start_idx = 0

        max_prompt_len = max(prompt_lens) if prompt_lens else 1
        for i, seq_group_metadata in enumerate(seq_group_metadata_list):
            seq_ids = list(seq_group_metadata.seq_data.keys())
            sampling_params = seq_group_metadata.sampling_params
            seq_groups.append((seq_ids, sampling_params))

            if seq_group_metadata.is_prompt:
                assert len(seq_ids) == 1
                prompt_len = prompt_lens[i]
                if sampling_params.prompt_logprobs is not None:
                    # NOTE: prompt token positions do not need sample, skip
                    categorized_sample_indices_start_idx += prompt_len - 1

                categorized_sample_indices[sampling_params.sampling_type].append(
                    categorized_sample_indices_start_idx
                )
                categorized_sample_indices_start_idx += 1

                if sampling_params.prompt_logprobs is not None:
                    selected_token_indices.extend(
                        range(
                            selected_token_start_idx,
                            selected_token_start_idx + prompt_len - 1,
                        )
                    )
                selected_token_indices.append(selected_token_start_idx + prompt_len - 1)
                selected_token_start_idx += max_prompt_len
            else:
                num_seqs = len(seq_ids)
                selected_token_indices.extend(
                    range(selected_token_start_idx, selected_token_start_idx + num_seqs)
                )
                selected_token_start_idx += num_seqs

                categorized_sample_indices[sampling_params.sampling_type].extend(
                    range(
                        categorized_sample_indices_start_idx,
                        categorized_sample_indices_start_idx + num_seqs,
                    )
                )
                categorized_sample_indices_start_idx += num_seqs

        selected_token_indices = _async_h2d(
            selected_token_indices, dtype=torch.long, pin_memory=not self.in_wsl
        )
        categorized_sample_indices = {
            t: _async_h2d(seq_ids, dtype=torch.int, pin_memory=not self.in_wsl)
            for t, seq_ids in categorized_sample_indices.items()
        }

        seq_data: Dict[int, SequenceData] = {}
        for seq_group_metadata in seq_group_metadata_list:
            seq_data.update(seq_group_metadata.seq_data)

        sampling_metadata = SamplingMetadata(
            seq_groups=seq_groups,
            seq_data=seq_data,
            prompt_lens=prompt_lens,
            selected_token_indices=selected_token_indices,
            categorized_sample_indices=categorized_sample_indices,
        )
        return sampling_metadata

    def prepare_input_tensors(
        self,
        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],
    ) -> Tuple[torch.Tensor, torch.Tensor, InputMetadata, SamplingMetadata]:
        if self.is_driver_worker:
            # NOTE: We assume that all sequences in the group are all prompts or
            # all decodes.
            is_prompt = seq_group_metadata_list[0].is_prompt
            # Prepare input tensors.
            if is_prompt:
                (input_tokens, input_positions, input_metadata, prompt_lens) = (
                    self._prepare_prompt(seq_group_metadata_list)
                )
            else:
                (input_tokens, input_positions, input_metadata) = self._prepare_decode(
                    seq_group_metadata_list
                )
                prompt_lens = []
            sampling_metadata = self._prepare_sample(
                seq_group_metadata_list, prompt_lens
            )

            def get_size_or_none(x: Optional[torch.Tensor]):
                return x.size() if x is not None else None

            # Broadcast the input data. For input tensors, we first broadcast
            # its shape and then broadcast the tensor to avoid high
            # serialization cost.
            py_data = {
                "input_tokens_size": input_tokens.size(),
                "input_positions_size": input_positions.size(),
                "is_prompt": input_metadata.is_prompt,
                "slot_mapping_size": get_size_or_none(input_metadata.slot_mapping),
                "max_context_len": input_metadata.max_context_len,
                "context_lens_size": get_size_or_none(input_metadata.context_lens),
                "block_tables_size": get_size_or_none(input_metadata.block_tables),
                "use_cuda_graph": input_metadata.use_cuda_graph,
                "selected_token_indices_size": sampling_metadata.selected_token_indices.size(),
            }
            broadcast_object_list([py_data], src=0)
            # TODO(zhuohan): Combine the broadcasts or set async_op=True.
            broadcast(input_tokens, src=0)
            broadcast(input_positions, src=0)
            if input_metadata.slot_mapping is not None:
                broadcast(input_metadata.slot_mapping, src=0)
            if input_metadata.context_lens is not None:
                broadcast(input_metadata.context_lens, src=0)
            if input_metadata.block_tables is not None:
                broadcast(input_metadata.block_tables, src=0)
            broadcast(sampling_metadata.selected_token_indices, src=0)
        else:
            receving_list = [None]
            broadcast_object_list(receving_list, src=0)
            py_data = receving_list[0]
            input_tokens = torch.empty(
                *py_data["input_tokens_size"], dtype=torch.long, device="cuda"
            )
            broadcast(input_tokens, src=0)
            input_positions = torch.empty(
                *py_data["input_positions_size"], dtype=torch.long, device="cuda"
            )
            broadcast(input_positions, src=0)
            if py_data["slot_mapping_size"] is not None:
                slot_mapping = torch.empty(
                    *py_data["slot_mapping_size"], dtype=torch.long, device="cuda"
                )
                broadcast(slot_mapping, src=0)
            else:
                slot_mapping = None
            if py_data["context_lens_size"] is not None:
                context_lens = torch.empty(
                    *py_data["context_lens_size"], dtype=torch.int, device="cuda"
                )
                broadcast(context_lens, src=0)
            else:
                context_lens = None
            if py_data["block_tables_size"] is not None:
                block_tables = torch.empty(
                    *py_data["block_tables_size"], dtype=torch.int, device="cuda"
                )
                broadcast(block_tables, src=0)
            else:
                block_tables = None
            selected_token_indices = torch.empty(
                *py_data["selected_token_indices_size"], dtype=torch.long, device="cuda"
            )
            broadcast(selected_token_indices, src=0)
            input_metadata = InputMetadata(
                is_prompt=py_data["is_prompt"],
                slot_mapping=slot_mapping,
                max_context_len=py_data["max_context_len"],
                context_lens=context_lens,
                block_tables=block_tables,
                use_cuda_graph=py_data["use_cuda_graph"],
            )
            sampling_metadata = SamplingMetadata(
                seq_groups=None,
                seq_data=None,
                prompt_lens=None,
                selected_token_indices=selected_token_indices,
                categorized_sample_indices=None,
                perform_sampling=False,
            )

        return input_tokens, input_positions, input_metadata, sampling_metadata

    @torch.inference_mode()
    def execute_model(
        self,
        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],
        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],
    ) -> Optional[SamplerOutput]:
        input_tokens, input_positions, input_metadata, sampling_metadata = (
            self.prepare_input_tensors(seq_group_metadata_list)
        )
        # print(sampling_metadata.seq_data)
        seq_groups = []
        input_tokens_history = []
        for i, rtn in enumerate(sampling_metadata.seq_groups):
            seq_groups.append(rtn[0][0])
            tokens_history = sampling_metadata.seq_data[rtn[0][0]].output_token_ids
            if len(tokens_history) >= 1:
                if len(tokens_history[0]) == 1:
                    tokens_history = [token[0] for token in tokens_history]
                else:
                    tokens_history = [list(token) for token in tokens_history]
            input_tokens_history.append(tokens_history)
        input_tokens_history = torch.tensor(input_tokens_history).to(
            input_tokens.device
        )
        # token_ids = rtn.outputs[0].token_ids
        # for j, token_id in enumerate(token_ids):
        #     if len(token_id) == 1:
        #         token_ids[j] = token_id[0]
        #     else:
        #         token_ids[j] = list(token_id)

        # Execute the model.
        # print("it1",input_tokens)
        if len(input_tokens.shape) == 2:
            input_tokens = input_tokens.unsqueeze(2).repeat(1, 1, 4)
        if len(input_tokens_history.shape) == 2:
            input_tokens_history = input_tokens_history.unsqueeze(2).repeat(1, 1, 4)
        # print(input_tokens_history.shape)
        # print("it2",input_tokens.shape)
        text_mask = input_tokens != 0
        text_mask = text_mask[:, :, 0]

        if input_metadata.use_cuda_graph:
            graph_batch_size = input_tokens.shape[0]
            model_executable = self.graph_runners[graph_batch_size]
        else:
            model_executable = self.model

        infer_text = sampling_metadata.seq_groups[0][1].infer_text
        temperture = sampling_metadata.seq_groups[0][1].temperature
        if not infer_text:
            temperture = torch.tensor(temperture).to(input_tokens.device)
        logits_processors, logits_warpers = sampling_metadata.seq_groups[0][
            1
        ].logits_processors
        # print(logits_processors, logits_warpers)
        min_new_token = sampling_metadata.seq_groups[0][1].min_new_token
        eos_token = sampling_metadata.seq_groups[0][1].eos_token
        start_idx = sampling_metadata.seq_groups[0][1].start_idx
        if input_tokens.shape[-2] == 1:
            if infer_text:
                input_emb: torch.Tensor = self.post_model.emb_text(
                    input_tokens[:, :, 0]
                )
            else:
                code_emb = [
                    self.post_model.emb_code[i](input_tokens[:, :, i])
                    for i in range(self.post_model.num_vq)
                ]
                input_emb = torch.stack(code_emb, 3).sum(3)
                start_idx = (
                    input_tokens_history.shape[-2] - 1
                    if input_tokens_history.shape[-2] > 0
                    else 0
                )
        else:
            input_emb = self.post_model(input_tokens, text_mask)
        # print(input_emb.shape)
        hidden_states = model_executable(
            input_emb=input_emb,
            positions=input_positions,
            kv_caches=kv_caches,
            input_metadata=input_metadata,
        )
        # print(hidden_states.shape)
        # print(input_tokens)
        B_NO_PAD = input_tokens_history.shape[0]
        input_tokens = input_tokens[:B_NO_PAD, :, :]
        hidden_states = hidden_states[:B_NO_PAD, :, :]
        idx_next, logprob, finish = self.sampler.sample(
            inputs_ids=(
                input_tokens
                if input_tokens_history.shape[-2] == 0
                else input_tokens_history
            ),
            hidden_states=hidden_states,
            infer_text=infer_text,
            temperature=temperture,
            logits_processors=logits_processors,
            logits_warpers=logits_warpers,
            min_new_token=min_new_token,
            now_length=1,
            eos_token=eos_token,
            start_idx=start_idx,
        )
        # print(logprob.shape, idx_next.shape)
        if len(logprob.shape) == 2:
            logprob = logprob[:, None, :]
        logprob = torch.gather(logprob, -1, idx_next.transpose(-1, -2))[:, :, 0]
        # print("测试",idx_next.shape, logprob.shape)
        # Sample the next token.
        # output = self.model.sample(
        #     hidden_states=hidden_states,
        #     sampling_metadata=sampling_metadata,
        # )
        results = []
        for i in range(idx_next.shape[0]):
            idx_next_i = idx_next[i, 0, :].tolist()
            logprob_i = logprob[i].tolist()
            tmp_hidden_states = hidden_states[i]
            if input_tokens[i].shape[-2] != 1:
                tmp_hidden_states = tmp_hidden_states[-1:, :]
            result = SequenceGroupOutput(
                samples=[
                    SequenceOutput(
                        parent_seq_id=seq_groups[i],
                        logprobs={tuple(idx_next_i): logprob_i},
                        output_token=tuple(idx_next_i),
                        hidden_states=tmp_hidden_states,
                        finished=finish[i].item(),
                    ),
                ],
                prompt_logprobs=None,
            )
            results.append(result)
        # print(results)
        # print(idx_next, idx_next.shape, logprob.shape)
        return results

    @torch.inference_mode()
    def profile_run(self) -> None:
        # Enable top-k sampling to reflect the accurate memory usage.
        vocab_size = self.model_config.get_vocab_size()
        sampling_params = SamplingParams(
            top_p=0.99, top_k=vocab_size - 1, infer_text=True
        )
        max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens
        max_num_seqs = self.scheduler_config.max_num_seqs

        # Profile memory usage with max_num_sequences sequences and the total
        # number of tokens equal to max_num_batched_tokens.
        seqs: List[SequenceGroupMetadata] = []
        for group_id in range(max_num_seqs):
            seq_len = max_num_batched_tokens // max_num_seqs + (
                group_id < max_num_batched_tokens % max_num_seqs
            )
            seq_data = SequenceData([0] * seq_len)
            seq = SequenceGroupMetadata(
                request_id=str(group_id),
                is_prompt=True,
                seq_data={group_id: seq_data},
                sampling_params=sampling_params,
                block_tables=None,
            )
            seqs.append(seq)

        # Run the model with the dummy inputs.
        num_layers = self.model_config.get_num_layers(self.parallel_config)
        kv_caches = [(None, None)] * num_layers
        self.execute_model(seqs, kv_caches)
        torch.cuda.synchronize()
        return

    @torch.inference_mode()
    def capture_model(self, kv_caches: List[KVCache]) -> None:
        assert not self.model_config.enforce_eager
        logger.info(
            "Capturing the model for CUDA graphs. This may lead to "
            "unexpected consequences if the model is not static. To "
            "run the model in eager mode, set 'enforce_eager=True' or "
            "use '--enforce-eager' in the CLI."
        )
        logger.info(
            "CUDA graphs can take additional 1~3 GiB memory per GPU. "
            "If you are running out of memory, consider decreasing "
            "`gpu_memory_utilization` or enforcing eager mode."
        )
        start_time = time.perf_counter()

        # Prepare dummy inputs. These will be reused for all batch sizes.
        max_batch_size = max(_BATCH_SIZES_TO_CAPTURE)
        input_emb = torch.zeros(
            max_batch_size,
            1,
            self.model_config.get_hidden_size(),
            dtype=next(self.model.parameters()).dtype,
        ).cuda()
        input_positions = torch.zeros(max_batch_size, 1, dtype=torch.long).cuda()
        slot_mapping = torch.empty(max_batch_size, 1, dtype=torch.long).cuda()
        slot_mapping.fill_(_PAD_SLOT_ID)
        context_lens = torch.ones(max_batch_size, dtype=torch.int32).cuda()
        block_tables = torch.from_numpy(self.graph_block_tables).cuda()

        # NOTE: Capturing the largest batch size first may help reduce the
        # memory usage of CUDA graph.
        for batch_size in reversed(_BATCH_SIZES_TO_CAPTURE):
            # Create dummy input_metadata.
            input_metadata = InputMetadata(
                is_prompt=False,
                slot_mapping=slot_mapping[:batch_size],
                max_context_len=self.max_context_len_to_capture,
                context_lens=context_lens[:batch_size],
                block_tables=block_tables[:batch_size],
                use_cuda_graph=True,
            )

            graph_runner = CUDAGraphRunner(self.model)
            graph_runner.capture(
                input_emb[:batch_size],
                input_positions[:batch_size],
                kv_caches,
                input_metadata,
                memory_pool=self.graph_memory_pool,
            )
            self.graph_memory_pool = graph_runner.graph.pool()
            self.graph_runners[batch_size] = graph_runner

        end_time = time.perf_counter()
        elapsed_time = end_time - start_time
        # This usually takes < 10 seconds.
        logger.info(f"Graph capturing finished in {elapsed_time:.0f} secs.")


class CUDAGraphRunner:

    def __init__(self, model: nn.Module):
        self.model = model
        self.graph = None
        self.input_buffers: Dict[str, torch.Tensor] = {}
        self.output_buffers: Dict[str, torch.Tensor] = {}

    def capture(
        self,
        input_emb: torch.Tensor,
        positions: torch.Tensor,
        kv_caches: List[KVCache],
        input_metadata: InputMetadata,
        memory_pool,
    ) -> None:
        assert self.graph is None
        # Run the model once without capturing the graph.
        # This is to make sure that the captured graph does not include the
        # kernel launches for initial benchmarking (e.g., Triton autotune).
        self.model(
            input_emb,
            positions,
            kv_caches,
            input_metadata,
        )
        torch.cuda.synchronize()

        # Capture the graph.
        self.graph = torch.cuda.CUDAGraph()
        with torch.cuda.graph(self.graph, pool=memory_pool):
            hidden_states = self.model(
                input_emb,
                positions,
                kv_caches,
                input_metadata,
            )
        torch.cuda.synchronize()

        # Save the input and output buffers.
        self.input_buffers = {
            "input_emb": input_emb,
            "positions": positions,
            "kv_caches": kv_caches,
            "slot_mapping": input_metadata.slot_mapping,
            "context_lens": input_metadata.context_lens,
            "block_tables": input_metadata.block_tables,
        }
        self.output_buffers = {"hidden_states": hidden_states}
        return

    def forward(
        self,
        input_emb: torch.Tensor,
        positions: torch.Tensor,
        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],
        input_metadata: InputMetadata,
    ) -> torch.Tensor:
        # KV caches are fixed tensors, so we don't need to copy them.
        del kv_caches

        # Copy the input tensors to the input buffers.
        self.input_buffers["input_emb"].copy_(input_emb, non_blocking=True)
        self.input_buffers["positions"].copy_(positions, non_blocking=True)
        self.input_buffers["slot_mapping"].copy_(
            input_metadata.slot_mapping, non_blocking=True
        )
        self.input_buffers["context_lens"].copy_(
            input_metadata.context_lens, non_blocking=True
        )
        self.input_buffers["block_tables"].copy_(
            input_metadata.block_tables, non_blocking=True
        )

        # Run the graph.
        self.graph.replay()

        # Return the output tensor.
        return self.output_buffers["hidden_states"]

    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)


def _pad_to_max(x: List[int], max_len: int, pad: int) -> List[int]:
    assert len(x) <= max_len
    if len(x) == max_len:
        return list(x)
    return list(x) + [pad] * (max_len - len(x))


def _make_tensor_with_pad(
    x: List[List[int]],
    max_len: int,
    pad: int,
    dtype: torch.dtype,
    device: Union[str, torch.device] = "cuda",
    pin_memory: bool = False,
) -> torch.Tensor:
    padded_x = []
    for x_i in x:
        pad_i = pad
        if isinstance(x[0][0], tuple):
            pad_i = (0,) * len(x[0][0])
        padded_x.append(_pad_to_max(x_i, max_len, pad_i))

    return torch.tensor(
        padded_x,
        dtype=dtype,
        device=device,
        pin_memory=pin_memory and str(device) == "cpu",
    )


def _get_graph_batch_size(batch_size: int) -> int:
    if batch_size <= 2:
        return batch_size
    elif batch_size <= 4:
        return 4
    else:
        return (batch_size + 7) // 8 * 8


def _async_h2d(data: list, dtype, pin_memory):
    t = torch.tensor(data, dtype=dtype, pin_memory=pin_memory)
    return t.to(device="cuda", non_blocking=True)



================================================
FILE: ChatTTS/model/velocity/output.py
================================================
from typing import List, Optional
import torch

from .sequence import (
    PromptLogprobs,
    SampleLogprobs,
    SequenceGroup,
    SequenceStatus,
)


class CompletionOutput:
    """The output data of one completion output of a request.

    Args:
        index: The index of the output in the request.
        text: The generated output text.
        token_ids: The token IDs of the generated output text.
        cumulative_logprob: The cumulative log probability of the generated
            output text.
        logprobs: The log probabilities of the top probability words at each
            position if the logprobs are requested.
        finish_reason: The reason why the sequence is finished.
    """

    def __init__(
        self,
        index: int,
        text: str,
        token_ids: List[int],
        cumulative_logprob: float,
        logprobs: Optional[SampleLogprobs],
        finish_reason: Optional[str] = None,
        hidden_states: Optional[torch.Tensor] = None,
    ) -> None:
        self.index = index
        self.text = text
        self.token_ids = token_ids
        self.cumulative_logprob = cumulative_logprob
        self.logprobs = logprobs
        self.finish_reason = finish_reason
        self.hidden_states = hidden_states

    def finished(self) -> bool:
        return self.finish_reason is not None

    def __repr__(self) -> str:
        return (
            f"CompletionOutput(index={self.index}, "
            f"text={self.text!r}, "
            f"token_ids={self.token_ids}, "
            f"cumulative_logprob={self.cumulative_logprob}, "
            f"logprobs={self.logprobs}, "
            f"finish_reason={self.finish_reason}, "
            f"hidden_states={self.hidden_states.shape if self.hidden_states is not None else None})"
        )


class RequestOutput:
    """The output data of a request to the LLM.

    Args:
        request_id: The unique ID of the request.
        prompt: The prompt string of the request.
        prompt_token_ids: The token IDs of the prompt.
        prompt_logprobs: The log probabilities to return per prompt token.
        outputs: The output sequences of the request.
        finished: Whether the whole request is finished.
    """

    def __init__(
        self,
        request_id: str,
        prompt: str,
        prompt_token_ids: List[int],
        prompt_logprobs: Optional[PromptLogprobs],
        outputs: List[CompletionOutput],
        finished: bool,
    ) -> None:
        self.request_id = request_id
        self.prompt = prompt
        self.prompt_token_ids = prompt_token_ids
        self.prompt_logprobs = prompt_logprobs
        self.outputs = outputs
        self.finished = finished

    @classmethod
    def from_seq_group(cls, seq_group: SequenceGroup) -> "RequestOutput":
        # Get the top-n sequences.
        n = seq_group.sampling_params.n
        seqs = seq_group.get_seqs()
        if seq_group.sampling_params.use_beam_search:
            sorting_key = lambda seq: seq.get_beam_search_score(
                seq_group.sampling_params.length_penalty
            )
        else:
            sorting_key = lambda seq: seq.get_cumulative_logprob()
        sorted_seqs = sorted(seqs, key=sorting_key, reverse=True)
        top_n_seqs = sorted_seqs[:n]

        # Create the outputs.
        outputs: List[CompletionOutput] = []
        for seq in top_n_seqs:
            logprobs = seq.output_logprobs
            if seq_group.sampling_params.logprobs is None:
                # NOTE: We need to take care of this case because the sequence
                # always has the logprobs of the sampled tokens even if the
                # logprobs are not requested.
                logprobs = None
            finshed_reason = SequenceStatus.get_finished_reason(seq.status)
            output = CompletionOutput(
                seqs.index(seq),
                seq.output_text,
                seq.get_output_token_ids(),
                seq.get_cumulative_logprob(),
                logprobs,
                finshed_reason,
                seq.data.hidden_states,
            )
            outputs.append(output)

        # Every sequence in the sequence group should have the same prompt.
        prompt = seq_group.prompt
        prompt_token_ids = seq_group.prompt_token_ids
        prompt_logprobs = seq_group.prompt_logprobs
        finished = seq_group.is_finished()
        return cls(
            seq_group.request_id,
            prompt,
            prompt_token_ids,
            prompt_logprobs,
            outputs,
            finished,
        )

    def __repr__(self) -> str:
        return (
            f"RequestOutput(request_id={self.request_id}, "
            f"prompt={self.prompt!r}, "
            f"prompt_token_ids={self.prompt_token_ids}, "
            f"prompt_logprobs={self.prompt_logprobs}, "
            f"outputs={self.outputs}, "
            f"finished={self.finished})"
        )



================================================
FILE: ChatTTS/model/velocity/sampler.py
================================================
import torch
from torch.functional import F
from typing import List, Callable

from ..embed import Embed


class Sampler:
    def __init__(self, post_model: Embed, num_audio_tokens: int, num_vq: int):
        self.post_model = post_model
        self.device = next(self.post_model.parameters()).device
        self.num_audio_tokens = num_audio_tokens
        self.num_vq = num_vq

    def sample(
        self,
        inputs_ids: torch.Tensor,
        hidden_states: torch.Tensor,
        infer_text: bool = False,
        temperature: torch.Tensor = 1.0,
        logits_processors: List[Callable] = [
            lambda logits_token, logits: logits,
        ],
        logits_warpers: List[Callable] = [
            lambda logits_token, logits: logits,
        ],
        min_new_token: int = 0,
        now_length: int = 0,
        eos_token: int = 0,
        start_idx: int = 0,
    ):
        # print(inputs_ids.shape)
        B = hidden_states.shape[0]

        end_idx = torch.zeros(
            inputs_ids.shape[0], device=inputs_ids.device, dtype=torch.long
        )
        finish = torch.zeros(inputs_ids.shape[0], device=inputs_ids.device).bool()
        if not infer_text:
            temperature = (
                temperature.unsqueeze(0)
                .expand(inputs_ids.shape[0], -1)
                .contiguous()
                .view(-1, 1)
            )

        if infer_text:
            logits: torch.Tensor = self.post_model.head_text(hidden_states)
        else:
            # logits = torch.stack([self.head_code[i](hidden_states) for i in range(self.num_vq)], 3)
            logits = torch.empty(
                hidden_states.size(0),
                hidden_states.size(1),
                self.num_audio_tokens,
                self.num_vq,
                dtype=torch.float,
                device=self.device,
            )
            for num_vq_iter in range(self.num_vq):
                x: torch.Tensor = self.post_model.head_code[num_vq_iter](hidden_states)
                logits[..., num_vq_iter] = x
                del x

        del hidden_states

        # logits = logits[:, -1].float()
        logits = logits.narrow(1, -1, 1).squeeze_(1).float()

        if not infer_text:
            # logits = rearrange(logits, "b c n -> (b n) c")
            logits = logits.permute(0, 2, 1)
            logits = logits.reshape(-1, logits.size(2))
            # logits_token = rearrange(inputs_ids[:, start_idx:], "b c n -> (b n) c")
            inputs_ids_sliced = inputs_ids[:, start_idx:].permute(0, 2, 1)
            logits_token = inputs_ids_sliced.reshape(
                inputs_ids_sliced.size(0) * inputs_ids_sliced.size(1),
                -1,
            ).to(self.device)
        else:
            logits_token = inputs_ids[:, start_idx:, 0].to(self.device)

        logits /= temperature

        for logitsProcessors in logits_processors:
            logits = logitsProcessors(logits_token, logits)

        for logitsWarpers in logits_warpers:
            logits = logitsWarpers(logits_token, logits)

        del logits_token

        if now_length < min_new_token:
            logits[:, eos_token] = -torch.inf

        scores = F.softmax(logits, dim=-1)
        idx_next = torch.multinomial(scores, num_samples=1).to(finish.device)
        if not infer_text:
            scores = scores.reshape(B, -1, scores.shape[-1])
        if not infer_text:
            # idx_next = rearrange(idx_next, "(b n) 1 -> b n", n=self.num_vq)
            idx_next = idx_next.view(-1, self.num_vq)
            finish_or = idx_next.eq(eos_token).any(1)
            finish.logical_or_(finish_or)
            del finish_or
        else:
            finish_or = idx_next.eq(eos_token).any(1)
            finish.logical_or_(finish_or)
            del finish_or

        del inputs_ids

        not_finished = finish.logical_not().to(end_idx.device)

        end_idx.add_(not_finished.int())
        idx_next = idx_next[:, None, :]
        return (
            idx_next,
            torch.log(scores),
            finish,
        )



================================================
FILE: ChatTTS/model/velocity/sampling_params.py
================================================
"""Sampling parameters for text generation."""

from enum import IntEnum
from functools import cached_property
from typing import Callable, List, Optional, Union

import torch

_SAMPLING_EPS = 1e-5


class SamplingType(IntEnum):
    GREEDY = 0
    RANDOM = 1
    BEAM = 2


LogitsProcessor = Callable[[List[int], torch.Tensor], torch.Tensor]
"""LogitsProcessor is a function that takes a list of previously generated
tokens and a tensor of the logits for the next token, and returns a modified
tensor of logits to sample from."""


class SamplingParams:
    """Sampling parameters for text generation.

    Overall, we follow the sampling parameters from the OpenAI text completion
    API (https://platform.openai.com/docs/api-reference/completions/create).
    In addition, we support beam search, which is not supported by OpenAI.

    Args:
        n: Number of output sequences to return for the given prompt.
        best_of: Number of output sequences that are generated from the prompt.
            From these `best_of` sequences, the top `n` sequences are returned.
            `best_of` must be greater than or equal to `n`. This is treated as
            the beam width when `use_beam_search` is True. By default, `best_of`
            is set to `n`.
        presence_penalty: Float that penalizes new tokens based on whether they
            appear in the generated text so far. Values > 0 encourage the model
            to use new tokens, while values < 0 encourage the model to repeat
            tokens.
        frequency_penalty: Float that penalizes new tokens based on their
            frequency in the generated text so far. Values > 0 encourage the
            model to use new tokens, while values < 0 encourage the model to
            repeat tokens.
        repetition_penalty: Float that penalizes new tokens based on whether
            they appear in the prompt and the generated text so far. Values > 1
            encourage the model to use new tokens, while values < 1 encourage
            the model to repeat tokens.
        temperature: Float that controls the randomness of the sampling. Lower
            values make the model more deterministic, while higher values make
            the model more random. Zero means greedy sampling.
        top_p: Float that controls the cumulative probability of the top tokens
            to consider. Must be in (0, 1]. Set to 1 to consider all tokens.
        top_k: Integer that controls the number of top tokens to consider. Set
            to -1 to consider all tokens.
        min_p: Float that represents the minimum probability for a token to be
            considered, relative to the probability of the most likely token.
            Must be in [0, 1]. Set to 0 to disable this.
        use_beam_search: Whether to use beam search instead of sampling.
        length_penalty: Float that penalizes sequences based on their length.
            Used in beam search.
        early_stopping: Controls the stopping condition for beam search. It
            accepts the following values: `True`, where the generation stops as
            soon as there are `best_of` complete candidates; `False`, where an
            heuristic is applied and the generation stops when is it very
            unlikely to find better candidates; `"never"`, where the beam search
            procedure only stops when there cannot be better candidates
            (canonical beam search algorithm).
        stop: List of strings that stop the generation when they are generated.
            The returned output will not contain the stop strings.
        stop_token_ids: List of tokens that stop the generation when they are
            generated. The returned output will contain the stop tokens unless
            the stop tokens are special tokens.
        include_stop_str_in_output: Whether to include the stop strings in output
            text. Defaults to False.
        ignore_eos: Whether to ignore the EOS token and continue generating
            tokens after the EOS token is generated.
        max_tokens: Maximum number of tokens to generate per output sequence.
        logprobs: Number of log probabilities to return per output token.
            Note that the implementation follows the OpenAI API: The return
            result includes the log probabilities on the `logprobs` most likely
            tokens, as well the chosen tokens. The API will always return the
            log probability of the sampled token, so there  may be up to
            `logprobs+1` elements in the response.
        prompt_logprobs: Number of log probabilities to return per prompt token.
        skip_special_tokens: Whether to skip special tokens in the output.
        spaces_between_special_tokens: Whether to add spaces between special
            tokens in the output.  Defaults to True.
        logits_processors: List of functions that modify logits based on
            previously generated tokens.
    """

    def __init__(
        self,
        n: int = 1,
        best_of: Optional[int] = None,
        presence_penalty: float = 0.0,
        frequency_penalty: float = 0.0,
        repetition_penalty: float = 1.0,
        temperature: float = 1.0,
        top_p: float = 1.0,
        top_k: int = -1,
        min_p: float = 0.0,
        use_beam_search: bool = False,
        length_penalty: float = 1.0,
        early_stopping: Union[bool, str] = False,
        stop: Optional[Union[str, List[str]]] = None,
        stop_token_ids: Optional[List[int]] = None,
        include_stop_str_in_output: bool = False,
        ignore_eos: bool = False,
        max_tokens: int = 16,
        logprobs: Optional[int] = None,
        prompt_logprobs: Optional[int] = None,
        skip_special_tokens: bool = True,
        spaces_between_special_tokens: bool = True,
        logits_processors: Optional[List[LogitsProcessor]] = (
            [
                lambda logits_token, logits: logits,
            ],
            [
                lambda logits_token, logits: logits,
            ],
        ),
        min_new_token: int = 0,
        max_new_token: int = 8192,
        infer_text: bool = False,
        eos_token: int = 0,
        spk_emb: str = None,
        start_idx: int = 0,
    ) -> None:
        self.n = n
        self.best_of = best_of if best_of is not None else n
        self.presence_penalty = presence_penalty
        self.frequency_penalty = frequency_penalty
        self.repetition_penalty = repetition_penalty
        self.temperature = temperature
        self.top_p = top_p
        self.top_k = top_k
        self.min_p = min_p
        self.use_beam_search = use_beam_search
        self.length_penalty = length_penalty
        self.early_stopping = early_stopping
        self.min_new_token = min_new_token
        self.max_new_token = max_new_token
        self.infer_text = infer_text
        self.eos_token = eos_token
        self.spk_emb = spk_emb
        self.start_idx = start_idx
        if stop is None:
            self.stop = []
        elif isinstance(stop, str):
            self.stop = [stop]
        else:
            self.stop = list(stop)
        if stop_token_ids is None:
            self.stop_token_ids = []
        else:
            self.stop_token_ids = list(stop_token_ids)
        self.ignore_eos = ignore_eos
        self.max_tokens = max_tokens
        self.logprobs = logprobs
        self.prompt_logprobs = prompt_logprobs
        self.skip_special_tokens = skip_special_tokens
        self.spaces_between_special_tokens = spaces_between_special_tokens
        self.logits_processors = logits_processors
        self.include_stop_str_in_output = include_stop_str_in_output
        self._verify_args()
        if self.use_beam_search:
            self._verify_beam_search()
        else:
            self._verify_non_beam_search()
            # if self.temperature < _SAMPLING_EPS:
            #     # Zero temperature means greedy sampling.
            #     self.top_p = 1.0
            #     self.top_k = -1
            #     self.min_p = 0.0
            #     self._verify_greedy_sampling()

    def _verify_args(self) -> None:
        if self.n < 1:
            raise ValueError(f"n must be at least 1, got {self.n}.")
        if self.best_of < self.n:
            raise ValueError(
                f"best_of must be greater than or equal to n, "
                f"got n={self.n} and best_of={self.best_of}."
            )
        if not -2.0 <= self.presence_penalty <= 2.0:
            raise ValueError(
                "presence_penalty must be in [-2, 2], got " f"{self.presence_penalty}."
            )
        if not -2.0 <= self.frequency_penalty <= 2.0:
            raise ValueError(
                "frequency_penalty must be in [-2, 2], got "
                f"{self.frequency_penalty}."
            )
        if not 0.0 < self.repetition_penalty <= 2.0:
            raise ValueError(
                "repetition_penalty must be in (0, 2], got "
                f"{self.repetition_penalty}."
            )
        # if self.temperature < 0.0:
        #     raise ValueError(
        #         f"temperature must be non-negative, got {self.temperature}.")
        if not 0.0 < self.top_p <= 1.0:
            raise ValueError(f"top_p must be in (0, 1], got {self.top_p}.")
        if self.top_k < -1 or self.top_k == 0:
            raise ValueError(
                f"top_k must be -1 (disable), or at least 1, " f"got {self.top_k}."
            )
        if not 0.0 <= self.min_p <= 1.0:
            raise ValueError("min_p must be in [0, 1], got " f"{self.min_p}.")
        if self.max_tokens < 1:
            raise ValueError(f"max_tokens must be at least 1, got {self.max_tokens}.")
        if self.logprobs is not None and self.logprobs < 0:
            raise ValueError(f"logprobs must be non-negative, got {self.logprobs}.")
        if self.prompt_logprobs is not None and self.prompt_logprobs < 0:
            raise ValueError(
                f"prompt_logprobs must be non-negative, got " f"{self.prompt_logprobs}."
            )

    def _verify_beam_search(self) -> None:
        if self.best_of == 1:
            raise ValueError(
                "best_of must be greater than 1 when using beam "
                f"search. Got {self.best_of}."
            )
        if self.temperature > _SAMPLING_EPS:
            raise ValueError("temperature must be 0 when using beam search.")
        if self.top_p < 1.0 - _SAMPLING_EPS:
            raise ValueError("top_p must be 1 when using beam search.")
        if self.top_k != -1:
            raise ValueError("top_k must be -1 when using beam search.")
        if self.early_stopping not in [True, False, "never"]:
            raise ValueError(
                f"early_stopping must be True, False, or 'never', "
                f"got {self.early_stopping}."
            )

    def _verify_non_beam_search(self) -> None:
        if self.early_stopping is not False:
            raise ValueError(
                "early_stopping is not effective and must be "
                "False when not using beam search."
            )
        if (
            self.length_penalty < 1.0 - _SAMPLING_EPS
            or self.length_penalty > 1.0 + _SAMPLING_EPS
        ):
            raise ValueError(
                "length_penalty is not effective and must be the "
                "default value of 1.0 when not using beam search."
            )

    def _verify_greedy_sampling(self) -> None:
        if self.best_of > 1:
            raise ValueError(
                "best_of must be 1 when using greedy sampling." f"Got {self.best_of}."
            )

    @cached_property
    def sampling_type(self) -> SamplingType:
        if self.use_beam_search:
            return SamplingType.BEAM
        # if self.temperature < _SAMPLING_EPS:
        #     return SamplingType.GREEDY
        return SamplingType.RANDOM

    def __repr__(self) -> str:
        return (
            f"SamplingParams(n={self.n}, "
            f"best_of={self.best_of}, "
            f"presence_penalty={self.presence_penalty}, "
            f"frequency_penalty={self.frequency_penalty}, "
            f"repetition_penalty={self.repetition_penalty}, "
            f"temperature={self.temperature}, "
            f"top_p={self.top_p}, "
            f"top_k={self.top_k}, "
            f"min_p={self.min_p}, "
            f"use_beam_search={self.use_beam_search}, "
            f"length_penalty={self.length_penalty}, "
            f"early_stopping={self.early_stopping}, "
            f"stop={self.stop}, "
            f"stop_token_ids={self.stop_token_ids}, "
            f"include_stop_str_in_output={self.include_stop_str_in_output}, "
            f"ignore_eos={self.ignore_eos}, "
            f"max_tokens={self.max_tokens}, "
            f"logprobs={self.logprobs}, "
            f"prompt_logprobs={self.prompt_logprobs}, "
            f"skip_special_tokens={self.skip_special_tokens}, "
            "spaces_between_special_tokens="
            f"{self.spaces_between_special_tokens}), "
            f"max_new_token={self.max_new_token}), "
            f"min_new_token={self.min_new_token}), "
            f"infer_text={self.infer_text})"
        )



================================================
FILE: ChatTTS/model/velocity/scheduler.py
================================================
import enum
import time
from typing import Dict, Iterable, List, Optional, Tuple, Union

from vllm.config import CacheConfig, SchedulerConfig
from .block_manager import AllocStatus, BlockSpaceManager
from vllm.core.policy import PolicyFactory
from vllm.logger import init_logger
from .sequence import (
    Sequence,
    SequenceData,
    SequenceGroup,
    SequenceGroupMetadata,
    SequenceStatus,
)

logger = init_logger(__name__)


class PreemptionMode(enum.Enum):
    """Preemption modes.

    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory
    and swap them back in when the sequences are resumed.
    2. Recomputation: Discard the blocks of the preempted sequences and
    recompute them when the sequences are resumed, treating the sequences as
    new prompts.
    """

    SWAP = enum.auto()
    RECOMPUTE = enum.auto()


class SchedulerOutputs:

    def __init__(
        self,
        scheduled_seq_groups: List[SequenceGroup],
        prompt_run: bool,
        num_batched_tokens: int,
        blocks_to_swap_in: Dict[int, int],
        blocks_to_swap_out: Dict[int, int],
        blocks_to_copy: Dict[int, List[int]],
        ignored_seq_groups: List[SequenceGroup],
    ) -> None:
        self.scheduled_seq_groups = scheduled_seq_groups
        self.prompt_run = prompt_run
        self.num_batched_tokens = num_batched_tokens
        self.blocks_to_swap_in = blocks_to_swap_in
        self.blocks_to_swap_out = blocks_to_swap_out
        self.blocks_to_copy = blocks_to_copy
        # Swap in and swap out should never happen at the same time.
        assert not (blocks_to_swap_in and blocks_to_swap_out)
        self.ignored_seq_groups = ignored_seq_groups

    def is_empty(self) -> bool:
        # NOTE: We do not consider the ignored sequence groups.
        return (
            not self.scheduled_seq_groups
            and not self.blocks_to_swap_in
            and not self.blocks_to_swap_out
            and not self.blocks_to_copy
        )


class Scheduler:

    def __init__(
        self,
        scheduler_config: SchedulerConfig,
        cache_config: CacheConfig,
    ) -> None:
        self.scheduler_config = scheduler_config
        self.cache_config = cache_config

        self.prompt_limit = min(
            self.scheduler_config.max_model_len,
            self.scheduler_config.max_num_batched_tokens,
        )

        # Instantiate the scheduling policy.
        self.policy = PolicyFactory.get_policy(policy_name="fcfs")
        # Create the block space manager.
        self.block_manager = BlockSpaceManager(
            block_size=self.cache_config.block_size,
            num_gpu_blocks=self.cache_config.num_gpu_blocks,
            num_cpu_blocks=self.cache_config.num_cpu_blocks,
            sliding_window=self.cache_config.sliding_window,
        )

        # TODO(zhuohan): Use deque instead of list for better performance.
        # Sequence groups in the WAITING state.
        self.waiting: List[SequenceGroup] = []
        # Sequence groups in the RUNNING state.
        self.running: List[SequenceGroup] = []
        # Sequence groups in the SWAPPED state.
        self.swapped: List[SequenceGroup] = []

    def add_seq_group(self, seq_group: SequenceGroup) -> None:
        # Add sequence groups to the waiting queue.
        self.waiting.append(seq_group)

    def abort_seq_group(self, request_id: Union[str, Iterable[str]]) -> None:
        if isinstance(request_id, str):
            request_id = (request_id,)
        request_ids = set(request_id)
        for state_queue in [self.waiting, self.running, self.swapped]:
            # We need to reverse the list as we are removing elements
            # from it as we iterate over it. If we don't do it,
            # indices will get messed up and we will skip over elements.
            for seq_group in reversed(state_queue):
                if seq_group.request_id in request_ids:
                    # Remove the sequence group from the state queue.
                    state_queue.remove(seq_group)
                    for seq in seq_group.get_seqs():
                        if seq.is_finished():
                            continue
                        seq.status = SequenceStatus.FINISHED_ABORTED
                        self.free_seq(seq)
                    request_ids.remove(seq_group.request_id)
                    if not request_ids:
                        return

    def has_unfinished_seqs(self) -> bool:
        return self.waiting or self.running or self.swapped

    def get_num_unfinished_seq_groups(self) -> int:
        return len(self.waiting) + len(self.running) + len(self.swapped)

    def _schedule(self) -> SchedulerOutputs:
        # Blocks that need to be swaped or copied before model execution.
        blocks_to_swap_in: Dict[int, int] = {}
        blocks_to_swap_out: Dict[int, int] = {}
        blocks_to_copy: Dict[int, List[int]] = {}

        # Fix the current time.
        now = time.monotonic()

        # Join waiting sequences if possible.
        if not self.swapped:
            ignored_seq_groups: List[SequenceGroup] = []
            scheduled: List[SequenceGroup] = []
            # The total number of sequences on the fly, including the
            # requests in the generation phase.
            num_curr_seqs = sum(
                seq_group.get_max_num_running_seqs() for seq_group in self.running
            )
            seq_lens: List[int] = []

            # Optimization: We do not sort the waiting queue since the preempted
            # sequence groups are added to the front and the new sequence groups
            # are added to the back.
            while self.waiting:
                seq_group = self.waiting[0]

                waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)
                assert len(waiting_seqs) == 1, (
                    "Waiting sequence group should have only one prompt " "sequence."
                )
                num_prompt_tokens = waiting_seqs[0].get_len()
                if num_prompt_tokens > self.prompt_limit:
                    logger.warning(
                        f"Input prompt ({num_prompt_tokens} tokens) is too long"
                        f" and exceeds limit of {self.prompt_limit}"
                    )
                    for seq in waiting_seqs:
                        seq.status = SequenceStatus.FINISHED_IGNORED
                    ignored_seq_groups.append(seq_group)
                    self.waiting.pop(0)
                    continue

                # If the sequence group cannot be allocated, stop.
                can_allocate = self.block_manager.can_allocate(seq_group)
                if can_allocate == AllocStatus.LATER:
                    break
                elif can_allocate == AllocStatus.NEVER:
                    logger.warning(
                        f"Input prompt ({num_prompt_tokens} tokens) is too long"
                        f" and exceeds the capacity of block_manager"
                    )
                    for seq in waiting_seqs:
                        seq.status = SequenceStatus.FINISHED_IGNORED
                    ignored_seq_groups.append(seq_group)
                    self.waiting.pop(0)
                    continue

                # If the number of batched tokens exceeds the limit, stop.
                new_seq_lens = seq_lens + [num_prompt_tokens]
                num_batched_tokens = len(new_seq_lens) * max(new_seq_lens)
                if num_batched_tokens > self.scheduler_config.max_num_batched_tokens:
                    break

                # The total number of sequences in the RUNNING state should not
                # exceed the maximum number of sequences.
                num_new_seqs = seq_group.get_max_num_running_seqs()
                if num_curr_seqs + num_new_seqs > self.scheduler_config.max_num_seqs:
                    break

                num_paddings = num_batched_tokens - sum(new_seq_lens)
                if num_paddings > self.scheduler_config.max_paddings:
                    break
                seq_lens = new_seq_lens

                seq_group = self.waiting.pop(0)
                self._allocate(seq_group)
                self.running.append(seq_group)
                num_curr_seqs += num_new_seqs
                scheduled.append(seq_group)

            if scheduled or ignored_seq_groups:
                scheduler_outputs = SchedulerOutputs(
                    scheduled_seq_groups=scheduled,
                    prompt_run=True,
                    num_batched_tokens=len(seq_lens) * max(seq_lens) if seq_lens else 0,
                    blocks_to_swap_in=blocks_to_swap_in,
                    blocks_to_swap_out=blocks_to_swap_out,
                    blocks_to_copy=blocks_to_copy,
                    ignored_seq_groups=ignored_seq_groups,
                )
                return scheduler_outputs

        # NOTE(woosuk): Preemption happens only when there is no available slot
        # to keep all the sequence groups in the RUNNING state.
        # In this case, the policy is responsible for deciding which sequence
        # groups to preempt.
        self.running = self.policy.sort_by_priority(now, self.running)

        # Reserve new token slots for the running sequence groups.
        running: List[SequenceGroup] = []
        preempted: List[SequenceGroup] = []
        while self.running:
            seq_group = self.running.pop(0)
            while not self.block_manager.can_append_slot(seq_group):
                if self.running:
                    # Preempt the lowest-priority sequence groups.
                    victim_seq_group = self.running.pop(-1)
                    self._preempt(victim_seq_group, blocks_to_swap_out)
                    preempted.append(victim_seq_group)
                else:
                    # No other sequence groups can be preempted.
                    # Preempt the current sequence group.
                    self._preempt(seq_group, blocks_to_swap_out)
                    preempted.append(seq_group)
                    break
            else:
                # Append new slots to the sequence group.
                self._append_slot(seq_group, blocks_to_copy)
                running.append(seq_group)
        self.running = running

        # Swap in the sequence groups in the SWAPPED state if possible.
        self.swapped = self.policy.sort_by_priority(now, self.swapped)
        if not preempted:
            num_curr_seqs = sum(
                seq_group.get_max_num_running_seqs() for seq_group in self.running
            )

            while self.swapped:
                seq_group = self.swapped[0]
                # If the sequence group cannot be swapped in, stop.
                if not self.block_manager.can_swap_in(seq_group):
                    break

                # The total number of sequences in the RUNNING state should not
                # exceed the maximum number of sequences.
                num_new_seqs = seq_group.get_max_num_running_seqs()
                if num_curr_seqs + num_new_seqs > self.scheduler_config.max_num_seqs:
                    break

                seq_group = self.swapped.pop(0)
                self._swap_in(seq_group, blocks_to_swap_in)
                self._append_slot(seq_group, blocks_to_copy)
                num_curr_seqs += num_new_seqs
                self.running.append(seq_group)

        # Each sequence in the generation phase only takes one token slot.
        # Therefore, the number of batched tokens is equal to the number of
        # sequences in the RUNNING state.
        num_batched_tokens = sum(
            seq_group.num_seqs(status=SequenceStatus.RUNNING)
            for seq_group in self.running
        )

        scheduler_outputs = SchedulerOutputs(
            scheduled_seq_groups=self.running,
            prompt_run=False,
            num_batched_tokens=num_batched_tokens,
            blocks_to_swap_in=blocks_to_swap_in,
            blocks_to_swap_out=blocks_to_swap_out,
            blocks_to_copy=blocks_to_copy,
            ignored_seq_groups=[],
        )
        return scheduler_outputs

    def schedule(self) -> Tuple[List[SequenceGroupMetadata], SchedulerOutputs]:
        # Schedule sequence groups.
        # This function call changes the internal states of the scheduler
        # such as self.running, self.swapped, and self.waiting.
        scheduler_outputs = self._schedule()

        # Create input data structures.
        seq_group_metadata_list: List[SequenceGroupMetadata] = []
        for seq_group in scheduler_outputs.scheduled_seq_groups:
            seq_data: Dict[int, SequenceData] = {}
            block_tables: Dict[int, List[int]] = {}
            for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
                seq_id = seq.seq_id
                seq_data[seq_id] = seq.data
                block_tables[seq_id] = self.block_manager.get_block_table(seq)

            seq_group_metadata = SequenceGroupMetadata(
                request_id=seq_group.request_id,
                is_prompt=scheduler_outputs.prompt_run,
                seq_data=seq_data,
                sampling_params=seq_group.sampling_params,
                block_tables=block_tables,
            )
            seq_group_metadata_list.append(seq_group_metadata)
        return seq_group_metadata_list, scheduler_outputs

    def fork_seq(self, parent_seq: Sequence, child_seq: Sequence) -> None:
        self.block_manager.fork(parent_seq, child_seq)

    def free_seq(self, seq: Sequence) -> None:
        self.block_manager.free(seq)

    def free_finished_seq_groups(self) -> None:
        self.running = [
            seq_group for seq_group in self.running if not seq_group.is_finished()
        ]

    def _allocate(self, seq_group: SequenceGroup) -> None:
        self.block_manager.allocate(seq_group)
        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):
            seq.status = SequenceStatus.RUNNING

    def _append_slot(
        self,
        seq_group: SequenceGroup,
        blocks_to_copy: Dict[int, List[int]],
    ) -> None:
        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
            ret = self.block_manager.append_slot(seq)
            if ret is not None:
                src_block, dst_block = ret
                if src_block in blocks_to_copy:
                    blocks_to_copy[src_block].append(dst_block)
                else:
                    blocks_to_copy[src_block] = [dst_block]

    def _preempt(
        self,
        seq_group: SequenceGroup,
        blocks_to_swap_out: Dict[int, int],
        preemption_mode: Optional[PreemptionMode] = None,
    ) -> None:
        # If preemption mode is not specified, we determine the mode as follows:
        # We use recomputation by default since it incurs lower overhead than
        # swapping. However, when the sequence group has multiple sequences
        # (e.g., beam search), recomputation is not currently supported. In
        # such a case, we use swapping instead.
        # FIXME(woosuk): This makes our scheduling policy a bit bizarre.
        # As swapped sequences are prioritized over waiting sequences,
        # sequence groups with multiple sequences are implicitly prioritized
        # over sequence groups with a single sequence.
        # TODO(woosuk): Support recomputation for sequence groups with multiple
        # sequences. This may require a more sophisticated CUDA kernel.
        if preemption_mode is None:
            if seq_group.get_max_num_running_seqs() == 1:
                preemption_mode = PreemptionMode.RECOMPUTE
            else:
                preemption_mode = PreemptionMode.SWAP
        if preemption_mode == PreemptionMode.RECOMPUTE:
            self._preempt_by_recompute(seq_group)
        elif preemption_mode == PreemptionMode.SWAP:
            self._preempt_by_swap(seq_group, blocks_to_swap_out)
        else:
            raise AssertionError("Invalid preemption mode.")

    def _preempt_by_recompute(
        self,
        seq_group: SequenceGroup,
    ) -> None:
        seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)
        assert len(seqs) == 1
        for seq in seqs:
            seq.status = SequenceStatus.WAITING
            self.block_manager.free(seq)
        # NOTE: For FCFS, we insert the preempted sequence group to the front
        # of the waiting queue.
        self.waiting.insert(0, seq_group)

    def _preempt_by_swap(
        self,
        seq_group: SequenceGroup,
        blocks_to_swap_out: Dict[int, int],
    ) -> None:
        self._swap_out(seq_group, blocks_to_swap_out)
        self.swapped.append(seq_group)

    def _swap_in(
        self,
        seq_group: SequenceGroup,
        blocks_to_swap_in: Dict[int, int],
    ) -> None:
        mapping = self.block_manager.swap_in(seq_group)
        blocks_to_swap_in.update(mapping)
        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):
            seq.status = SequenceStatus.RUNNING

    def _swap_out(
        self,
        seq_group: SequenceGroup,
        blocks_to_swap_out: Dict[int, int],
    ) -> None:
        if not self.block_manager.can_swap_out(seq_group):
            # FIXME(woosuk): Abort the sequence group instead of aborting the
            # entire engine.
            raise RuntimeError(
                "Aborted due to the lack of CPU swap space. Please increase "
                "the swap space to avoid this error."
            )
        mapping = self.block_manager.swap_out(seq_group)
        blocks_to_swap_out.update(mapping)
        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
            seq.status = SequenceStatus.SWAPPED



================================================
FILE: ChatTTS/model/velocity/sequence.py
================================================
"""Sequence and its related classes."""

import copy
import enum
from typing import Dict, List, Optional, Union
import torch
from vllm.block import LogicalTokenBlock
from .sampling_params import SamplingParams

PromptLogprobs = List[Optional[Dict[int, float]]]
SampleLogprobs = List[Dict[int, float]]


class SequenceStatus(enum.Enum):
    """Status of a sequence."""

    WAITING = enum.auto()
    RUNNING = enum.auto()
    SWAPPED = enum.auto()
    FINISHED_STOPPED = enum.auto()
    FINISHED_LENGTH_CAPPED = enum.auto()
    FINISHED_ABORTED = enum.auto()
    FINISHED_IGNORED = enum.auto()

    @staticmethod
    def is_finished(status: "SequenceStatus") -> bool:
        return status in [
            SequenceStatus.FINISHED_STOPPED,
            SequenceStatus.FINISHED_LENGTH_CAPPED,
            SequenceStatus.FINISHED_ABORTED,
            SequenceStatus.FINISHED_IGNORED,
        ]

    @staticmethod
    def get_finished_reason(status: "SequenceStatus") -> Union[str, None]:
        if status == SequenceStatus.FINISHED_STOPPED:
            finish_reason = "stop"
        elif status == SequenceStatus.FINISHED_LENGTH_CAPPED:
            finish_reason = "length"
        elif status == SequenceStatus.FINISHED_ABORTED:
            finish_reason = "abort"
        elif status == SequenceStatus.FINISHED_IGNORED:
            # The ignored sequences are the sequences whose prompt lengths
            # are longer than the model's length cap. Therefore, the stop
            # reason should also be "length" as in OpenAI API.
            finish_reason = "length"
        else:
            finish_reason = None
        return finish_reason


class SequenceData:
    """Data associated with a sequence.


    Args:
        prompt_token_ids: The token IDs of the prompt.

    Attributes:
        prompt_token_ids: The token IDs of the prompt.
        output_token_ids: The token IDs of the output.
        cumulative_logprob: The cumulative log probability of the output.
    """

    def __init__(
        self,
        prompt_token_ids: List[int],
    ) -> None:
        self.prompt_token_ids = prompt_token_ids
        self.output_token_ids: List[int] = []
        self.cumulative_logprob = 0.0
        self.hidden_states: Optional[torch.Tensor] = None
        self.finished = False

    def append_token_id(self, token_id: int, logprob: float) -> None:
        if isinstance(self.cumulative_logprob, float):
            self.cumulative_logprob = [
                0.0,
            ] * len(logprob)
        self.output_token_ids.append(token_id)
        for i in range(len(self.cumulative_logprob)):
            self.cumulative_logprob[i] += logprob[i]

    def append_hidden_states(self, hidden_states: torch.Tensor) -> None:
        if self.hidden_states is None:
            self.hidden_states = hidden_states
        else:
            self.hidden_states = torch.cat([self.hidden_states, hidden_states], dim=0)

    def get_len(self) -> int:
        return len(self.output_token_ids) + len(self.prompt_token_ids)

    def get_prompt_len(self) -> int:
        return len(self.prompt_token_ids)

    def get_output_len(self) -> int:
        return len(self.output_token_ids)

    def get_token_ids(self) -> List[int]:
        return self.prompt_token_ids + self.output_token_ids

    def get_last_token_id(self) -> int:
        if not self.output_token_ids:
            return self.prompt_token_ids[-1]
        return self.output_token_ids[-1]

    def __repr__(self) -> str:
        return (
            f"SequenceData("
            f"prompt_token_ids={self.prompt_token_ids}, "
            f"output_token_ids={self.output_token_ids}, "
            f"cumulative_logprob={self.cumulative_logprob}), "
            f"hidden_states={self.hidden_states.shape if self.hidden_states is not None else None}, "
            f"finished={self.finished})"
        )


class Sequence:
    """Stores the data, status, and block information of a sequence.

    Args:
        seq_id: The ID of the sequence.
        prompt: The prompt of the sequence.
        prompt_token_ids: The token IDs of the prompt.
        block_size: The block size of the sequence. Should be the same as the
            block size used by the block manager and cache engine.
    """

    def __init__(
        self,
        seq_id: int,
        prompt: str,
        prompt_token_ids: List[int],
        block_size: int,
    ) -> None:
        self.seq_id = seq_id
        self.prompt = prompt
        self.block_size = block_size

        self.data = SequenceData(prompt_token_ids)
        self.output_logprobs: SampleLogprobs = []
        self.output_text = ""

        self.logical_token_blocks: List[LogicalTokenBlock] = []
        # Initialize the logical token blocks with the prompt token ids.
        self._append_tokens_to_blocks(prompt_token_ids)
        self.status = SequenceStatus.WAITING

        # Used for incremental detokenization
        self.prefix_offset = 0
        self.read_offset = 0
        # Input + output tokens
        self.tokens: Optional[List[str]] = None

    def _append_logical_block(self) -> None:
        block = LogicalTokenBlock(
            block_number=len(self.logical_token_blocks),
            block_size=self.block_size,
        )
        self.logical_token_blocks.append(block)

    def _append_tokens_to_blocks(self, token_ids: List[int]) -> None:
        cursor = 0
        while cursor < len(token_ids):
            if not self.logical_token_blocks:
                self._append_logical_block()

            last_block = self.logical_token_blocks[-1]
            if last_block.is_full():
                self._append_logical_block()
                last_block = self.logical_token_blocks[-1]

            num_empty_slots = last_block.get_num_empty_slots()
            last_block.append_tokens(token_ids[cursor : cursor + num_empty_slots])
            cursor += num_empty_slots

    def append_token_id(
        self,
        token_id: int,
        logprobs: Dict[int, float],
        hidden_states: Optional[torch.Tensor] = None,
        finished: bool = False,
    ) -> None:
        assert token_id in logprobs
        self._append_tokens_to_blocks([token_id])
        self.output_logprobs.append(logprobs)
        self.data.append_token_id(token_id, logprobs[token_id])
        self.data.append_hidden_states(hidden_states)
        self.data.finished = finished

    def get_len(self) -> int:
        return self.data.get_len()

    def get_prompt_len(self) -> int:
        return self.data.get_prompt_len()

    def get_output_len(self) -> int:
        return self.data.get_output_len()

    def get_token_ids(self) -> List[int]:
        return self.data.get_token_ids()

    def get_last_token_id(self) -> int:
        return self.data.get_last_token_id()

    def get_output_token_ids(self) -> List[int]:
        return self.data.output_token_ids

    def get_cumulative_logprob(self) -> float:
        return self.data.cumulative_logprob

    def get_beam_search_score(
        self,
        length_penalty: float = 0.0,
        seq_len: Optional[int] = None,
        eos_token_id: Optional[int] = None,
    ) -> float:
        """Calculate the beam search score with length penalty.

        Adapted from

        https://github.com/huggingface/transformers/blob/ccb92be23def445f2afdea94c31286f84b89eb5b/src/transformers/generation/beam_search.py#L938
        """
        if seq_len is None:
            seq_len = self.get_len()
            # NOTE: HF implementation does not count the EOS token
            # towards the length, we align with that here for testing.
            if eos_token_id is not None and self.get_last_token_id() == eos_token_id:
                seq_len -= 1
        return self.get_cumulative_logprob() / (seq_len**length_penalty)

    def is_finished(self) -> bool:
        return SequenceStatus.is_finished(self.status)

    def fork(self, new_seq_id: int) -> "Sequence":
        new_seq = copy.deepcopy(self)
        new_seq.seq_id = new_seq_id
        return new_seq

    def __repr__(self) -> str:
        return (
            f"Sequence(seq_id={self.seq_id}, "
            f"status={self.status.name}, "
            f"num_blocks={len(self.logical_token_blocks)})"
        )


class SequenceGroup:
    """A group of sequences that are generated from the same prompt.

    Args:
        request_id: The ID of the request.
        seqs: The list of sequences.
        sampling_params: The sampling parameters used to generate the outputs.
        arrival_time: The arrival time of the request.
    """

    def __init__(
        self,
        request_id: str,
        seqs: List[Sequence],
        sampling_params: SamplingParams,
        arrival_time: float,
    ) -> None:
        self.request_id = request_id
        self.seqs_dict = {seq.seq_id: seq for seq in seqs}
        self.sampling_params = sampling_params
        self.arrival_time = arrival_time
        self.prompt_logprobs: Optional[PromptLogprobs] = None

    @property
    def prompt(self) -> str:
        # All sequences in the group should have the same prompt.
        # We use the prompt of an arbitrary sequence.
        return next(iter(self.seqs_dict.values())).prompt

    @property
    def prompt_token_ids(self) -> List[int]:
        # All sequences in the group should have the same prompt.
        # We use the prompt of an arbitrary sequence.
        return next(iter(self.seqs_dict.values())).data.prompt_token_ids

    def get_max_num_running_seqs(self) -> int:
        """The maximum number of sequences running in parallel in the remaining
        lifetime of the request."""
        if self.sampling_params.use_beam_search:
            # For beam search, maximally there will always be `best_of` beam
            # candidates running in the future.
            return self.sampling_params.best_of
        else:
            if self.sampling_params.best_of > self.num_seqs():
                # At prompt stage, the sequence group is not yet filled up
                # and only have one sequence running. However, in the
                # generation stage, we will have `best_of` sequences running.
                return self.sampling_params.best_of
            # At sampling stages, return the number of actual sequences
            # that are not finished yet.
            return self.num_unfinished_seqs()

    def get_seqs(
        self,
        status: Optional[SequenceStatus] = None,
    ) -> List[Sequence]:
        if status is None:
            return list(self.seqs_dict.values())
        else:
            return [seq for seq in self.seqs_dict.values() if seq.status == status]

    def get_unfinished_seqs(self) -> List[Sequence]:
        return [seq for seq in self.seqs_dict.values() if not seq.is_finished()]

    def get_finished_seqs(self) -> List[Sequence]:
        return [seq for seq in self.seqs_dict.values() if seq.is_finished()]

    def num_seqs(self, status: Optional[SequenceStatus] = None) -> int:
        return len(self.get_seqs(status))

    def num_unfinished_seqs(self) -> int:
        return len(self.get_unfinished_seqs())

    def num_finished_seqs(self) -> int:
        return len(self.get_finished_seqs())

    def find(self, seq_id: int) -> Sequence:
        if seq_id not in self.seqs_dict:
            raise ValueError(f"Sequence {seq_id} not found.")
        return self.seqs_dict[seq_id]

    def add(self, seq: Sequence) -> None:
        if seq.seq_id in self.seqs_dict:
            raise ValueError(f"Sequence {seq.seq_id} already exists.")
        self.seqs_dict[seq.seq_id] = seq

    def remove(self, seq_id: int) -> None:
        if seq_id not in self.seqs_dict:
            raise ValueError(f"Sequence {seq_id} not found.")
        del self.seqs_dict[seq_id]

    def is_finished(self) -> bool:
        return all(seq.is_finished() for seq in self.get_seqs())

    def __repr__(self) -> str:
        return (
            f"SequenceGroup(request_id={self.request_id}, "
            f"sampling_params={self.sampling_params}, "
            f"num_seqs={len(self.seqs_dict)})"
        )


class SequenceGroupMetadata:
    """Metadata for a sequence group. Used to create `InputMetadata`.


    Args:
        request_id: The ID of the request.
        is_prompt: Whether the request is at prompt stage.
        seq_data: The sequence data. (Seq id -> sequence data)
        sampling_params: The sampling parameters used to generate the outputs.
        block_tables: The block tables. (Seq id -> list of physical block
            numbers)
    """

    def __init__(
        self,
        request_id: str,
        is_prompt: bool,
        seq_data: Dict[int, SequenceData],
        sampling_params: SamplingParams,
        block_tables: Dict[int, List[int]],
    ) -> None:
        self.request_id = request_id
        self.is_prompt = is_prompt
        self.seq_data = seq_data
        self.sampling_params = sampling_params
        self.block_tables = block_tables


class SequenceOutput:
    """The model output associated with a sequence.

    Args:
        parent_seq_id: The ID of the parent sequence (for forking in beam
            search).
        output_token: The output token ID.
        logprobs: The logprobs of the output token.
            (Token id -> logP(x_i+1 | x_0, ..., x_i))
    """

    def __init__(
        self,
        parent_seq_id: int,
        output_token: int,
        logprobs: Dict[int, float],
        hidden_states: Optional[torch.Tensor] = None,
        finished: bool = False,
    ) -> None:
        self.parent_seq_id = parent_seq_id
        self.output_token = output_token
        self.logprobs = logprobs
        self.finished = finished
        self.hidden_states = hidden_states

    def __repr__(self) -> str:
        return (
            f"SequenceOutput(parent_seq_id={self.parent_seq_id}, "
            f"output_token={self.output_token}, "
            f"logprobs={self.logprobs}),"
            f"finished={self.finished}),"
            f"hidden_states={self.hidden_states.shape if self.hidden_states is not None else None}"
        )

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, SequenceOutput):
            raise NotImplementedError()
        return (
            self.parent_seq_id == other.parent_seq_id
            and self.output_token == other.output_token
            and self.logprobs == other.logprobs
        )


class SequenceGroupOutput:
    """The model output associated with a sequence group."""

    def __init__(
        self,
        samples: List[SequenceOutput],
        prompt_logprobs: Optional[PromptLogprobs],
    ) -> None:
        self.samples = samples
        self.prompt_logprobs = prompt_logprobs

    def __repr__(self) -> str:
        return (
            f"SequenceGroupOutput(samples={self.samples}, "
            f"prompt_logprobs={self.prompt_logprobs})"
        )

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, SequenceGroupOutput):
            raise NotImplementedError()
        return (
            self.samples == other.samples
            and self.prompt_logprobs == other.prompt_logprobs
        )


# For each sequence group, we generate a list of SequenceOutput object,
# each of which contains one possible candidate for the next token.
SamplerOutput = List[SequenceGroupOutput]



================================================
FILE: ChatTTS/model/velocity/worker.py
================================================
"""A GPU worker class."""

import os
from typing import Dict, List, Optional, Tuple

import torch
import torch.distributed

from vllm.config import CacheConfig, ModelConfig, ParallelConfig, SchedulerConfig
from vllm.model_executor import set_random_seed
from vllm.model_executor.parallel_utils.communication_op import broadcast_object_list
from vllm.model_executor.parallel_utils.parallel_state import initialize_model_parallel
from vllm.sequence import SamplerOutput, SequenceGroupMetadata
from vllm.worker.cache_engine import CacheEngine

from .model_runner import ModelRunner


class Worker:
    """A worker class that executes (a partition of) the model on a GPU.

    Each worker is associated with a single GPU. The worker is responsible for
    maintaining the KV cache and executing the model on the GPU. In case of
    distributed inference, each worker is assigned a partition of the model.
    """

    def __init__(
        self,
        model_config: ModelConfig,
        parallel_config: ParallelConfig,
        scheduler_config: SchedulerConfig,
        local_rank: int,
        rank: int,
        distributed_init_method: str,
        post_model_path: str,
        is_driver_worker: bool = False,
    ) -> None:
        self.model_config = model_config
        self.parallel_config = parallel_config
        self.scheduler_config = scheduler_config
        self.local_rank = local_rank
        self.rank = rank
        self.distributed_init_method = distributed_init_method
        self.is_driver_worker = is_driver_worker
        self.post_model_path = post_model_path

        if self.is_driver_worker:
            assert self.rank == 0, "The driver worker must have rank 0."

        self.model_runner = ModelRunner(
            model_config,
            parallel_config,
            scheduler_config,
            is_driver_worker,
            post_model_path,
        )
        # Uninitialized cache engine. Will be initialized by
        # self.init_cache_engine().
        self.cache_config = None
        self.cache_engine = None
        self.cache_events = None
        self.gpu_cache = None

    def init_model(self) -> None:
        # torch.distributed.all_reduce does not free the input tensor until
        # the synchronization point. This causes the memory usage to grow
        # as the number of all_reduce calls increases. This env var disables
        # this behavior.
        # Related issue:
        # https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573
        os.environ["TORCH_NCCL_AVOID_RECORD_STREAMS"] = "1"

        # This env var set by Ray causes exceptions with graph building.
        os.environ.pop("NCCL_ASYNC_ERROR_HANDLING", None)
        self.device = torch.device(f"cuda:{self.local_rank}")
        torch.cuda.set_device(self.device)

        _check_if_gpu_supports_dtype(self.model_config.dtype)

        # Initialize the distributed environment.
        _init_distributed_environment(
            self.parallel_config, self.rank, self.distributed_init_method
        )

        # Initialize the model.
        set_random_seed(self.model_config.seed)

    def load_model(self):
        self.model_runner.load_model()

    @torch.inference_mode()
    def profile_num_available_blocks(
        self,
        block_size: int,
        gpu_memory_utilization: float,
        cpu_swap_space: int,
    ) -> Tuple[int, int]:
        # Profile the memory usage of the model and get the maximum number of
        # cache blocks that can be allocated with the remaining free memory.
        torch.cuda.empty_cache()

        # Execute a forward pass with dummy inputs to profile the memory usage
        # of the model.
        self.model_runner.profile_run()

        # Calculate the number of blocks that can be allocated with the
        # profiled peak memory.
        torch.cuda.synchronize()
        free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()
        peak_memory = total_gpu_memory - free_gpu_memory

        cache_block_size = CacheEngine.get_cache_block_size(
            block_size, self.model_config, self.parallel_config
        )
        num_gpu_blocks = int(
            (total_gpu_memory * gpu_memory_utilization - peak_memory)
            // cache_block_size
        )
        num_cpu_blocks = int(cpu_swap_space // cache_block_size)
        num_gpu_blocks = max(num_gpu_blocks, 0)
        num_cpu_blocks = max(num_cpu_blocks, 0)
        torch.cuda.empty_cache()
        return num_gpu_blocks, num_cpu_blocks

    def init_cache_engine(self, cache_config: CacheConfig) -> None:
        self.cache_config = cache_config
        self.cache_engine = CacheEngine(
            self.cache_config, self.model_config, self.parallel_config
        )
        self.cache_events = self.cache_engine.events
        self.gpu_cache = self.cache_engine.gpu_cache
        self.model_runner.set_block_size(self.cache_engine.block_size)

    def warm_up_model(self) -> None:
        if not self.model_config.enforce_eager:
            self.model_runner.capture_model(self.gpu_cache)
        # Reset the seed to ensure that the random state is not affected by
        # the model initialization and profiling.
        set_random_seed(self.model_config.seed)

    def cache_swap(
        self,
        blocks_to_swap_in: Dict[int, int],
        blocks_to_swap_out: Dict[int, int],
        blocks_to_copy: Dict[int, List[int]],
    ) -> None:
        # Issue cache operations.
        issued_cache_op = False
        if blocks_to_swap_in:
            self.cache_engine.swap_in(blocks_to_swap_in)
            issued_cache_op = True
        if blocks_to_swap_out:
            self.cache_engine.swap_out(blocks_to_swap_out)
            issued_cache_op = True
        if blocks_to_copy:
            self.cache_engine.copy(blocks_to_copy)
            issued_cache_op = True

        cache_events = self.cache_events if issued_cache_op else None

        # Wait for cache operations to finish.
        # TODO(woosuk): Profile swapping overhead and optimize if needed.
        if cache_events is not None:
            for event in cache_events:
                event.wait()

    @torch.inference_mode()
    def execute_model(
        self,
        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None,
        blocks_to_swap_in: Optional[Dict[int, int]] = None,
        blocks_to_swap_out: Optional[Dict[int, int]] = None,
        blocks_to_copy: Optional[Dict[int, List[int]]] = None,
    ) -> Optional[SamplerOutput]:
        if self.is_driver_worker:
            assert seq_group_metadata_list is not None
            num_seq_groups = len(seq_group_metadata_list)
            assert blocks_to_swap_in is not None
            assert blocks_to_swap_out is not None
            assert blocks_to_copy is not None
            block_swapping_info = [
                blocks_to_swap_in,
                blocks_to_swap_out,
                blocks_to_copy,
            ]
            broadcast_object_list([num_seq_groups] + block_swapping_info, src=0)
        else:
            # num_seq_groups, blocks_to_swap_in, blocks_to_swap_out,
            # blocks_to_copy (4 elements)
            recv_data = [None] * 4
            broadcast_object_list(recv_data, src=0)
            num_seq_groups = recv_data[0]
            block_swapping_info = recv_data[1:]

        self.cache_swap(*block_swapping_info)

        # If there is no input, we don't need to execute the model.
        if num_seq_groups == 0:
            return {}

        output = self.model_runner.execute_model(
            seq_group_metadata_list, self.gpu_cache
        )
        return output


def _init_distributed_environment(
    parallel_config: ParallelConfig,
    rank: int,
    distributed_init_method: Optional[str] = None,
) -> None:
    """Initialize the distributed environment."""
    if torch.distributed.is_initialized():
        torch_world_size = torch.distributed.get_world_size()
        if torch_world_size != parallel_config.world_size:
            raise RuntimeError(
                "torch.distributed is already initialized but the torch world "
                "size does not match parallel_config.world_size "
                f"({torch_world_size} vs. {parallel_config.world_size})."
            )
    elif not distributed_init_method:
        raise ValueError(
            "distributed_init_method must be set if torch.distributed "
            "is not already initialized"
        )
    else:
        torch.distributed.init_process_group(
            backend="nccl",
            world_size=parallel_config.world_size,
            rank=rank,
            init_method=distributed_init_method,
        )

    # A small all_reduce for warmup.
    torch.distributed.all_reduce(torch.zeros(1).cuda())
    initialize_model_parallel(
        parallel_config.tensor_parallel_size, parallel_config.pipeline_parallel_size
    )


def _check_if_gpu_supports_dtype(torch_dtype: torch.dtype):
    # Check if the GPU supports the dtype.
    if torch_dtype == torch.bfloat16:
        compute_capability = torch.cuda.get_device_capability()
        if compute_capability[0] < 8:
            gpu_name = torch.cuda.get_device_name()
            raise ValueError(
                "Bfloat16 is only supported on GPUs with compute capability "
                f"of at least 8.0. Your {gpu_name} GPU has compute capability "
                f"{compute_capability[0]}.{compute_capability[1]}."
            )



================================================
FILE: ChatTTS/res/__init__.py
================================================



================================================
FILE: ChatTTS/res/homophones_map.json
================================================
{
    "粡": "同",
    "為": "位",
    "瀹": "月",
    "滆": "格",
    "摲": "颤",
    "渹": "轰",
    "於": "鱼",
    "満": "满",
    "鍑": "父",
    "與": "雨",
    "阃": "捆",
    "橹": "鲁",
    "骞": "前",
    "岀": "出",
    "铓": "忙",
    "杩": "骂",
    "鍞": "坑",
    "擼": "鲁",
    "後": "后",
    "來": "来",
    "喔": "哦",
    "會": "会",
    "爰": "原",
    "煡": "进",
    "鐗": "减",
    "個": "个",
    "澶": "缠",
    "時": "石",
    "噢": "哦",
    "铚": "至",
    "栧": "意",
    "浘": "伟",
    "這": "这",
    "穞": "旅",
    "妳": "你",
    "佷": "很",
    "對": "对",
    "並": "病",
    "國": "国",
    "戠": "知",
    "笟": "姑",
    "唔": "无",
    "勫": "翻",
    "徃": "网",
    "還": "孩",
    "將": "将",
    "閮": "停",
    "屽": "汉",
    "過": "过",
    "網": "网",
    "紅": "红",
    "說": "说",
    "開": "开",
    "區": "区",
    "該": "该",
    "從": "从",
    "鑹": "窜",
    "硗": "敲",
    "內": "内",
    "涘": "四",
    "屄": "逼",
    "種": "种",
    "爾": "耳",
    "讓": "让",
    "搧": "山",
    "甯": "凝",
    "晢": "哲",
    "愛": "爱",
    "吖": "呀",
    "沒": "梅",
    "長": "长",
    "線": "现",
    "熱": "热",
    "肏": "操",
    "鎶": "歌",
    "傚": "笑",
    "無": "无",
    "號": "号",
    "兩": "两",
    "榫": "损",
    "強": "强",
    "瓒": "赞",
    "發": "发",
    "機": "机",
    "達": "达",
    "圖": "图",
    "稱": "称",
    "則": "则",
    "間": "间",
    "馬": "马",
    "點": "点",
    "級": "极",
    "輪": "伦",
    "壹": "一",
    "東": "东",
    "鍚": "羊",
    "給": "给",
    "學": "学",
    "數": "树",
    "軍": "君",
    "當": "当",
    "嗎": "吗",
    "爲": "位",
    "鏂": "欧",
    "場": "场",
    "寮": "聊",
    "鏈": "练",
    "獎": "讲",
    "約": "约",
    "現": "现",
    "裏": "李",
    "縣": "现",
    "阌": "文",
    "婬": "银",
    "們": "们",
    "萬": "万",
    "亞": "亚",
    "經": "精",
    "艹": "草",
    "張": "张",
    "隊": "对",
    "進": "进",
    "帶": "带",
    "镩": "窜",
    "曽": "层",
    "麽": "魔",
    "閲": "月",
    "賽": "赛",
    "鐮": "连",
    "見": "见",
    "書": "书",
    "祂": "他",
    "鐭": "玉",
    "龍": "龙",
    "犳": "着",
    "墝": "敲",
    "門": "门",
    "連": "连",
    "嘅": "凯",
    "處": "处",
    "設": "设",
    "屬": "鼠",
    "戹": "恶",
    "員": "原",
    "體": "体",
    "車": "车",
    "裡": "李",
    "羅": "罗",
    "憺": "蛋",
    "頭": "头",
    "類": "泪",
    "戰": "战",
    "話": "话",
    "動": "动",
    "麼": "么",
    "組": "组",
    "別": "别",
    "嚕": "鲁",
    "請": "请",
    "許": "许",
    "總": "总",
    "島": "导",
    "華": "华",
    "卻": "确",
    "編": "编",
    "業": "夜",
    "鐜": "堆",
    "師": "诗",
    "雙": "双",
    "準": "准",
    "黃": "黄",
    "涁": "肾",
    "報": "报",
    "倫": "伦",
    "視": "是",
    "選": "选",
    "揄": "鱼",
    "鍙": "互",
    "純": "纯",
    "樻": "溃",
    "幾": "几",
    "濈": "极",
    "辦": "办",
    "應": "应",
    "極": "极",
    "柊": "中",
    "納": "那",
    "铰": "角",
    "蹇": "减",
    "係": "系",
    "掑": "其",
    "飾": "是",
    "闱": "维",
    "團": "团",
    "歳": "岁",
    "變": "变",
    "陳": "陈",
    "娍": "成",
    "義": "意",
    "兒": "而",
    "碼": "马",
    "條": "条",
    "轉": "转",
    "難": "南",
    "曜": "要",
    "邊": "编",
    "項": "向",
    "積": "机",
    "滿": "满",
    "幹": "干",
    "奧": "奥",
    "裝": "装",
    "町": "听",
    "哜": "记",
    "笫": "子",
    "絑": "朱",
    "風": "风",
    "黨": "挡",
    "晟": "成",
    "闃": "去",
    "蘭": "蓝",
    "樣": "样",
    "魚": "鱼",
    "較": "教",
    "聲": "生",
    "戝": "贼",
    "單": "单",
    "氣": "气",
    "獲": "或",
    "語": "雨",
    "樂": "乐",
    "實": "石",
    "親": "亲",
    "沖": "冲",
    "鎮": "镇",
    "阒": "去",
    "褚": "楚",
    "玖": "久",
    "樓": "楼",
    "楽": "乐",
    "闾": "驴",
    "夠": "够",
    "嬪": "贫",
    "宮": "工",
    "殇": "伤",
    "劉": "刘",
    "舉": "举",
    "尻": "烤",
    "園": "原",
    "傳": "传",
    "蹙": "促",
    "聯": "连",
    "維": "维",
    "鐵": "铁",
    "宸": "陈",
    "砲": "炮",
    "試": "是",
    "導": "导",
    "製": "至",
    "勝": "胜",
    "萊": "来",
    "絕": "觉",
    "僅": "紧",
    "聖": "胜",
    "睇": "地",
    "隨": "随",
    "浜": "帮",
    "慾": "玉",
    "億": "意",
    "屆": "借",
    "硒": "西",
    "電": "电",
    "玥": "月",
    "權": "全",
    "殺": "沙",
    "埚": "锅",
    "買": "买",
    "創": "创",
    "雲": "云",
    "記": "记",
    "臺": "台",
    "誰": "谁",
    "夢": "梦",
    "關": "关",
    "荜": "必",
    "叁": "三",
    "層": "层",
    "寶": "宝",
    "寫": "写",
    "問": "问",
    "啲": "低",
    "備": "被",
    "靑": "青",
    "務": "物",
    "灏": "号",
    "镑": "棒",
    "佢": "取",
    "諾": "诺",
    "鍧": "轰",
    "葉": "夜",
    "節": "节",
    "樹": "树",
    "舊": "就",
    "煜": "玉",
    "飛": "飞",
    "歲": "岁",
    "擊": "机",
    "灣": "弯",
    "蘇": "苏",
    "調": "掉",
    "產": "铲",
    "謝": "谢",
    "髮": "发",
    "冇": "帽",
    "劇": "巨",
    "優": "优",
    "費": "费",
    "铬": "个",
    "姦": "间",
    "廣": "广",
    "論": "论",
    "锰": "猛",
    "眾": "重",
    "遠": "远",
    "絲": "思",
    "灬": "标",
    "麗": "利",
    "覃": "谈",
    "烬": "进",
    "魯": "鲁",
    "橋": "桥",
    "钴": "古",
    "館": "管",
    "辊": "滚",
    "讀": "毒",
    "統": "统",
    "筆": "比",
    "蕩": "荡",
    "橫": "横",
    "盡": "紧",
    "錢": "钱",
    "柟": "南",
    "賞": "赏",
    "贰": "二",
    "題": "提",
    "戶": "互",
    "奷": "前",
    "泓": "红",
    "嘭": "砰",
    "楊": "羊",
    "計": "记",
    "薩": "萨",
    "陣": "镇",
    "茗": "明",
    "專": "专",
    "鬆": "松",
    "庫": "裤",
    "焱": "燕",
    "銉": "玉",
    "滅": "灭",
    "闄": "咬",
    "澤": "则",
    "梓": "子",
    "鄉": "相",
    "側": "册",
    "質": "至",
    "佔": "战",
    "復": "父",
    "剛": "刚",
    "吳": "无",
    "洩": "谢",
    "巳": "四",
    "鍦": "诗",
    "漢": "汉",
    "異": "意",
    "終": "中",
    "聽": "听",
    "職": "直",
    "標": "标",
    "賛": "赞",
    "咁": "干",
    "離": "离",
    "歡": "欢",
    "劍": "见",
    "靈": "零",
    "運": "运",
    "榛": "真",
    "叧": "寡",
    "歐": "欧",
    "領": "领",
    "搽": "茶",
    "換": "换",
    "绫": "零",
    "餘": "鱼",
    "椿": "春",
    "瀬": "赖",
    "萫": "向",
    "溆": "续",
    "藥": "要",
    "頂": "顶",
    "徵": "睁",
    "吋": "寸",
    "環": "环",
    "玮": "伟",
    "結": "节",
    "陽": "羊",
    "據": "巨",
    "興": "性",
    "詞": "瓷",
    "貝": "被",
    "敗": "败",
    "陸": "路",
    "儘": "紧",
    "亂": "乱",
    "紀": "记",
    "貴": "贵",
    "議": "意",
    "術": "树",
    "詩": "诗",
    "銀": "银",
    "锏": "减",
    "叻": "乐",
    "捻": "年",
    "嬴": "营",
    "確": "确",
    "砼": "同",
    "載": "在",
    "乜": "灭",
    "盅": "中",
    "頓": "顿",
    "鲲": "昆",
    "廠": "场",
    "钑": "萨",
    "姧": "间",
    "盤": "盘",
    "衛": "位",
    "讬": "拖",
    "獸": "瘦",
    "駐": "助",
    "発": "发",
    "盃": "杯",
    "喏": "诺",
    "镢": "觉",
    "黒": "黑",
    "僕": "葡",
    "偈": "记",
    "週": "周",
    "绾": "碗",
    "蓋": "概",
    "锺": "中",
    "溫": "温",
    "齊": "其",
    "幫": "帮",
    "斷": "断",
    "姝": "书",
    "飚": "标",
    "堝": "锅",
    "璟": "井",
    "祢": "迷",
    "乇": "拖",
    "藍": "蓝",
    "況": "矿",
    "昱": "玉",
    "綫": "现",
    "傷": "伤",
    "鐘": "中",
    "臧": "脏",
    "栾": "鸾",
    "鏄": "团",
    "囗": "维",
    "涧": "见",
    "護": "互",
    "罡": "刚",
    "浗": "求",
    "輕": "青",
    "馀": "鱼",
    "額": "额",
    "隻": "知",
    "湪": "团",
    "臨": "林",
    "奚": "西",
    "锛": "奔",
    "婧": "静",
    "錯": "错",
    "順": "顺",
    "浠": "西",
    "芮": "瑞",
    "続": "续",
    "奪": "夺",
    "預": "玉",
    "須": "需",
    "孫": "孙",
    "淩": "零",
    "賣": "卖",
    "細": "系",
    "顯": "显",
    "頁": "夜",
    "灞": "爸",
    "邬": "乌",
    "昇": "生",
    "茲": "姿",
    "彈": "蛋",
    "実": "石",
    "贏": "营",
    "狀": "壮",
    "營": "营",
    "菁": "精",
    "勢": "是",
    "畫": "话",
    "讚": "赞",
    "價": "驾",
    "闆": "板",
    "壓": "呀",
    "螭": "吃",
    "獨": "毒",
    "麥": "卖",
    "資": "姿",
    "牠": "他",
    "針": "真",
    "瑪": "马",
    "遊": "由",
    "講": "讲",
    "決": "觉",
    "説": "说",
    "喺": "习",
    "豔": "燕",
    "繼": "记",
    "姉": "子",
    "傑": "节",
    "斮": "错",
    "歸": "归",
    "歷": "利",
    "譯": "意",
    "鄭": "挣",
    "沢": "则",
    "錄": "路",
    "槍": "枪",
    "廳": "听",
    "趙": "照",
    "霏": "飞",
    "咻": "修",
    "鍏": "维",
    "貓": "猫",
    "殑": "情",
    "觀": "关",
    "慱": "团",
    "盧": "芦",
    "讜": "挡",
    "祇": "其",
    "扠": "插",
    "欸": "哀",
    "圍": "维",
    "炜": "伟",
    "缃": "相",
    "絶": "觉",
    "銈": "机",
    "綱": "刚",
    "態": "太",
    "騎": "其",
    "莊": "装",
    "麵": "面",
    "顔": "颜",
    "櫠": "费",
    "烏": "乌",
    "腳": "角",
    "談": "谈",
    "钼": "木",
    "寬": "宽",
    "榮": "容",
    "镉": "格",
    "莪": "额",
    "啐": "翠",
    "濠": "豪",
    "續": "续",
    "聞": "文",
    "钨": "乌",
    "蕙": "会",
    "課": "客",
    "嚒": "么",
    "毓": "玉",
    "漸": "见",
    "黶": "眼",
    "寧": "凝",
    "簡": "减",
    "養": "养",
    "湯": "汤",
    "垣": "原",
    "證": "挣",
    "旡": "记",
    "際": "记",
    "鈥": "火",
    "減": "减",
    "莳": "石",
    "韓": "含",
    "锭": "定",
    "凱": "凯",
    "響": "想",
    "珏": "觉",
    "伱": "你",
    "砷": "深",
    "禮": "李",
    "覺": "觉",
    "劵": "倦",
    "泠": "零",
    "濮": "葡",
    "諸": "朱",
    "艦": "见",
    "祗": "知",
    "醫": "一",
    "豐": "风",
    "卐": "万",
    "複": "父",
    "慶": "庆",
    "訂": "定",
    "賴": "赖",
    "谙": "安",
    "旳": "地",
    "織": "知",
    "硼": "朋",
    "鳥": "尿",
    "晁": "朝",
    "葭": "家",
    "鄰": "林",
    "涊": "年",
    "誠": "成",
    "爭": "睁",
    "補": "捕",
    "妺": "墨",
    "冊": "册",
    "笺": "间",
    "樽": "尊",
    "臉": "脸",
    "圩": "维",
    "塊": "快",
    "瑛": "应",
    "岱": "带",
    "紝": "任",
    "緒": "续",
    "識": "石",
    "円": "原",
    "巻": "倦",
    "負": "父",
    "枰": "平",
    "緊": "紧",
    "歆": "心",
    "敵": "敌",
    "鐐": "聊",
    "昃": "责",
    "藝": "意",
    "瀛": "营",
    "叟": "搜",
    "隼": "损",
    "翎": "零",
    "恁": "嫩",
    "専": "专",
    "願": "院",
    "気": "气",
    "埠": "不",
    "蔺": "吝",
    "參": "参",
    "轧": "亚",
    "睨": "逆",
    "铀": "由",
    "恵": "会",
    "採": "采",
    "仟": "前",
    "謂": "位",
    "韋": "维",
    "佘": "蛇",
    "鰭": "其",
    "岡": "刚",
    "込": "迂",
    "戲": "系",
    "捲": "卷",
    "煨": "微",
    "捌": "八",
    "飯": "饭",
    "黍": "鼠",
    "鎴": "习",
    "擡": "台",
    "紙": "指",
    "鎵": "家",
    "閫": "捆",
    "咲": "笑",
    "銆": "墨",
    "筱": "小",
    "軟": "软",
    "敤": "可",
    "啜": "踹",
    "顧": "固",
    "儀": "宜",
    "迩": "耳",
    "髙": "高",
    "賱": "允",
    "庾": "雨",
    "評": "平",
    "昀": "云",
    "責": "则",
    "併": "病",
    "喙": "会",
    "踫": "碰",
    "婕": "节",
    "逹": "达",
    "溴": "秀",
    "煸": "编",
    "铣": "洗",
    "鐧": "间",
    "倉": "仓",
    "稥": "相",
    "輝": "灰",
    "驚": "精",
    "翊": "意",
    "坶": "母",
    "様": "样",
    "遷": "前",
    "農": "农",
    "綠": "绿",
    "鸢": "冤",
    "鑽": "钻",
    "帛": "博",
    "忓": "干",
    "蔣": "讲",
    "緣": "原",
    "媓": "黄",
    "炁": "气",
    "镌": "捐",
    "钖": "羊",
    "輛": "亮",
    "壊": "坏",
    "钒": "烦",
    "厝": "错",
    "鋼": "刚",
    "馗": "奎",
    "釆": "变",
    "聿": "玉",
    "娆": "扰",
    "褃": "肯",
    "噴": "喷",
    "匯": "会",
    "嚴": "颜",
    "雞": "机",
    "莘": "深",
    "蛹": "永",
    "垸": "院",
    "晔": "夜",
    "谥": "是",
    "舐": "是",
    "孑": "节",
    "峯": "风",
    "镪": "枪",
    "圓": "原",
    "渇": "可",
    "剌": "啦",
    "涜": "毒",
    "掛": "挂",
    "測": "册",
    "貼": "贴",
    "睬": "采",
    "適": "是",
    "珈": "家",
    "徳": "德",
    "轄": "侠",
    "兇": "胸",
    "認": "任",
    "嶅": "敖",
    "惡": "恶",
    "嬫": "容",
    "鍛": "断",
    "狃": "纽",
    "協": "鞋",
    "査": "扎",
    "劃": "话",
    "巽": "训",
    "練": "练",
    "貨": "或",
    "嗳": "哀",
    "範": "饭",
    "寤": "物",
    "簽": "前",
    "祯": "真",
    "卅": "萨",
    "郜": "告",
    "趕": "感",
    "湫": "角",
    "晉": "进",
    "揚": "羊",
    "廟": "庙",
    "脫": "拖",
    "濃": "农",
    "竴": "村",
    "啖": "蛋",
    "醐": "胡",
    "椁": "果",
    "妤": "鱼",
    "衆": "重",
    "昶": "场",
    "讷": "呢",
    "鮑": "报",
    "钜": "巨",
    "鐢": "烦",
    "祐": "右",
    "勮": "巨",
    "戀": "练",
    "颦": "贫",
    "嚯": "或",
    "璁": "聪",
    "壞": "坏",
    "琰": "眼",
    "勐": "猛",
    "靜": "静",
    "鐨": "费",
    "诇": "胸",
    "呎": "尺",
    "顆": "科",
    "戦": "战",
    "闀": "红",
    "瑄": "宣",
    "賢": "闲",
    "賀": "贺",
    "訪": "访",
    "氲": "晕",
    "锉": "错",
    "隅": "鱼",
    "陰": "因",
    "獕": "催",
    "呂": "旅",
    "厍": "设",
    "嗗": "挖",
    "監": "间",
    "媽": "妈",
    "曉": "小",
    "嬢": "娘",
    "涼": "良",
    "牝": "聘",
    "娑": "缩",
    "瘠": "极",
    "剐": "寡",
    "懷": "怀",
    "谛": "地",
    "尋": "寻",
    "閣": "格",
    "俾": "比",
    "镭": "雷",
    "婦": "父",
    "圃": "普",
    "乛": "呀",
    "勞": "劳",
    "腚": "定",
    "貫": "灌",
    "錦": "紧",
    "癣": "选",
    "頒": "班",
    "丅": "下",
    "賷": "机",
    "蓟": "记",
    "嬩": "鱼",
    "蓮": "连",
    "遒": "求",
    "忻": "心",
    "鍒": "柔",
    "堇": "紧",
    "輸": "书",
    "荿": "成",
    "菈": "拉",
    "豬": "朱",
    "谌": "陈",
    "珮": "配",
    "贮": "助",
    "閭": "驴",
    "凊": "庆",
    "贊": "赞",
    "徹": "撤",
    "札": "炸",
    "産": "铲",
    "焗": "局",
    "俪": "利",
    "呷": "嘎",
    "喬": "桥",
    "檔": "荡",
    "雖": "虽",
    "歎": "探",
    "闇": "按",
    "襲": "习",
    "嶨": "学",
    "凈": "静",
    "敕": "赤",
    "執": "直",
    "噸": "蹲",
    "枓": "抖",
    "飲": "引",
    "毗": "皮",
    "猢": "胡",
    "膘": "标",
    "廢": "费",
    "遺": "宜",
    "筠": "云",
    "壇": "谈",
    "敝": "必",
    "枧": "减",
    "犟": "降",
    "奮": "愤",
    "钡": "被",
    "財": "才",
    "塬": "原",
    "忚": "西",
    "孬": "脑",
    "氩": "亚",
    "槿": "紧",
    "钯": "靶",
    "髒": "脏",
    "夔": "奎",
    "曆": "利",
    "腦": "脑",
    "泤": "四",
    "浚": "俊",
    "嶂": "帐",
    "輻": "福",
    "杷": "爬",
    "轶": "意",
    "嘬": "踹",
    "燈": "灯",
    "髻": "记",
    "伧": "仓",
    "揩": "开",
    "娉": "平",
    "險": "显",
    "珞": "落",
    "剜": "弯",
    "構": "够",
    "濟": "记",
    "脈": "卖",
    "紗": "沙",
    "侑": "右",
    "頻": "贫",
    "誌": "至",
    "齒": "尺",
    "鍐": "宗",
    "幔": "慢",
    "姫": "机",
    "擬": "你",
    "伝": "云",
    "糸": "密",
    "抜": "拔",
    "宓": "密",
    "騷": "骚",
    "衝": "冲",
    "姣": "教",
    "規": "归",
    "蓓": "被",
    "謀": "谋",
    "毀": "毁",
    "譜": "普",
    "寵": "宠",
    "遑": "黄",
    "畀": "必",
    "銮": "鸾",
    "帋": "指",
    "箐": "庆",
    "搶": "抢",
    "锑": "踢",
    "鮮": "先",
    "缁": "姿",
    "阚": "罕",
    "掸": "胆",
    "桜": "应",
    "誤": "物",
    "瓤": "嚷",
    "蕭": "消",
    "噌": "层",
    "槌": "垂",
    "輔": "辅",
    "鏡": "静",
    "泷": "龙",
    "鳃": "塞",
    "椴": "断",
    "尙": "上",
    "雉": "至",
    "褌": "昆",
    "楔": "些",
    "邨": "村",
    "珩": "行",
    "溟": "明",
    "隱": "引",
    "氐": "低",
    "紋": "文",
    "冼": "显",
    "绥": "随",
    "淬": "翠",
    "抻": "陈",
    "馮": "逢",
    "贲": "奔",
    "駛": "使",
    "邝": "矿",
    "祼": "灌",
    "柘": "这",
    "斛": "胡",
    "掬": "居",
    "蕪": "无",
    "畢": "必",
    "鬥": "豆",
    "憑": "平",
    "習": "习",
    "檢": "减",
    "輯": "极",
    "坳": "奥",
    "購": "够",
    "啟": "起",
    "築": "助",
    "霁": "记",
    "釋": "是",
    "槼": "归",
    "弼": "必",
    "邰": "台",
    "蹶": "觉",
    "績": "机",
    "煦": "续",
    "嚟": "离",
    "対": "对",
    "緯": "伟",
    "矾": "烦",
    "賈": "假",
    "棄": "气",
    "瓯": "欧",
    "樾": "月",
    "徑": "静",
    "煙": "烟",
    "洟": "替",
    "殘": "残",
    "浔": "寻",
    "磬": "庆",
    "叴": "求",
    "闈": "维",
    "攝": "设",
    "稞": "科",
    "绉": "昼",
    "岫": "秀",
    "辭": "瓷",
    "増": "增",
    "塚": "种",
    "吣": "亲",
    "褋": "叠",
    "鷹": "应",
    "鳳": "奉",
    "燒": "烧",
    "瓴": "零",
    "鐡": "铁",
    "觸": "处",
    "氫": "青",
    "汛": "训",
    "跡": "机",
    "繞": "绕",
    "镙": "罗",
    "旻": "民",
    "咂": "匝",
    "玦": "觉",
    "噫": "一",
    "桿": "感",
    "爺": "爷",
    "卟": "捕",
    "訓": "训",
    "湴": "办",
    "沅": "原",
    "姹": "差",
    "嵇": "机",
    "滢": "营",
    "蟥": "黄",
    "缙": "进",
    "浼": "美",
    "顏": "颜",
    "垚": "摇",
    "獄": "玉",
    "堅": "间",
    "轟": "轰",
    "櫈": "凳",
    "戍": "树",
    "斬": "展",
    "瓚": "赞",
    "揖": "一",
    "帥": "帅",
    "扈": "互",
    "诪": "周",
    "呤": "另",
    "汆": "窜",
    "疣": "由",
    "亜": "亚",
    "軸": "轴",
    "昴": "帽",
    "箸": "助",
    "鑳": "见",
    "濕": "诗",
    "賓": "彬",
    "膣": "至",
    "瀨": "赖",
    "偉": "伟",
    "珺": "俊",
    "亓": "其",
    "啓": "起",
    "沣": "风",
    "澹": "蛋",
    "曳": "夜",
    "蒯": "快",
    "痂": "家",
    "爛": "烂",
    "階": "接",
    "彧": "玉",
    "蜃": "肾",
    "噙": "琴",
    "褉": "谢",
    "鄧": "凳",
    "壽": "瘦",
    "鷄": "机",
    "郗": "西",
    "偎": "微",
    "鈴": "零",
    "衿": "金",
    "俟": "其",
    "劑": "记",
    "賲": "宝",
    "蟲": "虫",
    "広": "广",
    "囍": "洗",
    "洵": "寻",
    "競": "静",
    "槃": "盘",
    "鶴": "贺",
    "锗": "者",
    "潤": "润",
    "殒": "允",
    "苓": "零",
    "縮": "缩",
    "厜": "嘴",
    "禌": "姿",
    "铳": "冲",
    "嶺": "领",
    "醚": "迷",
    "菀": "碗",
    "鹽": "颜",
    "癡": "吃",
    "宥": "右",
    "註": "助",
    "濂": "连",
    "薨": "轰",
    "鲛": "教",
    "舂": "冲",
    "棟": "动",
    "颉": "节",
    "閑": "闲",
    "淺": "浅",
    "彜": "宜",
    "捱": "埃",
    "齡": "零",
    "銷": "消",
    "慘": "惨",
    "滛": "银",
    "荃": "全",
    "辇": "年",
    "錶": "表",
    "審": "审",
    "訊": "训",
    "繪": "会",
    "軌": "鬼",
    "吶": "那",
    "勁": "进",
    "貞": "真",
    "讘": "聂",
    "雒": "落",
    "锆": "告",
    "妣": "比",
    "潞": "路",
    "吠": "费",
    "魈": "消",
    "鄢": "烟",
    "栫": "见",
    "倆": "俩",
    "嚐": "长",
    "佈": "不",
    "陂": "杯",
    "虛": "需",
    "诲": "会",
    "銅": "同",
    "谒": "夜",
    "埴": "直",
    "瘀": "迂",
    "噘": "绝",
    "違": "维",
    "鄌": "唐",
    "膷": "相",
    "骊": "离",
    "囯": "国",
    "暫": "赞",
    "鉄": "直",
    "処": "楚",
    "拚": "判",
    "錫": "西",
    "郦": "利",
    "嘢": "也",
    "閃": "闪",
    "仝": "同",
    "讛": "意",
    "潔": "节",
    "曞": "利",
    "濱": "彬",
    "耢": "烙",
    "邁": "卖",
    "鍾": "中",
    "讫": "气",
    "紹": "绍",
    "趺": "夫",
    "祎": "一",
    "緻": "至",
    "唿": "呼",
    "犲": "柴",
    "煳": "胡",
    "賵": "奉",
    "膠": "教",
    "噉": "蛋",
    "愬": "速",
    "恆": "横",
    "彥": "燕",
    "団": "团",
    "夥": "火",
    "锟": "昆",
    "琏": "脸",
    "綋": "红",
    "獻": "现",
    "戸": "互",
    "雜": "杂",
    "獅": "诗",
    "霧": "物",
    "憲": "现",
    "撃": "机",
    "贈": "赠",
    "飨": "想",
    "櫻": "应",
    "岬": "假",
    "卍": "万",
    "逄": "旁",
    "氳": "晕",
    "蒽": "恩",
    "祿": "路",
    "鐏": "尊",
    "遜": "训",
    "変": "变",
    "濯": "着",
    "擺": "摆",
    "垠": "银",
    "淨": "静",
    "煊": "宣",
    "軳": "袍",
    "旌": "精",
    "殁": "墨",
    "嶋": "导",
    "鍗": "提",
    "皋": "高",
    "骋": "成",
    "谞": "需",
    "譽": "玉",
    "蟻": "以",
    "搴": "前",
    "訴": "速",
    "鯉": "李",
    "蟿": "气",
    "獾": "欢",
    "菡": "汉",
    "丟": "丢",
    "偌": "弱",
    "嗄": "啊",
    "钺": "月",
    "崏": "民",
    "濉": "虽",
    "忿": "愤",
    "硌": "个",
    "璎": "应",
    "铄": "硕",
    "獒": "敖",
    "鋒": "风",
    "踞": "巨",
    "炀": "羊",
    "壯": "壮",
    "汙": "乌",
    "関": "关",
    "捨": "舍",
    "粵": "月",
    "琨": "昆",
    "颍": "影",
    "賜": "次",
    "夲": "掏",
    "郢": "影",
    "淚": "泪",
    "赊": "舍",
    "礎": "楚",
    "豊": "李",
    "夾": "家",
    "芈": "米",
    "穩": "稳",
    "诰": "告",
    "翌": "意",
    "閉": "必",
    "鳴": "明",
    "鍋": "锅",
    "垅": "垄",
    "鎖": "锁",
    "潃": "修",
    "缦": "慢",
    "爐": "芦",
    "嗟": "接",
    "縱": "宗",
    "摺": "哲",
    "鋪": "瀑",
    "昰": "是",
    "箓": "路",
    "澧": "李",
    "馥": "父",
    "侗": "动",
    "歩": "不",
    "毎": "美",
    "瑨": "进",
    "秆": "感",
    "姒": "四",
    "椽": "传",
    "蹤": "宗",
    "鹹": "闲",
    "畦": "其",
    "挿": "插",
    "掣": "撤",
    "綾": "零",
    "鏉": "瘦",
    "軒": "宣",
    "芠": "文",
    "齋": "摘",
    "岄": "月",
    "昝": "赞",
    "潛": "钱",
    "伢": "牙",
    "鐪": "鲁",
    "蕃": "翻",
    "恥": "尺",
    "討": "讨",
    "厠": "册",
    "箩": "罗",
    "彡": "山",
    "弁": "变",
    "貢": "共",
    "鴻": "红",
    "氬": "亚",
    "墻": "强",
    "芾": "费",
    "亳": "博",
    "鸩": "镇",
    "牆": "强",
    "譚": "谈",
    "鲧": "滚",
    "铖": "成",
    "氡": "东",
    "蕞": "最",
    "汚": "乌",
    "缨": "应",
    "埙": "熏",
    "饷": "想",
    "闊": "扩",
    "铱": "一",
    "钤": "钱",
    "漲": "长",
    "擢": "着",
    "殼": "咳",
    "饧": "唐",
    "枳": "指",
    "埂": "梗",
    "賊": "贼",
    "働": "动",
    "哂": "审",
    "鬲": "格",
    "弋": "意",
    "斫": "着",
    "瞇": "眯",
    "槊": "硕",
    "託": "拖",
    "嗬": "喝",
    "龜": "归",
    "锶": "思",
    "潵": "洒",
    "唸": "念",
    "杈": "插",
    "箕": "机",
    "崥": "皮",
    "濆": "坟",
    "繫": "系",
    "騰": "腾",
    "銘": "明",
    "蚩": "吃",
    "椤": "罗",
    "隗": "奎",
    "笅": "角",
    "祕": "密",
    "滾": "滚",
    "怛": "达",
    "檫": "茶",
    "療": "聊",
    "稅": "睡",
    "韻": "运",
    "皎": "角",
    "鍊": "练",
    "総": "总",
    "渑": "免",
    "铵": "俺",
    "椹": "肾",
    "舫": "访",
    "盜": "到",
    "遲": "持",
    "龐": "旁",
    "騙": "片",
    "煩": "烦",
    "溝": "勾",
    "俅": "求",
    "缐": "现",
    "飄": "飘",
    "颚": "恶",
    "雎": "居",
    "泗": "四",
    "賳": "灾",
    "褰": "前",
    "閒": "闲",
    "驗": "燕",
    "酆": "风",
    "炝": "呛",
    "犇": "奔",
    "猷": "由",
    "籌": "愁",
    "胫": "静",
    "塵": "陈",
    "荻": "敌",
    "暝": "明",
    "訾": "姿",
    "牒": "叠",
    "蟽": "达",
    "屍": "诗",
    "餌": "耳",
    "舖": "瀑",
    "铌": "尼",
    "劭": "绍",
    "熘": "溜",
    "盂": "鱼",
    "燮": "谢",
    "呔": "呆",
    "悅": "月",
    "尕": "尬",
    "彌": "迷",
    "獵": "裂",
    "恙": "样",
    "烃": "听",
    "涎": "闲",
    "岙": "奥",
    "貉": "豪",
    "铉": "炫",
    "崗": "岗",
    "鋁": "旅",
    "豕": "使",
    "蠻": "瞒",
    "俠": "侠",
    "桢": "真",
    "讗": "鞋",
    "镗": "汤",
    "埸": "意",
    "萼": "恶",
    "酔": "最",
    "餵": "位",
    "嬌": "教",
    "擋": "挡",
    "損": "损",
    "钽": "坦",
    "髯": "然",
    "幣": "必",
    "紒": "记",
    "囡": "喃",
    "淖": "闹",
    "覽": "懒",
    "氖": "奶",
    "轲": "科",
    "虬": "求",
    "鍥": "妾",
    "衾": "亲",
    "璜": "黄",
    "嵘": "容",
    "隣": "林",
    "揿": "亲",
    "拋": "抛",
    "峁": "帽",
    "莅": "利",
    "勬": "捐",
    "沱": "驮",
    "嗮": "赛",
    "绺": "柳",
    "甑": "赠",
    "鍵": "见",
    "篑": "溃",
    "迴": "回",
    "憩": "气",
    "図": "图",
    "証": "挣",
    "铟": "因",
    "愮": "摇",
    "粿": "果",
    "衹": "指",
    "牽": "前",
    "巿": "福",
    "棲": "七",
    "橾": "书",
    "晩": "碗",
    "録": "路",
    "豺": "柴",
    "璨": "灿",
    "诐": "必",
    "洺": "明",
    "颢": "号",
    "鬧": "闹",
    "艖": "插",
    "賺": "赚",
    "婀": "阿",
    "淦": "干",
    "绡": "消",
    "迳": "静",
    "悶": "闷",
    "艷": "燕",
    "勳": "熏",
    "蜇": "遮",
    "蒼": "仓",
    "笣": "包",
    "欤": "鱼",
    "嬲": "尿",
    "旸": "羊",
    "売": "卖",
    "蓦": "墨",
    "喪": "丧",
    "褔": "父",
    "颔": "汉",
    "苒": "染",
    "隠": "引",
    "觞": "伤",
    "兗": "眼",
    "弾": "蛋",
    "嬬": "如",
    "泑": "优",
    "姘": "拼",
    "琮": "从",
    "廋": "搜",
    "阍": "昏",
    "椋": "良",
    "嗕": "入",
    "囩": "云",
    "纾": "书",
    "絵": "会",
    "鐝": "觉",
    "祙": "妹",
    "堀": "哭",
    "铋": "必",
    "桉": "安",
    "礦": "矿",
    "鐫": "捐",
    "祚": "做",
    "幇": "帮",
    "滦": "鸾",
    "仞": "任",
    "鎻": "锁",
    "阕": "确",
    "彲": "吃",
    "傾": "青",
    "箬": "弱",
    "熶": "窜",
    "皴": "村",
    "馊": "搜",
    "湄": "梅",
    "骐": "其",
    "镓": "家",
    "靛": "电",
    "悪": "恶",
    "徕": "来",
    "鯰": "年",
    "駅": "意",
    "龋": "曲",
    "筵": "颜",
    "洙": "朱",
    "犼": "吼",
    "廿": "念",
    "霭": "矮",
    "鲎": "后",
    "芶": "狗",
    "呯": "平",
    "塗": "图",
    "妝": "装",
    "鹄": "古",
    "佚": "意",
    "嘁": "七",
    "樑": "良",
    "薦": "见",
    "嗚": "乌",
    "勸": "劝",
    "楮": "楚",
    "砣": "驮",
    "葳": "微",
    "棰": "垂",
    "舷": "闲",
    "菅": "间",
    "誘": "右",
    "夊": "虽",
    "頗": "坡",
    "潼": "同",
    "畈": "饭",
    "琬": "碗",
    "佸": "活",
    "邺": "夜",
    "姵": "配",
    "郵": "由",
    "儲": "楚",
    "鑑": "见",
    "旃": "占",
    "躍": "月",
    "珉": "民",
    "皬": "和",
    "苡": "以",
    "贻": "宜",
    "鹜": "物",
    "駕": "驾",
    "馔": "赚",
    "濛": "盟",
    "仺": "仓",
    "鍔": "恶",
    "遞": "地",
    "鈺": "玉",
    "災": "灾",
    "偸": "偷",
    "邕": "庸",
    "诤": "挣",
    "蜱": "皮",
    "砾": "利",
    "禛": "真",
    "翳": "意",
    "潋": "练",
    "钚": "不",
    "醴": "李",
    "紐": "纽",
    "殳": "书",
    "偃": "眼",
    "碴": "茶",
    "猶": "由",
    "偽": "伟",
    "嚌": "记",
    "谂": "审",
    "蜞": "其",
    "刪": "山",
    "婳": "话",
    "镒": "意",
    "慮": "绿",
    "菏": "和",
    "癥": "睁",
    "褍": "端",
    "樺": "话",
    "唵": "俺",
    "沤": "欧",
    "恣": "字",
    "欽": "亲",
    "栎": "利",
    "堜": "练",
    "潆": "营",
    "掃": "扫",
    "鎏": "刘",
    "汨": "密",
    "傝": "探",
    "繚": "聊",
    "唎": "利",
    "瀵": "愤",
    "賮": "进",
    "墜": "缀",
    "峒": "动",
    "曌": "照",
    "萘": "耐",
    "剋": "客",
    "鼐": "耐",
    "睑": "减",
    "楹": "营",
    "乗": "成",
    "讞": "燕",
    "謎": "迷",
    "鏃": "族",
    "魑": "吃",
    "卮": "知",
    "輩": "被",
    "賭": "堵",
    "菸": "烟",
    "醬": "降",
    "淵": "冤",
    "铯": "色",
    "煉": "练",
    "蔵": "脏",
    "杓": "标",
    "燊": "深",
    "渌": "路",
    "哋": "叠",
    "琚": "居",
    "榀": "品",
    "襹": "诗",
    "虢": "国",
    "瑗": "院",
    "睢": "虽",
    "繳": "角",
    "诘": "极",
    "罵": "骂",
    "铍": "批",
    "埄": "崩",
    "膑": "宾",
    "挶": "居",
    "鱸": "芦",
    "遙": "摇",
    "粈": "柔",
    "擾": "扰",
    "擔": "单",
    "罷": "爸",
    "潰": "溃",
    "炅": "窘",
    "诓": "框",
    "戕": "枪",
    "恔": "角",
    "壘": "垒",
    "遽": "巨",
    "鴨": "呀",
    "邴": "饼",
    "硪": "卧",
    "纭": "云",
    "虱": "诗",
    "揮": "灰",
    "狍": "袍",
    "堑": "欠",
    "廈": "煞",
    "囬": "回",
    "澍": "树",
    "诳": "狂",
    "飏": "羊",
    "磚": "专",
    "脳": "脑",
    "枋": "方",
    "浄": "静",
    "囿": "右",
    "讙": "欢",
    "眞": "真",
    "搾": "咋",
    "钭": "偷",
    "慳": "前",
    "谯": "桥",
    "倏": "书",
    "凪": "指",
    "唛": "骂",
    "缶": "否",
    "痈": "庸",
    "脣": "纯",
    "縛": "父",
    "憂": "优",
    "糧": "良",
    "螯": "敖",
    "畼": "唱",
    "翦": "减",
    "溼": "诗",
    "鐣": "称",
    "娌": "李",
    "釐": "离",
    "妫": "归",
    "蟺": "善",
    "墦": "烦",
    "钕": "女",
    "玑": "机",
    "钏": "串",
    "卩": "节",
    "堥": "毛",
    "氰": "情",
    "谗": "缠",
    "銳": "瑞",
    "璩": "取",
    "尐": "节",
    "樞": "书",
    "寳": "宝",
    "巖": "颜",
    "貸": "带",
    "绦": "掏",
    "銇": "泪",
    "疝": "善",
    "铑": "老",
    "膽": "胆",
    "詠": "永",
    "曬": "筛",
    "粲": "灿",
    "栀": "知",
    "钎": "前",
    "疃": "团",
    "愽": "博",
    "窮": "穷",
    "窩": "窝",
    "铡": "炸",
    "暢": "唱",
    "剎": "沙",
    "鵬": "朋",
    "搵": "问",
    "湧": "永",
    "笆": "八",
    "緩": "缓",
    "偻": "楼",
    "邈": "秒",
    "衞": "位",
    "郃": "和",
    "钷": "坡",
    "胄": "昼",
    "晞": "西",
    "泮": "判",
    "滁": "除",
    "霰": "现",
    "桁": "横",
    "掼": "灌",
    "闖": "闯",
    "撥": "波",
    "呖": "利",
    "嚇": "下",
    "棹": "照",
    "怏": "样",
    "埭": "带",
    "絡": "落",
    "镞": "族",
    "癒": "玉",
    "洌": "裂",
    "蹚": "汤",
    "収": "收",
    "涞": "来",
    "亻": "人",
    "綜": "宗",
    "懸": "旋",
    "燚": "意",
    "蓼": "了",
    "洇": "因",
    "痢": "利",
    "庢": "至",
    "濋": "楚",
    "帯": "带",
    "咝": "思",
    "罰": "罚",
    "彎": "弯",
    "讟": "毒",
    "阡": "前",
    "鍜": "侠",
    "膻": "山",
    "颡": "桑",
    "赓": "耕",
    "詳": "翔",
    "槎": "茶",
    "桷": "觉",
    "懋": "帽",
    "冫": "冰",
    "秸": "接",
    "醮": "教",
    "氽": "吞",
    "讴": "欧",
    "兂": "赞",
    "嵐": "蓝",
    "鸷": "至",
    "郞": "狼",
    "囉": "罗",
    "魃": "拔",
    "鼋": "原",
    "籠": "龙",
    "铧": "华",
    "撲": "铺",
    "擁": "庸",
    "莤": "速",
    "畲": "舍",
    "晌": "赏",
    "姮": "横",
    "儇": "宣",
    "澪": "零",
    "壅": "庸",
    "驅": "区",
    "禪": "缠",
    "靥": "夜",
    "詔": "照",
    "嗰": "葛",
    "鬃": "宗",
    "尓": "耳",
    "亟": "极",
    "糁": "三",
    "擤": "醒",
    "竜": "龙",
    "羸": "雷",
    "錾": "赞",
    "瑠": "刘",
    "湊": "凑",
    "楣": "梅",
    "巜": "快",
    "苫": "山",
    "芃": "朋",
    "罒": "网",
    "僮": "同",
    "哞": "某",
    "宀": "眠",
    "単": "单",
    "鎯": "狼",
    "媞": "是",
    "謙": "前",
    "愍": "敏",
    "虺": "灰",
    "鑰": "要",
    "樘": "唐",
    "沵": "米",
    "骅": "华",
    "菓": "果",
    "锝": "德",
    "醯": "西",
    "凍": "动",
    "钹": "博",
    "遼": "聊",
    "悌": "替",
    "餅": "饼",
    "瞋": "陈",
    "矽": "系",
    "蹟": "机",
    "奂": "换",
    "楓": "风",
    "撿": "减",
    "峤": "教",
    "佃": "电",
    "墒": "伤",
    "倖": "性",
    "蠍": "些",
    "眀": "明",
    "漿": "将",
    "檯": "台",
    "羣": "群",
    "疊": "叠",
    "佺": "全",
    "扦": "前",
    "蕈": "训",
    "吽": "轰",
    "嘆": "探",
    "涪": "福",
    "賤": "见",
    "珥": "耳",
    "孛": "被",
    "簋": "鬼",
    "苕": "勺",
    "鎰": "意",
    "藐": "秒",
    "氱": "养",
    "涸": "和",
    "撫": "辅",
    "閽": "昏",
    "畝": "母",
    "芩": "琴",
    "嘗": "长",
    "猹": "茶",
    "彘": "至",
    "聰": "聪",
    "淸": "青",
    "泫": "炫",
    "観": "关",
    "醜": "丑",
    "桅": "维",
    "瑷": "爱",
    "莜": "由",
    "庣": "挑",
    "癿": "茄",
    "氚": "穿",
    "眦": "字",
    "堃": "昆",
    "篙": "高",
    "瘋": "风",
    "淙": "从",
    "耒": "垒",
    "煅": "断",
    "穂": "岁",
    "鉆": "搀",
    "凜": "吝",
    "鐸": "夺",
    "獩": "会",
    "乄": "五",
    "阗": "田",
    "罂": "应",
    "蘅": "横",
    "啱": "颜",
    "瑭": "唐",
    "芪": "其",
    "屼": "物",
    "鬚": "需",
    "洧": "伟",
    "傢": "家",
    "阇": "督",
    "閘": "炸",
    "搖": "摇",
    "氘": "刀",
    "逑": "求",
    "镬": "或",
    "娛": "鱼",
    "嫒": "爱",
    "縫": "奉",
    "傘": "三",
    "嵋": "梅",
    "焓": "含",
    "蠀": "刺",
    "蠡": "离",
    "湟": "黄",
    "逯": "路",
    "袂": "妹",
    "耆": "其",
    "鉅": "巨",
    "褟": "他",
    "妗": "进",
    "綿": "眠",
    "賦": "父",
    "穔": "黄",
    "谝": "偏",
    "邙": "忙",
    "炔": "贵",
    "琇": "秀",
    "钅": "金",
    "篤": "堵",
    "嵴": "几",
    "穀": "古",
    "両": "两",
    "彙": "会",
    "铨": "全",
    "畑": "田",
    "祜": "互",
    "涙": "泪",
    "撷": "鞋",
    "牍": "毒",
    "刂": "刀",
    "夐": "胸",
    "吔": "也",
    "貘": "墨",
    "啻": "赤",
    "鎷": "马",
    "締": "地",
    "鏁": "锁",
    "舄": "系",
    "稔": "忍",
    "児": "而",
    "懶": "懒",
    "忤": "五",
    "貪": "贪",
    "艙": "仓",
    "铊": "他",
    "鱗": "林",
    "隷": "利",
    "蠹": "度",
    "骜": "奥",
    "砦": "寨",
    "垌": "动",
    "涐": "额",
    "湍": "团",
    "埕": "成",
    "镧": "蓝",
    "獐": "张",
    "綁": "绑",
    "铼": "来",
    "脩": "修",
    "岷": "民",
    "効": "笑",
    "駿": "俊",
    "廚": "除",
    "褘": "灰",
    "嫱": "强",
    "刈": "意",
    "孺": "如",
    "恚": "会",
    "灘": "贪",
    "貧": "贫",
    "綦": "其",
    "爨": "窜",
    "闂": "红",
    "顼": "需",
    "摀": "五",
    "滃": "瓮",
    "閿": "文",
    "溧": "利",
    "獣": "瘦",
    "紛": "分",
    "竦": "耸",
    "菽": "书",
    "莒": "举",
    "婐": "我",
    "畿": "机",
    "蚬": "显",
    "峋": "寻",
    "栢": "摆",
    "隸": "利",
    "缗": "民",
    "孓": "觉",
    "阆": "狼",
    "泺": "落",
    "霈": "配",
    "忝": "舔",
    "兖": "眼",
    "饯": "见",
    "戯": "呼",
    "呰": "子",
    "甙": "带",
    "嫚": "慢",
    "綴": "缀",
    "镄": "费",
    "癞": "赖",
    "迨": "带",
    "绔": "裤",
    "腆": "舔",
    "肅": "速",
    "臥": "卧",
    "圻": "其",
    "瘘": "漏",
    "屢": "旅",
    "酢": "促",
    "辻": "石",
    "擴": "扩",
    "撳": "亲",
    "従": "从",
    "憶": "意",
    "鯨": "精",
    "貿": "帽",
    "珣": "寻",
    "嵬": "维",
    "碩": "硕",
    "艶": "燕",
    "壩": "爸",
    "穎": "影",
    "曷": "和",
    "銜": "闲",
    "硐": "动",
    "韫": "运",
    "寛": "宽",
    "颀": "其",
    "谟": "魔",
    "滟": "燕",
    "笄": "机",
    "碁": "其",
    "閥": "罚",
    "缑": "勾",
    "鍝": "鱼",
    "钌": "了",
    "苌": "长",
    "豢": "换",
    "閩": "敏",
    "骶": "底",
    "棂": "零",
    "鹞": "要",
    "鑒": "见",
    "瓊": "穷",
    "儍": "傻",
    "愠": "运",
    "屙": "阿",
    "孖": "妈",
    "钿": "电",
    "兲": "天",
    "锷": "恶",
    "轭": "恶",
    "珪": "归",
    "礡": "博",
    "叕": "着",
    "丌": "机",
    "壺": "胡",
    "蛭": "至",
    "墎": "锅",
    "帏": "维",
    "徇": "训",
    "詢": "寻",
    "笪": "达",
    "吥": "不",
    "殛": "极",
    "杲": "搞",
    "滬": "互",
    "診": "枕",
    "隹": "追",
    "磙": "滚",
    "堣": "鱼",
    "乺": "锁",
    "苋": "现",
    "雹": "薄",
    "啶": "定",
    "繇": "摇",
    "凫": "福",
    "抟": "团",
    "沄": "云",
    "翕": "西",
    "玎": "丁",
    "鏍": "罗",
    "僑": "桥",
    "虧": "亏",
    "篁": "黄",
    "瑀": "雨",
    "嵊": "胜",
    "柰": "耐",
    "婺": "物",
    "洹": "环",
    "喰": "参",
    "钇": "以",
    "镊": "聂",
    "褲": "裤",
    "腓": "肥",
    "蕲": "其",
    "檄": "习",
    "笉": "寝",
    "陟": "至",
    "珲": "灰",
    "矇": "盟",
    "讻": "胸",
    "佗": "驮",
    "轸": "枕",
    "垴": "脑",
    "崧": "松",
    "揾": "问",
    "铈": "是",
    "渾": "魂",
    "丼": "井",
    "尛": "魔",
    "頌": "宋",
    "凼": "荡",
    "挛": "鸾",
    "鍖": "尘",
    "吡": "比",
    "挈": "妾",
    "佾": "意",
    "黱": "带",
    "俎": "组",
    "赭": "者",
    "趨": "区",
    "嬭": "奶",
    "撐": "称",
    "磯": "机",
    "臟": "藏",
    "滄": "仓",
    "薜": "必",
    "觚": "姑",
    "旖": "以",
    "艏": "手",
    "苻": "福",
    "経": "精",
    "獭": "塔",
    "纏": "缠",
    "纔": "才",
    "鑾": "鸾",
    "赟": "晕",
    "亍": "处",
    "芫": "颜",
    "呓": "意",
    "蓁": "真",
    "馳": "持",
    "禍": "或",
    "扞": "感",
    "帳": "帐",
    "氙": "先",
    "缈": "秒",
    "擘": "掰",
    "庖": "袍",
    "骧": "相",
    "訣": "觉",
    "墊": "电",
    "瀧": "龙",
    "嫰": "嫩",
    "刖": "月",
    "禦": "玉",
    "姩": "念",
    "尅": "客",
    "剣": "见",
    "醅": "培",
    "锕": "啊",
    "圪": "歌",
    "雛": "除",
    "鑲": "相",
    "繩": "生",
    "玠": "借",
    "掾": "院",
    "倌": "关",
    "笏": "互",
    "螫": "是",
    "茆": "毛",
    "瑁": "帽",
    "疽": "居",
    "県": "现",
    "擇": "则",
    "頃": "请",
    "绶": "瘦",
    "舛": "喘",
    "唌": "闲",
    "偵": "真",
    "欄": "蓝",
    "谪": "哲",
    "転": "转",
    "恽": "运",
    "谶": "衬",
    "褜": "袍",
    "恸": "痛",
    "萁": "其",
    "娈": "鸾",
    "飽": "宝",
    "饴": "宜",
    "丷": "八",
    "孃": "娘",
    "紮": "匝",
    "綉": "透",
    "羟": "抢",
    "鎬": "号",
    "鍟": "生",
    "囘": "回",
    "賬": "帐",
    "鄞": "银",
    "钍": "土",
    "闢": "屁",
    "錘": "垂",
    "郏": "夹",
    "脘": "碗",
    "鳅": "秋",
    "靚": "静",
    "楦": "炫",
    "膩": "逆",
    "辯": "变",
    "岢": "可",
    "鈈": "批",
    "燧": "岁",
    "債": "寨",
    "楸": "秋",
    "曠": "矿",
    "娚": "南",
    "龇": "姿",
    "绀": "干",
    "剡": "善",
    "慣": "灌",
    "腸": "长",
    "傩": "挪",
    "曈": "同",
    "蝾": "容",
    "歴": "利",
    "帰": "归",
    "磔": "哲",
    "郧": "云",
    "侶": "旅",
    "漁": "鱼",
    "佶": "极",
    "冨": "父",
    "臌": "古",
    "醪": "劳",
    "樯": "强",
    "魍": "网",
    "槸": "意",
    "佹": "鬼",
    "蝦": "虾",
    "鯛": "雕",
    "逅": "后",
    "鸫": "东",
    "鹳": "灌",
    "硷": "减",
    "嚩": "魔",
    "篠": "小",
    "锲": "妾",
    "沭": "树",
    "嚥": "燕",
    "乂": "意",
    "矬": "搓",
    "锘": "诺",
    "诮": "巧",
    "绠": "梗",
    "庠": "翔",
    "餓": "恶",
    "稜": "棱",
    "祉": "指",
    "掴": "乖",
    "噁": "饿",
    "薬": "要",
    "厩": "就",
    "籤": "前",
    "囱": "聪",
    "萸": "鱼",
    "読": "毒",
    "囹": "零",
    "擠": "几",
    "颏": "科",
    "繆": "谋",
    "栄": "容",
    "綺": "起",
    "穰": "嚷",
    "疔": "丁",
    "莼": "纯",
    "凃": "图",
    "叢": "从",
    "镝": "低",
    "鞑": "达",
    "蹼": "普",
    "槬": "话",
    "償": "长",
    "簟": "电",
    "腈": "精",
    "庹": "妥",
    "嗵": "通",
    "鎺": "组",
    "粍": "哲",
    "蘋": "平",
    "窈": "咬",
    "郄": "妾",
    "钲": "睁",
    "屾": "深",
    "旎": "你",
    "桡": "扰",
    "鹬": "玉",
    "暈": "晕",
    "蘿": "罗",
    "艂": "逢",
    "赉": "赖",
    "聶": "聂",
    "嶈": "枪",
    "瞑": "明",
    "焐": "物",
    "鏅": "修",
    "踵": "种",
    "滓": "子",
    "楗": "见",
    "巔": "颠",
    "碓": "对",
    "琊": "牙",
    "碛": "气",
    "嫵": "五",
    "庡": "以",
    "釣": "掉",
    "钬": "火",
    "厳": "颜",
    "冦": "扣",
    "襦": "如",
    "卝": "矿",
    "雫": "哪",
    "諡": "是",
    "苆": "切",
    "訷": "深",
    "咭": "机",
    "璘": "林",
    "喚": "换",
    "锨": "先",
    "惇": "蹲",
    "緑": "绿",
    "氭": "东",
    "玚": "唱",
    "柞": "咋",
    "锴": "凯",
    "毖": "必",
    "馐": "修",
    "湁": "赤",
    "鄣": "张",
    "儆": "井",
    "芘": "皮",
    "俚": "李",
    "埝": "念",
    "喑": "因",
    "觥": "工",
    "囟": "信",
    "肓": "荒",
    "蠛": "灭",
    "黜": "处",
    "歙": "设",
    "狹": "侠",
    "鬻": "玉",
    "亀": "归",
    "柢": "底",
    "誕": "蛋",
    "啮": "聂",
    "谰": "蓝",
    "亊": "是",
    "佽": "次",
    "亅": "觉",
    "峇": "八",
    "闩": "拴",
    "棢": "网",
    "圧": "呀",
    "樁": "装",
    "觪": "星",
    "夋": "群",
    "鋆": "云",
    "瀰": "迷",
    "呶": "脑",
    "氺": "水",
    "葯": "要",
    "圏": "圈",
    "螟": "明",
    "乀": "福",
    "攜": "鞋",
    "仵": "五",
    "粳": "精",
    "郅": "至",
    "爿": "盘",
    "醣": "唐",
    "濤": "掏",
    "荑": "提",
    "沨": "风",
    "臬": "聂",
    "垱": "荡",
    "榷": "确",
    "啭": "赚",
    "鐩": "岁",
    "钃": "竹",
    "膚": "夫",
    "潺": "缠",
    "跖": "直",
    "薙": "替",
    "闳": "红",
    "玳": "带",
    "铷": "如",
    "腴": "鱼",
    "幵": "间",
    "呒": "五",
    "鹗": "恶",
    "蟬": "缠",
    "戢": "极",
    "荇": "性",
    "欑": "攒",
    "誅": "朱",
    "鍘": "炸",
    "雠": "愁",
    "趐": "血",
    "陞": "生",
    "滘": "教",
    "攤": "贪",
    "値": "直",
    "槑": "梅",
    "牸": "字",
    "塩": "颜",
    "绗": "行",
    "賯": "胸",
    "笕": "减",
    "籃": "蓝",
    "寢": "寝",
    "垟": "羊",
    "鱷": "恶",
    "篦": "必",
    "鑻": "判",
    "俦": "愁",
    "眚": "省",
    "呋": "夫",
    "駒": "居",
    "胔": "字",
    "鑄": "助",
    "驷": "四",
    "楫": "极",
    "绌": "处",
    "漉": "路",
    "崝": "睁",
    "撚": "年",
    "唳": "利",
    "瑩": "营",
    "滝": "龙",
    "猊": "尼",
    "彀": "够",
    "焼": "烧",
    "笂": "完",
    "闘": "豆",
    "猗": "一",
    "嗐": "害",
    "叒": "弱",
    "凇": "松",
    "碲": "地",
    "菰": "姑",
    "堺": "借",
    "虻": "盟",
    "誼": "意",
    "珯": "老",
    "裾": "居",
    "憐": "连",
    "袢": "判",
    "姀": "和",
    "滙": "会",
    "検": "减",
    "溏": "唐",
    "髪": "发",
    "焘": "到",
    "槔": "高",
    "龅": "包",
    "魉": "两",
    "櫃": "贵",
    "拏": "拿",
    "脲": "尿",
    "缟": "搞",
    "媪": "袄",
    "厲": "利",
    "堯": "摇",
    "臘": "蜡",
    "兕": "四",
    "踅": "学",
    "箅": "必",
    "圉": "雨",
    "婠": "弯",
    "悭": "前",
    "敘": "续",
    "夂": "指",
    "褨": "锁",
    "笈": "极",
    "亾": "王",
    "鲢": "连",
    "賰": "春",
    "騒": "骚",
    "怿": "意",
    "稲": "到",
    "昐": "分",
    "樸": "普",
    "笳": "家",
    "疥": "借",
    "験": "燕",
    "箇": "个",
    "逋": "不",
    "欹": "一",
    "痾": "阿",
    "埵": "朵",
    "鵝": "额",
    "嗥": "豪",
    "砟": "眨",
    "誊": "腾",
    "茔": "营",
    "贇": "晕",
    "屮": "撤",
    "笞": "吃",
    "剝": "波",
    "闼": "踏",
    "鉛": "前",
    "钣": "板",
    "綝": "陈",
    "杪": "秒",
    "翀": "冲",
    "腭": "恶",
    "郤": "系",
    "僦": "就",
    "閱": "月",
    "钪": "抗",
    "鑿": "凿",
    "齣": "出",
    "舸": "葛",
    "汜": "四",
    "熷": "增",
    "崑": "昆",
    "僖": "西",
    "丄": "上",
    "厭": "燕",
    "諭": "玉",
    "枞": "聪",
    "烝": "睁",
    "嬰": "应",
    "咥": "系",
    "蚓": "引",
    "诜": "深",
    "溱": "琴",
    "犻": "博",
    "缄": "间",
    "鸮": "消",
    "篾": "灭",
    "菘": "松",
    "饒": "扰",
    "兘": "使",
    "唻": "赖",
    "鰻": "瞒",
    "璺": "问",
    "叡": "瑞",
    "劢": "卖",
    "湜": "石",
    "辶": "绰",
    "罅": "下",
    "窨": "熏",
    "檩": "吝",
    "罴": "皮",
    "邾": "朱",
    "黙": "墨",
    "鄒": "邹",
    "桖": "血",
    "婓": "飞",
    "缱": "浅",
    "荏": "忍",
    "洄": "回",
    "霂": "木",
    "睺": "喉",
    "頸": "井",
    "芰": "记",
    "掽": "碰",
    "蛏": "称",
    "琯": "管",
    "蔸": "兜",
    "倬": "捉",
    "覚": "觉",
    "烜": "选",
    "薮": "搜",
    "捰": "我",
    "眇": "秒",
    "阝": "父",
    "嶼": "雨",
    "峽": "侠",
    "纛": "到",
    "郷": "相",
    "绱": "上",
    "稟": "饼",
    "飼": "四",
    "嶄": "展",
    "彵": "妥",
    "勵": "利",
    "镲": "差",
    "豎": "树",
    "鄯": "善",
    "脅": "鞋",
    "崮": "固",
    "嗌": "爱",
    "鄱": "婆",
    "墀": "持",
    "颳": "瓜",
    "漯": "落",
    "蓑": "缩",
    "賠": "培",
    "疖": "接",
    "豈": "起",
    "绋": "福",
    "隈": "微",
    "洳": "入",
    "哙": "快",
    "鹂": "离",
    "瑤": "摇",
    "汊": "差",
    "麴": "区",
    "蜚": "飞",
    "絨": "容",
    "谉": "审",
    "釘": "丁",
    "勻": "云",
    "觴": "伤",
    "岘": "现",
    "骈": "偏",
    "壆": "学",
    "櫣": "连",
    "妪": "玉",
    "薹": "台",
    "鲈": "芦",
    "跶": "达",
    "坭": "尼",
    "姪": "直",
    "洮": "桃",
    "箧": "妾",
    "挹": "意",
    "囝": "减",
    "乩": "机",
    "芑": "起",
    "乚": "引",
    "睏": "困",
    "腫": "种",
    "偲": "猜",
    "荥": "行",
    "擲": "至",
    "蛔": "回",
    "讣": "父",
    "膦": "吝",
    "餮": "贴",
    "嶽": "月",
    "倨": "巨",
    "砭": "编",
    "牀": "床",
    "濞": "必",
    "謹": "紧",
    "腎": "肾",
    "圌": "传",
    "苜": "木",
    "肱": "工",
    "胴": "动",
    "纟": "思",
    "犽": "亚",
    "肼": "井",
    "酊": "丁",
    "綏": "虽",
    "籐": "腾",
    "渉": "设",
    "缯": "增",
    "悆": "玉",
    "锔": "居",
    "欏": "罗",
    "謇": "减",
    "橐": "驮",
    "霎": "煞",
    "殄": "舔",
    "禇": "者",
    "鉤": "勾",
    "噠": "答",
    "郇": "环",
    "蕖": "取",
    "戆": "杠",
    "勭": "同",
    "勖": "续",
    "婭": "亚",
    "咛": "凝",
    "燦": "灿",
    "肟": "卧",
    "抺": "妹",
    "弭": "米",
    "趸": "蹲",
    "蝕": "石",
    "浞": "着",
    "臎": "翠",
    "垵": "俺",
    "笱": "狗",
    "邛": "穷",
    "応": "应",
    "墮": "堕",
    "墳": "坟",
    "擞": "搜",
    "蚺": "然",
    "绐": "带",
    "髁": "科",
    "蹁": "偏",
    "簾": "连",
    "厶": "思",
    "頼": "赖",
    "苄": "变",
    "噤": "进",
    "鈞": "君",
    "蓖": "必",
    "廻": "回",
    "臵": "格",
    "慄": "利",
    "紘": "红",
    "柩": "就",
    "垩": "恶",
    "蠅": "营",
    "冏": "窘",
    "槭": "七",
    "幷": "病",
    "緝": "机",
    "纡": "迂",
    "鷲": "就",
    "迤": "宜",
    "芏": "度",
    "昉": "访",
    "沔": "免",
    "叵": "坡",
    "郯": "谈",
    "桠": "呀",
    "翹": "巧",
    "忄": "心",
    "驽": "努",
    "埧": "巨",
    "佥": "前",
    "蕤": "瑞",
    "笥": "四",
    "楃": "卧",
    "颞": "聂",
    "骓": "追",
    "嗪": "琴",
    "旒": "刘",
    "岜": "八",
    "偣": "烟",
    "禳": "嚷",
    "恫": "动",
    "権": "全",
    "匚": "方",
    "愆": "前",
    "牖": "有",
    "钐": "山",
    "鲟": "寻",
    "栞": "看",
    "诌": "周",
    "仉": "长",
    "坼": "撤",
    "鯊": "沙",
    "販": "饭",
    "硎": "行",
    "赍": "机",
    "洎": "记",
    "杌": "物",
    "鈔": "超",
    "遴": "林",
    "圜": "环",
    "齢": "零",
    "貳": "二",
    "瘿": "影",
    "镕": "容",
    "呭": "意",
    "憿": "角",
    "芎": "琼",
    "絞": "角",
    "頜": "和",
    "肫": "准",
    "颛": "专",
    "訳": "意",
    "鬱": "玉",
    "醌": "昆",
    "哝": "农",
    "紡": "访",
    "尢": "由",
    "忖": "存",
    "炘": "心",
    "铒": "耳",
    "磴": "凳",
    "諧": "鞋",
    "缳": "环",
    "纥": "歌",
    "嚧": "芦",
    "貶": "扁",
    "窯": "摇",
    "嬗": "善",
    "郸": "单",
    "襠": "当",
    "崙": "伦",
    "彊": "降",
    "氿": "鬼",
    "幛": "帐",
    "绻": "犬",
    "骢": "聪",
    "镛": "庸",
    "孀": "双",
    "粄": "板",
    "裨": "必",
    "荪": "孙",
    "礙": "爱",
    "胪": "芦",
    "戞": "夹",
    "脿": "标",
    "逖": "替",
    "螅": "西",
    "鏀": "鲁",
    "蘆": "芦",
    "萆": "必",
    "吚": "一",
    "麸": "夫",
    "矍": "觉",
    "燔": "烦",
    "浥": "意",
    "碣": "节",
    "椟": "毒",
    "鞯": "间",
    "荠": "记",
    "镫": "凳",
    "堍": "兔",
    "拰": "您",
    "芣": "福",
    "啫": "者",
    "揸": "扎",
    "狎": "侠",
    "咤": "咋",
    "郓": "运",
    "傛": "永",
    "悩": "脑",
    "呑": "吞",
    "攞": "罗",
    "菟": "图",
    "洰": "巨",
    "崆": "空",
    "屐": "机",
    "竽": "鱼",
    "嗾": "搜",
    "鳩": "纠",
    "俿": "虎",
    "艸": "草",
    "豸": "至",
    "攔": "蓝",
    "绂": "福",
    "儷": "利",
    "衲": "那",
    "苘": "请",
    "砜": "风",
    "洐": "行",
    "骖": "参",
    "琍": "离",
    "瘥": "拆",
    "佞": "宁",
    "彳": "赤",
    "龘": "达",
    "砀": "荡",
    "庝": "同",
    "盍": "和",
    "烀": "呼",
    "峄": "意",
    "穑": "色",
    "阊": "昌",
    "臋": "吞",
    "硖": "侠",
    "嗫": "聂",
    "苴": "居",
    "龔": "工",
    "誇": "夸",
    "槓": "杠",
    "愪": "云",
    "呙": "锅",
    "莛": "停",
    "鏆": "灌",
    "卣": "有",
    "聃": "单",
    "閻": "颜",
    "滀": "处",
    "怙": "互",
    "诒": "宜",
    "喫": "吃",
    "缂": "客",
    "枱": "台",
    "镱": "意",
    "薫": "熏",
    "讵": "巨",
    "橛": "觉",
    "蚧": "借",
    "蘊": "运",
    "庑": "五",
    "飧": "孙",
    "瑣": "锁",
    "惘": "网",
    "淪": "伦",
    "汅": "免",
    "儋": "单",
    "醭": "哺",
    "蟮": "善",
    "撘": "答",
    "癍": "班",
    "矸": "干",
    "苼": "生",
    "嘌": "票",
    "杼": "助",
    "婁": "楼",
    "铙": "脑",
    "荭": "红",
    "恏": "号",
    "捭": "摆",
    "羨": "现",
    "痩": "瘦",
    "缡": "离",
    "姞": "极",
    "閔": "敏",
    "棗": "早",
    "谡": "速",
    "糞": "愤",
    "颯": "萨",
    "緬": "免",
    "舁": "鱼",
    "敓": "夺",
    "谲": "觉",
    "龊": "绰",
    "艱": "间",
    "炆": "文",
    "庋": "鬼",
    "鈎": "勾",
    "逕": "静",
    "箜": "空",
    "荊": "精",
    "蠃": "裸",
    "稗": "败",
    "湉": "田",
    "婼": "绰",
    "蟼": "井",
    "蚶": "憨",
    "岽": "东",
    "眭": "虽",
    "塆": "弯",
    "呮": "气",
    "崁": "看",
    "斿": "由",
    "慊": "欠",
    "窉": "饼",
    "噂": "尊",
    "碇": "定",
    "楝": "练",
    "鱂": "将",
    "鈣": "概",
    "憣": "翻",
    "酽": "燕",
    "毬": "求",
    "汎": "饭",
    "徼": "角",
    "瑯": "狼",
    "揆": "奎",
    "揺": "摇",
    "铘": "爷",
    "芗": "相",
    "觐": "进",
    "蛩": "穷",
    "荳": "豆",
    "鋸": "巨",
    "镹": "久",
    "霊": "零",
    "镋": "躺",
    "濁": "着",
    "閤": "格",
    "珙": "拱",
    "袪": "区",
    "蚨": "福",
    "榧": "匪",
    "軽": "至",
    "冂": "窘",
    "畹": "碗",
    "坻": "持",
    "怍": "做",
    "滷": "鲁",
    "锒": "狼",
    "囷": "群",
    "胱": "光",
    "耄": "帽",
    "娠": "深",
    "熸": "间",
    "孒": "觉",
    "冴": "互",
    "砻": "龙",
    "沩": "维",
    "攵": "铺",
    "澀": "色",
    "怆": "创",
    "煥": "换",
    "氷": "冰",
    "侪": "柴",
    "弔": "掉",
    "妘": "云",
    "奁": "连",
    "鰓": "塞",
    "栉": "至",
    "啉": "林",
    "贠": "原",
    "濬": "俊",
    "荘": "装",
    "鼍": "驮",
    "俷": "费",
    "堛": "必",
    "萩": "秋",
    "蔭": "因",
    "髡": "昆",
    "鋈": "物",
    "鈪": "恶",
    "忕": "是",
    "铕": "有",
    "瘁": "翠",
    "嫑": "薄",
    "絆": "办",
    "帙": "至",
    "藁": "搞",
    "芴": "物",
    "済": "记",
    "觳": "胡",
    "黉": "红",
    "渃": "弱",
    "纖": "先",
    "贅": "缀",
    "埗": "不",
    "茭": "教",
    "鋅": "心",
    "搿": "格",
    "虜": "鲁",
    "鹕": "胡",
    "戜": "叠",
    "紳": "深",
    "瓙": "到",
    "谖": "宣",
    "锎": "开",
    "駁": "博",
    "蒌": "楼",
    "隍": "黄",
    "谀": "鱼",
    "葶": "停",
    "鏋": "满",
    "灑": "洒",
    "詈": "利",
    "堿": "减",
    "夼": "旷",
    "鑷": "聂",
    "弢": "掏",
    "塍": "成",
    "旂": "其",
    "羧": "缩",
    "帱": "愁",
    "墾": "肯",
    "鸪": "姑",
    "潴": "朱",
    "鏊": "奥",
    "睌": "满",
    "傥": "躺",
    "胍": "瓜",
    "萋": "七",
    "赀": "姿",
    "俣": "雨",
    "邳": "批",
    "讝": "占",
    "暱": "逆",
    "鲵": "尼",
    "亽": "极",
    "嚭": "匹",
    "赜": "则",
    "暾": "吞",
    "噑": "豪",
    "罟": "古",
    "燙": "烫",
    "奬": "讲",
    "醢": "海",
    "朮": "树",
    "骛": "物",
    "辺": "编",
    "勪": "觉",
    "溲": "搜",
    "憤": "愤",
    "覧": "懒",
    "暉": "灰",
    "儏": "灿",
    "闕": "确",
    "蒐": "搜",
    "斃": "必",
    "樨": "西",
    "鳕": "雪",
    "檠": "情",
    "媺": "美",
    "懼": "巨",
    "莖": "精",
    "葑": "风",
    "裎": "成",
    "俶": "处",
    "肜": "容",
    "鏌": "墨",
    "龢": "和",
    "闅": "文",
    "诨": "混",
    "龑": "眼",
    "镨": "普",
    "凖": "准",
    "艉": "伟",
    "濫": "烂",
    "仩": "长",
    "仮": "反",
    "碜": "尘",
    "牯": "古",
    "飆": "标",
    "眄": "免",
    "髂": "恰",
    "辔": "配",
    "滯": "至",
    "滗": "必",
    "浈": "真",
    "啗": "蛋",
    "冮": "刚",
    "璠": "烦",
    "瞞": "瞒",
    "絢": "炫",
    "跹": "先",
    "戗": "枪",
    "苣": "巨",
    "噹": "当",
    "泬": "觉",
    "蛸": "烧",
    "狻": "酸",
    "唁": "燕",
    "耋": "叠",
    "睜": "睁",
    "訫": "信",
    "剞": "机",
    "啝": "和",
    "夌": "零",
    "頑": "完",
    "琭": "路",
    "曛": "熏",
    "觯": "至",
    "廪": "吝",
    "弒": "是",
    "殓": "练",
    "繰": "早",
    "兌": "对",
    "鳐": "摇",
    "祧": "挑",
    "擄": "鲁",
    "茑": "尿",
    "檚": "楚",
    "浃": "家",
    "驕": "教",
    "営": "营",
    "蘖": "聂",
    "脇": "鞋",
    "侓": "路",
    "刋": "欠",
    "齑": "机",
    "稹": "枕",
    "鬣": "裂",
    "艿": "奶",
    "钆": "嘎",
    "鎗": "枪",
    "顛": "颠",
    "雩": "鱼",
    "麂": "几",
    "糺": "纠",
    "鍍": "度",
    "圬": "乌",
    "疋": "匹",
    "陉": "行",
    "婂": "眠",
    "戣": "奎",
    "毘": "皮",
    "玢": "彬",
    "摭": "直",
    "瘢": "班",
    "苁": "聪",
    "戻": "替",
    "嶇": "区",
    "惢": "锁",
    "荛": "扰",
    "諫": "见",
    "獬": "谢",
    "辂": "路",
    "楀": "雨",
    "錠": "定",
    "喾": "裤",
    "盪": "荡",
    "罍": "雷",
    "釀": "娘",
    "谿": "西",
    "濾": "绿",
    "榉": "举",
    "訥": "呢",
    "璽": "洗",
    "袛": "低",
    "冧": "林",
    "椐": "居",
    "詰": "节",
    "玹": "旋",
    "彂": "发",
    "笮": "则",
    "暍": "椰",
    "熳": "慢",
    "佤": "瓦",
    "閏": "润",
    "饔": "庸",
    "旆": "配",
    "貅": "修",
    "陬": "邹",
    "猱": "脑",
    "迓": "亚",
    "嗞": "姿",
    "頔": "敌",
    "酎": "昼",
    "緋": "飞",
    "铪": "哈",
    "懲": "成",
    "舎": "设",
    "聡": "聪",
    "髭": "姿",
    "礻": "是",
    "蘧": "取",
    "攴": "铺",
    "箨": "拓",
    "罨": "眼",
    "垓": "该",
    "勰": "鞋",
    "漬": "字",
    "蠟": "蜡",
    "鄄": "倦",
    "缷": "谢",
    "哕": "会",
    "圮": "匹",
    "诎": "区",
    "埽": "懆",
    "羙": "高",
    "廂": "相",
    "侔": "谋",
    "顎": "恶",
    "摈": "宾",
    "竄": "窜",
    "恿": "永",
    "侩": "快",
    "氤": "因",
    "鐖": "机",
    "蔥": "聪",
    "鹵": "鲁",
    "焜": "昆",
    "谘": "姿",
    "骠": "标",
    "攪": "角",
    "蟀": "帅",
    "錐": "追",
    "悛": "圈",
    "蠂": "设",
    "鎸": "捐",
    "枌": "坟",
    "瀕": "彬",
    "埤": "皮",
    "鲞": "想",
    "徴": "睁",
    "淆": "肖",
    "偠": "咬",
    "諒": "亮",
    "懇": "肯",
    "吿": "告",
    "暁": "小",
    "茏": "龙",
    "猭": "穿",
    "朊": "软",
    "俜": "平",
    "诿": "伟",
    "俳": "排",
    "倻": "椰",
    "勯": "单",
    "槁": "搞",
    "蚵": "和",
    "瓠": "互",
    "渫": "谢",
    "鲷": "雕",
    "葺": "气",
    "镏": "刘",
    "嶝": "凳",
    "鳎": "塔",
    "翮": "和",
    "禤": "宣",
    "钋": "坡",
    "搠": "硕",
    "攬": "懒",
    "踐": "见",
    "蛉": "零",
    "帑": "躺",
    "埌": "浪",
    "耪": "旁",
    "煄": "种",
    "衽": "任",
    "晧": "号",
    "犴": "按",
    "鲂": "房",
    "跬": "魁",
    "蠁": "想",
    "洸": "光",
    "傜": "摇",
    "隰": "习",
    "喟": "溃",
    "錨": "毛",
    "剰": "胜",
    "襪": "袜",
    "锇": "额",
    "伲": "逆",
    "忔": "气",
    "匏": "袍",
    "繡": "秀",
    "弐": "二",
    "衄": "女",
    "鎧": "凯",
    "陲": "垂",
    "驛": "意",
    "滂": "旁",
    "谮": "怎",
    "髹": "修",
    "璿": "旋",
    "龕": "看",
    "嗉": "速",
    "铥": "丢",
    "詣": "意",
    "蝽": "春",
    "堟": "赚",
    "甦": "苏",
    "匐": "福",
    "厣": "眼",
    "嶆": "曹",
    "媸": "吃",
    "煖": "暖",
    "殤": "伤",
    "倮": "裸",
    "簌": "速",
    "寔": "石",
    "捩": "裂",
    "竇": "豆",
    "鮋": "由",
    "珵": "成",
    "呬": "系",
    "萜": "贴",
    "吇": "子",
    "梱": "捆",
    "笹": "踢",
    "妁": "硕",
    "瑱": "镇",
    "旯": "啦",
    "窪": "挖",
    "槄": "掏",
    "圹": "矿",
    "儡": "垒",
    "缌": "思",
    "鹛": "梅",
    "敎": "教",
    "撈": "捞",
    "徂": "促",
    "呺": "消",
    "彖": "团",
    "枨": "成",
    "舘": "管",
    "宬": "成",
    "鲠": "梗",
    "鈦": "太",
    "椂": "路",
    "噺": "心",
    "鞞": "饼",
    "蠓": "猛",
    "悻": "性",
    "蓠": "离",
    "泅": "求",
    "妧": "万",
    "镡": "缠",
    "嗘": "机",
    "瞽": "古",
    "酃": "零",
    "衪": "宜",
    "覓": "密",
    "誦": "宋",
    "蠈": "贼",
    "粋": "翠",
    "缧": "雷",
    "柽": "称",
    "剤": "记",
    "匦": "鬼",
    "鞣": "柔",
    "澆": "教",
    "梃": "挺",
    "潅": "灌",
    "癢": "养",
    "冪": "密",
    "溦": "微",
    "沬": "妹",
    "鮨": "意",
    "暐": "伟",
    "囵": "伦",
    "雱": "旁",
    "浯": "无",
    "瀾": "蓝",
    "蛴": "其",
    "躇": "除",
    "镆": "墨",
    "侀": "行",
    "熺": "西",
    "窬": "鱼",
    "疴": "科",
    "薈": "会",
    "仂": "乐",
    "霑": "占",
    "铽": "特",
    "媾": "够",
    "逭": "换",
    "黾": "敏",
    "燐": "林",
    "酞": "太",
    "钄": "蓝",
    "哌": "派",
    "粧": "装",
    "鄹": "邹",
    "仼": "王",
    "柝": "拓",
    "渦": "窝",
    "翥": "助",
    "缬": "鞋",
    "纮": "红",
    "蘼": "迷",
    "黥": "情",
    "堞": "叠",
    "鲇": "年",
    "嬖": "必",
    "柙": "侠",
    "箻": "绿",
    "硃": "朱",
    "怹": "贪",
    "諱": "会",
    "埘": "石",
    "鍩": "舔",
    "菹": "居",
    "愀": "巧",
    "隳": "灰",
    "垜": "朵",
    "犺": "抗",
    "螢": "营",
    "觜": "姿",
    "矚": "主",
    "侖": "伦",
    "觊": "记",
    "蠲": "捐",
    "燿": "要",
    "坵": "秋",
    "嬿": "燕",
    "崱": "责",
    "怄": "欧",
    "跗": "夫",
    "诔": "垒",
    "鴉": "呀",
    "饑": "机",
    "睞": "赖",
    "悳": "德",
    "襯": "衬",
    "垁": "至",
    "滲": "肾",
    "柁": "堕",
    "玒": "红",
    "屛": "平",
    "篪": "持",
    "籼": "先",
    "崘": "伦",
    "佉": "区",
    "囲": "通",
    "癮": "引",
    "倜": "替",
    "胝": "知",
    "徭": "摇",
    "讠": "颜",
    "荽": "虽",
    "偆": "春",
    "鵰": "雕",
    "嗙": "旁",
    "宄": "鬼",
    "镘": "慢",
    "鼉": "驮",
    "殲": "间",
    "伋": "极",
    "諜": "叠",
    "莴": "窝",
    "儂": "农",
    "佻": "挑",
    "銎": "穷",
    "疳": "干",
    "犵": "歌",
    "嫫": "魔",
    "娡": "至",
    "褕": "鱼",
    "漤": "懒",
    "帔": "配",
    "澘": "山",
    "旄": "毛",
    "锍": "柳",
    "豇": "将",
    "栌": "芦",
    "垧": "赏",
    "閾": "玉",
    "晳": "西",
    "賗": "串",
    "夁": "一",
    "粜": "跳",
    "拊": "辅",
    "暸": "聊",
    "褛": "旅",
    "瘗": "意",
    "鍓": "极",
    "羰": "汤",
    "儵": "书",
    "賍": "脏",
    "驢": "驴",
    "嘏": "古",
    "蚣": "工",
    "鎼": "下",
    "鍪": "谋",
    "兝": "分",
    "梟": "消",
    "懔": "吝",
    "繖": "三",
    "臿": "插",
    "峩": "额",
    "篩": "筛",
    "猓": "果",
    "泐": "乐",
    "撄": "应",
    "鈍": "顿",
    "绁": "谢",
    "驍": "消",
    "褫": "尺",
    "瑋": "伟",
    "閨": "归",
    "覗": "四",
    "秣": "墨",
    "挙": "举",
    "瓿": "不",
    "繃": "崩",
    "豳": "彬",
    "噓": "需",
    "埒": "裂",
    "涠": "维",
    "涑": "速",
    "僞": "伟",
    "儉": "减",
    "黼": "辅",
    "贛": "干",
    "窕": "挑",
    "揹": "杯",
    "涴": "卧",
    "峿": "雨",
    "盌": "碗",
    "囨": "偏",
    "痱": "费",
    "淏": "号",
    "蓐": "入",
    "湅": "练",
    "穢": "会",
    "綔": "互",
    "潑": "坡",
    "酐": "干",
    "莠": "有",
    "摳": "口",
    "鈉": "那",
    "頤": "宜",
    "谠": "挡",
    "聙": "精",
    "詐": "咋",
    "勛": "熏",
    "榖": "古",
    "竊": "妾",
    "兎": "兔",
    "尥": "料",
    "囓": "聂",
    "逡": "群",
    "鸱": "吃",
    "筮": "是",
    "嘯": "笑",
    "巛": "穿",
    "嚜": "么",
    "払": "反",
    "絃": "闲",
    "顫": "颤",
    "抔": "剖",
    "嶷": "宜",
    "鳜": "贵",
    "鱈": "雪",
    "攏": "垄",
    "僉": "前",
    "皤": "婆",
    "廬": "芦",
    "陜": "侠",
    "锪": "霍",
    "僳": "速",
    "鞒": "桥",
    "薤": "谢",
    "霪": "银",
    "詭": "鬼",
    "戙": "动",
    "扃": "窘",
    "罘": "福",
    "鎂": "美",
    "胗": "真",
    "絹": "倦",
    "鼖": "坟",
    "祅": "邀",
    "躏": "吝",
    "寃": "冤",
    "冹": "福",
    "囪": "聪",
    "橼": "原",
    "豨": "西",
    "鸬": "芦",
    "閬": "浪",
    "餍": "燕",
    "瑧": "真",
    "犭": "犬",
    "氾": "饭",
    "邶": "被",
    "嬱": "欠",
    "酹": "泪",
    "擗": "匹",
    "塎": "永",
    "缢": "意",
    "鹼": "减",
    "涫": "灌",
    "栲": "考",
    "謊": "谎",
    "胙": "做",
    "馃": "果",
    "厓": "牙",
    "呿": "去",
    "継": "记",
    "驟": "昼",
    "闍": "督",
    "襙": "操",
    "婱": "闲",
    "湡": "鱼",
    "劼": "节",
    "疠": "利",
    "窓": "窗",
    "媵": "硬",
    "槠": "朱",
    "縁": "原",
    "仏": "佛",
    "駭": "害",
    "跎": "驮",
    "宍": "肉",
    "袴": "裤",
    "硶": "尘",
    "駱": "落",
    "躔": "缠",
    "朐": "取",
    "茕": "穷",
    "逶": "微",
    "亶": "胆",
    "媄": "美",
    "暦": "利",
    "瓘": "灌",
    "洨": "肖",
    "綄": "环",
    "尪": "汪",
    "脹": "帐",
    "跫": "穷",
    "肊": "意",
    "氹": "荡",
    "鎳": "聂",
    "哚": "朵",
    "戋": "间",
    "哿": "葛",
    "骘": "至",
    "钀": "聂",
    "菔": "福",
    "冭": "太",
    "渶": "应",
    "溷": "混",
    "沚": "指",
    "蛘": "羊",
    "碚": "被",
    "筌": "全",
    "鵺": "夜",
    "挲": "撒",
    "钫": "方",
    "铩": "沙",
    "撓": "脑",
    "贽": "至",
    "绨": "提",
    "觇": "搀",
    "炪": "捉",
    "崾": "咬",
    "堋": "朋",
    "仃": "丁",
    "骟": "善",
    "晷": "鬼",
    "塱": "浪",
    "浐": "铲",
    "殂": "促",
    "缒": "缀",
    "叏": "怪",
    "珰": "当",
    "綸": "伦",
    "倣": "访",
    "薑": "将",
    "朂": "续",
    "蔔": "博",
    "銃": "冲",
    "悕": "西",
    "恂": "寻",
    "鴿": "歌",
    "糾": "纠",
    "睃": "缩",
    "耧": "楼",
    "胛": "假",
    "嘧": "密",
    "帼": "国",
    "髀": "必",
    "镥": "鲁",
    "嘹": "聊",
    "茈": "瓷",
    "狁": "允",
    "颙": "庸",
    "樗": "出",
    "狷": "倦",
    "劬": "取",
    "袤": "帽",
    "栊": "龙",
    "儓": "台",
    "郫": "皮",
    "铗": "夹",
    "莨": "浪",
    "赕": "胆",
    "砉": "或",
    "楯": "顿",
    "壱": "一",
    "聴": "听",
    "謠": "摇",
    "螈": "原",
    "炲": "台",
    "甾": "灾",
    "櫺": "零",
    "螽": "中",
    "阄": "纠",
    "禎": "真",
    "叆": "爱",
    "佧": "卡",
    "儸": "罗",
    "攢": "赞",
    "蚴": "右",
    "轎": "教",
    "垆": "芦",
    "饫": "玉",
    "紑": "否",
    "崤": "肖",
    "蕏": "除",
    "縄": "生",
    "踔": "戳",
    "墠": "善",
    "墁": "慢",
    "繭": "减",
    "徨": "黄",
    "掤": "冰",
    "銹": "秀",
    "琌": "零",
    "衤": "一",
    "俢": "修",
    "魨": "吞",
    "骝": "刘",
    "橥": "朱",
    "戡": "看",
    "晡": "不",
    "耜": "四",
    "姤": "够",
    "匜": "宜",
    "鈾": "由",
    "盞": "展",
    "緹": "提",
    "隕": "允",
    "敠": "多",
    "汏": "大",
    "罠": "民",
    "埏": "山",
    "擝": "蒙",
    "斝": "假",
    "噾": "因",
    "鳔": "标",
    "诹": "邹",
    "悊": "哲",
    "侽": "南",
    "榘": "举",
    "鲔": "伟",
    "瞓": "愤",
    "蝼": "楼",
    "驲": "日",
    "蓥": "营",
    "泖": "帽",
    "涒": "吞",
    "妯": "轴",
    "晝": "昼",
    "睚": "牙",
    "篼": "兜",
    "簫": "消",
    "屦": "巨",
    "熻": "西",
    "擱": "歌",
    "鄴": "夜",
    "殻": "巧",
    "岣": "狗",
    "嚮": "向",
    "嗍": "缩",
    "煺": "退",
    "苊": "恶",
    "鷺": "路",
    "蜮": "玉",
    "脽": "谁",
    "葸": "洗",
    "邗": "含",
    "锞": "客",
    "夤": "银",
    "竈": "造",
    "锜": "其",
    "傭": "庸",
    "鎭": "镇",
    "朩": "等",
    "悽": "七",
    "簷": "颜",
    "踣": "博",
    "臾": "鱼",
    "掮": "钱",
    "榊": "神",
    "攉": "霍",
    "脔": "鸾",
    "歺": "恶",
    "罾": "增",
    "洶": "胸",
    "毌": "灌",
    "耱": "墨",
    "挻": "山",
    "陝": "闪",
    "纜": "懒",
    "枥": "利",
    "畠": "田",
    "浍": "会",
    "嬨": "瓷",
    "茍": "记",
    "淝": "肥",
    "椼": "眼",
    "濊": "会",
    "瀞": "静",
    "鳙": "庸",
    "竑": "红",
    "灝": "号",
    "呪": "昼",
    "踰": "鱼",
    "菖": "昌",
    "冩": "写",
    "蜆": "现",
    "滏": "辅",
    "汩": "古",
    "勅": "赤",
    "墿": "意",
    "垕": "后",
    "酩": "命",
    "觌": "敌",
    "縷": "旅",
    "鶯": "应",
    "愰": "荒",
    "綵": "采",
    "呴": "许",
    "脛": "静",
    "垎": "贺",
    "蓿": "须",
    "胨": "动",
    "坜": "利",
    "刳": "哭",
    "鸺": "修",
    "巣": "朝",
    "獴": "猛",
    "尒": "耳",
    "皯": "感",
    "茳": "将",
    "虿": "拆",
    "馭": "玉",
    "蠔": "豪",
    "愦": "溃",
    "泭": "福",
    "鏢": "标",
    "秫": "熟",
    "鲳": "昌",
    "筘": "扣",
    "瘕": "假",
    "喎": "歪",
    "岿": "亏",
    "睥": "屁",
    "軀": "区",
    "菴": "安",
    "曘": "如",
    "煕": "西",
    "捜": "搜",
    "苾": "必",
    "桤": "七",
    "籹": "女",
    "甏": "甭",
    "缣": "间",
    "釦": "扣",
    "娿": "阿",
    "犸": "骂",
    "偗": "省",
    "鉢": "波",
    "聼": "听",
    "崟": "银",
    "塢": "物",
    "琤": "称",
    "髌": "宾",
    "砹": "爱",
    "笁": "竹",
    "髟": "标",
    "姟": "该",
    "孥": "努",
    "艽": "教",
    "莣": "王",
    "脢": "梅",
    "澔": "号",
    "伕": "夫",
    "砘": "顿",
    "儛": "五",
    "紺": "干",
    "盄": "招",
    "懆": "草",
    "輥": "滚",
    "鬘": "瞒",
    "痠": "酸",
    "樼": "真",
    "哓": "消",
    "脬": "抛",
    "価": "四",
    "伉": "抗",
    "蕓": "云",
    "偅": "重",
    "扱": "西",
    "乸": "哪",
    "盉": "和",
    "漭": "忙",
    "鸞": "鸾",
    "窺": "亏",
    "搗": "导",
    "愔": "因",
    "蓍": "诗",
    "幄": "卧",
    "赧": "男",
    "饩": "系",
    "醃": "烟",
    "鑼": "罗",
    "鞏": "拱",
    "噯": "哀",
    "姳": "命",
    "崃": "来",
    "痍": "宜",
    "儑": "岸",
    "喹": "奎",
    "坉": "吞",
    "鏇": "炫",
    "鲅": "爸",
    "笾": "编",
    "鲋": "父",
    "悫": "确",
    "鹍": "昆",
    "禚": "着",
    "筇": "穷",
    "檑": "雷",
    "斎": "摘",
    "鎚": "垂",
    "鳶": "冤",
    "镔": "彬",
    "綬": "瘦",
    "愎": "必",
    "苎": "助",
    "郐": "快",
    "楁": "和",
    "诖": "挂",
    "崐": "昆",
    "荸": "鼻",
    "韪": "伟",
    "皺": "昼",
    "骣": "铲",
    "谇": "岁",
    "瘰": "裸",
    "岃": "任",
    "銓": "全",
    "鑺": "取",
    "粬": "区",
    "飑": "标",
    "輿": "鱼",
    "褴": "蓝",
    "轍": "哲",
    "旀": "妹",
    "硏": "颜",
    "泩": "生",
    "畛": "枕",
    "龉": "雨",
    "骱": "借",
    "冖": "密",
    "褞": "允",
    "鮎": "年",
    "殚": "单",
    "暻": "井",
    "嫻": "闲",
    "覇": "爸",
    "啞": "哑",
    "堙": "因",
    "黟": "一",
    "耥": "汤",
    "褏": "秀",
    "悝": "亏",
    "蚋": "瑞",
    "籴": "敌",
    "鏖": "敖",
    "掱": "爬",
    "俬": "思",
    "撻": "踏",
    "蝰": "奎",
    "鉻": "落",
    "粢": "姿",
    "脨": "促",
    "肄": "意",
    "駆": "区",
    "炤": "照",
    "酡": "驮",
    "鬍": "胡",
    "愷": "凯",
    "遅": "持",
    "鈊": "心",
    "遄": "传",
    "绲": "滚",
    "馓": "三",
    "烔": "同",
    "靼": "达",
    "畾": "雷",
    "跣": "显",
    "湳": "男",
    "榇": "衬",
    "蠊": "连",
    "砵": "波",
    "禿": "突",
    "秕": "比",
    "鋰": "李",
    "絷": "直",
    "歯": "尺",
    "懐": "怀",
    "燠": "玉",
    "卌": "系",
    "坌": "笨",
    "浬": "李",
    "堠": "后",
    "弝": "爸",
    "夶": "比",
    "筲": "烧",
    "瑢": "容",
    "龀": "衬",
    "墘": "钱",
    "炱": "台",
    "夈": "摘",
    "蝮": "父",
    "筅": "显",
    "熾": "赤",
    "欐": "利",
    "艡": "当",
    "渟": "停",
    "郾": "眼",
    "跞": "利",
    "鄺": "矿",
    "闡": "铲",
    "惱": "脑",
    "鳯": "奉",
    "曇": "谈",
    "硯": "燕",
    "脥": "浅",
    "庒": "装",
    "寲": "宜",
    "輒": "哲",
    "欎": "玉",
    "淒": "七",
    "盥": "灌",
    "婃": "从",
    "沆": "行",
    "庅": "魔",
    "亼": "极",
    "羈": "机",
    "跄": "枪",
    "瘳": "抽",
    "雑": "杂",
    "氅": "场",
    "饣": "石",
    "欒": "鸾",
    "柵": "山",
    "芄": "完",
    "槻": "归",
    "垨": "手",
    "骺": "喉",
    "岈": "牙",
    "椛": "花",
    "傕": "觉",
    "裟": "沙",
    "穦": "拼",
    "鈹": "批",
    "埍": "卷",
    "蒉": "溃",
    "甕": "瓮",
    "嗻": "遮",
    "涚": "睡",
    "鐑": "妾",
    "癜": "电",
    "濄": "锅",
    "悎": "号",
    "薷": "如",
    "觏": "够",
    "斂": "脸",
    "廁": "册",
    "喈": "接",
    "囦": "冤",
    "暘": "羊",
    "槲": "胡",
    "眬": "龙",
    "鲮": "零",
    "镦": "对",
    "庥": "修",
    "袷": "夹",
    "噍": "教",
    "裥": "减",
    "趼": "减",
    "坫": "电",
    "岺": "零",
    "愭": "其",
    "缏": "变",
    "锸": "插",
    "尨": "忙",
    "畨": "攀",
    "愩": "工",
    "縢": "腾",
    "秊": "年",
    "搥": "垂",
    "駡": "骂",
    "芨": "机",
    "玘": "起",
    "廾": "拱",
    "谳": "燕",
    "誐": "额",
    "茌": "持",
    "貯": "助",
    "眏": "养",
    "漈": "记",
    "賃": "吝",
    "垡": "罚",
    "鹣": "间",
    "诂": "古",
    "鏟": "铲",
    "鉗": "钱",
    "獗": "觉",
    "洿": "乌",
    "棨": "起",
    "燉": "顿",
    "閪": "色",
    "槙": "颠",
    "蠖": "或",
    "龠": "月",
    "鑱": "缠",
    "綍": "福",
    "倥": "空",
    "戥": "等",
    "圄": "雨",
    "鼗": "桃",
    "哾": "说",
    "鏽": "秀",
    "僱": "固",
    "睍": "现",
    "鉀": "假",
    "裼": "替",
    "鷗": "欧",
    "晫": "着",
    "涏": "挺",
    "濇": "色",
    "櫒": "萨",
    "鱻": "先",
    "籺": "和",
    "嫘": "雷",
    "傧": "彬",
    "卬": "昂",
    "枡": "生",
    "菂": "地",
    "鸸": "而",
    "屺": "起",
    "簲": "排",
    "閰": "局",
    "峓": "宜",
    "枃": "进",
    "裢": "连",
    "囙": "因",
    "脗": "稳",
    "祆": "先",
    "奭": "是",
    "舡": "传",
    "痼": "固",
    "骀": "带",
    "貊": "墨",
    "皵": "确",
    "贶": "矿",
    "祓": "福",
    "謦": "请",
    "銭": "钱",
    "褎": "秀",
    "鮫": "教",
    "秾": "农",
    "憷": "处",
    "魟": "红",
    "亰": "精",
    "蔟": "促",
    "匂": "胸",
    "湰": "龙",
    "仳": "匹",
    "歃": "煞",
    "嬅": "话",
    "栬": "最",
    "辏": "凑",
    "鬯": "唱",
    "岍": "前",
    "逦": "李",
    "檻": "砍",
    "庀": "匹",
    "艚": "曹",
    "辎": "姿",
    "裉": "肯",
    "媗": "宣",
    "鲭": "青",
    "琻": "金",
    "褑": "院",
    "扆": "以",
    "駌": "冤",
    "撴": "蹲",
    "箌": "到",
    "蔘": "深",
    "堢": "宝",
    "敫": "角",
    "尯": "溃",
    "嵛": "鱼",
    "甗": "眼",
    "朠": "应",
    "砑": "亚",
    "凵": "浅",
    "鍣": "招",
    "庁": "听",
    "縻": "迷",
    "惀": "伦",
    "峠": "恰",
    "彐": "记",
    "鸹": "瓜",
    "煋": "星",
    "廛": "缠",
    "茚": "印",
    "倶": "巨",
    "櫛": "至",
    "僾": "爱",
    "芲": "花",
    "畋": "田",
    "婄": "剖",
    "蠶": "残",
    "轺": "摇",
    "迕": "物",
    "曟": "陈",
    "廝": "思",
    "揞": "俺",
    "煒": "伟",
    "紵": "助",
    "褡": "答",
    "燄": "燕",
    "袓": "巨",
    "朶": "朵",
    "摻": "灿",
    "煇": "灰",
    "澌": "思",
    "甪": "路",
    "鉯": "以",
    "蒹": "间",
    "偘": "砍",
    "鰍": "秋",
    "彫": "雕",
    "椀": "碗",
    "燭": "竹",
    "伈": "信",
    "抎": "允",
    "瓑": "利",
    "葙": "相",
    "伥": "昌",
    "玏": "乐",
    "鲆": "平",
    "鞴": "被",
    "狆": "重",
    "鈕": "纽",
    "芟": "山",
    "筯": "助",
    "癲": "颠",
    "菫": "紧",
    "搮": "利",
    "鈳": "科",
    "芤": "口",
    "晥": "碗",
    "婲": "花",
    "笸": "坡",
    "骉": "标",
    "儶": "会",
    "戓": "歌",
    "鹱": "互",
    "姌": "染",
    "咴": "灰",
    "玆": "姿",
    "絀": "处",
    "栥": "姿",
    "煶": "是",
    "膰": "烦",
    "鲊": "眨",
    "舳": "竹",
    "帻": "则",
    "塅": "断",
    "塄": "棱",
    "敉": "米",
    "黠": "侠",
    "挊": "弄",
    "孳": "姿",
    "紬": "愁",
    "夎": "错",
    "嚅": "如",
    "黧": "离",
    "冚": "砍",
    "矧": "审",
    "揷": "插",
    "忉": "刀",
    "胂": "肾",
    "馴": "寻",
    "菿": "到",
    "鳏": "关",
    "欍": "就",
    "儴": "嚷",
    "瞢": "盟",
    "杺": "心",
    "赳": "纠",
    "祌": "重",
    "瘛": "赤",
    "驺": "邹",
    "茛": "根",
    "磲": "取",
    "搦": "诺",
    "朹": "鬼",
    "掙": "睁",
    "讐": "愁",
    "襞": "必",
    "忮": "至",
    "祫": "侠",
    "膮": "消",
    "瀉": "谢",
    "偪": "逼",
    "鍃": "霍",
    "呡": "稳",
    "誣": "乌",
    "苧": "凝",
    "莸": "由",
    "滌": "敌",
    "罱": "懒",
    "鵜": "提",
    "穫": "或",
    "烎": "银",
    "簦": "灯",
    "鐒": "劳",
    "犱": "几",
    "猟": "裂",
    "铫": "掉",
    "檗": "薄",
    "繕": "善",
    "伄": "掉",
    "鎽": "风",
    "篌": "喉",
    "甍": "盟",
    "麣": "颜",
    "鲽": "叠",
    "麺": "面",
    "毴": "逼",
    "矯": "角",
    "脶": "罗",
    "軋": "亚",
    "粝": "利",
    "廴": "引",
    "贖": "熟",
    "厷": "工",
    "潲": "绍",
    "浤": "红",
    "咘": "不",
    "栱": "拱",
    "咑": "答",
    "垲": "凯",
    "崦": "烟",
    "湀": "鬼",
    "轾": "至",
    "渀": "笨",
    "悗": "瞒",
    "酲": "成",
    "劓": "意",
    "簒": "窜",
    "僪": "局",
    "廨": "谢",
    "鸲": "取",
    "幝": "铲",
    "跏": "家",
    "躜": "钻",
    "荦": "落",
    "隴": "垄",
    "憍": "教",
    "闶": "康",
    "鞫": "居",
    "仫": "木",
    "艄": "烧",
    "缽": "波",
    "圝": "鸾",
    "脰": "豆",
    "窳": "雨",
    "襻": "判",
    "舻": "芦",
    "囑": "主",
    "槳": "讲",
    "鼙": "皮",
    "畚": "本",
    "刎": "稳",
    "皲": "君",
    "皞": "号",
    "莩": "福",
    "仡": "歌",
    "廃": "费",
    "挾": "鞋",
    "姏": "瞒",
    "籲": "玉",
    "迺": "奶",
    "拝": "败",
    "鳊": "编",
    "圯": "宜",
    "遘": "够",
    "乣": "久",
    "狨": "容",
    "銑": "显",
    "鹩": "聊",
    "荩": "进",
    "涢": "云",
    "撺": "窜",
    "籀": "昼",
    "籬": "离",
    "峣": "摇",
    "嘩": "花",
    "麹": "区",
    "伅": "顿",
    "蛄": "姑",
    "哢": "弄",
    "廯": "先",
    "狲": "孙",
    "瓨": "翔",
    "竷": "砍",
    "焃": "贺",
    "猡": "罗",
    "虓": "消",
    "鑸": "垒",
    "洣": "米",
    "渼": "美",
    "汸": "方",
    "墖": "塔",
    "樶": "嘴",
    "兿": "意",
    "踟": "持",
    "搋": "揣",
    "寘": "至",
    "岵": "互",
    "庤": "至",
    "蕹": "瓮",
    "摯": "至",
    "矅": "要",
    "鎹": "宋",
    "洏": "而",
    "稂": "狼",
    "呉": "无",
    "諦": "地",
    "鉑": "博",
    "搛": "间",
    "艋": "猛",
    "鳇": "黄",
    "繘": "玉",
    "桴": "福",
    "缍": "朵",
    "蠉": "宣",
    "婅": "局",
    "诼": "着",
    "澗": "见",
    "迣": "至",
    "爇": "弱",
    "鲩": "换",
    "簮": "赞",
    "龃": "举",
    "簖": "断",
    "盱": "需",
    "砬": "啦",
    "畳": "叠",
    "潗": "极",
    "緂": "田",
    "闿": "凯",
    "黻": "福",
    "苐": "提",
    "挢": "角",
    "鹾": "搓",
    "懑": "闷",
    "獃": "呆",
    "聳": "耸",
    "塁": "垒",
    "謨": "魔",
    "淠": "屁",
    "佀": "四",
    "砩": "福",
    "窆": "扁",
    "拡": "扩",
    "鰈": "叠",
    "埜": "也",
    "箝": "钱",
    "吢": "亲",
    "脤": "肾",
    "簬": "路",
    "氶": "整",
    "蛯": "老",
    "袮": "迷",
    "諷": "奉",
    "驊": "华",
    "檜": "贵",
    "汭": "瑞",
    "儱": "垄",
    "錚": "睁",
    "雋": "倦",
    "痺": "必",
    "湇": "气",
    "炴": "养",
    "苺": "梅",
    "泚": "此",
    "獍": "静",
    "鮭": "归",
    "泔": "干",
    "秭": "子",
    "翾": "宣",
    "箦": "则",
    "蒨": "欠",
    "蜢": "猛",
    "镠": "刘",
    "粨": "摆",
    "笤": "条",
    "穸": "西",
    "茼": "同",
    "栝": "瓜",
    "陔": "该",
    "洅": "在",
    "訟": "宋",
    "飢": "机",
    "棬": "圈",
    "笊": "照",
    "僰": "博",
    "礪": "利",
    "旰": "干",
    "趵": "报",
    "佴": "二",
    "郛": "福",
    "滐": "节",
    "栨": "次",
    "畬": "舍",
    "鳟": "尊",
    "儔": "愁",
    "湎": "免",
    "嗛": "浅",
    "箣": "册",
    "羮": "耕",
    "瓞": "叠",
    "徉": "羊",
    "蒔": "石",
    "毳": "翠",
    "麿": "迷",
    "鹧": "这",
    "耔": "子",
    "虼": "个",
    "缛": "入",
    "枊": "盎",
    "莶": "先",
    "穌": "苏",
    "糈": "许",
    "犷": "广",
    "亠": "头",
    "侉": "垮",
    "舾": "西",
    "苳": "东",
    "柃": "零",
    "岖": "区",
    "彯": "飘",
    "蚱": "咋",
    "颎": "窘",
    "踬": "至",
    "伒": "进",
    "硭": "忙",
    "暀": "网",
    "澐": "云",
    "檆": "山",
    "嫠": "离",
    "熀": "谎",
    "豝": "八",
    "桕": "就",
    "圊": "青",
    "瘆": "肾",
    "牴": "底",
    "缵": "钻",
    "壸": "捆",
    "泶": "学",
    "夬": "怪",
    "蛍": "营",
    "酏": "以",
    "趿": "他",
    "餉": "想",
    "珽": "挺",
    "嘖": "则",
    "弎": "三",
    "禘": "地",
    "鲡": "离",
    "馿": "驴",
    "讦": "节",
    "忸": "纽",
    "埇": "永",
    "栘": "宜",
    "砗": "车",
    "溇": "楼",
    "疒": "呢",
    "儁": "俊",
    "忎": "人",
    "湔": "间",
    "葦": "伟",
    "脷": "利",
    "潓": "会",
    "笌": "牙",
    "箞": "前",
    "摰": "聂",
    "镅": "梅",
    "鍠": "黄",
    "祔": "父",
    "彴": "着",
    "儹": "赞",
    "忋": "改",
    "疄": "吝",
    "譻": "应",
    "龒": "龙",
    "迮": "则",
    "馡": "飞",
    "葚": "任",
    "嫀": "琴",
    "紜": "云",
    "奨": "讲",
    "嫃": "真",
    "嬮": "烟",
    "裣": "脸",
    "噐": "气",
    "滈": "号",
    "橦": "同",
    "偍": "提",
    "蘩": "烦",
    "猁": "利",
    "樫": "间",
    "亇": "吗",
    "苠": "民",
    "囥": "抗",
    "甓": "屁",
    "忪": "松",
    "嵯": "搓",
    "浿": "配",
    "誮": "花",
    "趄": "居",
    "蟛": "朋",
    "讧": "红",
    "軎": "位",
    "慝": "特",
    "郕": "成",
    "詤": "谎",
    "萑": "环",
    "潶": "黑",
    "擐": "换",
    "鱉": "憋",
    "箋": "间",
    "炻": "石",
    "褊": "扁",
    "轫": "任",
    "択": "则",
    "硇": "脑",
    "縦": "宗",
    "禊": "系",
    "梶": "伟",
    "鋭": "瑞",
    "镤": "葡",
    "眈": "单",
    "顗": "以",
    "凥": "居",
    "蠱": "古",
    "崋": "话",
    "垤": "叠",
    "慟": "痛",
    "跚": "山",
    "礤": "擦",
    "綘": "逢",
    "獞": "同",
    "瘡": "窗",
    "颱": "台",
    "藴": "运",
    "栦": "愁",
    "綽": "绰",
    "藔": "聊",
    "輶": "由",
    "簍": "楼",
    "齤": "全",
    "瓏": "龙",
    "彿": "福",
    "嬶": "鼻",
    "龖": "达",
    "嫲": "吗",
    "譲": "让",
    "阋": "系",
    "箪": "单",
    "麈": "主",
    "丆": "罕",
    "碹": "炫",
    "厤": "利",
    "裒": "剖",
    "廸": "敌",
    "瘐": "雨",
    "籓": "翻",
    "筍": "损",
    "钶": "科",
    "瑺": "长",
    "眢": "冤",
    "龤": "鞋",
    "翚": "灰",
    "仴": "卧",
    "撙": "尊",
    "俍": "良",
    "緱": "勾",
    "悓": "欠",
    "繹": "意",
    "丗": "是",
    "邠": "彬",
    "侷": "局",
    "狅": "狂",
    "傞": "缩",
    "蜩": "条",
    "褓": "宝",
    "汔": "气",
    "掟": "整",
    "毵": "三",
    "臁": "连",
    "繋": "记",
    "曡": "叠",
    "埼": "其",
    "缋": "会",
    "嶉": "催",
    "虮": "几",
    "瞾": "照",
    "鸶": "思",
    "瑆": "星",
    "歿": "墨",
    "慉": "续",
    "謁": "夜",
    "呾": "达",
    "濺": "见",
    "涳": "空",
    "龁": "和",
    "艧": "或",
    "貍": "离",
    "嶶": "微",
    "菥": "西",
    "篃": "妹",
    "肷": "浅",
    "絳": "降",
    "碶": "气",
    "鯖": "睁",
    "翛": "消",
    "阏": "恶",
    "轵": "指",
    "萣": "定",
    "鶏": "机",
    "訛": "额",
    "儯": "腾",
    "眛": "妹",
    "悱": "匪",
    "鋬": "判",
    "絜": "节",
    "珹": "成",
    "瀝": "利",
    "盦": "安",
    "銊": "续",
    "眵": "吃",
    "婹": "咬",
    "嶃": "展",
    "簏": "路",
    "襤": "蓝",
    "咾": "老",
    "疎": "书",
    "恠": "怪",
    "脧": "捐",
    "牮": "见",
    "涗": "睡",
    "腙": "宗",
    "捽": "做",
    "礅": "蹲",
    "滺": "优",
    "竅": "巧",
    "捯": "刀",
    "崡": "含",
    "颥": "如",
    "霙": "应",
    "茓": "学",
    "駄": "驮",
    "頰": "夹",
    "蔴": "麻",
    "窭": "巨",
    "燻": "熏",
    "鴞": "消",
    "锾": "环",
    "儞": "你",
    "蟊": "毛",
    "桫": "缩",
    "棿": "尼",
    "犮": "拔",
    "箢": "冤",
    "棧": "战",
    "崚": "棱",
    "韜": "掏",
    "涇": "精",
    "贄": "至",
    "叄": "参",
    "覌": "关",
    "鉨": "洗",
    "蒇": "铲",
    "琀": "含",
    "幟": "至",
    "澥": "谢",
    "鯤": "昆",
    "羥": "抢",
    "剉": "错",
    "餸": "宋",
    "螚": "耐",
    "幞": "福",
    "嗆": "枪",
    "剀": "凯",
    "簨": "损",
    "鎊": "棒",
    "嬸": "审",
    "衕": "痛",
    "偄": "软",
    "蚿": "闲",
    "蕺": "极",
    "曖": "爱",
    "紓": "书",
    "儚": "盟",
    "枵": "消",
    "譙": "巧",
    "伀": "中",
    "囄": "离",
    "纙": "落",
    "躋": "机",
    "鎾": "温",
    "渕": "冤",
    "幋": "盘",
    "揵": "钱",
    "躅": "竹",
    "莐": "陈",
    "溽": "入",
    "羝": "低",
    "昜": "羊",
    "棼": "坟",
    "邡": "方",
    "鈻": "四",
    "竤": "红",
    "缫": "骚",
    "疍": "蛋",
    "掲": "接",
    "駙": "父",
    "烠": "回",
    "埖": "花",
    "妡": "心",
    "栆": "早",
    "怫": "福",
    "偾": "愤",
    "蘚": "显",
    "趱": "赞",
    "嫜": "张",
    "觋": "习",
    "艣": "鲁",
    "牋": "间",
    "嘍": "楼",
    "刿": "贵",
    "媿": "溃",
    "脠": "山",
    "琎": "进",
    "婇": "采",
    "耩": "讲",
    "锖": "枪",
    "衳": "中",
    "仌": "冰",
    "偁": "称",
    "鹆": "玉",
    "缲": "敲",
    "饸": "和",
    "勣": "机",
    "旛": "翻",
    "褀": "其",
    "痲": "麻",
    "梏": "固",
    "禱": "导",
    "冑": "昼",
    "灉": "庸",
    "舗": "瀑",
    "嵗": "岁",
    "勹": "包",
    "犰": "求",
    "棻": "分",
    "侟": "存",
    "塒": "石",
    "蛑": "谋",
    "嫇": "明",
    "爀": "贺",
    "屻": "任",
    "脕": "万",
    "邽": "归",
    "镟": "炫",
    "炵": "通",
    "躶": "裸",
    "亖": "四",
    "錳": "猛",
    "鐾": "被",
    "餡": "现",
    "笀": "忙",
    "鐤": "顶",
    "劄": "扎",
    "冋": "窘",
    "嫋": "尿",
    "啁": "招",
    "扺": "指",
    "玙": "鱼",
    "禰": "迷",
    "榎": "假",
    "夨": "责",
    "壒": "爱",
    "隂": "因",
    "揲": "叠",
    "栭": "而",
    "苀": "行",
    "曚": "盟",
    "竝": "病",
    "蠋": "竹",
    "觎": "鱼",
    "怮": "优",
    "睱": "下",
    "慬": "琴",
    "礿": "月",
    "楢": "由",
    "搌": "展",
    "丬": "强",
    "駝": "驮",
    "徧": "变",
    "粞": "西",
    "溻": "他",
    "俆": "徐",
    "濜": "进",
    "嵫": "姿",
    "颃": "行",
    "勩": "意",
    "刭": "井",
    "锼": "搜",
    "脝": "哼",
    "鎌": "连",
    "慫": "耸",
    "叚": "假",
    "坲": "佛",
    "墍": "系",
    "氼": "逆",
    "鹮": "环",
    "偰": "谢",
    "崞": "锅",
    "檎": "琴",
    "瘜": "西",
    "廲": "离",
    "毆": "欧",
    "洫": "续",
    "萏": "蛋",
    "棩": "冤",
    "崄": "显",
    "傈": "利",
    "槦": "庸",
    "馱": "驮",
    "陥": "现",
    "庨": "消",
    "塝": "棒",
    "樿": "善",
    "琸": "着",
    "瘝": "关",
    "龆": "条",
    "艟": "冲",
    "嗱": "拿",
    "顸": "憨",
    "蚍": "皮",
    "鬈": "全",
    "檛": "抓",
    "砕": "岁",
    "舣": "以",
    "趔": "裂",
    "谵": "占",
    "蚰": "由",
    "沺": "田",
    "斉": "其",
    "烺": "浪",
    "勍": "情",
    "嶠": "教",
    "吙": "霍",
    "鰲": "敖",
    "竒": "其",
    "蠼": "取",
    "冃": "帽",
    "叾": "了",
    "獯": "熏",
    "倳": "字",
    "脙": "修",
    "賁": "必",
    "脡": "挺",
    "潯": "寻",
    "猞": "舍",
    "曁": "记",
    "黴": "梅",
    "勲": "熏",
    "咢": "恶",
    "顒": "庸",
    "朣": "同",
    "膂": "旅",
    "膿": "农",
    "甁": "平",
    "湲": "原",
    "挒": "裂",
    "甶": "福",
    "灺": "谢",
    "艼": "听",
    "侘": "差",
    "皢": "小",
    "妽": "深",
    "笢": "敏",
    "戔": "间",
    "锃": "赠",
    "俵": "标",
    "鉬": "木",
    "琲": "被",
    "蒴": "硕",
    "屲": "挖",
    "斻": "行",
    "掊": "剖",
    "悃": "捆",
    "儖": "蓝",
    "鹚": "瓷",
    "奀": "恩",
    "佲": "命",
    "尜": "嘎",
    "菪": "荡",
    "蛞": "扩",
    "脒": "米",
    "氍": "取",
    "愯": "耸",
    "鳚": "位",
    "莰": "砍",
    "鰨": "塔",
    "杄": "前",
    "螣": "特",
    "跽": "记",
    "嗭": "直",
    "仛": "拖",
    "戽": "互",
    "賄": "会",
    "釵": "拆",
    "寖": "进",
    "橀": "西",
    "珃": "染",
    "锳": "应",
    "諮": "姿",
    "卽": "极",
    "箏": "睁",
    "譏": "机",
    "淰": "年",
    "珧": "摇",
    "涶": "拖",
    "魮": "皮",
    "脟": "裂",
    "毐": "矮",
    "蓅": "刘",
    "侊": "光",
    "畊": "耕",
    "廒": "敖",
    "亣": "大",
    "跢": "堕",
    "鏘": "枪",
    "厙": "设",
    "妋": "夫",
    "酤": "姑",
    "僬": "教",
    "峫": "鞋",
    "眰": "叠",
    "祄": "谢",
    "鈡": "中",
    "郿": "梅",
    "忞": "民",
    "枘": "瑞",
    "枼": "夜",
    "偒": "躺",
    "莀": "陈",
    "偊": "雨",
    "屸": "龙",
    "癔": "意",
    "硂": "全",
    "錡": "其",
    "裰": "多",
    "迗": "额",
    "铹": "劳",
    "埯": "俺",
    "唴": "呛",
    "畤": "至",
    "眳": "明",
    "浉": "诗",
    "斁": "意",
    "鱧": "李",
    "黡": "眼",
    "癱": "贪",
    "轹": "利",
    "跸": "必",
    "偂": "钱",
    "贓": "脏",
    "仈": "八",
    "暠": "搞",
    "壷": "胡",
    "撶": "华",
    "锓": "寝",
    "馇": "插",
    "阼": "做",
    "譁": "华",
    "摅": "书",
    "莈": "墨",
    "揃": "减",
    "脪": "信",
    "苈": "利",
    "躰": "体",
    "労": "劳",
    "蟙": "直",
    "閹": "烟",
    "玊": "速",
    "筧": "减",
    "怴": "续",
    "伖": "躺",
    "旼": "民",
    "冄": "染",
    "崀": "浪",
    "檤": "到",
    "脭": "成",
    "璾": "姿",
    "墼": "机",
    "惣": "总",
    "誾": "银",
    "熼": "意",
    "馑": "紧",
    "掭": "天",
    "綮": "起",
    "誥": "告",
    "亱": "夜",
    "舲": "零",
    "窰": "摇",
    "卲": "绍",
    "拶": "匝",
    "鑚": "钻",
    "礽": "仍",
    "湓": "盆",
    "訖": "气",
    "朿": "次",
    "儼": "眼",
    "脞": "搓",
    "桄": "光",
    "帀": "匝",
    "鹎": "杯",
    "斺": "铲",
    "琞": "胜",
    "鹇": "闲",
    "瘮": "肾",
    "赱": "走",
    "趯": "替",
    "曱": "约",
    "綑": "捆",
    "燹": "显",
    "渖": "审",
    "彁": "歌",
    "穏": "稳",
    "棤": "错",
    "鲀": "吞",
    "磡": "看",
    "逺": "远",
    "垙": "光",
    "暌": "奎",
    "舯": "中",
    "莂": "别",
    "覀": "西",
    "岕": "借",
    "渋": "色",
    "堎": "愣",
    "暎": "硬",
    "燁": "夜",
    "贳": "是",
    "鳢": "李",
    "蓳": "紧",
    "蚡": "坟",
    "秂": "人",
    "馘": "国",
    "焌": "俊",
    "叿": "轰",
    "乬": "巨",
    "儕": "柴",
    "舴": "则",
    "杻": "丑",
    "鍨": "奎",
    "鑴": "西",
    "钔": "门",
    "髑": "毒",
    "伛": "雨",
    "璶": "进",
    "聩": "溃",
    "朲": "人",
    "锩": "卷",
    "祘": "算",
    "栤": "病",
    "浕": "进",
    "韡": "伟",
    "牬": "被",
    "濅": "进",
    "栴": "占",
    "埛": "窘",
    "絺": "吃",
    "玡": "牙",
    "蓆": "习",
    "懾": "设",
    "淯": "玉",
    "摽": "标",
    "拠": "巨",
    "麪": "面",
    "弍": "二",
    "剷": "铲",
    "牐": "炸",
    "榃": "谈",
    "糨": "降",
    "芐": "互",
    "鹪": "教",
    "珝": "许",
    "纓": "应",
    "腘": "国",
    "癯": "取",
    "紶": "区",
    "昺": "饼",
    "袞": "滚",
    "膞": "专",
    "蓊": "瓮",
    "洀": "盘",
    "撱": "伟",
    "垪": "病",
    "潽": "铺",
    "緢": "描",
    "嚆": "蒿",
    "黐": "吃",
    "氇": "撸",
    "嬈": "扰",
    "癀": "黄",
    "氈": "占",
    "儳": "缠",
    "菑": "灾",
    "偧": "扎",
    "炰": "袍",
    "佇": "助",
    "瞫": "审",
    "蛱": "夹",
    "赙": "父",
    "涖": "利",
    "踯": "直",
    "镚": "甭",
    "廑": "紧",
    "蝥": "毛",
    "殪": "意",
    "殢": "替",
    "榱": "催",
    "瞹": "爱",
    "窞": "蛋",
    "澋": "红",
    "巯": "求",
    "籣": "蓝",
    "熴": "昆",
    "庯": "不",
    "瀣": "谢",
    "秏": "号",
    "裑": "深",
    "镈": "博",
    "锫": "培",
    "轱": "姑",
    "踽": "举",
    "囁": "聂",
    "蜉": "福",
    "撹": "角",
    "鏗": "坑",
    "墕": "燕",
    "碥": "扁",
    "脦": "的",
    "賧": "探",
    "螓": "琴",
    "蕻": "红",
    "悘": "一",
    "囂": "消",
    "蔰": "互",
    "旇": "批",
    "羑": "有",
    "祤": "雨",
    "腧": "树",
    "跘": "盘",
    "怱": "聪",
    "粛": "速",
    "墬": "地",
    "鯶": "换",
    "繍": "秀",
    "麇": "君",
    "锿": "哀",
    "艨": "盟",
    "罝": "居",
    "罈": "谈",
    "澉": "感",
    "凕": "命",
    "惓": "全",
    "誨": "会",
    "岆": "咬",
    "踴": "永",
    "瘉": "玉",
    "瀷": "意",
    "憳": "坦",
    "啍": "吞",
    "忛": "翻",
    "揎": "宣",
    "詮": "全",
    "縯": "眼",
    "糰": "团",
    "頵": "晕",
    "寍": "凝",
    "溿": "判",
    "鎢": "乌",
    "偬": "总",
    "茝": "拆",
    "皒": "额",
    "骵": "体",
    "紥": "匝",
    "嚄": "霍",
    "阬": "坑",
    "蛲": "脑",
    "幓": "山",
    "檸": "凝",
    "蟳": "寻",
    "樋": "通",
    "畎": "犬",
    "哖": "年",
    "爣": "躺",
    "蜊": "离",
    "頹": "推",
    "陴": "皮",
    "櫓": "鲁",
    "眙": "宜",
    "矞": "玉",
    "顱": "芦",
    "遯": "顿",
    "勧": "劝",
    "謚": "是",
    "椾": "间",
    "甡": "深",
    "脜": "有",
    "嵝": "楼",
    "辋": "网",
    "彆": "蹩",
    "璣": "机",
    "荌": "按",
    "腠": "凑",
    "莔": "盟",
    "樰": "雪",
    "襌": "单",
    "罥": "倦",
    "鵟": "狂",
    "緵": "宗",
    "璉": "脸",
    "蟜": "角",
    "釈": "是",
    "緞": "断",
    "瘵": "寨",
    "猋": "标",
    "摟": "楼",
    "亁": "干",
    "坩": "干",
    "誡": "借",
    "飈": "标",
    "嗶": "必",
    "擰": "凝",
    "劧": "指",
    "鰕": "虾",
    "蒗": "浪",
    "勶": "撤",
    "闔": "和",
    "菉": "路",
    "鑣": "标",
    "眕": "枕",
    "徠": "来",
    "忾": "开",
    "噵": "到",
    "澂": "成",
    "蒄": "关",
    "幙": "木",
    "牜": "牛",
    "蛦": "宜",
    "鈷": "古",
    "猀": "沙",
    "偢": "丑",
    "簰": "排",
    "瘧": "虐",
    "繙": "翻",
    "薊": "记",
    "偡": "战",
    "熥": "腾",
    "鹨": "六",
    "檺": "搞",
    "彛": "宜",
    "窀": "准",
    "沲": "驮",
    "幀": "挣",
    "咃": "拖",
    "鸀": "楚",
    "玗": "鱼",
    "礌": "雷",
    "訶": "喝",
    "甌": "欧",
    "饋": "溃",
    "嘠": "嘎",
    "鼴": "眼",
    "淂": "德",
    "蓌": "错",
    "泦": "局",
    "縠": "胡",
    "玧": "门",
    "恝": "夹",
    "筊": "肖",
    "潟": "系",
    "嫐": "脑",
    "餼": "系",
    "醑": "许",
    "訇": "轰",
    "唅": "含",
    "鄠": "互",
    "檳": "彬",
    "渓": "西",
    "塭": "温",
    "媱": "摇",
    "閞": "变",
    "鲱": "飞",
    "冿": "间",
    "褙": "被",
    "笵": "饭",
    "鉉": "炫",
    "掕": "零",
    "弇": "眼",
    "赑": "必",
    "攰": "贵",
    "獑": "缠",
    "錕": "昆",
    "耑": "端",
    "茺": "冲",
    "楨": "真",
    "褁": "果",
    "聻": "你",
    "訁": "颜",
    "佝": "勾",
    "緈": "性",
    "紱": "福",
    "幈": "平",
    "咵": "垮",
    "覈": "和",
    "揶": "爷",
    "萢": "抛",
    "蕗": "路",
    "癃": "龙",
    "塲": "长",
    "玅": "庙",
    "瞶": "贵",
    "墚": "良",
    "荅": "答",
    "揑": "捏",
    "鈧": "抗",
    "鲦": "条",
    "硚": "桥",
    "斾": "配",
    "浛": "含",
    "榍": "谢",
    "諳": "安",
    "輋": "舍",
    "眂": "是",
    "鞬": "间",
    "漣": "连",
    "荬": "买",
    "髄": "髓",
    "卋": "是",
    "汧": "前",
    "玍": "尬",
    "酴": "图",
    "榼": "科",
    "刬": "铲",
    "瞜": "楼",
    "垾": "汉",
    "锬": "谈",
    "胼": "偏",
    "鞲": "勾",
    "嵁": "看",
    "劏": "汤",
    "璢": "刘",
    "掓": "书",
    "袼": "歌",
    "粺": "败",
    "驒": "驮",
    "燬": "毁",
    "焠": "翠",
    "蟄": "哲",
    "劅": "着",
    "疇": "愁",
    "偤": "由",
    "緥": "宝",
    "皛": "小",
    "荄": "该",
    "娸": "七",
    "砫": "助",
    "屣": "洗",
    "絫": "垒",
    "穨": "推",
    "緃": "宗",
    "銨": "俺",
    "厛": "听",
    "洑": "福",
    "箹": "约",
    "蹓": "溜",
    "倝": "干",
    "瘊": "喉",
    "汌": "串",
    "懟": "对",
    "狳": "鱼",
    "鸚": "应",
    "泇": "家",
    "辮": "变",
    "祵": "捆",
    "佋": "招",
    "狌": "生",
    "慜": "敏",
    "挱": "撒",
    "鑵": "灌",
    "瞀": "帽",
    "甴": "炸",
    "鬏": "纠",
    "枻": "意",
    "齧": "聂",
    "誸": "闲",
    "鮰": "回",
    "锱": "姿",
    "扙": "帐",
    "壟": "垄",
    "敹": "聊",
    "夛": "多",
    "豋": "灯",
    "楄": "偏",
    "帹": "煞",
    "鉞": "月",
    "莯": "木",
    "洓": "色",
    "敍": "续",
    "鬨": "红",
    "藪": "搜",
    "幆": "意",
    "埡": "呀",
    "侞": "如",
    "狴": "必",
    "蜍": "除",
    "鐥": "善",
    "讁": "哲",
    "蘀": "拓",
    "嚶": "应",
    "酾": "筛",
    "竻": "乐",
    "鐓": "对",
    "竫": "静",
    "覬": "记",
    "肀": "玉",
    "荖": "老",
    "怃": "五",
    "渧": "地",
    "糬": "鼠",
    "靐": "病",
    "吲": "引",
    "瀟": "消",
    "饗": "想",
    "纁": "熏",
    "巒": "鸾",
    "肈": "照",
    "饹": "了",
    "啀": "埃",
    "濰": "维",
    "欵": "款",
    "葜": "掐",
    "椠": "欠",
    "恹": "烟",
    "亸": "朵",
    "荝": "册",
    "鍼": "真",
    "珛": "秀",
    "倢": "节",
    "揀": "减",
    "跂": "其",
    "袑": "绍",
    "倓": "谈",
    "閙": "闹",
    "鱽": "刀",
    "絝": "裤",
    "檊": "干",
    "鼩": "取",
    "吀": "灭",
    "鱨": "长",
    "鉚": "柳",
    "禸": "柔",
    "掍": "混",
    "咹": "恶",
    "澴": "环",
    "悒": "意",
    "貎": "尼",
    "坈": "容",
    "貽": "宜",
    "垯": "哒",
    "芼": "帽",
    "巉": "缠",
    "筢": "爬",
    "鞔": "瞒",
    "邇": "耳",
    "嚙": "聂",
    "鄔": "乌",
    "玕": "干",
    "唦": "沙",
    "黹": "指",
    "儜": "凝",
    "溾": "哀",
    "笓": "必",
    "瘑": "锅",
    "簠": "辅",
    "籶": "深",
    "痄": "咋",
    "憚": "蛋",
    "囮": "额",
    "躄": "必",
    "輓": "碗",
    "姽": "鬼",
    "傗": "处",
    "簩": "劳",
    "晸": "整",
    "胬": "努",
    "姈": "零",
    "聹": "凝",
    "欷": "西",
    "帇": "聂",
    "苖": "敌",
    "瘅": "单",
    "譺": "爱",
    "媖": "应",
    "顕": "显",
    "羖": "古",
    "綅": "亲",
    "蹰": "除",
    "眊": "帽",
    "簳": "感",
    "粦": "林",
    "醵": "巨",
    "镎": "拿",
    "橈": "扰",
    "獀": "搜",
    "頷": "汉",
    "礫": "利",
    "啘": "夜",
    "跐": "刺",
    "衎": "看",
    "柸": "培",
    "湚": "印",
    "掞": "善",
    "肸": "西",
    "賑": "镇",
    "湣": "敏",
    "皊": "零",
    "壈": "懒",
    "翙": "会",
    "匋": "桃",
    "暟": "凯",
    "辁": "全",
    "捑": "责",
    "嫄": "原",
    "宔": "主",
    "岋": "恶",
    "逇": "顿",
    "兊": "对",
    "緌": "瑞",
    "耲": "怀",
    "铞": "掉",
    "聱": "敖",
    "飐": "展",
    "仹": "风",
    "讱": "任",
    "昳": "叠",
    "臓": "藏",
    "謾": "瞒",
    "喁": "庸",
    "偓": "卧",
    "毊": "消",
    "甃": "昼",
    "飖": "摇",
    "鄚": "帽",
    "澝": "宁",
    "钁": "觉",
    "飴": "宜",
    "轳": "芦",
    "儠": "裂",
    "犢": "毒",
    "麯": "区",
    "皟": "则",
    "瓩": "前",
    "舨": "板",
    "萠": "攀",
    "鎔": "容",
    "蟪": "会",
    "莋": "做",
    "姍": "山",
    "钂": "躺",
    "敔": "雨",
    "弙": "乌",
    "賶": "仓",
    "栿": "福",
    "詁": "古",
    "鲥": "石",
    "燂": "谈",
    "胩": "卡",
    "檓": "毁",
    "厾": "督",
    "萻": "安",
    "儨": "至",
    "鈅": "月",
    "稃": "夫",
    "簃": "宜",
    "楛": "互",
    "舢": "山",
    "驥": "记",
    "芻": "除",
    "鍀": "德",
    "竮": "平",
    "堔": "深",
    "鑛": "矿",
    "蝻": "男",
    "鐙": "凳",
    "薆": "爱",
    "祦": "无",
    "瞤": "润",
    "鄗": "号",
    "籔": "搜",
    "恧": "女",
    "皕": "必",
    "瀘": "芦",
    "釗": "招",
    "傒": "西",
    "刕": "离",
    "軚": "带",
    "蟓": "向",
    "濩": "或",
    "枒": "呀",
    "灜": "营",
    "蝨": "诗",
    "蹻": "绝",
    "拃": "眨",
    "濓": "连",
    "嘡": "汤",
    "蹧": "糟",
    "奼": "差",
    "歛": "憨",
    "獺": "塔",
    "洜": "落",
    "礓": "将",
    "倞": "静",
    "潏": "玉",
    "櫧": "朱",
    "梾": "来",
    "蚸": "利",
    "巘": "眼",
    "辵": "绰",
    "詇": "样",
    "姸": "颜",
    "唽": "西",
    "覩": "堵",
    "鵲": "确",
    "侁": "深",
    "槗": "桥",
    "晙": "俊",
    "綯": "桃",
    "壴": "助",
    "伾": "批",
    "儭": "衬",
    "壠": "垄",
    "塴": "甭",
    "鼡": "鼠",
    "肭": "那",
    "瀍": "缠",
    "挌": "格",
    "牱": "歌",
    "劦": "鞋",
    "銫": "色",
    "朙": "明",
    "耖": "吵",
    "妸": "阿",
    "觭": "机",
    "麤": "粗",
    "碞": "颜",
    "彔": "路",
    "璂": "其",
    "氥": "西",
    "鑶": "藏",
    "侫": "宁",
    "禩": "四",
    "绖": "叠",
    "劂": "觉",
    "袆": "灰",
    "槚": "假",
    "亙": "根",
    "菋": "位",
    "絪": "因",
    "謫": "哲",
    "櫟": "利",
    "膪": "踹",
    "嘫": "然",
    "穤": "诺",
    "谫": "减",
    "餬": "胡",
    "晅": "选",
    "訢": "心",
    "鍢": "父",
    "唊": "夹",
    "惪": "德",
    "仧": "长",
    "杧": "忙",
    "萳": "男",
    "埆": "确",
    "蛻": "退",
    "楅": "逼",
    "髈": "绑",
    "楱": "揍",
    "朧": "龙",
    "鞮": "低",
    "狛": "博",
    "鐺": "当",
    "磞": "砰",
    "珨": "侠",
    "茤": "记",
    "咰": "树",
    "欙": "雷",
    "帴": "散",
    "庳": "必",
    "嶏": "配",
    "襕": "蓝",
    "謌": "歌",
    "瀋": "审",
    "惗": "聂",
    "屚": "漏",
    "璈": "敖",
    "鉏": "除",
    "癬": "选",
    "奌": "点",
    "荮": "昼",
    "鬪": "豆",
    "琁": "旋",
    "鲻": "姿",
    "蟎": "满",
    "塙": "确",
    "欖": "懒",
    "覑": "偏",
    "疰": "助",
    "瑉": "民",
    "朅": "妾",
    "枂": "卧",
    "蔹": "脸",
    "騫": "前",
    "檙": "成",
    "蕐": "华",
    "卺": "紧",
    "珴": "额",
    "寜": "凝",
    "牤": "忙",
    "鈐": "钱",
    "搨": "踏",
    "偝": "被",
    "釒": "金",
    "劁": "敲",
    "璥": "井",
    "竾": "持",
    "娰": "四",
    "軼": "意",
    "奓": "扎",
    "纍": "雷",
    "莮": "南",
    "鲣": "间",
    "鹁": "博",
    "傺": "赤",
    "莢": "夹",
    "聟": "续",
    "仐": "三",
    "羕": "样",
    "潾": "林",
    "謡": "摇",
    "吅": "宣",
    "倷": "奶",
    "迀": "干",
    "嫺": "闲",
    "堬": "鱼",
    "掔": "前",
    "蕥": "哑",
    "蕎": "桥",
    "絙": "环",
    "滹": "呼",
    "毹": "书",
    "惛": "昏",
    "逬": "甭",
    "愨": "确",
    "鶇": "东",
    "滠": "设",
    "罺": "朝",
    "紸": "助",
    "胠": "区",
    "悇": "图",
    "葰": "虽",
    "欓": "挡",
    "驵": "脏",
    "嫪": "烙",
    "翃": "红",
    "鯽": "贼",
    "伓": "批",
    "寀": "采",
    "揠": "亚",
    "脮": "内",
    "齮": "以",
    "緾": "缠",
    "忭": "变",
    "棶": "来",
    "闁": "包",
    "聾": "龙",
    "轷": "呼",
    "礀": "见",
    "斱": "着",
    "邏": "罗",
    "賸": "胜",
    "烴": "听",
    "謕": "提",
    "棐": "匪",
    "廆": "归",
    "痳": "林",
    "憡": "册",
    "乁": "宜",
    "萇": "长",
    "嵎": "鱼",
    "綪": "欠",
    "軛": "恶",
    "粠": "红",
    "羼": "颤",
    "痦": "物",
    "蟴": "思",
    "淃": "倦",
    "檒": "风",
    "蔀": "不",
    "嚸": "点",
    "偑": "风",
    "鲙": "快",
    "傉": "怒",
    "纞": "练",
    "洚": "降",
    "堦": "接",
    "驋": "波",
    "瓌": "归",
    "髎": "聊",
    "縴": "欠",
    "紈": "完",
    "挗": "觉",
    "唍": "碗",
    "襬": "摆",
    "醸": "娘",
    "漀": "请",
    "祊": "崩",
    "礳": "墨",
    "鬢": "宾",
    "簑": "缩",
    "灋": "法",
    "扂": "电",
    "覷": "去",
    "鮃": "平",
    "儣": "旷",
    "儻": "躺",
    "滉": "荒",
    "铏": "行",
    "蒺": "极",
    "啋": "采",
    "嬡": "爱",
    "錵": "花",
    "嚨": "龙",
    "扡": "拖",
    "譟": "造",
    "悧": "利",
    "馫": "心",
    "彶": "极",
    "鲼": "愤",
    "瘼": "墨",
    "螬": "曹",
    "絾": "成",
    "餚": "摇",
    "轅": "原",
    "崯": "银",
    "崕": "牙",
    "羋": "米",
    "捥": "万",
    "唢": "锁",
    "螵": "飘",
    "蹀": "叠",
    "莬": "问",
    "婗": "尼",
    "摢": "互",
    "秖": "知",
    "胲": "海",
    "葇": "柔",
    "綷": "翠",
    "珗": "先",
    "縂": "总",
    "錒": "科",
    "蔌": "速",
    "輦": "年",
    "媃": "柔",
    "繎": "然",
    "饌": "赚",
    "糹": "思",
    "秠": "批",
    "躪": "吝",
    "觧": "解",
    "潁": "影",
    "鼽": "求",
    "柾": "就",
    "鲐": "台",
    "鄫": "增",
    "吘": "偶",
    "鎓": "瓮",
    "磜": "气",
    "罽": "记",
    "糇": "喉",
    "伹": "区",
    "慇": "因",
    "婞": "性",
    "圕": "图",
    "珋": "柳",
    "簺": "赛",
    "梼": "桃",
    "鋤": "除",
    "柅": "你",
    "覦": "鱼",
    "乧": "抖",
    "壢": "利",
    "驃": "标",
    "啯": "锅",
    "嵨": "物",
    "蹠": "直",
    "瘯": "促",
    "仸": "咬",
    "臑": "闹",
    "愝": "眼",
    "钸": "不",
    "姁": "许",
    "穠": "农",
    "袊": "领",
    "謘": "持",
    "敱": "埃",
    "蓧": "掉",
    "洖": "无",
    "嬤": "妈",
    "鈽": "不",
    "瞍": "搜",
    "徬": "旁",
    "儎": "在",
    "唣": "造",
    "曢": "了",
    "毞": "皮",
    "鏞": "庸",
    "鼔": "古",
    "蒡": "棒",
    "枟": "运",
    "賨": "从",
    "簕": "乐",
    "稈": "感",
    "湑": "需",
    "楙": "帽",
    "唞": "抖",
    "紦": "八",
    "膾": "快",
    "閟": "必",
    "邉": "编",
    "洯": "妾",
    "裛": "意",
    "鍌": "显",
    "籥": "月",
    "歓": "欢",
    "衊": "灭",
    "摱": "慢",
    "琋": "西",
    "旣": "记",
    "睒": "闪",
    "翫": "完",
    "貰": "是",
    "懮": "有",
    "岞": "做",
    "輟": "绰",
    "嗏": "插",
    "茇": "拔",
    "仚": "先",
    "姇": "夫",
    "琩": "昌",
    "卹": "续",
    "偱": "寻",
    "筳": "停",
    "硓": "老",
    "堌": "固",
    "垬": "红",
    "骒": "客",
    "妭": "拔",
    "凎": "干",
    "橎": "烦",
    "粶": "路",
    "鞚": "控",
    "潕": "五",
    "垇": "奥",
    "尰": "种",
    "蔮": "国",
    "燾": "到",
    "猄": "精",
    "幪": "盟",
    "凔": "创",
    "畯": "俊",
    "紭": "红",
    "嬾": "懒",
    "垻": "爸",
    "殹": "意",
    "襛": "农",
    "蓣": "玉",
    "牿": "固",
    "昫": "续",
    "廌": "至",
    "秈": "先",
    "唚": "亲",
    "銖": "朱",
    "傦": "古",
    "潩": "意",
    "穉": "至",
    "躐": "裂",
    "腍": "任",
    "帍": "互",
    "赇": "求",
    "舃": "系",
    "曕": "燕",
    "犏": "偏",
    "緖": "续",
    "鍎": "图",
    "纻": "助",
    "虒": "思",
    "瘌": "蜡",
    "訚": "银",
    "杬": "原",
    "蕰": "温",
    "樉": "双",
    "鏻": "林",
    "夆": "逢",
    "盨": "许",
    "搆": "够",
    "怩": "尼",
    "偟": "黄",
    "嘸": "辅",
    "暏": "鼠",
    "劻": "框",
    "媤": "思",
    "瀏": "刘",
    "幗": "国",
    "洝": "按",
    "瑒": "唱",
    "垞": "茶",
    "禋": "因",
    "絔": "摆",
    "耵": "丁",
    "縊": "意",
    "埶": "意",
    "鈸": "博",
    "閆": "颜",
    "曺": "曹",
    "孿": "鸾",
    "墈": "看",
    "筥": "举",
    "磉": "桑",
    "暚": "摇",
    "卄": "念",
    "仯": "吵",
    "羴": "山",
    "萺": "帽",
    "縈": "营",
    "狝": "显",
    "憝": "对",
    "洢": "一",
    "綻": "战",
    "篚": "匪",
    "纩": "矿",
    "酈": "利",
    "秷": "至",
    "乿": "至",
    "蠪": "龙",
    "潚": "速",
    "驩": "欢",
    "旈": "刘",
    "搷": "田",
    "冱": "互",
    "誫": "镇",
    "圞": "鸾",
    "瑈": "柔",
    "拫": "很",
    "垝": "鬼",
    "淉": "果",
    "悰": "从",
    "饞": "缠",
    "掎": "几",
    "灪": "玉",
    "鲚": "记",
    "杋": "烦",
    "僊": "先",
    "峧": "教",
    "嵙": "科",
    "厖": "旁",
    "坣": "唐",
    "悜": "成",
    "蝧": "应",
    "祃": "骂",
    "窣": "苏",
    "婥": "闹",
    "闌": "蓝",
    "俫": "来",
    "黩": "毒",
    "櫶": "显",
    "乆": "久",
    "韘": "设",
    "眍": "口",
    "珎": "真",
    "勊": "客",
    "擭": "卧",
    "荈": "喘",
    "蕑": "间",
    "倀": "昌",
    "鄀": "弱",
    "郪": "七",
    "炑": "木",
    "浽": "虽",
    "丵": "着",
    "媏": "端",
    "漖": "教",
    "兏": "长",
    "鰺": "深",
    "屝": "费",
    "敮": "侠",
    "穣": "嚷",
    "偙": "地",
    "璆": "求",
    "苝": "被",
    "縞": "搞",
    "灃": "风",
    "韮": "久",
    "讃": "赞",
    "颩": "标",
    "鲴": "固",
    "秪": "知",
    "崌": "居",
    "坬": "挂",
    "銼": "错",
    "忥": "系",
    "菛": "门",
    "缊": "晕",
    "筚": "必",
    "轆": "路",
    "秞": "由",
    "笘": "山",
    "釹": "女",
    "姴": "裂",
    "姱": "夸",
    "鎿": "拿",
    "燜": "闷",
    "茖": "格",
    "覲": "进",
    "瑅": "提",
    "鄜": "夫",
    "綃": "消",
    "嵂": "绿",
    "朓": "挑",
    "瘽": "琴",
    "枔": "信",
    "愶": "鞋",
    "籏": "其",
    "珄": "生",
    "綖": "颜",
    "韌": "任",
    "枙": "饿",
    "苭": "咬",
    "餃": "角",
    "褢": "怀",
    "漨": "逢",
    "炌": "开",
    "赿": "持",
    "圀": "国",
    "聺": "茄",
    "觖": "觉",
    "熒": "营",
    "宑": "井",
    "嘼": "处",
    "侚": "训",
    "鮠": "维",
    "棌": "菜",
    "攣": "鸾",
    "粅": "物",
    "頞": "恶",
    "錀": "伦",
    "邚": "如",
    "佂": "睁",
    "艌": "念",
    "妠": "那",
    "貙": "出",
    "驪": "离",
    "疬": "利",
    "燼": "进",
    "埫": "宠",
    "圴": "着",
    "炩": "另",
    "仒": "冰",
    "嫙": "旋",
    "觃": "燕",
    "鳮": "机",
    "燋": "教",
    "憇": "气",
    "茀": "福",
    "嘙": "婆",
    "哳": "扎",
    "喥": "夺",
    "鴇": "宝",
    "颋": "挺",
    "櫌": "优",
    "鵠": "胡",
    "姃": "睁",
    "羓": "八",
    "儐": "彬",
    "釿": "金",
    "堚": "魂",
    "惔": "谈",
    "慚": "残",
    "寚": "宝",
    "臯": "高",
    "龗": "零",
    "裀": "因",
    "橞": "会",
    "訝": "亚",
    "爍": "硕",
    "捃": "俊",
    "殽": "肖",
    "澱": "电",
    "堽": "刚",
    "哃": "同",
    "浡": "博",
    "蝭": "提",
    "濏": "色",
    "蒱": "葡",
    "炶": "闪",
    "蝤": "求",
    "艅": "鱼",
    "鮀": "驮",
    "媜": "睁",
    "苩": "博",
    "鏑": "敌",
    "眔": "大",
    "羶": "山",
    "渁": "冤",
    "韁": "将",
    "菝": "拔",
    "溡": "石",
    "晛": "现",
    "栳": "老",
    "敺": "区",
    "楋": "蜡",
    "徯": "西",
    "緟": "虫",
    "朥": "劳",
    "閡": "爱",
    "晣": "哲",
    "踺": "见",
    "桭": "真",
    "鲺": "诗",
    "溋": "营",
    "姙": "任",
    "犂": "离",
    "葎": "绿",
    "籾": "尼",
    "猸": "梅",
    "颟": "慢",
    "娖": "绰",
    "魜": "人",
    "嘮": "劳",
    "俁": "雨",
    "諗": "审",
    "殍": "漂",
    "靦": "舔",
    "堉": "玉",
    "嫈": "应",
    "檝": "极",
    "寑": "寝",
    "珌": "必",
    "琹": "琴",
    "裈": "昆",
    "溘": "客",
    "鐲": "着",
    "偋": "病",
    "尞": "料",
    "鱺": "离",
    "枹": "包",
    "殭": "将",
    "裵": "培",
    "虍": "呼",
    "弸": "朋",
    "諺": "燕",
    "讒": "缠",
    "砞": "墨",
    "魴": "房",
    "峴": "现",
    "儬": "庆",
    "玨": "觉",
    "炟": "达",
    "臔": "现",
    "嶧": "意",
    "靄": "矮",
    "鋐": "红",
    "壙": "矿",
    "瞷": "见",
    "哯": "现",
    "稆": "旅",
    "虯": "求",
    "鷇": "扣",
    "鈿": "田",
    "斈": "学",
    "槡": "丧",
    "偔": "恶",
    "瑝": "黄",
    "卾": "恶",
    "姖": "巨",
    "苪": "饼",
    "坮": "台",
    "鏜": "汤",
    "粰": "福",
    "睪": "意",
    "鈭": "姿",
    "羆": "皮",
    "嫭": "互",
    "葁": "将",
    "伩": "信",
    "苮": "先",
    "鄕": "相",
    "猇": "消",
    "芺": "袄",
    "皀": "极",
    "齦": "肯",
    "筴": "册",
    "懽": "欢",
    "梠": "旅",
    "棈": "欠",
    "懺": "颤",
    "妏": "问",
    "喦": "聂",
    "舦": "太",
    "爜": "从",
    "鈇": "夫",
    "廩": "吝",
    "蝸": "窝",
    "阩": "生",
    "滳": "伤",
    "驡": "龙",
    "筁": "区",
    "皐": "高",
    "挍": "教",
    "篺": "皮",
    "絬": "谢",
    "姛": "动",
    "葷": "昏",
    "祹": "桃",
    "攷": "考",
    "藺": "吝",
    "彨": "吃",
    "筎": "如",
    "婏": "饭",
    "澁": "色",
    "晻": "按",
    "慡": "双",
    "溞": "骚",
    "拑": "钱",
    "釁": "信",
    "暔": "南",
    "膄": "瘦",
    "曄": "夜",
    "糢": "魔",
    "飭": "赤",
    "弌": "一",
    "犋": "巨",
    "詪": "很",
    "朢": "忘",
    "敭": "羊",
    "榦": "干",
    "寕": "凝",
    "鈤": "日",
    "鱲": "裂",
    "瞵": "林",
    "憫": "敏",
    "聛": "比",
    "睘": "穷",
    "眅": "攀",
    "薗": "原",
    "芚": "吞",
    "驫": "标",
    "畐": "福",
    "朏": "匪",
    "鲹": "深",
    "栻": "是",
    "鼯": "无",
    "疭": "宗",
    "賥": "岁",
    "蟞": "憋",
    "嘰": "机",
    "犾": "银",
    "緆": "西",
    "嘔": "偶",
    "窸": "西",
    "鹓": "冤",
    "驤": "相",
    "姎": "养",
    "聍": "凝",
    "櫂": "照",
    "嫽": "聊",
    "醤": "降",
    "嶸": "容",
    "曵": "夜",
    "辚": "林",
    "潪": "这",
    "牾": "五",
    "儗": "你",
    "蕡": "坟",
    "煠": "炸",
    "嚦": "利",
    "哊": "右",
    "罙": "深",
    "勑": "赤",
    "阯": "指",
    "沷": "发",
    "剻": "捧",
    "葽": "邀",
    "唪": "奉",
    "儰": "伟",
    "倧": "宗",
    "釩": "反",
    "魕": "几",
    "玭": "贫",
    "絘": "次",
    "塥": "格",
    "棫": "玉",
    "鉾": "谋",
    "鱔": "善",
    "棸": "邹",
    "錬": "练",
    "巌": "颜",
    "蓺": "意",
    "羭": "鱼",
    "抇": "胡",
    "撖": "汉",
    "纴": "任",
    "暅": "更",
    "迚": "达",
    "铔": "呀",
    "栵": "裂",
    "盺": "心",
    "綢": "愁",
    "慥": "造",
    "孭": "灭",
    "鸛": "灌",
    "摑": "乖",
    "輜": "姿",
    "厰": "场",
    "僣": "铁",
    "焄": "熏",
    "幖": "标",
    "蚦": "然",
    "皁": "造",
    "帓": "墨",
    "驎": "林",
    "鴒": "零",
    "奡": "奥",
    "怊": "超",
    "鼷": "西",
    "魘": "眼",
    "昪": "变",
    "湸": "亮",
    "鸻": "横",
    "掻": "骚",
    "霽": "记",
    "敩": "笑",
    "瘨": "颠",
    "骃": "因",
    "倰": "愣",
    "緳": "鞋",
    "疌": "节",
    "貮": "二",
    "蝟": "位",
    "澛": "鲁",
    "妴": "院",
    "屃": "系",
    "厐": "旁",
    "柤": "扎",
    "玓": "地",
    "詬": "够",
    "邤": "心",
    "侂": "拖",
    "蘂": "瑞",
    "疂": "叠",
    "呧": "底",
    "遶": "绕",
    "倅": "翠",
    "楪": "夜",
    "臚": "芦",
    "紪": "七",
    "舭": "比",
    "囸": "日",
    "竡": "摆",
    "蓂": "明",
    "巇": "西",
    "筦": "管",
    "隄": "低",
    "厯": "利",
    "桊": "倦",
    "紞": "胆",
    "杇": "乌",
    "弜": "降",
    "殞": "允",
    "哱": "波",
    "縥": "枕",
    "喓": "邀",
    "庛": "次",
    "篸": "惨",
    "誧": "不",
    "苶": "捏",
    "裇": "需",
    "痃": "旋",
    "糥": "诺",
    "毦": "耳",
    "溚": "塔",
    "泲": "几",
    "琺": "发",
    "矟": "硕",
    "彇": "消",
    "緲": "秒",
    "澬": "姿",
    "柦": "蛋",
    "鷸": "玉",
    "軆": "体",
    "摛": "吃",
    "牎": "窗",
    "偐": "燕",
    "蒈": "凯",
    "胕": "辅",
    "旪": "鞋",
    "欬": "开",
    "畻": "成",
    "簊": "机",
    "塽": "双",
    "凨": "风",
    "牘": "毒",
    "唖": "哑",
    "禵": "提",
    "墺": "奥",
    "藌": "密",
    "髫": "条",
    "噏": "西",
    "扖": "入",
    "鹖": "和",
    "幎": "密",
    "柺": "拐",
    "唃": "古",
    "駮": "博",
    "暪": "闷",
    "奻": "暖",
    "徦": "假",
    "狘": "血",
    "閠": "润",
    "褆": "提",
    "瘖": "因",
    "妬": "度",
    "霫": "习",
    "埅": "房",
    "倐": "书",
    "阽": "电",
    "敾": "善",
    "覠": "君",
    "攲": "七",
    "噚": "寻",
    "湥": "突",
    "阧": "抖",
    "筭": "算",
    "鹋": "描",
    "耠": "霍",
    "湶": "全",
    "呫": "贴",
    "痋": "腾",
    "燴": "会",
    "觢": "是",
    "裊": "尿",
    "璱": "色",
    "砮": "努",
    "洊": "见",
    "掗": "亚",
    "槇": "颠",
    "赪": "称",
    "馂": "俊",
    "畵": "话",
    "粔": "巨",
    "渏": "一",
    "癟": "别",
    "钘": "行",
    "鷍": "消",
    "騁": "成",
    "螗": "唐",
    "惲": "运",
    "皝": "荒",
    "亴": "右",
    "銬": "烤",
    "沝": "缀",
    "彉": "锅",
    "莙": "君",
    "艴": "福",
    "籘": "腾",
    "珒": "金",
    "僆": "练",
    "紩": "至",
    "詫": "差",
    "痙": "静",
    "糀": "花",
    "昄": "板",
    "鮁": "波",
    "遈": "石",
    "沇": "眼",
    "襖": "袄",
    "鄖": "云",
    "餽": "溃",
    "崢": "睁",
    "姠": "向",
    "蒾": "迷",
    "遹": "玉",
    "衼": "知",
    "鲒": "节",
    "稙": "知",
    "窾": "款",
    "莦": "烧",
    "柉": "烦",
    "廍": "不",
    "嫤": "紧",
    "瓀": "软",
    "巟": "荒",
    "汣": "久",
    "砆": "夫",
    "鄘": "庸",
    "獙": "必",
    "蚮": "带",
    "痖": "哑",
    "頡": "鞋",
    "餴": "分",
    "狉": "批",
    "紟": "金",
    "儊": "处",
    "捚": "摘",
    "薺": "记",
    "抲": "喝",
    "脃": "翠",
    "瞙": "墨",
    "讖": "衬",
    "晹": "意",
    "姢": "捐",
    "閎": "红",
    "櫼": "间",
    "瞐": "墨",
    "弽": "设",
    "唗": "兜",
    "唓": "车",
    "髖": "宽",
    "褝": "单",
    "蔄": "慢",
    "鎘": "利",
    "抰": "养",
    "抷": "批",
    "鳆": "父",
    "餔": "不",
    "謟": "掏",
    "櫎": "谎",
    "汖": "聘",
    "縉": "进",
    "歀": "款",
    "踭": "睁",
    "睙": "裂",
    "簵": "路",
    "鯡": "费",
    "躾": "美",
    "渙": "换",
    "婻": "难",
    "弻": "必",
    "鼈": "憋",
    "綰": "碗",
    "釭": "刚",
    "刧": "节",
    "潡": "顿",
    "擓": "快",
    "瀼": "嚷",
    "烚": "侠",
    "炐": "胖",
    "垶": "星",
    "亹": "伟",
    "鴅": "欢",
    "襶": "带",
    "犪": "奎",
    "癩": "赖",
    "刄": "任",
    "佮": "格",
    "戉": "月",
    "褖": "团",
    "蹺": "敲",
    "繻": "需",
    "暋": "敏",
    "玂": "其",
    "鯧": "昌",
    "盵": "气",
    "贔": "必",
    "嚗": "博",
    "簀": "则",
    "曪": "裸",
    "狋": "宜",
    "嫹": "描",
    "偛": "插",
    "嫨": "憨",
    "衱": "节",
    "纀": "葡",
    "丏": "免",
    "銏": "善",
    "蔦": "尿",
    "酂": "搓",
    "梜": "家",
    "睎": "西",
    "礛": "间",
    "賔": "彬",
    "鯔": "姿",
    "挷": "朋",
    "辀": "周",
    "咅": "剖",
    "丠": "秋",
    "伃": "鱼",
    "翣": "煞",
    "泃": "居",
    "繀": "岁",
    "掜": "意",
    "铻": "无",
    "颺": "羊",
    "潙": "维",
    "昽": "龙",
    "蕅": "偶",
    "鑪": "芦",
    "躭": "单",
    "骹": "敲",
    "眹": "镇",
    "厼": "耳",
    "蘫": "汉",
    "顥": "号",
    "栁": "柳",
    "屟": "谢",
    "眴": "炫",
    "紂": "昼",
    "鲃": "八",
    "泿": "银",
    "縝": "陈",
    "蒎": "派",
    "勠": "路",
    "潄": "树",
    "煬": "羊",
    "奍": "圈",
    "搲": "挖",
    "臍": "其",
    "濶": "扩",
    "痷": "安",
    "辿": "搀",
    "涬": "性",
    "渷": "眼",
    "搣": "灭",
    "杮": "费",
    "踇": "母",
    "繊": "先",
    "鍕": "君",
    "淓": "方",
    "鸰": "零",
    "蒍": "伟",
    "琗": "色",
    "瀽": "减",
    "暣": "气",
    "嫆": "容",
    "鐚": "呀",
    "鴛": "冤",
    "杙": "意",
    "巺": "训",
    "鲕": "而",
    "陁": "驮",
    "烸": "海",
    "薁": "玉",
    "捊": "剖",
    "蒞": "利",
    "掋": "底",
    "瀁": "样",
    "碏": "确",
    "澇": "烙",
    "嚿": "或",
    "癈": "费",
    "衴": "胆",
    "鹟": "瓮",
    "騑": "飞",
    "蔲": "扣",
    "醨": "离",
    "潬": "善",
    "鐳": "雷",
    "麑": "尼",
    "鬽": "妹",
    "婍": "起",
    "廼": "奶",
    "鬭": "豆",
    "塹": "欠",
    "鹀": "无",
    "縵": "慢",
    "赆": "进",
    "喯": "喷",
    "咡": "二",
    "廄": "就",
    "軔": "任",
    "袃": "拆",
    "籟": "赖",
    "琿": "魂",
    "倗": "朋",
    "櫄": "春",
    "硈": "恰",
    "挃": "至",
    "弶": "降",
    "禔": "知",
    "輾": "展",
    "肶": "皮",
    "侌": "因",
    "箙": "福",
    "錛": "奔",
    "苽": "姑",
    "倁": "知",
    "琠": "舔",
    "洃": "灰",
    "硄": "框",
    "詟": "哲",
    "蝓": "鱼",
    "溍": "进",
    "堊": "恶",
    "蘘": "嚷",
    "秺": "度",
    "槅": "格",
    "叜": "搜",
    "靣": "面",
    "赻": "显",
    "蔶": "则",
    "瘺": "漏",
    "晽": "林",
    "綆": "梗",
    "謢": "撸",
    "僢": "喘",
    "昋": "贵",
    "婌": "熟",
    "謐": "密",
    "嬋": "缠",
    "唲": "而",
    "鑊": "或",
    "粖": "墨",
    "餾": "六",
    "楉": "弱",
    "櫨": "芦",
    "悵": "唱",
    "倕": "垂",
    "棽": "深",
    "蝝": "原",
    "釬": "汉",
    "盁": "营",
    "笶": "使",
    "廡": "五",
    "鰩": "摇",
    "嚞": "哲",
    "桻": "风",
    "緗": "相",
    "斲": "着",
    "謋": "或",
    "譞": "宣",
    "恇": "框",
    "畞": "母",
    "瀆": "毒",
    "巆": "容",
    "漧": "干",
    "鵑": "捐",
    "窛": "扣",
    "竪": "树",
    "硙": "维",
    "飝": "飞",
    "釧": "串",
    "壵": "壮",
    "纘": "钻",
    "簭": "是",
    "憙": "西",
    "晲": "你",
    "蝣": "由",
    "掆": "刚",
    "磘": "摇",
    "鐔": "信",
    "騏": "其",
    "巠": "精",
    "藠": "教",
    "膋": "聊",
    "抅": "居",
    "黢": "区",
    "殮": "练",
    "曀": "意",
    "墫": "尊",
    "殩": "窜",
    "宆": "穷",
    "琓": "完",
    "櫤": "降",
    "蔯": "陈",
    "薀": "运",
    "睆": "换",
    "轡": "配",
    "匼": "科",
    "樷": "从",
    "煿": "博",
    "洘": "考",
    "祏": "石",
    "皦": "角",
    "阷": "称",
    "鋳": "助",
    "詡": "许",
    "贍": "善",
    "锽": "黄",
    "龥": "玉",
    "唒": "求",
    "悾": "空",
    "嵥": "节",
    "迆": "宜",
    "秴": "和",
    "瑍": "换",
    "煐": "应",
    "惂": "砍",
    "蓀": "孙",
    "娤": "装",
    "脎": "萨",
    "皥": "号",
    "箰": "损",
    "儢": "旅",
    "爝": "觉",
    "瓟": "博",
    "俛": "辅",
    "腨": "涮",
    "硑": "砰",
    "窅": "咬",
    "篭": "龙",
    "褦": "耐",
    "楒": "思",
    "翯": "贺",
    "饃": "魔",
    "榠": "明",
    "黓": "意",
    "笖": "以",
    "蜑": "蛋",
    "熲": "窘",
    "鮱": "老",
    "抪": "不",
    "糌": "赞",
    "箠": "垂",
    "牗": "有",
    "縹": "漂",
    "嘜": "骂",
    "鶸": "弱",
    "詓": "曲",
    "棑": "排",
    "橢": "妥",
    "眗": "居",
    "炣": "可",
    "蓰": "洗",
    "繄": "一",
    "晵": "起",
    "嵆": "机",
    "菻": "吝",
    "軫": "枕",
    "餹": "唐",
    "徍": "网",
    "刼": "节",
    "豷": "意",
    "紇": "和",
    "蒻": "弱",
    "賡": "耕",
    "鳪": "哺",
    "澣": "换",
    "妼": "必",
    "楬": "节",
    "袕": "学",
    "裯": "愁",
    "敼": "以",
    "柷": "处",
    "鑀": "爱",
    "湭": "求",
    "擩": "乳",
    "鴈": "燕",
    "苰": "红",
    "喌": "周",
    "軻": "科",
    "鯓": "深",
    "撔": "红",
    "闉": "因",
    "浵": "同",
    "摂": "设",
    "緘": "间",
    "溁": "营",
    "軦": "矿",
    "湋": "维",
    "鶚": "恶",
    "蜣": "枪",
    "峘": "环",
    "嘦": "教",
    "絣": "崩",
    "銙": "垮",
    "丂": "考",
    "朷": "刀",
    "撾": "窝",
    "垏": "绿",
    "銻": "提",
    "獥": "教",
    "惉": "占",
    "廕": "印",
    "烱": "窘",
    "泴": "灌",
    "噰": "庸",
    "擷": "鞋",
    "挴": "美",
    "囻": "国",
    "厎": "底",
    "瑳": "搓",
    "盻": "系",
    "垰": "卡",
    "劚": "竹",
    "俙": "西",
    "勷": "嚷",
    "礚": "科",
    "蘗": "薄",
    "淢": "玉",
    "焮": "信",
    "娮": "颜",
    "鍺": "朵",
    "匄": "概",
    "匽": "眼",
    "鉍": "必",
    "聦": "聪",
    "埳": "砍",
    "頫": "辅",
    "呅": "梅",
    "呏": "生",
    "溸": "速",
    "噝": "思",
    "惝": "场",
    "搢": "进",
    "骕": "速",
    "窊": "挖",
    "銥": "一",
    "阢": "物",
    "撗": "逛",
    "鎱": "原",
    "槺": "康",
    "翺": "敖",
    "鲌": "爸",
    "粯": "现",
    "亷": "连",
    "骍": "星",
    "奣": "瓮",
    "騧": "瓜",
    "箘": "俊",
    "訕": "善",
    "唝": "共",
    "薳": "伟",
    "厽": "垒",
    "墽": "敲",
    "侢": "带",
    "娒": "梅",
    "癘": "利",
    "頴": "影",
    "諴": "闲",
    "瀯": "营",
    "聨": "连",
    "鉱": "矿",
    "蘺": "离",
    "鮓": "眨",
    "蒓": "纯",
    "燏": "玉",
    "瓛": "环",
    "遆": "提",
    "矷": "子",
    "殣": "进",
    "剅": "楼",
    "敳": "埃",
    "粙": "昼",
    "畱": "刘",
    "釷": "土",
    "麅": "袍",
    "觝": "底",
    "謗": "棒",
    "凲": "干",
    "嚰": "魔",
    "惈": "果",
    "撣": "胆",
    "焢": "轰",
    "藞": "啦",
    "擫": "夜",
    "賚": "赖",
    "鯙": "纯",
    "躉": "蹲",
    "姅": "办",
    "爼": "组",
    "惤": "间",
    "圵": "荡",
    "葂": "免",
    "羛": "意",
    "柈": "办",
    "敧": "机",
    "篜": "睁",
    "鯷": "提",
    "臸": "知",
    "筩": "同",
    "氂": "毛",
    "騮": "刘",
    "歞": "恶",
    "孅": "前",
    "纒": "缠",
    "劐": "霍",
    "啣": "闲",
    "偞": "谢",
    "欥": "意",
    "昻": "昂",
    "琂": "颜",
    "邘": "鱼",
    "瓅": "利",
    "諠": "宣",
    "迋": "忘",
    "欱": "喝",
    "侹": "挺",
    "棪": "眼",
    "乢": "概",
    "筬": "成",
    "喅": "玉",
    "礞": "盟",
    "盿": "民",
    "獟": "要",
    "遡": "速",
    "弪": "静",
    "槪": "概",
    "傪": "参",
    "鴝": "取",
    "刲": "亏",
    "狥": "训",
    "摡": "概",
    "挓": "扎",
    "瑮": "利",
    "聝": "国",
    "瓲": "哇",
    "詀": "占",
    "滽": "庸",
    "鋇": "被",
    "儅": "荡",
    "獏": "墨",
    "硿": "空",
    "咍": "孩",
    "謿": "朝",
    "犅": "刚",
    "蹌": "枪",
    "铇": "报",
    "戤": "概",
    "廰": "听",
    "謅": "周",
    "豄": "毒",
    "簹": "当",
    "摷": "角",
    "栃": "利",
    "蟫": "银",
    "迖": "达",
    "羃": "密",
    "籇": "豪",
    "鼇": "敖",
    "媷": "入",
    "罿": "冲",
    "糡": "降",
    "扐": "乐",
    "齶": "恶",
    "閼": "恶",
    "邲": "必",
    "膬": "翠",
    "栂": "梅",
    "讎": "愁",
    "掅": "庆",
    "杽": "丑",
    "砢": "科",
    "暒": "情",
    "慼": "七",
    "摶": "团",
    "锠": "昌",
    "僇": "路",
    "缾": "平",
    "铦": "先",
    "禫": "蛋",
    "詎": "巨",
    "笩": "罚",
    "湩": "动",
    "廘": "路",
    "泂": "窘",
    "鴷": "裂",
    "駟": "四",
    "醡": "咋",
    "薔": "强",
    "舩": "传",
    "膕": "国",
    "焔": "燕",
    "鲰": "邹",
    "繬": "色",
    "璫": "当",
    "蹍": "年",
    "櫥": "除",
    "狢": "和",
    "錞": "纯",
    "筈": "扩",
    "璹": "熟",
    "踼": "唐",
    "魊": "玉",
    "抃": "变",
    "癹": "拔",
    "穓": "意",
    "魎": "两",
    "崠": "东",
    "擿": "踢",
    "葢": "概",
    "佒": "养",
    "秨": "做",
    "腃": "溃",
    "筣": "离",
    "鈀": "靶",
    "禗": "思",
    "嫿": "话",
    "諶": "陈",
    "歠": "绰",
    "鐟": "赞",
    "駢": "偏",
    "鐕": "赞",
    "夀": "瘦",
    "蜾": "果",
    "唙": "敌",
    "挐": "拿",
    "雰": "分",
    "蹾": "蹲",
    "鏤": "漏",
    "酖": "镇",
    "剳": "达",
    "鴟": "吃",
    "鉭": "坦",
    "鳘": "敏",
    "噀": "训",
    "愺": "草",
    "諟": "是",
    "坰": "窘",
    "摣": "扎",
    "煚": "窘",
    "櫲": "玉",
    "椆": "愁",
    "筿": "小",
    "魆": "需",
    "綇": "修",
    "宼": "扣",
    "祡": "柴",
    "俻": "被",
    "螋": "搜",
    "鳣": "占",
    "繛": "绰",
    "猙": "睁",
    "眎": "是",
    "穘": "消",
    "菢": "报",
    "荙": "达",
    "辒": "温",
    "譎": "觉",
    "矦": "喉",
    "縋": "缀",
    "旓": "烧",
    "盢": "续",
    "榞": "原",
    "瀀": "优",
    "躡": "聂",
    "穋": "路",
    "屇": "田",
    "墰": "谈",
    "浭": "耕",
    "鉸": "角",
    "奤": "哈",
    "嶲": "西",
    "椑": "杯",
    "擧": "举",
    "碡": "毒",
    "耤": "极",
    "剕": "费",
    "妶": "闲",
    "觤": "鬼",
    "滧": "摇",
    "朤": "浪",
    "銣": "如",
    "鯱": "虎",
    "厡": "原",
    "嗼": "墨",
    "簣": "溃",
    "鍫": "敲",
    "俥": "车",
    "唼": "煞",
    "溂": "蜡",
    "篋": "妾",
    "蟌": "聪",
    "匸": "系",
    "瑸": "彬",
    "喼": "接",
    "勗": "续",
    "唜": "墨",
    "陑": "而",
    "赼": "姿",
    "獰": "凝",
    "椪": "碰",
    "敇": "册",
    "萡": "波",
    "蠎": "忙",
    "狖": "右",
    "緔": "上",
    "毉": "一",
    "鎣": "营",
    "谸": "前",
    "娧": "退",
    "芉": "干",
    "瑔": "全",
    "鱟": "后",
    "聑": "贴",
    "縺": "连",
    "慤": "确",
    "嗢": "袜",
    "豲": "环",
    "蔿": "伟",
    "襷": "举",
    "楤": "耸",
    "朒": "女",
    "饅": "瞒",
    "麕": "君",
    "骯": "肮",
    "仭": "任",
    "飠": "石",
    "襜": "搀",
    "瑎": "鞋",
    "弬": "宜",
    "嵒": "颜",
    "聵": "溃",
    "蘤": "花",
    "奐": "换",
    "槢": "习",
    "飬": "倦",
    "蟣": "几",
    "鰧": "腾",
    "澏": "含",
    "礬": "烦",
    "抳": "你",
    "皃": "帽",
    "剗": "铲",
    "兠": "兜",
    "俤": "地",
    "嫗": "玉",
    "冁": "铲",
    "葒": "红",
    "凩": "木",
    "廮": "影",
    "莑": "朋",
    "緤": "谢",
    "晠": "胜",
    "噣": "昼",
    "銲": "汉",
    "喛": "换",
    "漷": "火",
    "碕": "其",
    "愙": "客",
    "萉": "费",
    "啇": "地",
    "挀": "掰",
    "戇": "壮",
    "殯": "宾",
    "篥": "利",
    "壌": "嚷",
    "鞪": "木",
    "觔": "金",
    "礐": "确",
    "糵": "聂",
    "犨": "抽",
    "噛": "聂",
    "闋": "确",
    "鳉": "将",
    "癵": "鸾",
    "愼": "肾",
    "萿": "扩",
    "猃": "显",
    "傃": "速",
    "矖": "洗",
    "珖": "光",
    "劋": "角",
    "鉲": "卡",
    "苃": "有",
    "狪": "同",
    "跔": "居",
    "眮": "同",
    "禆": "必",
    "硾": "缀",
    "枩": "松",
    "牳": "母",
    "籮": "罗",
    "疈": "屁",
    "鏐": "刘",
    "晿": "昌",
    "嘇": "山",
    "赽": "觉",
    "瑊": "间",
    "侒": "安",
    "轂": "古",
    "洔": "指",
    "璊": "门",
    "鹢": "意",
    "蓶": "维",
    "揜": "眼",
    "鈮": "你",
    "靷": "引",
    "脻": "接",
    "阨": "恶",
    "冾": "恰",
    "懡": "抹",
    "淲": "标",
    "砯": "平",
    "葻": "蓝",
    "滒": "歌",
    "砐": "恶",
    "疐": "至",
    "顰": "贫",
    "昅": "节",
    "鲗": "贼",
    "兟": "深",
    "獁": "骂",
    "艎": "黄",
    "瀺": "缠",
    "冘": "银",
    "勥": "降",
    "倠": "虽",
    "躞": "谢",
    "摐": "窗",
    "熇": "贺",
    "脴": "匹",
    "鈩": "芦",
    "蹯": "烦",
    "圡": "土",
    "飔": "思",
    "桟": "战",
    "籙": "路",
    "祴": "该",
    "鬬": "豆",
    "軏": "月",
    "嚤": "魔",
    "烋": "修",
    "玼": "此",
    "仠": "感",
    "焾": "年",
    "猂": "汉",
    "獦": "格",
    "綎": "听",
    "幏": "驾",
    "欉": "从",
    "豑": "至",
    "鹙": "秋",
    "衘": "闲",
    "彣": "文",
    "犜": "蹲",
    "攼": "干",
    "荗": "树",
    "緷": "运",
    "縳": "倦",
    "檁": "吝",
    "仾": "低",
    "飱": "孙",
    "畗": "达",
    "皔": "汉",
    "咈": "福",
    "藹": "矮",
    "笭": "零",
    "訏": "需",
    "殅": "生",
    "噭": "教",
    "媂": "地",
    "禕": "一",
    "偦": "许",
    "鴎": "欧",
    "硉": "路",
    "鼕": "东",
    "踡": "全",
    "哻": "憨",
    "櫙": "欧",
    "鱮": "续",
    "鎅": "借",
    "熯": "汉",
    "眥": "字",
    "騵": "原",
    "潒": "荡",
    "栟": "奔",
    "旉": "夫",
    "髣": "访",
    "叐": "拔",
    "彃": "必",
    "蓢": "浪",
    "劒": "见",
    "譪": "矮",
    "糼": "工",
    "跼": "局",
    "揦": "啦",
    "矼": "刚",
    "偖": "扯",
    "稌": "图",
    "萰": "练",
    "鐻": "巨",
    "鮡": "照",
    "麩": "夫",
    "漥": "挖",
    "汫": "井",
    "苲": "眨",
    "諎": "则",
    "飃": "飘",
    "疁": "刘",
    "魭": "原",
    "蕶": "零",
    "枲": "洗",
    "琱": "雕",
    "敊": "处",
    "枬": "占",
    "暕": "减",
    "呌": "教",
    "玜": "红",
    "狯": "快",
    "樛": "纠",
    "邅": "占",
    "矴": "定",
    "媎": "解",
    "儺": "挪",
    "湙": "意",
    "薲": "贫",
    "芔": "会",
    "錮": "固",
    "棓": "棒",
    "籪": "断",
    "珷": "五",
    "畄": "刘",
    "磳": "增",
    "媧": "挖",
    "翴": "连",
    "觫": "速",
    "茪": "光",
    "灄": "设",
    "瑬": "刘",
    "廇": "六",
    "訌": "红",
    "槱": "有",
    "紆": "迂",
    "兪": "鱼",
    "孋": "离",
    "櫆": "奎",
    "磾": "低",
    "諨": "福",
    "謔": "血",
    "檞": "解",
    "鴀": "否",
    "夿": "八",
    "爫": "找",
    "箑": "煞",
    "唂": "姑",
    "琈": "福",
    "沕": "密",
    "帊": "怕",
    "焺": "生",
    "糳": "做",
    "芞": "气",
    "鬶": "归",
    "聢": "定",
    "傖": "仓",
    "譔": "赚",
    "犦": "博",
    "淜": "平",
    "膤": "雪",
    "冡": "盟",
    "篴": "敌",
    "倎": "舔",
    "魷": "由",
    "壎": "熏",
    "搩": "眨",
    "衖": "向",
    "挵": "弄",
    "壼": "捆",
    "箄": "比",
    "輗": "尼",
    "楳": "梅",
    "慴": "设",
    "忺": "先",
    "勨": "向",
    "璝": "归",
    "桯": "听",
    "幨": "搀",
    "熦": "觉",
    "赩": "系",
    "芓": "字",
    "遖": "南",
    "镮": "环",
    "遝": "踏",
    "貲": "姿",
    "耞": "家",
    "媴": "原",
    "祋": "对",
    "嘪": "买",
    "鉈": "诗",
    "捄": "就",
    "拤": "恰",
    "慯": "伤",
    "氆": "普",
    "蠏": "谢",
    "鰐": "恶",
    "鄮": "帽",
    "汍": "完",
    "黪": "惨",
    "塂": "向",
    "棁": "捉",
    "贗": "燕",
    "毪": "木",
    "玱": "枪",
    "帨": "睡",
    "茻": "忙",
    "緜": "眠",
    "漃": "记",
    "蟗": "秋",
    "柹": "是",
    "啈": "哼",
    "鳓": "乐",
    "緍": "民",
    "狊": "局",
    "媯": "归",
    "瀅": "营",
    "硁": "坑",
    "徏": "至",
    "巶": "招",
    "犤": "排",
    "濙": "营",
    "謜": "原",
    "拸": "宜",
    "橲": "洗",
    "菍": "聂",
    "楿": "相",
    "潥": "速",
    "埈": "俊",
    "縟": "入",
    "嚬": "贫",
    "羢": "容",
    "槾": "慢",
    "胘": "闲",
    "詧": "茶",
    "袏": "做",
    "粂": "摘",
    "榑": "福",
    "岤": "学",
    "姷": "右",
    "筼": "云",
    "蟰": "消",
    "磥": "垒",
    "娭": "哀",
    "莾": "忙",
    "冐": "帽",
    "霅": "咋",
    "雊": "够",
    "犌": "家",
    "叞": "位",
    "臛": "或",
    "葄": "做",
    "芧": "续",
    "珇": "组",
    "氀": "驴",
    "嚑": "熏",
    "惎": "记",
    "梹": "彬",
    "杅": "鱼",
    "錧": "管",
    "镴": "蜡",
    "娞": "内",
    "餑": "波",
    "鋯": "告",
    "柇": "和",
    "髽": "抓",
    "癦": "么",
    "靭": "任",
    "斅": "笑",
    "瘎": "陈",
    "攺": "以",
    "箟": "俊",
    "嵿": "顶",
    "臫": "角",
    "昡": "炫",
    "坿": "父",
    "舙": "话",
    "勀": "客",
    "牚": "称",
    "潻": "鼠",
    "諍": "挣",
    "僥": "角",
    "臈": "蜡",
    "柲": "必",
    "罣": "挂",
    "瞛": "聪",
    "狓": "皮",
    "巗": "颜",
    "鹐": "前",
    "杣": "眠",
    "崲": "黄",
    "碙": "脑",
    "歔": "需",
    "蓏": "裸",
    "鋋": "缠",
    "桼": "七",
    "煭": "裂",
    "媻": "盘",
    "蕟": "发",
    "叅": "参",
    "瘃": "竹",
    "瑫": "掏",
    "絛": "掏",
    "伨": "训",
    "鰾": "标",
    "滮": "标",
    "廙": "意",
    "稾": "搞",
    "沴": "利",
    "哵": "八",
    "渄": "飞",
    "璄": "井",
    "鶉": "纯",
    "爩": "玉",
    "鱒": "尊",
    "漶": "换",
    "頬": "夹",
    "夣": "梦",
    "渰": "眼",
    "釔": "以",
    "跧": "全",
    "鱵": "真",
    "嵃": "眼",
    "泆": "意",
    "籦": "中",
    "頽": "推",
    "棊": "其",
    "昤": "零",
    "搯": "掏",
    "趑": "姿",
    "觽": "西",
    "皚": "埃",
    "顓": "专",
    "踦": "以",
    "歟": "鱼",
    "彮": "永",
    "挄": "扩",
    "鮻": "缩",
    "叀": "专",
    "叓": "是",
    "抯": "扎",
    "齪": "绰",
    "辧": "变",
    "偭": "免",
    "朳": "八",
    "砳": "乐",
    "鯢": "尼",
    "倂": "病",
    "煴": "晕",
    "絰": "叠",
    "纕": "嚷",
    "磻": "盘",
    "鵙": "局",
    "劯": "朱",
    "敻": "胸",
    "韞": "运",
    "欌": "藏",
    "猨": "原",
    "罉": "称",
    "徥": "是",
    "恊": "鞋",
    "裻": "毒",
    "讂": "炫",
    "彟": "约",
    "栺": "意",
    "棡": "刚",
    "暤": "号",
    "俓": "静",
    "搤": "恶",
    "焅": "裤",
    "狶": "西",
    "虡": "巨",
    "箖": "林",
    "焪": "穷",
    "茘": "利",
    "璸": "彬",
    "儩": "四",
    "撝": "灰",
    "輌": "亮",
    "畺": "将",
    "礮": "炮",
    "痜": "突",
    "犫": "抽",
    "鳀": "提",
    "皠": "脆",
    "妜": "月",
    "笯": "努",
    "玃": "觉",
    "莃": "西",
    "熤": "意",
    "腄": "垂",
    "婨": "伦",
    "桮": "杯",
    "铴": "汤",
    "泙": "平",
    "瑥": "温",
    "箂": "来",
    "鰹": "间",
    "矰": "增",
    "竚": "助",
    "楴": "替",
    "祍": "任",
    "磗": "专",
    "塀": "平",
    "駉": "窘",
    "帺": "其",
    "鞕": "硬",
    "扟": "深",
    "哬": "和",
    "苿": "位",
    "鵡": "五",
    "簙": "博",
    "錆": "枪",
    "珆": "宜",
    "縌": "逆",
    "饦": "拖",
    "矝": "金",
    "眖": "矿",
    "臒": "卧",
    "烇": "犬",
    "磧": "气",
    "鉽": "是",
    "莗": "车",
    "聜": "底",
    "蹆": "腿",
    "嵈": "换",
    "丩": "纠",
    "癶": "波",
    "鱘": "寻",
    "甀": "缀",
    "掿": "诺",
    "滶": "敖",
    "禠": "思",
    "鳧": "福",
    "鯥": "路",
    "迊": "匝",
    "嘂": "教",
    "璪": "早",
    "昩": "墨",
    "徤": "见",
    "瀲": "练",
    "咉": "养",
    "昷": "温",
    "唫": "进",
    "鴖": "民",
    "粻": "张",
    "驀": "墨",
    "栠": "忍",
    "靿": "要",
    "癆": "劳",
    "庘": "呀",
    "簞": "单",
    "舺": "侠",
    "宨": "挑",
    "礒": "以",
    "瑑": "赚",
    "怳": "谎",
    "謏": "小",
    "郈": "后",
    "湢": "必",
    "鱥": "贵",
    "琖": "展",
    "蒟": "举",
    "皸": "君",
    "蕯": "龙",
    "峈": "落",
    "眒": "深",
    "顳": "聂",
    "蒳": "那",
    "耂": "老",
    "瘭": "标",
    "盬": "古",
    "旙": "翻",
    "蕚": "恶",
    "惷": "春",
    "啨": "应",
    "輅": "和",
    "腽": "袜",
    "嫮": "互",
    "鮈": "居",
    "諲": "因",
    "瑂": "梅",
    "僎": "赚",
    "搊": "抽",
    "蝢": "鞋",
    "醱": "发",
    "蓸": "曹",
    "埥": "青",
    "聧": "亏",
    "絟": "全",
    "苅": "意",
    "濳": "钱",
    "凅": "固",
    "眆": "访",
    "喠": "种",
    "桋": "宜",
    "頩": "平",
    "盩": "周",
    "槨": "果",
    "薾": "耳",
    "嶒": "层",
    "憀": "聊",
    "鮸": "免",
    "灤": "鸾",
    "険": "显",
    "塈": "记",
    "聸": "单",
    "豭": "家",
    "笲": "烦",
    "鞦": "秋",
    "抦": "饼",
    "枏": "南",
    "縒": "刺",
    "峮": "群",
    "淟": "舔",
    "嵜": "其",
    "魋": "推",
    "懗": "下",
    "蠵": "西",
    "糶": "跳",
    "吂": "忙",
    "蔠": "中",
    "酺": "葡",
    "襚": "岁",
    "腂": "垒",
    "碿": "速",
    "鍬": "敲",
    "讌": "燕",
    "霤": "六",
    "籵": "烦",
    "溵": "因",
    "鉓": "赤",
    "韾": "因",
    "煵": "男",
    "塟": "藏",
    "楩": "偏",
    "髏": "楼",
    "誑": "狂",
    "粃": "比",
    "嵚": "亲",
    "聅": "撤",
    "梛": "挪",
    "犿": "欢",
    "羘": "脏",
    "坒": "必",
    "鹯": "占",
    "旴": "需",
    "陧": "聂",
    "彺": "王",
    "聫": "连",
    "偨": "刺",
    "娢": "含",
    "簻": "抓",
    "泒": "姑",
    "蕫": "懂",
    "昸": "东",
    "楡": "鱼",
    "詨": "笑",
    "鳡": "感",
    "饉": "紧",
    "籨": "连",
    "靸": "洒",
    "渱": "红",
    "鼌": "朝",
    "癴": "鸾",
    "摥": "烫",
    "鼆": "盟",
    "淥": "路",
    "癭": "影",
    "袝": "父",
    "椇": "举",
    "頦": "孩",
    "佱": "法",
    "鶄": "精",
    "岠": "巨",
    "袿": "归",
    "痁": "山",
    "羠": "宜",
    "馹": "日",
    "摼": "坑",
    "泹": "蛋",
    "鍶": "松",
    "苙": "利",
    "懓": "爱",
    "摝": "路",
    "痶": "舔",
    "醗": "破",
    "乥": "互",
    "鉷": "红",
    "慒": "从",
    "趌": "极",
    "喨": "亮",
    "鐠": "普",
    "懌": "意",
    "錱": "真",
    "佪": "回",
    "磦": "标",
    "溙": "太",
    "苨": "你",
    "蚖": "原",
    "銦": "因",
    "驁": "奥",
    "詒": "宜",
    "鳤": "管",
    "爊": "凹",
    "妱": "招",
    "佫": "贺",
    "鱤": "感",
    "琡": "处",
    "牓": "绑",
    "礱": "龙",
    "淈": "古",
    "榚": "咬",
    "仱": "钱",
    "蕝": "觉",
    "熈": "西",
    "憟": "速",
    "漰": "砰",
    "狟": "环",
    "沘": "比",
    "捫": "门",
    "珦": "向",
    "暊": "许",
    "玶": "平",
    "壡": "瑞",
    "渢": "烦",
    "毱": "局",
    "睲": "醒",
    "翵": "喉",
    "汋": "着",
    "誃": "宜",
    "緇": "姿",
    "靁": "雷",
    "狺": "银",
    "篶": "烟",
    "岒": "钱",
    "橷": "兜",
    "巂": "归",
    "啩": "挂",
    "椗": "定",
    "甽": "镇",
    "啅": "着",
    "鯪": "零",
    "瞼": "减",
    "泝": "速",
    "躱": "朵",
    "鄃": "书",
    "鴦": "养",
    "釨": "子",
    "豽": "那",
    "秝": "利",
    "湼": "聂",
    "砃": "单",
    "鬾": "记",
    "醖": "运",
    "醂": "懒",
    "鬂": "宾",
    "汯": "红",
    "筽": "欧",
    "璦": "爱",
    "儫": "豪",
    "灊": "钱",
    "紉": "任",
    "薌": "相",
    "碂": "宗",
    "誔": "挺",
    "瞏": "穷",
    "堨": "夜",
    "翬": "灰",
    "侭": "紧",
    "膁": "浅",
    "螞": "马",
    "娕": "绰",
    "崈": "虫",
    "荢": "字",
    "頠": "伟",
    "悮": "物",
    "禖": "梅",
    "坔": "地",
    "箛": "姑",
    "鷂": "要",
    "蓜": "配",
    "熋": "奶",
    "鏷": "葡",
    "趪": "黄",
    "晈": "角",
    "詆": "底",
    "筶": "告",
    "蚳": "持",
    "絯": "该",
    "粐": "互",
    "莄": "梗",
    "圂": "混",
    "瓖": "相",
    "蓱": "平",
    "昦": "号",
    "皷": "古",
    "橚": "速",
    "澦": "玉",
    "峃": "学",
    "侎": "米",
    "摖": "气",
    "痡": "夫",
    "敁": "颠",
    "勼": "纠",
    "靺": "墨",
    "洉": "后",
    "窎": "掉",
    "屰": "逆",
    "鑭": "烂",
    "磵": "见",
    "舠": "刀",
    "晼": "碗",
    "秬": "巨",
    "雚": "灌",
    "忬": "玉",
    "鋹": "场",
    "瑏": "穿",
    "牷": "全",
    "瓻": "吃",
    "笻": "穷",
    "饍": "善",
    "绤": "系",
    "燶": "农",
    "咇": "别",
    "牥": "方",
    "雬": "否",
    "閶": "昌",
    "蚃": "想",
    "緫": "聪",
    "聤": "停",
    "魢": "几",
    "堼": "哼",
    "婔": "飞",
    "昬": "昏",
    "窋": "竹",
    "璚": "穷",
    "爞": "虫",
    "輮": "柔",
    "竵": "歪",
    "畇": "云",
    "鑨": "龙",
    "瓕": "迷",
    "敋": "格",
    "鮴": "修",
    "帉": "分",
    "刓": "完",
    "祻": "固",
    "倱": "混",
    "攋": "蜡",
    "鄲": "单",
    "閂": "拴",
    "鋌": "定",
    "垍": "记",
    "婖": "天",
    "髃": "鱼",
    "洈": "维",
    "裧": "搀",
    "羇": "机",
    "槩": "概",
    "銠": "老",
    "蓹": "玉",
    "蹱": "中",
    "嘓": "锅",
    "蔕": "地",
    "膉": "意",
    "耎": "软",
    "勔": "免",
    "彍": "锅",
    "逰": "由",
    "膟": "绿",
    "帢": "恰",
    "厺": "去",
    "賾": "则",
    "緭": "位",
    "鮪": "伟",
    "犧": "西",
    "寽": "绿",
    "鋃": "狼",
    "俰": "或",
    "尃": "夫",
    "繽": "彬",
    "囀": "赚",
    "蘡": "应",
    "瀦": "朱",
    "芖": "至",
    "卨": "谢",
    "諄": "准",
    "嫢": "归",
    "罃": "应",
    "眘": "肾",
    "琝": "民",
    "貭": "至",
    "緼": "运",
    "跕": "点",
    "厔": "至",
    "閦": "处",
    "磪": "催",
    "懞": "盟",
    "媢": "帽",
    "陏": "堕",
    "墐": "进",
    "鵼": "空",
    "乊": "一",
    "醲": "农",
    "侐": "续",
    "椻": "燕",
    "籫": "钻",
    "柛": "深",
    "潎": "屁",
    "闚": "亏",
    "箎": "持",
    "鎛": "博",
    "爧": "零",
    "歭": "持",
    "梿": "连",
    "祲": "进",
    "蕜": "匪",
    "嶌": "导",
    "尫": "汪",
    "貚": "谈",
    "敯": "敏",
    "彞": "宜",
    "檷": "你",
    "愴": "创",
    "盫": "安",
    "搒": "棒",
    "稰": "许",
    "絤": "现",
    "灡": "蓝",
    "厵": "原",
    "璲": "岁",
    "琜": "来",
    "蒅": "染",
    "玞": "夫",
    "伮": "努",
    "巃": "龙",
    "绹": "桃",
    "嵶": "弱",
    "焽": "胸",
    "眽": "墨",
    "徫": "伟",
    "羬": "钱",
    "眡": "是",
    "莿": "次",
    "蠄": "琴",
    "徝": "至",
    "靨": "夜",
    "竂": "聊",
    "摎": "纠",
    "騲": "草",
    "苂": "银",
    "宻": "密",
    "瓁": "卧",
    "尟": "显",
    "麨": "吵",
    "炏": "开",
    "墭": "胜",
    "骻": "跨",
    "褩": "班",
    "蓴": "纯",
    "軾": "是",
    "敶": "镇",
    "觿": "西",
    "騾": "罗",
    "紾": "枕",
    "鑠": "硕",
    "淐": "昌",
    "稊": "提",
    "隤": "推",
    "茞": "陈",
    "搫": "盘",
    "妑": "趴",
    "皘": "欠",
    "咊": "和",
    "椚": "们",
    "楆": "邀",
    "棯": "忍",
    "阾": "领",
    "叝": "极",
    "槫": "团",
    "鮄": "福",
    "怑": "办",
    "廵": "寻",
    "垈": "带",
    "嫍": "掏",
    "晄": "谎",
    "碦": "客",
    "巼": "八",
    "梲": "着",
    "犖": "落",
    "僤": "蛋",
    "颰": "拔",
    "懃": "琴",
    "獊": "仓",
    "椥": "知",
    "蛧": "网",
    "臞": "取",
    "醠": "盎",
    "忳": "吞",
    "灥": "寻",
    "臝": "裸",
    "礂": "西",
    "塰": "海",
    "瓥": "利",
    "硋": "爱",
    "沋": "由",
    "繒": "增",
    "鴴": "横",
    "餭": "黄",
    "齬": "雨",
    "抆": "稳",
    "酙": "真",
    "鼒": "姿",
    "啴": "铲",
    "瑡": "诗",
    "捀": "逢",
    "獚": "黄",
    "聣": "尼",
    "碪": "真",
    "鷃": "燕",
    "媐": "宜",
    "棝": "固",
    "鋦": "居",
    "喩": "玉",
    "鋶": "柳",
    "汘": "前",
    "冣": "巨",
    "皌": "墨",
    "閸": "捆",
    "佡": "先",
    "踎": "谋",
    "挰": "成",
    "吤": "借",
    "勦": "超",
    "庿": "庙",
    "掫": "周",
    "侕": "而",
    "椄": "接",
    "昞": "饼",
    "掄": "伦",
    "誩": "静",
    "鰆": "春",
    "剘": "其",
    "脵": "古",
    "畷": "缀",
    "訐": "节",
    "賒": "舍",
    "鑖": "灭",
    "嚵": "缠",
    "嫼": "墨",
    "硤": "侠",
    "鉮": "环",
    "惻": "册",
    "鰥": "关",
    "媔": "眠",
    "磑": "维",
    "虀": "机",
    "湝": "接",
    "豿": "狗",
    "垺": "夫",
    "疪": "必",
    "麁": "粗",
    "詘": "区",
    "硔": "红",
    "捗": "不",
    "絋": "矿",
    "埢": "全",
    "鰰": "神",
    "韤": "袜",
    "襾": "亚",
    "嚛": "互",
    "櫘": "会",
    "薍": "万",
    "碽": "工",
    "杕": "地",
    "鶡": "和",
    "獶": "脑",
    "歜": "处",
    "尗": "书",
    "餞": "见",
    "矙": "看",
    "椓": "着",
    "飮": "引",
    "渿": "耐",
    "憯": "惨",
    "橃": "罚",
    "訽": "够",
    "涭": "瘦",
    "慆": "掏",
    "吪": "额",
    "飡": "参",
    "詺": "命",
    "闓": "凯",
    "騃": "埃",
    "鯀": "滚",
    "禴": "月",
    "祾": "零",
    "滎": "行",
    "衜": "到",
    "謄": "腾",
    "衭": "夫",
    "牁": "科",
    "鰋": "眼",
    "轤": "芦",
    "抐": "呢",
    "筻": "杠",
    "芛": "伟",
    "惒": "和",
    "礲": "龙",
    "篈": "风",
    "蠧": "度",
    "撦": "扯",
    "慭": "印",
    "曓": "报",
    "丱": "灌",
    "疢": "衬",
    "萚": "拓",
    "棭": "意",
    "奿": "饭",
    "膇": "缀",
    "閧": "红",
    "袠": "至",
    "糤": "三",
    "鰣": "石",
    "蓇": "古",
    "玤": "棒",
    "恴": "德",
    "駸": "亲",
    "傫": "垒",
    "姄": "民",
    "匱": "贵",
    "辷": "一",
    "滸": "虎",
    "瑵": "找",
    "蒝": "原",
    "錸": "来",
    "僸": "进",
    "涷": "东",
    "鋽": "掉",
    "掯": "肯",
    "霝": "零",
    "熜": "聪",
    "眜": "墨",
    "弅": "愤",
    "蜼": "位",
    "焑": "烟",
    "濪": "庆",
    "鐦": "开",
    "鳛": "习",
    "崰": "姿",
    "硺": "着",
    "怣": "由",
    "藟": "垒",
    "緿": "带",
    "髕": "宾",
    "畓": "多",
    "蹏": "提",
    "壻": "续",
    "叺": "尺",
    "朸": "利",
    "毑": "解",
    "糉": "宗",
    "襴": "蓝",
    "鉎": "生",
    "筸": "干",
    "捔": "觉",
    "玴": "意",
    "繾": "浅",
    "藦": "墨",
    "麃": "袍",
    "尌": "树",
    "姺": "深",
    "匨": "脏",
    "殰": "毒",
    "踈": "书",
    "廞": "心",
    "鐎": "教",
    "縲": "雷",
    "紨": "夫",
    "訧": "由",
    "烕": "灭",
    "朚": "荒",
    "虖": "呼",
    "崼": "是",
    "淴": "呼",
    "琷": "确",
    "嚱": "系",
    "齾": "亚",
    "栔": "气",
    "罇": "尊",
    "籿": "寸",
    "淍": "周",
    "亯": "想",
    "靉": "爱",
    "鄩": "寻",
    "謆": "善",
    "旐": "照",
    "纆": "墨",
    "牞": "纠",
    "扵": "鱼",
    "颕": "影",
    "僩": "现",
    "彽": "低",
    "銝": "修",
    "枿": "聂",
    "傎": "颠",
    "蹽": "了",
    "怺": "永",
    "湞": "真",
    "啺": "唐",
    "迏": "达",
    "絁": "诗",
    "痗": "妹",
    "槤": "连",
    "瑃": "春",
    "熝": "路",
    "崿": "恶",
    "穡": "色",
    "韂": "颤",
    "宭": "群",
    "嘥": "塞",
    "煣": "柔",
    "忈": "人",
    "蝂": "板",
    "鳋": "骚",
    "玸": "福",
    "狔": "你",
    "灴": "轰",
    "祒": "条",
    "崒": "族",
    "胐": "匪",
    "蜎": "冤",
    "儙": "欠",
    "爔": "西",
    "鉥": "树",
    "澊": "村",
    "甿": "盟",
    "銶": "求",
    "砽": "用",
    "鋂": "梅",
    "騖": "物",
    "柀": "比",
    "櫸": "举",
    "騶": "邹",
    "耭": "机",
    "韺": "应",
    "糱": "聂",
    "鴵": "消",
    "渘": "柔",
    "蔾": "离",
    "嬁": "灯",
    "酳": "印",
    "摜": "灌",
    "齛": "谢",
    "聭": "溃",
    "猧": "窝",
    "髠": "昆",
    "齙": "包",
    "盝": "路",
    "笗": "东",
    "鵾": "昆",
    "堁": "客",
    "啿": "蛋",
    "猺": "摇",
    "穕": "妾",
    "娝": "剖",
    "詊": "判",
    "旲": "台",
    "榶": "唐",
    "洤": "全",
    "袔": "贺",
    "丒": "丑",
    "摃": "康",
    "簱": "其",
    "昢": "破",
    "柎": "夫",
    "磆": "华",
    "塡": "田",
    "峎": "恩",
    "毚": "缠",
    "粎": "米",
    "囼": "胎",
    "洦": "破",
    "篯": "间",
    "桝": "节",
    "螀": "将",
    "镸": "长",
    "袗": "枕",
    "幰": "显",
    "爓": "燕",
    "餗": "速",
    "繶": "意",
    "彚": "会",
    "聠": "平",
    "鷟": "着",
    "齎": "机",
    "鱯": "互",
    "魙": "占",
    "螝": "归",
    "橕": "称",
    "鏠": "风",
    "伡": "车",
    "漚": "欧",
    "徰": "睁",
    "灷": "赚",
    "媮": "偷",
    "浰": "练",
    "幃": "维",
    "蝜": "父",
    "菼": "坦",
    "蜺": "尼",
    "掦": "替",
    "糭": "宗",
    "伷": "昼",
    "鱇": "康",
    "膥": "村",
    "亐": "鱼",
    "弖": "互",
    "胊": "取",
    "歾": "墨",
    "篔": "云",
    "鴆": "镇",
    "羫": "枪",
    "吷": "血",
    "玬": "胆",
    "埑": "哲",
    "衒": "炫",
    "橴": "子",
    "獓": "敖",
    "妚": "否",
    "篢": "垄",
    "迵": "动",
    "穐": "秋",
    "裠": "群",
    "摵": "设",
    "鄦": "许",
    "寯": "俊",
    "檵": "记",
    "騄": "路",
    "澫": "万",
    "哫": "族",
    "鹡": "极",
    "墤": "快",
    "黖": "系",
    "楇": "或",
    "諛": "鱼",
    "薉": "会",
    "黫": "烟",
    "迍": "准",
    "曏": "想",
    "耏": "耐",
    "玔": "串",
    "僂": "楼",
    "鸧": "仓",
    "剸": "团",
    "跅": "拓",
    "頎": "其",
    "琣": "崩",
    "礵": "双",
    "珚": "烟",
    "頊": "需",
    "汈": "雕",
    "笴": "感",
    "嫬": "遮",
    "僿": "赛",
    "柆": "拉",
    "蔳": "欠",
    "刅": "窗",
    "骎": "亲",
    "鸜": "取",
    "嬧": "进",
    "颿": "翻",
    "訞": "邀",
    "纚": "离",
    "綳": "崩",
    "鬒": "枕",
    "繷": "农",
    "抂": "狂",
    "凢": "烦",
    "覔": "密",
    "砏": "彬",
    "頖": "判",
    "穈": "梅",
    "詷": "同",
    "頲": "挺",
    "磱": "劳",
    "顙": "桑",
    "辡": "变",
    "蕇": "点",
    "戱": "系",
    "黽": "免",
    "蠣": "利",
    "暙": "春",
    "刦": "节",
    "犛": "毛",
    "鶩": "物",
    "朌": "坟",
    "椮": "森",
    "鰽": "求",
    "忲": "太",
    "誋": "记",
    "嫊": "速",
    "稭": "接",
    "漻": "聊",
    "餂": "舔",
    "僡": "会",
    "甧": "深",
    "猠": "点",
    "鬛": "裂",
    "緡": "民",
    "觩": "求",
    "晆": "奎",
    "盽": "风",
    "髆": "博",
    "桒": "丧",
    "藼": "宣",
    "闞": "看",
    "瑴": "觉",
    "豖": "处",
    "絅": "窘",
    "緦": "思",
    "鵽": "堕",
    "譴": "浅",
    "齲": "曲",
    "湌": "参",
    "缞": "催",
    "晬": "最",
    "筨": "含",
    "痟": "消",
    "蓨": "条",
    "珶": "地",
    "玐": "八",
    "囶": "国",
    "秗": "玉",
    "櫹": "消",
    "硊": "伟",
    "箒": "肘",
    "撯": "着",
    "瑇": "带",
    "媟": "谢",
    "忼": "康",
    "汼": "牛",
    "奺": "久",
    "罌": "应",
    "謳": "欧",
    "橿": "将",
    "伇": "意",
    "頣": "审",
    "萮": "鱼",
    "蘄": "其",
    "竕": "分",
    "跱": "至",
    "夑": "谢",
    "杦": "久",
    "駑": "努",
    "薡": "顶",
    "洴": "平",
    "僷": "夜",
    "煍": "角",
    "抝": "袄",
    "媣": "染",
    "媼": "袄",
    "麐": "林",
    "羗": "枪",
    "橆": "五",
    "柍": "养",
    "煈": "奉",
    "厊": "哑",
    "昚": "肾",
    "虂": "路",
    "礠": "瓷",
    "粷": "局",
    "鬰": "玉",
    "痏": "伟",
    "鈰": "是",
    "穽": "井",
    "蝖": "宣",
    "詌": "干",
    "鐍": "觉",
    "韙": "伟",
    "薱": "对",
    "侻": "退",
    "狤": "极",
    "帤": "如",
    "殫": "单",
    "璏": "位",
    "惄": "逆",
    "鼫": "石",
    "惁": "西",
    "衯": "分",
    "瞴": "谋",
    "藯": "位",
    "荁": "环",
    "嗇": "色",
    "砠": "居",
    "捒": "树",
    "魖": "需",
    "篐": "姑",
    "銛": "先",
    "瞉": "扣",
    "髳": "毛",
    "珼": "被",
    "呠": "喷",
    "楧": "养",
    "碻": "确",
    "碠": "定",
    "雝": "庸",
    "眿": "墨",
    "厹": "柔",
    "覡": "习",
    "皜": "号",
    "篗": "月",
    "汮": "君",
    "猔": "宗",
    "膅": "唐",
    "啙": "子",
    "儥": "玉",
    "饈": "修",
    "侱": "成",
    "窵": "掉",
    "哷": "裂",
    "宧": "宜",
    "鯒": "永",
    "莇": "助",
    "胾": "字",
    "錩": "昌",
    "詝": "主",
    "髢": "敌",
    "罫": "挂",
    "騔": "格",
    "磝": "敖",
    "侼": "博",
    "猘": "至",
    "迯": "桃",
    "漙": "团",
    "灩": "燕",
    "囈": "意",
    "峳": "由",
    "汵": "干",
    "綈": "提",
    "鉂": "使",
    "膆": "速",
    "肨": "胖",
    "衧": "鱼",
    "綟": "利",
    "伣": "欠",
    "臜": "匝",
    "厑": "牙",
    "窡": "着",
    "紁": "差",
    "玝": "五",
    "灦": "显",
    "啚": "比",
    "獂": "原",
    "磀": "额",
    "檨": "舍",
    "濘": "宁",
    "歨": "不",
    "埻": "准",
    "夻": "话",
    "碭": "荡",
    "簤": "带",
    "烼": "续",
    "棏": "德",
    "柂": "宜",
    "睠": "倦",
    "齷": "卧",
    "刵": "二",
    "屜": "替",
    "霗": "零",
    "鮟": "按",
    "畩": "一",
    "檪": "利",
    "鋏": "夹",
    "妀": "几",
    "僜": "称",
    "橪": "染",
    "蛶": "借",
    "虰": "丁",
    "漴": "壮",
    "嬑": "意",
    "嫕": "意",
    "蔎": "设",
    "矻": "哭",
    "觮": "路",
    "哴": "亮",
    "灒": "赞",
    "挕": "叠",
    "傁": "搜",
    "棃": "离",
    "砡": "玉",
    "栒": "寻",
    "枺": "墨",
    "恛": "回",
    "鴕": "驮",
    "琄": "炫",
    "晊": "至",
    "琟": "维",
    "灮": "光",
    "訸": "和",
    "捴": "总",
    "簥": "教",
    "嵕": "宗",
    "寗": "凝",
    "戨": "歌",
    "衠": "准",
    "暆": "宜",
    "諤": "恶",
    "愐": "免",
    "筃": "因",
    "塋": "营",
    "潧": "真",
    "朞": "机",
    "騳": "毒",
    "桸": "西",
    "醙": "搜",
    "眤": "逆",
    "睟": "岁",
    "篘": "抽",
    "闐": "田",
    "漼": "脆",
    "侰": "窘",
    "仦": "吵",
    "魡": "掉",
    "荍": "桥",
    "呇": "起",
    "肧": "培",
    "捝": "拖",
    "鰤": "诗",
    "譌": "额",
    "偩": "父",
    "檰": "眠",
    "鴂": "觉",
    "棴": "福",
    "鍱": "夜",
    "岨": "区",
    "焫": "弱",
    "垉": "袍",
    "蠆": "拆",
    "輈": "周",
    "梂": "求",
    "俽": "心",
    "郳": "尼",
    "鈑": "板",
    "虁": "奎",
    "儈": "快",
    "諂": "铲",
    "詛": "组",
    "鑁": "宗",
    "衵": "意",
    "淛": "这",
    "阸": "恶",
    "昮": "宗",
    "掁": "成",
    "鉿": "家",
    "瓫": "盆",
    "徎": "成",
    "郉": "行",
    "飂": "六",
    "卆": "族",
    "萹": "扁",
    "凐": "因",
    "峸": "成",
    "虵": "蛇",
    "綣": "犬",
    "扢": "古",
    "詖": "必",
    "豗": "灰",
    "韝": "勾",
    "藋": "掉",
    "樆": "离",
    "闗": "关",
    "瓃": "雷",
    "脁": "挑",
    "瘞": "意",
    "蟟": "聊",
    "嚚": "银",
    "膌": "极",
    "糴": "敌",
    "燨": "西",
    "鵷": "冤",
    "遧": "张",
    "衐": "取",
    "杗": "忙",
    "崳": "鱼",
    "愒": "开",
    "篛": "弱",
    "杍": "子",
    "欅": "举",
    "鷁": "意",
    "圑": "普",
    "闥": "踏",
    "猻": "孙",
    "繦": "抢",
    "鵐": "无",
    "頳": "称",
    "悈": "借",
    "謑": "洗",
    "愢": "塞",
    "旵": "铲",
    "瘍": "羊",
    "梺": "下",
    "顠": "漂",
    "眃": "云",
    "榣": "摇",
    "炛": "光",
    "厸": "林",
    "坋": "笨",
    "挏": "动",
    "斶": "处",
    "榐": "展",
    "墣": "葡",
    "椈": "局",
    "坴": "路",
    "郚": "无",
    "黿": "原",
    "圥": "路",
    "纗": "嘴",
    "秮": "活",
    "畍": "借",
    "噧": "谢",
    "攃": "擦",
    "雘": "卧",
    "揳": "些",
    "竉": "垄",
    "胻": "横",
    "悑": "不",
    "甹": "平",
    "跓": "助",
    "餒": "内",
    "繐": "岁",
    "珸": "无",
    "楟": "停",
    "栯": "有",
    "槖": "驮",
    "麷": "风",
    "灕": "离",
    "酓": "眼",
    "鰌": "秋",
    "胵": "吃",
    "莝": "错",
    "蝿": "营",
    "熰": "欧",
    "藳": "搞",
    "縗": "催",
    "鞄": "袍",
    "喞": "机",
    "攨": "挖",
    "昹": "矮",
    "殏": "求",
    "瓂": "概",
    "睖": "愣",
    "醆": "展",
    "傆": "院",
    "臏": "宾",
    "簈": "平",
    "凘": "思",
    "膙": "讲",
    "犆": "直",
    "澼": "屁",
    "舋": "信",
    "紤": "久",
    "舝": "侠",
    "鎲": "躺",
    "慅": "骚",
    "綶": "果",
    "鉦": "睁",
    "彄": "口",
    "羉": "鸾",
    "葀": "扩",
    "殗": "夜",
    "聬": "瓮",
    "儃": "缠",
    "埞": "低",
    "豘": "吞",
    "镵": "缠",
    "煆": "虾",
    "箮": "宣",
    "趆": "低",
    "暰": "聪",
    "浻": "窘",
    "繯": "环",
    "甂": "编",
    "攧": "颠",
    "髇": "消",
    "筰": "做",
    "廹": "拍",
    "箥": "跛",
    "鵩": "福",
    "揈": "轰",
    "懠": "其",
    "凣": "烦",
    "睸": "妹",
    "袥": "拖",
    "輙": "哲",
    "蒛": "缺",
    "梘": "减",
    "桵": "瑞",
    "玣": "变",
    "乲": "姿",
    "鵁": "教",
    "蚫": "报",
    "觶": "至",
    "璮": "坦",
    "竼": "朋",
    "哹": "福",
    "禣": "父",
    "梄": "有",
    "鮒": "父",
    "孻": "奶",
    "賂": "路",
    "篰": "不",
    "嶰": "谢",
    "緉": "两",
    "糒": "被",
    "褻": "谢",
    "阓": "会",
    "飜": "翻",
    "痌": "通",
    "朘": "嘴",
    "慲": "瞒",
    "溰": "埃",
    "琑": "锁",
    "眣": "叠",
    "隩": "奥",
    "滭": "必",
    "皧": "爱",
    "淿": "博",
    "阰": "皮",
    "逈": "窘",
    "乷": "沙",
    "噈": "促",
    "毶": "三",
    "岦": "利",
    "哸": "虽",
    "砱": "零",
    "塜": "种",
    "喐": "玉",
    "揔": "总",
    "汃": "彬",
    "夗": "院",
    "膴": "呼",
    "傋": "讲",
    "鍅": "法",
    "侴": "丑",
    "庈": "琴",
    "秱": "同",
    "鬋": "减",
    "捁": "角",
    "槮": "森",
    "蝋": "蜡",
    "鞶": "盘",
    "慿": "平",
    "竛": "零",
    "芢": "人",
    "屧": "谢",
    "袚": "波",
    "嬀": "归",
    "璓": "秀",
    "塼": "专",
    "聥": "举",
    "韆": "前",
    "崬": "东",
    "旑": "以",
    "敨": "偷",
    "狦": "山",
    "癎": "闲",
    "蒵": "习",
    "罓": "刚",
    "琫": "崩",
    "軿": "平",
    "鱠": "快",
    "傌": "骂",
    "繝": "见",
    "蛁": "雕",
    "嬞": "懂",
    "姶": "恶",
    "諉": "伟",
    "蝀": "东",
    "膍": "皮",
    "葅": "租",
    "圫": "玉",
    "斠": "教",
    "礋": "则",
    "煑": "主",
    "厫": "敖",
    "耇": "狗",
    "攱": "鬼",
    "螄": "思",
    "箷": "宜",
    "靹": "那",
    "樌": "灌",
    "蒫": "搓",
    "縀": "侠",
    "顚": "颠",
    "颸": "思",
    "囌": "苏",
    "磈": "伟",
    "墲": "木",
    "妟": "燕",
    "蘹": "怀",
    "熿": "黄",
    "垽": "印",
    "孌": "鸾",
    "椯": "朵",
    "詂": "父",
    "猌": "印",
    "捖": "完",
    "尭": "摇",
    "亗": "岁",
    "虳": "觉",
    "玪": "间",
    "簓": "雕",
    "珜": "羊",
    "偮": "极",
    "嬝": "尿",
    "鐹": "果",
    "湦": "生",
    "釤": "善",
    "蚑": "其",
    "鈼": "做",
    "浂": "意",
    "冓": "够",
    "暃": "飞",
    "旟": "鱼",
    "桽": "稳",
    "鶲": "瓮",
    "欋": "取",
    "秡": "博",
    "懍": "吝",
    "卭": "穷",
    "餠": "饼",
    "弨": "超",
    "佖": "必",
    "柭": "八",
    "堓": "按",
    "拻": "灰",
    "鏹": "抢",
    "漄": "牙",
    "恌": "挑",
    "詏": "要",
    "躹": "局",
    "麀": "优",
    "禒": "显",
    "磓": "堆",
    "鬿": "其",
    "瀸": "间",
    "鹲": "盟",
    "崶": "风",
    "鐽": "达",
    "酫": "绰",
    "蹔": "赞",
    "碸": "风",
    "玌": "求",
    "鞡": "拉",
    "喿": "造",
    "揰": "冲",
    "詶": "昼",
    "凴": "平",
    "脺": "翠",
    "縭": "离",
    "絚": "耕",
    "猯": "团",
    "蹣": "盘",
    "趍": "持",
    "暲": "张",
    "瘓": "换",
    "靘": "庆",
    "昍": "宣",
    "淶": "来",
    "奵": "顶",
    "誏": "浪",
    "牂": "脏",
    "檮": "桃",
    "柶": "四",
    "欦": "前",
    "耹": "琴",
    "嵓": "颜",
    "擸": "裂",
    "塳": "朋",
    "晍": "同",
    "鋗": "宣",
    "茢": "裂",
    "蔂": "雷",
    "矪": "周",
    "瞱": "夜",
    "攙": "搀",
    "迠": "撤",
    "嫏": "狼",
    "摋": "萨",
    "葴": "真",
    "誄": "垒",
    "鳽": "间",
    "梒": "含",
    "穄": "记",
    "嵖": "茶",
    "闬": "汉",
    "梔": "知",
    "逌": "优",
    "醩": "糟",
    "騭": "至",
    "徖": "从",
    "蘻": "记",
    "硸": "虐",
    "灆": "蓝",
    "筪": "侠",
    "窇": "薄",
    "欗": "蓝",
    "桹": "狼",
    "饐": "意",
    "皾": "毒",
    "塯": "六",
    "緛": "软",
    "媭": "需",
    "琧": "恶",
    "鯵": "深",
    "墏": "抢",
    "髤": "修",
    "龎": "旁",
    "禟": "唐",
    "溈": "维",
    "栐": "永",
    "挋": "镇",
    "巵": "知",
    "膶": "润",
    "譮": "话",
    "簔": "缩",
    "倲": "东",
    "焞": "吞",
    "婑": "瑞",
    "嗹": "连",
    "樒": "密",
    "橑": "老",
    "攠": "迷",
    "鴠": "蛋",
    "荶": "银",
    "丯": "借",
    "孨": "转",
    "蘶": "位",
    "鴣": "姑",
    "玿": "勺",
    "宷": "审",
    "鼢": "坟",
    "剠": "情",
    "婤": "抽",
    "藎": "进",
    "隃": "树",
    "貏": "比",
    "麞": "张",
    "襝": "脸",
    "輖": "周",
    "旍": "精",
    "鍮": "偷",
    "訉": "饭",
    "鈜": "红",
    "糷": "烂",
    "惙": "绰",
    "彾": "零",
    "覘": "搀",
    "伆": "物",
    "薸": "漂",
    "辬": "班",
    "誶": "岁",
    "拵": "存",
    "觵": "工",
    "銩": "丢",
    "鶥": "梅",
    "譩": "一",
    "縐": "昼",
    "竢": "四",
    "鐶": "环",
    "韨": "福",
    "捈": "图",
    "鱚": "洗",
    "辠": "最",
    "奩": "连",
    "菗": "愁",
    "鏣": "树",
    "讉": "宜",
    "怗": "贴",
    "烆": "横",
    "噇": "床",
    "擣": "导",
    "徛": "记",
    "黦": "月",
    "噲": "快",
    "妛": "吃",
    "銕": "铁",
    "敿": "角",
    "瘏": "图",
    "馜": "你",
    "駂": "宝",
    "裞": "睡",
    "糎": "离",
    "蘥": "月",
    "劙": "离",
    "孞": "信",
    "龏": "工",
    "蟇": "麻",
    "閗": "豆",
    "淎": "捧",
    "鬮": "纠",
    "漺": "双",
    "鉁": "真",
    "啹": "局",
    "嫟": "逆",
    "颭": "展",
    "羂": "倦",
    "逨": "来",
    "鬐": "其",
    "鍇": "凯",
    "甛": "田",
    "僝": "缠",
    "飊": "标",
    "娊": "现",
    "蟢": "洗",
    "甖": "应",
    "饟": "想",
    "萓": "宜",
    "熖": "燕",
    "搙": "怒",
    "翆": "翠",
    "鋩": "忙",
    "憓": "会",
    "搟": "显",
    "斵": "着",
    "蕬": "思",
    "膖": "旁",
    "礭": "确",
    "迉": "七",
    "砈": "饿",
    "咼": "锅",
    "戩": "减",
    "琒": "风",
    "慍": "运",
    "穜": "种",
    "馌": "夜",
    "橒": "云",
    "挜": "亚",
    "蠙": "贫",
    "馺": "萨",
    "纇": "泪",
    "夘": "帽",
    "佁": "以",
    "棷": "邹",
    "噳": "雨",
    "愊": "必",
    "灂": "着",
    "楲": "微",
    "癰": "庸",
    "竎": "父",
    "悏": "妾",
    "斘": "生",
    "躂": "达",
    "渞": "求",
    "昑": "寝",
    "鄾": "优",
    "驌": "速",
    "躧": "洗",
    "饤": "定",
    "齄": "扎",
    "嶮": "显",
    "恅": "老",
    "潣": "敏",
    "彑": "记",
    "宊": "突",
    "俴": "见",
    "饾": "豆",
    "鯮": "宗",
    "辢": "蜡",
    "媌": "描",
    "帄": "丁",
    "鞁": "被",
    "萭": "雨",
    "襃": "包",
    "絎": "行",
    "峝": "同",
    "諝": "需",
    "噃": "翻",
    "梀": "速",
    "磌": "田",
    "鯈": "条",
    "畮": "母",
    "恉": "指",
    "杫": "四",
    "倛": "七",
    "絠": "改",
    "鐱": "见",
    "刣": "中",
    "囇": "利",
    "僠": "波",
    "覜": "跳",
    "咮": "昼",
    "牰": "右",
    "慙": "残",
    "槀": "搞",
    "迒": "行",
    "鼘": "冤",
    "顣": "促",
    "瘔": "裤",
    "剮": "寡",
    "衶": "重",
    "赗": "奉",
    "挭": "梗",
    "禝": "记",
    "黷": "毒",
    "幑": "灰",
    "斸": "主",
    "貆": "环",
    "珓": "教",
    "啛": "翠",
    "訆": "教",
    "鱊": "玉",
    "碈": "民",
    "咷": "桃",
    "紕": "批",
    "笇": "算",
    "藂": "从",
    "圔": "亚",
    "踨": "宗",
    "葹": "诗",
    "鱬": "如",
    "戅": "杠",
    "艻": "乐",
    "憞": "对",
    "莧": "现",
    "泍": "奔",
    "燀": "铲",
    "楻": "黄",
    "煾": "恩",
    "僔": "尊",
    "襭": "鞋",
    "尷": "干",
    "淽": "指",
    "訃": "父",
    "碨": "位",
    "椷": "间",
    "嵏": "宗",
    "浀": "区",
    "幚": "帮",
    "鲄": "和",
    "辌": "良",
    "筜": "当",
    "苸": "呼",
    "滫": "修",
    "彸": "中",
    "妷": "直",
    "痓": "赤",
    "硰": "沙",
    "餿": "搜",
    "娬": "五",
    "鋮": "成",
    "晱": "闪",
    "秌": "秋",
    "熛": "标",
    "箤": "族",
    "胹": "而",
    "壖": "软",
    "霥": "梦",
    "殸": "庆",
    "硥": "忙",
    "惃": "滚",
    "彅": "减",
    "穊": "记",
    "幘": "则",
    "膧": "同",
    "愾": "开",
    "唕": "造",
    "魽": "含",
    "莕": "性",
    "鞆": "饼",
    "媬": "宝",
    "苚": "用",
    "椌": "枪",
    "軨": "零",
    "樕": "速",
    "屴": "利",
    "灠": "懒",
    "捇": "或",
    "婘": "全",
    "壂": "电",
    "禡": "骂",
    "勄": "敏",
    "澨": "是",
    "漍": "国",
    "尠": "显",
    "岼": "平",
    "鈫": "琴",
    "豵": "宗",
    "纋": "优",
    "怞": "愁",
    "泜": "知",
    "逥": "回",
    "湵": "有",
    "雮": "木",
    "飇": "标",
    "靬": "钱",
    "籧": "取",
    "猳": "家",
    "鹥": "一",
    "溹": "所",
    "歁": "砍",
    "賖": "舍",
    "窂": "劳",
    "譕": "无",
    "妦": "风",
    "浹": "家",
    "繮": "将",
    "鷘": "赤",
    "刜": "福",
    "嚠": "刘",
    "涥": "哼",
    "縡": "在",
    "鲉": "由",
    "笷": "帽",
    "銪": "有",
    "鷚": "六",
    "螘": "以",
    "卙": "极",
    "佌": "此",
    "眲": "呢",
    "埿": "尼",
    "俹": "亚",
    "玾": "假",
    "嶴": "奥",
    "虋": "门",
    "琾": "借",
    "醁": "路",
    "璒": "灯",
    "蕼": "四",
    "沍": "互",
    "唡": "两",
    "餳": "唐",
    "漘": "纯",
    "鶻": "古",
    "殧": "就",
    "粓": "干",
    "儤": "报",
    "柧": "姑",
    "帪": "真",
    "噟": "硬",
    "伭": "闲",
    "鹔": "速",
    "圤": "葡",
    "赒": "周",
    "鷔": "敖",
    "癇": "闲",
    "鲿": "长",
    "璵": "鱼",
    "悙": "哼",
    "泈": "中",
    "軑": "带",
    "弚": "推",
    "揅": "颜",
    "睄": "绍",
    "烮": "裂",
    "罶": "柳",
    "諢": "混",
    "麛": "迷",
    "辪": "靴",
    "迌": "兔",
    "袙": "怕",
    "躓": "至",
    "諈": "缀",
    "蝯": "原",
    "綼": "必",
    "洂": "夜",
    "褗": "眼",
    "繈": "抢",
    "眧": "吵",
    "渂": "问",
    "翧": "宣",
    "鉰": "思",
    "穬": "矿",
    "鸝": "离",
    "浱": "纯",
    "垔": "因",
    "愖": "陈",
    "騨": "驮",
    "蜨": "叠",
    "姰": "君",
    "囜": "您",
    "抶": "赤",
    "褳": "连",
    "骭": "干",
    "錇": "剖",
    "涆": "汉",
    "呟": "卷",
    "齏": "机",
    "峕": "石",
    "萵": "窝",
    "堧": "软",
    "蒪": "破",
    "灨": "干",
    "譶": "踏",
    "鱓": "善",
    "藧": "换",
    "傴": "雨",
    "袌": "报",
    "躥": "窜",
    "唋": "突",
    "唭": "气",
    "醞": "运",
    "朾": "成",
    "軃": "朵",
    "龂": "银",
    "薶": "买",
    "鴽": "如",
    "鵯": "杯",
    "獌": "慢",
    "屩": "绝",
    "娯": "鱼",
    "嶢": "摇",
    "郔": "颜",
    "榯": "石",
    "膵": "翠",
    "藅": "罚",
    "詵": "深",
    "緎": "玉",
    "乻": "鱼",
    "鷖": "一",
    "趫": "桥",
    "俀": "腿",
    "貺": "矿",
    "裌": "夹",
    "爁": "烂",
    "粩": "捞",
    "韈": "袜",
    "劘": "魔",
    "輹": "父",
    "喣": "许",
    "茷": "罚",
    "虌": "憋",
    "紌": "求",
    "塤": "熏",
    "鰼": "习",
    "耼": "单",
    "殙": "昏",
    "秔": "精",
    "恓": "西",
    "腷": "必",
    "駃": "觉",
    "圁": "银",
    "壄": "也",
    "榙": "他",
    "阛": "环",
    "醶": "燕",
    "鮶": "君",
    "寎": "病",
    "埰": "菜",
    "噦": "月",
    "笎": "原",
    "嚻": "消",
    "甮": "奉",
    "唹": "迂",
    "鐿": "意",
    "壋": "荡",
    "筫": "至",
    "釺": "前",
    "殜": "叠",
    "銧": "光",
    "桱": "静",
    "哶": "灭",
    "峏": "而",
    "珻": "梅",
    "匴": "算",
    "玈": "芦",
    "匒": "达",
    "鄅": "雨",
    "椏": "呀",
    "莵": "兔",
    "唶": "则",
    "偳": "端",
    "櫜": "高",
    "硞": "确",
    "蕣": "顺",
    "楥": "炫",
    "趉": "觉",
    "惌": "冤",
    "冸": "判",
    "抴": "夜",
    "謃": "星",
    "瘒": "文",
    "孧": "右",
    "兦": "王",
    "醳": "意",
    "絸": "减",
    "阤": "至",
    "跍": "哭",
    "簗": "助",
    "攽": "班",
    "籈": "真",
    "蛺": "夹",
    "釂": "教",
    "葲": "全",
    "燑": "同",
    "慛": "催",
    "釕": "了",
    "冞": "迷",
    "纈": "鞋",
    "沰": "拖",
    "奆": "倦",
    "鴫": "田",
    "羱": "原",
    "岇": "昂",
    "愓": "荡",
    "棎": "缠",
    "韃": "达",
    "踄": "不",
    "瘈": "赤",
    "隺": "胡",
    "郺": "庸",
    "鱀": "记",
    "鷦": "教",
    "倴": "笨",
    "疉": "叠",
    "埲": "崩",
    "絓": "挂",
    "嶁": "楼",
    "峢": "李",
    "澑": "六",
    "揂": "纠",
    "欀": "相",
    "敀": "破",
    "圗": "图",
    "僈": "瞒",
    "攆": "年",
    "沑": "女",
    "罧": "肾",
    "檣": "强",
    "硍": "现",
    "瓔": "应",
    "儮": "利",
    "蠚": "喝",
    "璖": "取",
    "汬": "井",
    "箉": "拐",
    "縩": "菜",
    "瑐": "减",
    "媥": "偏",
    "摠": "总",
    "鑌": "彬",
    "扤": "物",
    "鰂": "贼",
    "蒶": "坟",
    "鏨": "赞",
    "鸑": "月",
    "磹": "谈",
    "矱": "约",
    "閺": "文",
    "嘷": "豪",
    "泘": "呼",
    "坢": "办",
    "璡": "进",
    "蝱": "盟",
    "晜": "昆",
    "睗": "是",
    "缻": "否",
    "滍": "至",
    "閴": "去",
    "鯿": "编",
    "瓈": "离",
    "壃": "将",
    "剏": "创",
    "晀": "挑",
    "喤": "黄",
    "銚": "摇",
    "橓": "顺",
    "炋": "批",
    "枮": "先",
    "灈": "取",
    "蘪": "梅",
    "梡": "魂",
    "矉": "贫",
    "褾": "表",
    "淔": "直",
    "鉝": "利",
    "酅": "西",
    "禬": "贵",
    "颣": "泪",
    "轘": "环",
    "鴏": "带",
    "瀙": "亲",
    "諏": "邹",
    "鴳": "燕",
    "饘": "占",
    "鸌": "互",
    "鶖": "秋",
    "蚔": "其",
    "礉": "和",
    "潖": "爬",
    "藄": "其",
    "娐": "夫",
    "繑": "敲",
    "韽": "安",
    "茽": "重",
    "駔": "脏",
    "悢": "亮",
    "趽": "放",
    "牕": "窗",
    "滵": "密",
    "篽": "玉",
    "皡": "号",
    "萂": "和",
    "繂": "绿",
    "笰": "福",
    "咓": "瓦",
    "贋": "燕",
    "皻": "扎",
    "螾": "引",
    "惞": "心",
    "猰": "亚",
    "懜": "猛",
    "侅": "该",
    "煻": "唐",
    "莥": "纽",
    "鐁": "思",
    "奾": "先",
    "禜": "永",
    "鼑": "顶",
    "鎝": "答",
    "鎕": "唐",
    "曐": "星",
    "虣": "报",
    "秹": "忍",
    "梽": "至",
    "鋨": "铁",
    "葍": "福",
    "眫": "米",
    "欞": "零",
    "抌": "胆",
    "揝": "赞",
    "蔢": "婆",
    "婈": "零",
    "愜": "妾",
    "堾": "春",
    "佄": "憨",
    "剺": "离",
    "貹": "胜",
    "鮗": "东",
    "闑": "聂",
    "蹕": "必",
    "岊": "节",
    "鑢": "绿",
    "煃": "魁",
    "壚": "芦",
    "賎": "见",
    "摏": "冲",
    "魣": "续",
    "鞓": "听",
    "埊": "地",
    "鋕": "至",
    "鱣": "占",
    "嵧": "刘",
    "鎩": "沙",
    "筓": "机",
    "卼": "物",
    "媑": "重",
    "榅": "温",
    "粆": "沙",
    "錤": "机",
    "蠘": "节",
    "榾": "古",
    "甞": "长",
    "騩": "归",
    "幉": "叠",
    "巄": "龙",
    "鰱": "连",
    "腯": "图",
    "翋": "拉",
    "覊": "机",
    "桾": "君",
    "柌": "瓷",
    "礟": "炮",
    "碔": "五",
    "倵": "五",
    "櫚": "驴",
    "妌": "静",
    "俉": "五",
    "澭": "庸",
    "肂": "四",
    "骙": "奎",
    "爕": "谢",
    "誹": "匪",
    "胷": "胸",
    "筀": "贵",
    "夽": "允",
    "桺": "柳",
    "妔": "坑",
    "慓": "飘",
    "窻": "窗",
    "刴": "堕",
    "剼": "山",
    "貁": "右",
    "榳": "停",
    "繟": "铲",
    "鳦": "以",
    "緄": "滚",
    "虙": "福",
    "墡": "善",
    "螻": "楼",
    "僺": "巧",
    "闠": "会",
    "旽": "吞",
    "簉": "造",
    "癝": "吝",
    "挆": "朵",
    "揌": "塞",
    "竃": "造",
    "囋": "杂",
    "蟝": "取",
    "傂": "至",
    "穟": "岁",
    "荵": "忍",
    "忴": "钱",
    "楕": "妥",
    "瀓": "成",
    "淭": "取",
    "譭": "毁",
    "摮": "敖",
    "熆": "和",
    "僯": "吝",
    "蓎": "唐",
    "汒": "忙",
    "椫": "善",
    "睷": "间",
    "峖": "安",
    "獪": "快",
    "艛": "楼",
    "餧": "位",
    "萪": "科",
    "焿": "耕",
    "崍": "来",
    "蟈": "锅",
    "坆": "梅",
    "逴": "戳",
    "镃": "姿",
    "噆": "赞",
    "輧": "平",
    "絇": "取",
    "潨": "从",
    "瀡": "髓",
    "珔": "见",
    "蚚": "其",
    "饙": "分",
    "灙": "挡",
    "蘯": "荡",
    "泟": "称",
    "賙": "周",
    "僴": "现",
    "渻": "省",
    "藨": "标",
    "悤": "聪",
    "鈢": "洗",
    "籂": "诗",
    "蚞": "木",
    "魒": "飘",
    "薵": "愁",
    "闏": "风",
    "樅": "聪",
    "榿": "七",
    "瑲": "枪",
    "匊": "居",
    "蝛": "微",
    "杁": "入",
    "嚂": "烂",
    "氻": "乐",
    "蝡": "如",
    "唈": "意",
    "浧": "影",
    "髲": "必",
    "垀": "呼",
    "祑": "至",
    "芿": "仍",
    "偼": "节",
    "饎": "赤",
    "褸": "旅",
    "阣": "概",
    "輷": "轰",
    "乕": "虎",
    "櫫": "朱",
    "糝": "三",
    "嚫": "衬",
    "帟": "意",
    "楖": "至",
    "鈶": "四",
    "牪": "燕",
    "矠": "则",
    "鬺": "伤",
    "鷯": "聊",
    "煁": "陈",
    "鄑": "姿",
    "斄": "离",
    "襍": "杂",
    "窼": "招",
    "圐": "哭",
    "浢": "豆",
    "袇": "然",
    "硲": "玉",
    "犘": "麻",
    "裪": "桃",
    "聮": "连",
    "嶓": "波",
    "剾": "口",
    "卪": "节",
    "徆": "西",
    "敜": "聂",
    "瀿": "烦",
    "霣": "允",
    "鷉": "踢",
    "荋": "而",
    "嗺": "嘴",
    "綀": "书",
    "囃": "擦",
    "騂": "星",
    "誽": "逆",
    "隞": "敖",
    "忦": "夹",
    "濼": "落",
    "痽": "堆",
    "溠": "咋",
    "劔": "见",
    "廔": "楼",
    "儦": "标",
    "跲": "夹",
    "虷": "含",
    "蚻": "炸",
    "骴": "刺",
    "墑": "地",
    "韠": "必",
    "輴": "春",
    "僐": "善",
    "鲪": "君",
    "剫": "夺",
    "蕋": "瑞",
    "罳": "思",
    "錋": "朋",
    "鱝": "愤",
    "抈": "月",
    "飍": "修",
    "榬": "原",
    "懣": "闷",
    "澮": "会",
    "憈": "区",
    "鋚": "条",
    "嚈": "夜",
    "朜": "吞",
    "犉": "纯",
    "馉": "古",
    "駘": "台",
    "壪": "弯",
    "竏": "前",
    "蓙": "做",
    "伻": "崩",
    "垿": "续",
    "舕": "探",
    "檟": "假",
    "稧": "系",
    "檇": "最",
    "蕠": "如",
    "茠": "蒿",
    "孼": "聂",
    "矺": "哲",
    "瀠": "营",
    "氊": "占",
    "娻": "东",
    "暯": "墨",
    "隡": "萨",
    "垳": "行",
    "憜": "堕",
    "駹": "忙",
    "崹": "提",
    "蹐": "极",
    "喴": "微",
    "鳼": "文",
    "棆": "伦",
    "喕": "免",
    "烾": "赤",
    "禑": "无",
    "挦": "闲",
    "澸": "胆",
    "伔": "胆",
    "鯻": "蜡",
    "鈒": "萨",
    "楶": "节",
    "鼪": "生",
    "薽": "真",
    "屨": "巨",
    "疻": "指",
    "觍": "舔",
    "妎": "害",
    "奅": "炮",
    "湬": "角",
    "丣": "有",
    "軰": "被",
    "呁": "俊",
    "忟": "稳",
    "勚": "意",
    "齨": "就",
    "窴": "田",
    "漮": "康",
    "樀": "敌",
    "飣": "定",
    "弉": "藏",
    "翏": "六",
    "尣": "汪",
    "羄": "照",
    "粸": "其",
    "袺": "节",
    "鐉": "圈",
    "厈": "罕",
    "腶": "断",
    "駞": "驮",
    "諌": "懂",
    "搱": "至",
    "袐": "必",
    "魠": "拖",
    "迻": "宜",
    "咺": "选",
    "莟": "汉",
    "暜": "普",
    "詃": "减",
    "鱄": "专",
    "鵳": "间",
    "萛": "纠",
    "頋": "饿",
    "擕": "鞋",
    "搰": "胡",
    "獆": "豪",
    "嘺": "桥",
    "爖": "龙",
    "碫": "断",
    "樃": "浪",
    "羏": "羊",
    "褭": "尿",
    "橅": "魔",
    "肎": "肯",
    "禥": "其",
    "寙": "雨",
    "旹": "石",
    "鈁": "方",
    "蘳": "灰",
    "乭": "石",
    "凙": "夺",
    "覰": "区",
    "粁": "前",
    "堐": "牙",
    "倸": "采",
    "挮": "体",
    "籱": "着",
    "軘": "吞",
    "罎": "谈",
    "蔍": "路",
    "烖": "灾",
    "臷": "叠",
    "沜": "判",
    "湏": "会",
    "儽": "雷",
    "蒀": "晕",
    "黌": "红",
    "綞": "朵",
    "轝": "玉",
    "鄐": "处",
    "峂": "同",
    "骦": "双",
    "窶": "巨",
    "餱": "喉",
    "輞": "网",
    "猤": "贵",
    "菶": "崩",
    "璴": "楚",
    "竍": "石",
    "膹": "愤",
    "箈": "台",
    "黈": "偷",
    "藢": "指",
    "臠": "鸾",
    "捿": "七",
    "裬": "零",
    "蜅": "辅",
    "曶": "呼",
    "穛": "捉",
    "獈": "意",
    "崪": "族",
    "誜": "刷",
    "嫸": "展",
    "衂": "女",
    "嵄": "美",
    "萯": "父",
    "胏": "子",
    "馝": "必",
    "薠": "烦",
    "郣": "博",
    "釴": "意",
    "琕": "贫",
    "謩": "魔",
    "帵": "弯",
    "侇": "宜",
    "潫": "弯",
    "盠": "离",
    "剟": "多",
    "芵": "觉",
    "潠": "训",
    "皅": "趴",
    "嫧": "则",
    "畣": "达",
    "靇": "龙",
    "菎": "昆",
    "剚": "字",
    "籩": "编",
    "犃": "剖",
    "賫": "机",
    "蓚": "条",
    "嘨": "笑",
    "蹅": "差",
    "皰": "炮",
    "諕": "豪",
    "藷": "鼠",
    "鋘": "华",
    "籯": "营",
    "櫪": "利",
    "鴊": "挣",
    "徻": "会",
    "昣": "枕",
    "賅": "该",
    "耡": "除",
    "瀜": "容",
    "娀": "松",
    "婫": "昆",
    "緪": "耕",
    "搕": "科",
    "忢": "物",
    "蚘": "回",
    "怤": "夫",
    "貐": "雨",
    "勱": "卖",
    "虤": "颜",
    "麄": "粗",
    "獹": "芦",
    "侸": "树",
    "糲": "利",
    "鏛": "长",
    "匤": "区",
    "蠭": "风",
    "巋": "亏",
    "譓": "会",
    "瀇": "网",
    "鴍": "文",
    "撢": "胆",
    "繸": "岁",
    "埨": "伦",
    "坱": "养",
    "潱": "椰",
    "忣": "极",
    "獮": "显",
    "礶": "灌",
    "擯": "宾",
    "埀": "垂",
    "酇": "赞",
    "岥": "坡",
    "榥": "荒",
    "顁": "定",
    "椙": "昌",
    "吺": "兜",
    "鏵": "华",
    "漟": "唐",
    "蒢": "除",
    "俇": "逛",
    "犠": "西",
    "珿": "处",
    "甤": "瑞",
    "奱": "鸾",
    "鉋": "报",
    "犙": "三",
    "幤": "必",
    "屪": "聊",
    "綊": "鞋",
    "塇": "宣",
    "淗": "局",
    "沗": "旁",
    "髧": "蛋",
    "詑": "宜",
    "凷": "快",
    "漦": "持",
    "凮": "风",
    "僟": "机",
    "秄": "子",
    "殀": "邀",
    "窐": "归",
    "邆": "腾",
    "憒": "溃",
    "汻": "虎",
    "芇": "眠",
    "叇": "带",
    "拕": "拖",
    "浟": "由",
    "繵": "蛋",
    "鱅": "庸",
    "鎡": "姿",
    "蓭": "安",
    "脄": "梅",
    "鹺": "搓",
    "捬": "辅",
    "幜": "井",
    "棔": "昏",
    "韑": "伟",
    "沶": "宜",
    "浲": "逢",
    "盇": "和",
    "蒃": "赚",
    "卛": "帅",
    "眻": "羊",
    "箯": "编",
    "蓽": "必",
    "刐": "胆",
    "趚": "速",
    "踧": "促",
    "娷": "缀",
    "煟": "位",
    "瘲": "宗",
    "秼": "朱",
    "紷": "零",
    "揫": "纠",
    "俌": "辅",
    "滊": "系",
    "匛": "就",
    "蘢": "龙",
    "姯": "光",
    "虅": "腾",
    "竸": "静",
    "臶": "见",
    "倯": "松",
    "礊": "客",
    "佊": "比",
    "靂": "利",
    "驄": "聪",
    "爚": "月",
    "迼": "节",
    "梪": "豆",
    "禢": "踏",
    "颻": "摇",
    "飩": "吞",
    "煯": "接",
    "輤": "欠",
    "氎": "叠",
    "聐": "亚",
    "熕": "工",
    "婯": "利",
    "肙": "院",
    "縼": "炫",
    "濎": "顶",
    "鞵": "鞋",
    "恟": "胸",
    "焛": "吝",
    "獷": "广",
    "鯁": "梗",
    "魸": "片",
    "翜": "煞",
    "縧": "掏",
    "芅": "意",
    "砛": "金",
    "曗": "夜",
    "僽": "昼",
    "蝍": "节",
    "頍": "魁",
    "焟": "西",
    "欶": "硕",
    "巪": "巨",
    "罞": "毛",
    "鶕": "安",
    "搉": "确",
    "檲": "团",
    "巎": "脑",
    "吜": "丑",
    "靌": "宝",
    "壝": "伟",
    "洍": "四",
    "觲": "星",
    "宂": "容",
    "罏": "芦",
    "謖": "速",
    "櫱": "聂",
    "廱": "庸",
    "遫": "赤",
    "椊": "做",
    "珟": "速",
    "忹": "狂",
    "謉": "溃",
    "鵞": "额",
    "璔": "增",
    "爮": "袍",
    "爯": "称",
    "浖": "裂",
    "灀": "双",
    "墪": "蹲",
    "蛓": "次",
    "貟": "原",
    "葼": "宗",
    "觬": "尼",
    "訡": "银",
    "漹": "烟",
    "蓷": "推",
    "峗": "维",
    "扲": "钱",
    "狽": "被",
    "乹": "干",
    "鷊": "意",
    "媊": "钱",
    "騋": "来",
    "迡": "逆",
    "鐄": "黄",
    "碝": "软",
    "赸": "善",
    "媠": "妥",
    "緺": "瓜",
    "鐬": "会",
    "兺": "分",
    "瞯": "闲",
    "麳": "来",
    "熁": "鞋",
    "澕": "和",
    "膓": "长",
    "梉": "装",
    "鐀": "溃",
    "鷆": "田",
    "凧": "睁",
    "緁": "妾",
    "咶": "坏",
    "矂": "懆",
    "洬": "速",
    "楺": "柔",
    "鳑": "旁",
    "緸": "因",
    "譖": "怎",
    "颷": "标",
    "炓": "料",
    "籰": "月",
    "巁": "利",
    "厀": "西",
    "嘒": "会",
    "饀": "桃",
    "蝐": "帽",
    "闤": "环",
    "鉺": "二",
    "羀": "柳",
    "唀": "右",
    "莁": "无",
    "縆": "耕",
    "儧": "赞",
    "飤": "四",
    "欝": "玉",
    "齟": "举",
    "圷": "下",
    "喗": "允",
    "萅": "春",
    "愄": "微",
    "滪": "玉",
    "揙": "编",
    "慠": "奥",
    "螎": "容",
    "筺": "框",
    "坃": "熏",
    "噅": "灰",
    "枴": "拐",
    "祬": "知",
    "牣": "任",
    "矑": "芦",
    "汳": "变",
    "謈": "婆",
    "鏾": "三",
    "剨": "霍",
    "蒁": "树",
    "惍": "金",
    "嵵": "石",
    "鷝": "必",
    "蝬": "宗",
    "胓": "平",
    "峊": "父",
    "碋": "贺",
    "遰": "地",
    "魓": "必",
    "舤": "烦",
    "洭": "框",
    "阹": "区",
    "夅": "降",
    "揕": "镇",
    "繲": "谢",
    "娫": "颜",
    "嬟": "意",
    "錽": "万",
    "梴": "搀",
    "橝": "电",
    "窽": "款",
    "瑽": "聪",
    "爌": "矿",
    "宒": "准",
    "謓": "陈",
    "獢": "消",
    "郘": "旅",
    "燖": "寻",
    "蝞": "妹",
    "錑": "泪",
    "猽": "明",
    "轃": "真",
    "躃": "必",
    "冝": "宜",
    "殕": "否",
    "趖": "缩",
    "飫": "玉",
    "涰": "绰",
    "翂": "分",
    "婣": "因",
    "橖": "唐",
    "汦": "指",
    "扜": "迂",
    "咟": "或",
    "懕": "烟",
    "纉": "钻",
    "鬷": "宗",
    "悺": "灌",
    "郲": "来",
    "鱫": "爱",
    "椱": "父",
    "颼": "搜",
    "繤": "钻",
    "褱": "怀",
    "曨": "龙",
    "爃": "容",
    "黵": "展",
    "伿": "意",
    "釰": "日",
    "艤": "以",
    "芀": "条",
    "畕": "将",
    "靫": "茶",
    "齝": "吃",
    "縿": "山",
    "荺": "允",
    "歮": "色",
    "麌": "雨",
    "煝": "妹",
    "阭": "允",
    "陾": "仍",
    "踆": "村",
    "峆": "和",
    "劤": "进",
    "霨": "位",
    "玵": "岸",
    "鍉": "低",
    "啒": "古",
    "紲": "谢",
    "萴": "册",
    "籋": "聂",
    "獧": "倦",
    "鉫": "家",
    "墌": "直",
    "鉐": "石",
    "斨": "枪",
    "蟡": "鬼",
    "磎": "西",
    "杝": "离",
    "杸": "书",
    "疕": "比",
    "鉔": "匝",
    "蝚": "柔",
    "躣": "取",
    "冎": "寡",
    "珤": "宝",
    "蚆": "八",
    "篵": "聪",
    "湕": "减",
    "朻": "纠",
    "荰": "度",
    "穻": "迂",
    "僃": "被",
    "嚖": "会",
    "鞿": "机",
    "摬": "影",
    "檦": "表",
    "奰": "必",
    "嘵": "消",
    "爅": "墨",
    "燡": "意",
    "劥": "坑",
    "杶": "春",
    "葿": "梅",
    "蕳": "间",
    "逎": "求",
    "鰫": "庸",
    "蒆": "靴",
    "騣": "宗",
    "匬": "雨",
    "驂": "参",
    "啑": "煞",
    "綹": "柳",
    "廐": "就",
    "鯾": "编",
    "詻": "恶",
    "櫬": "衬",
    "鄋": "搜",
    "鞀": "桃",
    "鋡": "含",
    "媁": "维",
    "鞟": "扩",
    "餜": "果",
    "錼": "耐",
    "盳": "忘",
    "虘": "搓",
    "葃": "做",
    "簁": "筛",
    "輳": "凑",
    "盋": "波",
    "靔": "天",
    "貒": "团",
    "戧": "枪",
    "畟": "册",
    "禼": "谢",
    "鹴": "双",
    "廦": "必",
    "釖": "刀",
    "垹": "帮",
    "穵": "挖",
    "趠": "绰",
    "撧": "绝",
    "脼": "两",
    "偀": "应",
    "婾": "偷",
    "稉": "精",
    "嬍": "美",
    "詍": "意",
    "蛣": "七",
    "嶔": "亲",
    "酘": "豆",
    "擵": "魔",
    "憭": "了",
    "瑿": "一",
    "貀": "那",
    "閅": "门",
    "煢": "穷",
    "醎": "闲",
    "菳": "琴",
    "薋": "瓷",
    "劖": "缠",
    "鷐": "陈",
    "筄": "要",
    "毿": "三",
    "覂": "奉",
    "檍": "意",
    "寷": "风",
    "釪": "华",
    "湹": "缠",
    "憃": "冲",
    "槕": "捉",
    "轪": "带",
    "煱": "瓜",
    "鴜": "瓷",
    "歊": "消",
    "鰯": "弱",
    "陃": "饼",
    "舑": "贪",
    "夒": "脑",
    "戺": "是",
    "潳": "图",
    "蒧": "点",
    "鲾": "逼",
    "媋": "春",
    "鬙": "僧",
    "齕": "和",
    "蟩": "觉",
    "袵": "任",
    "腤": "安",
    "鮹": "烧",
    "奜": "匪",
    "靯": "度",
    "戄": "觉",
    "塨": "工",
    "泀": "思",
    "鯝": "固",
    "寊": "真",
    "耴": "意",
    "魌": "七",
    "鱦": "硬",
    "踠": "碗",
    "笍": "缀",
    "痝": "忙",
    "鯹": "星",
    "贁": "败",
    "銗": "向",
    "斆": "笑",
    "寋": "见",
    "峟": "右",
    "灧": "燕",
    "鋄": "万",
    "綌": "系",
    "藇": "续",
    "鷙": "至",
    "踘": "居",
    "頄": "奎",
    "嚲": "朵",
    "恀": "是",
    "嬓": "教",
    "嬜": "心",
    "鼀": "促",
    "俋": "意",
    "枍": "意",
    "嫝": "康",
    "紃": "寻",
    "醓": "坦",
    "榤": "节",
    "旿": "五",
    "堻": "金",
    "懰": "刘",
    "揗": "寻",
    "娪": "无",
    "簂": "贵",
    "蚼": "狗",
    "斢": "挑",
    "乤": "下",
    "鄨": "必",
    "菧": "底",
    "螆": "次",
    "忊": "定",
    "樠": "瞒",
    "碃": "庆",
    "烰": "福",
    "鸇": "占",
    "鵻": "追",
    "縕": "运",
    "鉼": "饼",
    "裃": "卡",
    "駈": "区",
    "鞻": "楼",
    "劜": "亚",
    "禐": "院",
    "弿": "减",
    "酛": "原",
    "衏": "院",
    "鯎": "成",
    "蛖": "忙",
    "潹": "缠",
    "堘": "成",
    "裩": "昆",
    "嫾": "连",
    "炡": "睁",
    "葌": "间",
    "秳": "活",
    "袨": "炫",
    "趷": "科",
    "饠": "罗",
    "縚": "掏",
    "聕": "号",
    "輊": "至",
    "凞": "西",
    "猒": "燕",
    "罖": "罗",
    "涽": "昏",
    "褧": "窘",
    "曮": "眼",
    "軞": "毛",
    "趜": "局",
    "燳": "照",
    "埓": "裂",
    "騛": "飞",
    "痵": "记",
    "煹": "够",
    "櫞": "原",
    "璍": "夜",
    "穚": "教",
    "椶": "宗",
    "獜": "林",
    "鏦": "聪",
    "趮": "造",
    "觕": "粗",
    "耰": "优",
    "衟": "到",
    "礹": "颜",
    "巓": "颠",
    "緅": "邹",
    "豻": "按",
    "鄷": "风",
    "鼃": "挖",
    "籝": "营",
    "丮": "几",
    "堹": "重",
    "璅": "锁",
    "祳": "肾",
    "繏": "炫",
    "鲏": "皮",
    "姼": "石",
    "揻": "微",
    "繅": "骚",
    "鶒": "赤",
    "幠": "呼",
    "嬣": "凝",
    "躝": "蓝",
    "陼": "主",
    "鴓": "灭",
    "鼳": "局",
    "臩": "广",
    "犕": "被",
    "礨": "垒",
    "弆": "举",
    "葧": "博",
    "靤": "报",
    "輑": "引",
    "驘": "罗",
    "絻": "免",
    "鞧": "秋",
    "鞨": "和",
    "馠": "憨",
    "牨": "刚",
    "譤": "机",
    "獡": "硕",
    "倽": "煞",
    "砨": "恶",
    "欿": "砍",
    "籛": "减",
    "巰": "求",
    "瞆": "溃",
    "鼂": "朝",
    "粇": "康",
    "襈": "赚",
    "碢": "驮",
    "眝": "助",
    "猵": "编",
    "敒": "深",
    "簄": "互",
    "觹": "西",
    "匁": "文",
    "娾": "矮",
    "禂": "导",
    "肁": "照",
    "鰶": "记",
    "爢": "迷",
    "瘻": "漏",
    "枛": "照",
    "醽": "零",
    "圿": "夹",
    "詙": "拔",
    "瑦": "五",
    "椸": "宜",
    "鰏": "逼",
    "尶": "干",
    "斕": "蓝",
    "溓": "连",
    "觱": "必",
    "蚄": "方",
    "圎": "原",
    "櫴": "赖",
    "媈": "灰",
    "仜": "红",
    "刱": "创",
    "櫑": "雷",
    "轢": "利",
    "雥": "杂",
    "坧": "知",
    "棖": "成",
    "螠": "意",
    "窢": "需",
    "鶽": "损",
    "砅": "利",
    "疷": "知",
    "挸": "减",
    "堷": "印",
    "妉": "单",
    "甠": "情",
    "蘷": "奎",
    "朁": "惨",
    "鞾": "靴",
    "烒": "是",
    "澒": "红",
    "歰": "色",
    "枅": "机",
    "癁": "福",
    "宱": "咋",
    "矓": "龙",
    "洷": "至",
    "簶": "路",
    "橉": "吝",
    "罛": "姑",
    "塿": "楼",
    "陘": "行",
    "礜": "玉",
    "敂": "扣",
    "僼": "风",
    "蜯": "棒",
    "醰": "谈",
    "謷": "敖",
    "岏": "完",
    "餷": "插",
    "塮": "谢",
    "齜": "柴",
    "膃": "袜",
    "扸": "西",
    "卂": "训",
    "臦": "逛",
    "藾": "赖",
    "鱶": "想",
    "缹": "否",
    "厧": "颠",
    "匃": "概",
    "呩": "是",
    "輭": "软",
    "滻": "铲",
    "辤": "瓷",
    "剦": "烟",
    "搘": "知",
    "帗": "波",
    "窌": "教",
    "贑": "干",
    "甝": "含",
    "甈": "气",
    "礸": "擦",
    "辳": "农",
    "騜": "黄",
    "麖": "精",
    "乶": "辅",
    "阘": "达",
    "逓": "地",
    "穅": "康",
    "槰": "朋",
    "剢": "督",
    "揥": "替",
    "釡": "辅",
    "雼": "荡",
    "菭": "台",
    "鴗": "利",
    "媙": "微",
    "駓": "批",
    "鴄": "匹",
    "覿": "敌",
    "綡": "良",
    "懯": "夫",
    "煏": "必",
    "艑": "变",
    "袧": "勾",
    "阠": "信",
    "穧": "记",
    "蚇": "尺",
    "毣": "木",
    "蜧": "利",
    "韍": "福",
    "蠦": "芦",
    "齺": "邹",
    "閁": "骂",
    "榟": "子",
    "鍤": "插",
    "曂": "荒",
    "鹒": "耕",
    "磂": "刘",
    "蒭": "除",
    "峱": "脑",
    "磏": "连",
    "荂": "夫",
    "拺": "册",
    "梖": "被",
    "耬": "楼",
    "葔": "喉",
    "柕": "帽",
    "礇": "玉",
    "逽": "诺",
    "乨": "使",
    "鮞": "而",
    "暵": "汉",
    "邍": "原",
    "枀": "松",
    "鰃": "微",
    "屓": "谢",
    "袡": "然",
    "圇": "伦",
    "虗": "需",
    "鞎": "很",
    "崊": "林",
    "矁": "丑",
    "俈": "裤",
    "馎": "博",
    "抭": "咬",
    "篨": "除",
    "蕌": "垒",
    "箶": "胡",
    "餈": "瓷",
    "虪": "树",
    "囕": "懒",
    "踳": "喘",
    "蝲": "蜡",
    "媹": "刘",
    "嶤": "摇",
    "眑": "咬",
    "濨": "瓷",
    "檿": "眼",
    "檏": "普",
    "劌": "贵",
    "慁": "混",
    "嶗": "劳",
    "圢": "挺",
    "霳": "龙",
    "艈": "玉",
    "醹": "如",
    "庼": "请",
    "欂": "博",
    "酧": "愁",
    "醄": "桃",
    "哣": "剖",
    "醕": "纯",
    "窫": "亚",
    "甒": "五",
    "峬": "不",
    "篊": "黄",
    "兤": "谎",
    "橊": "刘",
    "繢": "会",
    "獽": "嚷",
    "茩": "够",
    "邿": "诗",
    "醷": "意",
    "孴": "你",
    "鐃": "脑",
    "礧": "雷",
    "厞": "费",
    "嚁": "敌",
    "椖": "朋",
    "薃": "号",
    "刌": "存",
    "翽": "会",
    "噥": "农",
    "煪": "求",
    "硠": "狼",
    "秙": "裤",
    "薖": "科",
    "襆": "福",
    "椃": "豪",
    "瓉": "赞",
    "堭": "黄",
    "踶": "地",
    "懙": "雨",
    "墸": "助",
    "剄": "井",
    "釙": "破",
    "畖": "挖",
    "焸": "胸",
    "抏": "完",
    "磿": "利",
    "牶": "劝",
    "襩": "鼠",
    "椘": "楚",
    "虨": "彬",
    "寉": "贺",
    "哠": "号",
    "緙": "客",
    "靧": "会",
    "袽": "如",
    "扚": "掉",
    "涺": "居",
    "諘": "表",
    "峍": "路",
    "鏳": "称",
    "皩": "荒",
    "踤": "族",
    "鄆": "运",
    "慹": "直",
    "怌": "培",
    "摴": "出",
    "汓": "求",
    "呥": "然",
    "抧": "指",
    "碬": "侠",
    "鴸": "朱",
    "濵": "彬",
    "褈": "虫",
    "鶪": "局",
    "嫎": "旁",
    "鸊": "屁",
    "韊": "蓝",
    "鷡": "无",
    "旕": "鱼",
    "蜙": "松",
    "捸": "突",
    "蹢": "敌",
    "皉": "此",
    "臽": "现",
    "霐": "红",
    "燣": "蓝",
    "煗": "暖",
    "帿": "喉",
    "鄳": "盟",
    "漒": "强",
    "粫": "而",
    "岹": "条",
    "銍": "至",
    "圅": "含",
    "俒": "混",
    "蛃": "饼",
    "奫": "晕",
    "夵": "眼",
    "瞂": "罚",
    "劸": "挖",
    "侳": "做",
    "鲖": "同",
    "桙": "鱼",
    "齌": "记",
    "鑞": "蜡",
    "巹": "紧",
    "裭": "尺",
    "豦": "巨",
    "啌": "相",
    "莚": "颜",
    "郱": "平",
    "溎": "燕",
    "璬": "角",
    "錂": "零",
    "薢": "谢",
    "稬": "诺",
    "狇": "木",
    "鸓": "垒",
    "樏": "垒",
    "躩": "觉",
    "蛒": "格",
    "塓": "密",
    "剱": "见",
    "蒩": "租",
    "圚": "会",
    "鍳": "见",
    "峚": "密",
    "蚲": "平",
    "慖": "国",
    "瑓": "练",
    "塏": "凯",
    "襡": "鼠",
    "荹": "不",
    "氜": "羊",
    "爟": "灌",
    "箆": "必",
    "赬": "称",
    "齆": "瓮",
    "餋": "倦",
    "噮": "院",
    "糂": "三",
    "鶠": "眼",
    "氒": "觉",
    "葮": "断",
    "蠐": "其",
    "錏": "呀",
    "趒": "条",
    "摚": "称",
    "狏": "驮",
    "鑓": "浅",
    "湤": "诗",
    "檾": "请",
    "綒": "夫",
    "罯": "俺",
    "稑": "路",
    "曅": "夜",
    "澟": "吝",
    "獉": "真",
    "鑥": "鲁",
    "暶": "旋",
    "堩": "更",
    "鸂": "西",
    "斳": "琴",
    "蜋": "狼",
    "嘑": "呼",
    "瓬": "访",
    "誖": "被",
    "燵": "达",
    "乵": "眼",
    "臙": "烟",
    "雟": "西",
    "袀": "君",
    "厱": "蓝",
    "蹹": "他",
    "摙": "脸",
    "堒": "昆",
    "豀": "西",
    "挳": "坑",
    "瞡": "归",
    "諪": "停",
    "犎": "风",
    "鏝": "慢",
    "纎": "先",
    "爎": "聊",
    "襂": "森",
    "搸": "真",
    "楘": "木",
    "漎": "从",
    "坹": "血",
    "鄤": "慢",
    "鞌": "安",
    "渜": "暖",
    "顴": "全",
    "厃": "伟",
    "蓪": "通",
    "欘": "竹",
    "穇": "惨",
    "顑": "砍",
    "霛": "零",
    "紏": "偷",
    "縑": "间",
    "賻": "父",
    "戭": "眼",
    "亝": "其",
    "扴": "夹",
    "膐": "旅",
    "鈏": "引",
    "鴃": "觉",
    "礑": "荡",
    "櫉": "除",
    "嬽": "冤",
    "皪": "利",
    "駶": "局",
    "蕸": "侠",
    "纊": "矿",
    "牉": "判",
    "甆": "瓷",
    "戁": "男",
    "惸": "穷",
    "邧": "原",
    "犗": "借",
    "閈": "汉",
    "儌": "角",
    "灚": "角",
    "醻": "愁",
    "畒": "母",
    "挩": "拖",
    "莏": "缩",
    "欭": "意",
    "齱": "邹",
    "覤": "系",
    "誚": "巧",
    "牻": "忙",
    "殾": "训",
    "鶤": "昆",
    "鉶": "行",
    "鄡": "敲",
    "舮": "芦",
    "憮": "五",
    "墴": "黄",
    "譐": "尊",
    "聏": "而",
    "栰": "罚",
    "帾": "堵",
    "厪": "紧",
    "昛": "巨",
    "踛": "路",
    "閵": "吝",
    "礘": "恶",
    "悥": "意",
    "礄": "桥",
    "顬": "如",
    "頙": "撤",
    "燗": "烂",
    "郰": "邹",
    "嵡": "瓮",
    "焴": "玉",
    "臐": "熏",
    "邐": "李",
    "襢": "坦",
    "蒥": "刘",
    "霔": "助",
    "繜": "尊",
    "岅": "板",
    "砎": "借",
    "磟": "六",
    "傔": "欠",
    "喒": "杂",
    "濹": "么",
    "鷜": "驴",
    "芁": "教",
    "撠": "几",
    "骾": "梗",
    "曤": "或",
    "諰": "洗",
    "灲": "消",
    "徶": "别",
    "鵹": "离",
    "欴": "狼",
    "誷": "网",
    "馚": "坟",
    "鰔": "感",
    "鞜": "踏",
    "鼜": "气",
    "摫": "归",
    "漑": "概",
    "嶵": "嘴",
    "檋": "局",
    "鏸": "会",
    "謍": "营",
    "兛": "前",
    "僄": "票",
    "謸": "敖",
    "鱞": "关",
    "綧": "准",
    "辸": "仍",
    "跰": "偏",
    "陿": "侠",
    "穾": "要",
    "籒": "昼",
    "橸": "精",
    "簆": "扣",
    "偹": "被",
    "稫": "屁",
    "廧": "强",
    "鹠": "刘",
    "峞": "维",
    "敚": "夺",
    "衦": "感",
    "箾": "硕",
    "菺": "间",
    "鵿": "生",
    "喭": "燕",
    "鸴": "学",
    "蛫": "鬼",
    "傤": "在",
    "琘": "民",
    "艭": "双",
    "鵦": "路",
    "趦": "姿",
    "槜": "最",
    "穖": "几",
    "胮": "旁",
    "怰": "炫",
    "僗": "劳",
    "珘": "周",
    "阺": "底",
    "埮": "探",
    "湺": "闲",
    "嵅": "含",
    "汱": "犬",
    "尩": "汪",
    "趛": "引",
    "昖": "颜",
    "昘": "访",
    "劽": "裂",
    "錙": "姿",
    "畡": "该",
    "楜": "胡",
    "屵": "恶",
    "蓡": "深",
    "谼": "红",
    "繉": "魂",
    "膡": "硬",
    "郟": "夹",
    "鷛": "庸",
    "孆": "应",
    "冺": "敏",
    "淊": "烟",
    "傿": "燕",
    "嬃": "需",
    "鬊": "顺",
    "讋": "哲",
    "鰎": "减",
    "圼": "聂",
    "鷓": "这",
    "鐇": "烦",
    "秎": "愤",
    "颲": "裂",
    "胈": "拔",
    "鼣": "费",
    "珫": "冲",
    "卥": "西",
    "坽": "零",
    "笡": "妾",
    "鑗": "离",
    "隉": "聂",
    "熣": "虽",
    "痚": "消",
    "汥": "知",
    "嚾": "欢",
    "踖": "极",
    "敃": "敏",
    "茐": "聪",
    "麰": "谋",
    "苢": "以",
    "鴌": "奉",
    "巙": "奎",
    "堗": "突",
    "邫": "帮",
    "轓": "翻",
    "悋": "吝",
    "韣": "毒",
    "疞": "需",
    "桪": "寻",
    "婡": "来",
    "犝": "同",
    "杛": "工",
    "湨": "局",
    "訒": "任",
    "悷": "利",
    "蹖": "冲",
    "奒": "开",
    "輂": "局",
    "怚": "巨",
    "湆": "气",
    "葊": "安",
    "凁": "搜",
    "澽": "巨",
    "檡": "宅",
    "嵉": "停",
    "畉": "福",
    "疘": "刚",
    "蘠": "强",
    "櫝": "毒",
    "輍": "玉",
    "楰": "鱼",
    "椵": "假",
    "皭": "教",
    "礷": "蓝",
    "熅": "运",
    "雔": "愁",
    "剒": "错",
    "蒰": "盘",
    "祮": "告",
    "璕": "寻",
    "侜": "周",
    "岧": "条",
    "襵": "者",
    "箼": "乌",
    "絏": "谢",
    "凾": "含",
    "垖": "堆",
    "颶": "巨",
    "頚": "井",
    "鯸": "喉",
    "巚": "眼",
    "齠": "条",
    "塻": "墨",
    "竓": "豪",
    "嵣": "荡",
    "胣": "尺",
    "媦": "位",
    "傱": "耸",
    "毸": "塞",
    "欪": "处",
    "纐": "角",
    "螉": "瓮",
    "摪": "将",
    "殠": "臭",
    "磒": "允",
    "誎": "促",
    "鶵": "除",
    "蒖": "真",
    "璼": "蓝",
    "睋": "额",
    "秐": "云",
    "筞": "册",
    "鵪": "安",
    "迬": "助",
    "勌": "倦",
    "哤": "忙",
    "苉": "匹",
    "愋": "宣",
    "獘": "必",
    "颵": "烧",
    "瀴": "营",
    "歏": "进",
    "孶": "姿",
    "擥": "懒",
    "牼": "坑",
    "躌": "五",
    "祱": "睡",
    "糿": "右",
    "洕": "印",
    "豰": "博",
    "餤": "谈",
    "梩": "离",
    "蒠": "西",
    "脀": "成",
    "誆": "框",
    "禙": "被",
    "鸕": "芦",
    "馽": "直",
    "笜": "竹",
    "阞": "乐",
    "褵": "离",
    "倈": "来",
    "墱": "凳",
    "侾": "消",
    "躦": "搓",
    "詼": "灰",
    "妢": "坟",
    "薂": "习",
    "酦": "破",
    "葖": "突",
    "藸": "除",
    "腏": "绰",
    "謞": "贺",
    "茋": "指",
    "鋀": "偷",
    "渳": "米",
    "絭": "倦",
    "棦": "称",
    "蚗": "觉",
    "寴": "亲",
    "茡": "字",
    "觻": "利",
    "轖": "色",
    "椺": "习",
    "鰉": "黄",
    "捼": "弱",
    "鷰": "燕",
    "鈙": "琴",
    "偺": "杂",
    "枆": "毛",
    "堳": "梅",
    "癤": "接",
    "坺": "拔",
    "珢": "银",
    "涾": "踏",
    "縸": "木",
    "籸": "深",
    "瓓": "烂",
    "籭": "思",
    "婮": "居",
    "嚳": "裤",
    "爂": "标",
    "茣": "无",
    "鍡": "伟",
    "屳": "先",
    "譱": "善",
    "瀻": "带",
    "匇": "意",
    "峜": "记",
    "鮢": "朱",
    "睰": "骂",
    "憖": "印",
    "蟭": "教",
    "斞": "雨",
    "緓": "应",
    "菬": "桥",
    "嶣": "教",
    "櫰": "怀",
    "鵶": "呀",
    "柣": "至",
    "笿": "落",
    "韉": "间",
    "銾": "红",
    "眐": "睁",
    "撜": "整",
    "鮦": "同",
    "黳": "一",
    "櫕": "攒",
    "趂": "衬",
    "鎦": "刘",
    "蚹": "父",
    "痑": "贪",
    "枑": "互",
    "屷": "会",
    "芆": "拆",
    "樍": "则",
    "酕": "毛",
    "臮": "记",
    "魗": "丑",
    "騠": "提",
    "柗": "松",
    "塠": "堆",
    "摗": "搜",
    "劀": "瓜",
    "祩": "助",
    "鞼": "贵",
    "菄": "东",
    "撁": "前",
    "礝": "软",
    "啽": "岸",
    "鳬": "辅",
    "蘓": "苏",
    "窷": "料",
    "薻": "早",
    "閄": "或",
    "恘": "秋",
    "舚": "天",
    "墋": "尘",
    "湗": "奉",
    "圶": "恰",
    "幐": "腾",
    "虭": "雕",
    "邞": "夫",
    "霚": "物",
    "睩": "路",
    "鴔": "福",
    "橧": "增",
    "趈": "占",
    "餲": "爱",
    "鍆": "门",
    "錿": "虎",
    "惿": "提",
    "菆": "邹",
    "辥": "靴",
    "僶": "敏",
    "蔞": "楼",
    "嬙": "强",
    "譊": "脑",
    "畽": "吞",
    "絖": "矿",
    "蛈": "铁",
    "嘾": "蛋",
    "陓": "迂",
    "衁": "荒",
    "玽": "狗",
    "鰗": "胡",
    "禨": "机",
    "杘": "赤",
    "洠": "谋",
    "熗": "呛",
    "慔": "木",
    "檶": "前",
    "凟": "毒",
    "墷": "夜",
    "瞦": "西",
    "豞": "后",
    "稡": "最",
    "馻": "允",
    "郥": "被",
    "迶": "右",
    "怶": "必",
    "荓": "平",
    "饂": "温",
    "兾": "记",
    "墯": "堕",
    "徾": "梅",
    "靏": "贺",
    "緮": "父",
    "蟦": "肥",
    "龝": "秋",
    "槧": "欠",
    "瓍": "随",
    "鋻": "见",
    "夝": "情",
    "鯩": "伦",
    "剙": "创",
    "匳": "连",
    "傊": "运",
    "櫡": "着",
    "簝": "聊",
    "蟶": "称",
    "湷": "装",
    "薿": "你",
    "膲": "教",
    "矒": "盟",
    "鏕": "路",
    "鼟": "腾",
    "倿": "宁",
    "鈖": "分",
    "嚹": "啦",
    "姕": "姿",
    "炗": "光",
    "鞳": "踏",
    "杴": "先",
    "煓": "团",
    "傮": "糟",
    "璗": "荡",
    "艩": "其",
    "靱": "任",
    "拹": "鞋",
    "饜": "燕",
    "赯": "唐",
    "訨": "指",
    "霺": "维",
    "菕": "伦",
    "葠": "深",
    "祰": "告",
    "徲": "提",
    "怢": "突",
    "忯": "其",
    "蚈": "前",
    "飀": "刘",
    "剴": "凯",
    "檖": "岁",
    "呄": "格",
    "襘": "贵",
    "裋": "树",
    "顡": "外",
    "靋": "利",
    "蛕": "回",
    "刡": "敏",
    "黮": "胆",
    "齞": "眼",
    "偫": "至",
    "烳": "普",
    "畃": "寻",
    "怉": "宝",
    "挬": "博",
    "皽": "招",
    "逷": "替",
    "劗": "减",
    "萐": "煞",
    "鶆": "来",
    "穭": "旅",
    "乯": "呼",
    "嫓": "屁",
    "竁": "翠",
    "朄": "引",
    "睯": "昏",
    "蕁": "钱",
    "齭": "楚",
    "荎": "持",
    "憸": "先",
    "盰": "干",
    "棜": "玉",
    "壨": "雷",
    "縶": "直",
    "轌": "雪",
    "轔": "林",
    "騱": "习",
    "瀫": "胡",
    "嵞": "图",
    "踒": "窝",
    "榓": "密",
    "焻": "唱",
    "蝳": "毒",
    "穱": "捉",
    "鴋": "方",
    "袩": "哲",
    "猼": "博",
    "秥": "年",
    "簼": "勾",
    "絿": "求",
    "餀": "害",
    "樲": "二",
    "澓": "福",
    "猲": "些",
    "奙": "本",
    "旾": "春",
    "鬹": "归",
    "胟": "母",
    "貤": "宜",
    "菾": "田",
    "蓲": "秋",
    "耮": "烙",
    "蜪": "桃",
    "嶦": "占",
    "漞": "密",
    "顢": "瞒",
    "洡": "泪",
    "攄": "书",
    "岻": "持",
    "疶": "靴",
    "鍰": "环",
    "陊": "堕",
    "篅": "传",
    "嬥": "挑",
    "詅": "零",
    "寈": "青",
    "韟": "高",
    "渮": "和",
    "岰": "奥",
    "栮": "耳",
    "獋": "豪",
    "焂": "书",
    "屘": "满",
    "窔": "要",
    "鉧": "母",
    "楍": "本",
    "藙": "意",
    "鉩": "洗",
    "髾": "烧",
    "慽": "七",
    "硦": "落",
    "媕": "安",
    "韏": "劝",
    "蹛": "带",
    "秅": "茶",
    "鉳": "北",
    "竬": "曲",
    "濦": "引",
    "齔": "衬",
    "呍": "轰",
    "燸": "如",
    "氠": "深",
    "唩": "窝",
    "湠": "探",
    "胒": "逆",
    "爉": "蜡",
    "譣": "显",
    "寪": "伟",
    "嶕": "教",
    "淾": "引",
    "荱": "伟",
    "礩": "至",
    "坅": "寝",
    "篍": "秋",
    "厗": "提",
    "穼": "深",
    "齯": "尼",
    "傯": "总",
    "賏": "应",
    "攅": "赞",
    "曧": "容",
    "郩": "肖",
    "矊": "眠",
    "頥": "宜",
    "搳": "华",
    "簜": "荡",
    "騤": "奎",
    "諵": "南",
    "鞈": "格",
    "邔": "起",
    "忂": "取",
    "荾": "虽",
    "藽": "亲",
    "粀": "帐",
    "婟": "互",
    "濲": "古",
    "茮": "教",
    "鶬": "仓",
    "齰": "则",
    "嶪": "夜",
    "懹": "让",
    "迾": "裂",
    "劰": "墨",
    "歂": "喘",
    "滖": "虽",
    "鼲": "魂",
    "騉": "昆",
    "皶": "扎",
    "姂": "罚",
    "鋲": "冰",
    "淧": "密",
    "鰳": "乐",
    "焬": "西",
    "鐌": "向",
    "癐": "贵",
    "麏": "君",
    "濧": "对",
    "癙": "鼠",
    "斏": "狼",
    "篻": "漂",
    "躆": "巨",
    "汑": "拖",
    "婰": "点",
    "嘄": "教",
    "捛": "旅",
    "挧": "雨",
    "抩": "南",
    "髥": "然",
    "揇": "男",
    "桇": "如",
    "蠿": "捉",
    "蹜": "速",
    "櫐": "垒",
    "闒": "踏",
    "堈": "刚",
    "霿": "盟",
    "憕": "成",
    "枖": "邀",
    "輠": "果",
    "蓒": "宣",
    "鄪": "必",
    "蕀": "极",
    "岶": "破",
    "蹵": "促",
    "痆": "聂",
    "嵺": "聊",
    "礔": "批",
    "雡": "六",
    "膸": "髓",
    "枎": "福",
    "滱": "扣",
    "凓": "利",
    "婩": "按",
    "耈": "狗",
    "緀": "七",
    "蝺": "曲",
    "鍭": "喉",
    "傡": "病",
    "蝏": "停",
    "鯃": "无",
    "襮": "博",
    "譅": "色",
    "麫": "面",
    "狚": "蛋",
    "綕": "知",
    "鉃": "是",
    "櫽": "引",
    "嫛": "一",
    "孾": "应",
    "厴": "眼",
    "嗿": "坦",
    "蔁": "张",
    "鵮": "前",
    "偯": "以",
    "豯": "西",
    "敄": "物",
    "綩": "碗",
    "扏": "求",
    "攦": "利",
    "獝": "续",
    "刾": "次",
    "鸼": "周",
    "朰": "木",
    "潐": "教",
    "頇": "憨",
    "娙": "行",
    "犚": "位",
    "孍": "颜",
    "椢": "贵",
    "襀": "机",
    "姲": "燕",
    "扻": "至",
    "媉": "卧",
    "裿": "以",
    "悁": "冤",
    "嶡": "贵",
    "衚": "胡",
    "翄": "赤",
    "麮": "去",
    "漋": "龙",
    "羾": "共",
    "罸": "罚",
    "醦": "尘",
    "蝁": "恶",
    "鬦": "豆",
    "擪": "夜",
    "鎍": "锁",
    "喍": "柴",
    "蚅": "恶",
    "鞢": "谢",
    "鰛": "温",
    "扨": "任",
    "昈": "互",
    "憊": "被",
    "梻": "佛",
    "靝": "天",
    "褅": "替",
    "簴": "巨",
    "齴": "眼",
    "軄": "直",
    "郂": "该",
    "庴": "极",
    "嶞": "堕",
    "柡": "永",
    "恈": "谋",
    "栙": "翔",
    "濽": "赞",
    "梫": "寝",
    "烓": "微",
    "鷫": "速",
    "稺": "至",
    "媆": "软",
    "艒": "木",
    "甎": "专",
    "鮊": "爸",
    "鈃": "行",
    "骽": "腿",
    "淕": "路",
    "犓": "除",
    "菙": "垂",
    "葟": "黄",
    "鶛": "接",
    "啎": "五",
    "奛": "谎",
    "轜": "而",
    "帎": "蛋",
    "莭": "节",
    "蘜": "局",
    "逿": "荡",
    "怭": "必",
    "栶": "因",
    "僫": "恶",
    "蓃": "搜",
    "幦": "密",
    "盭": "利",
    "榡": "速",
    "韛": "败",
    "鴐": "歌",
    "釾": "爷",
    "幍": "掏",
    "憱": "促",
    "吰": "红",
    "橔": "蹲",
    "輘": "棱",
    "珡": "琴",
    "篏": "欠",
    "蝩": "虫",
    "騞": "霍",
    "飺": "瓷",
    "砪": "母",
    "嗊": "红",
    "縨": "谎",
    "暓": "帽",
    "僓": "腿",
    "瘹": "掉",
    "懀": "位",
    "夃": "古",
    "鰄": "微",
    "勆": "狼",
    "惏": "蓝",
    "笧": "册",
    "枦": "芦",
    "匷": "觉",
    "梐": "必",
    "誀": "二",
    "垗": "照",
    "壔": "导",
    "鍄": "亮",
    "帞": "墨",
    "肐": "歌",
    "曔": "静",
    "棳": "捉",
    "恮": "圈",
    "楌": "颜",
    "釅": "燕",
    "櫋": "眠",
    "臇": "卷",
    "屭": "系",
    "嫯": "奥",
    "蓤": "零",
    "陻": "因",
    "拁": "家",
    "羜": "助",
    "膫": "聊",
    "膯": "腾",
    "兣": "李",
    "侙": "吃",
    "曒": "角",
    "粊": "必",
    "襧": "指",
    "籷": "哲",
    "疺": "罚",
    "錝": "宗",
    "猚": "牙",
    "磛": "缠",
    "鉒": "助",
    "棙": "利",
    "螶": "取",
    "峔": "母",
    "熢": "朋",
    "鶼": "间",
    "齗": "银",
    "樧": "沙",
    "駊": "坡",
    "瀌": "标",
    "魰": "文",
    "腟": "赤",
    "蒬": "冤",
    "烿": "容",
    "鱭": "记",
    "湻": "纯",
    "椡": "到",
    "蟕": "嘴",
    "銱": "掉",
    "熩": "互",
    "癳": "裸",
    "娗": "挺",
    "跦": "朱",
    "斦": "银",
    "鬴": "辅",
    "銤": "米",
    "炾": "谎",
    "腢": "偶",
    "琔": "电",
    "螹": "见",
    "貛": "欢",
    "捘": "尊",
    "壜": "谈",
    "篂": "星",
    "柋": "带",
    "鹸": "减",
    "甔": "单",
    "贜": "脏",
    "睂": "梅",
    "雃": "前",
    "宎": "咬",
    "挅": "朵",
    "霦": "彬",
    "蓫": "处",
    "剹": "路",
    "鍂": "偏",
    "瑹": "书",
    "薝": "占",
    "鬄": "敌",
    "鎞": "逼",
    "觛": "蛋",
    "鼱": "精",
    "縓": "全",
    "栍": "生",
    "麊": "迷",
    "髷": "区",
    "訑": "宜",
    "袯": "博",
    "牅": "庸",
    "笚": "答",
    "氝": "内",
    "魀": "尬",
    "輐": "万",
    "趤": "荡",
    "聎": "挑",
    "旝": "快",
    "颾": "搜",
    "豜": "间",
    "孊": "米",
    "鑔": "差",
    "跴": "采",
    "暿": "洗",
    "羍": "达",
    "鼏": "密",
    "噽": "匹",
    "瀭": "书",
    "沯": "杂",
    "懨": "烟",
    "纃": "其",
    "蜵": "冤",
    "袣": "意",
    "箊": "迂",
    "忇": "乐",
    "兞": "毛",
    "蕢": "溃",
    "鰇": "柔",
    "魼": "区",
    "赺": "引",
    "肕": "任",
    "稯": "宗",
    "覯": "够",
    "鄶": "快",
    "綗": "窘",
    "禈": "灰",
    "圠": "亚",
    "轠": "雷",
    "慦": "就",
    "矄": "熏",
    "螇": "西",
    "禓": "羊",
    "瞃": "卧",
    "怋": "民",
    "砊": "康",
    "傐": "号",
    "嚊": "屁",
    "俔": "欠",
    "熂": "系",
    "璯": "会",
    "匞": "降",
    "騅": "追",
    "鯗": "想",
    "堸": "逢",
    "譝": "生",
    "鐼": "坟",
    "湐": "墨",
    "壀": "皮",
    "顖": "信",
    "硣": "消",
    "妕": "重",
    "嗠": "烙",
    "璑": "无",
    "蠸": "全",
    "螙": "度",
    "藊": "扁",
    "珁": "瓷",
    "焇": "消",
    "驣": "腾",
    "豮": "坟",
    "絴": "翔",
    "厬": "鬼",
    "堶": "驮",
    "絥": "福",
    "鄍": "明",
    "煼": "吵",
    "鉵": "同",
    "蔆": "零",
    "捠": "帮",
    "甐": "吝",
    "臖": "性",
    "嬄": "一",
    "傸": "闯",
    "虩": "系",
    "鐯": "着",
    "痻": "民",
    "棅": "饼",
    "攖": "应",
    "槵": "换",
    "蘰": "慢",
    "巑": "攒",
    "繓": "左",
    "饄": "唐",
    "黚": "钱",
    "燓": "坟",
    "恦": "上",
    "僛": "七",
    "絽": "旅",
    "蘱": "泪",
    "榗": "见",
    "愃": "宣",
    "穪": "称",
    "闝": "漂",
    "樝": "扎",
    "衺": "鞋",
    "鑕": "至",
    "蕘": "扰",
    "鱆": "张",
    "酟": "天",
    "鏮": "康",
    "拲": "拱",
    "殎": "恰",
    "櫩": "颜",
    "鬅": "朋",
    "垷": "现",
    "巊": "影",
    "懭": "旷",
    "籄": "溃",
    "疧": "其",
    "郼": "一",
    "祶": "地",
    "諞": "偏",
    "皨": "星",
    "竘": "曲",
    "埱": "处",
    "蜲": "微",
    "轗": "砍",
    "罁": "刚",
    "鰢": "马",
    "糏": "谢",
    "裓": "格",
    "娋": "绍",
    "焀": "胡",
    "睕": "碗",
    "桚": "赞",
    "謪": "伤",
    "鱾": "几",
    "烣": "灰",
    "泋": "会",
    "鋟": "寝",
    "爄": "利",
    "詗": "胸",
    "嬇": "溃",
    "滼": "饭",
    "枈": "必",
    "澙": "系",
    "襫": "是",
    "蟁": "文",
    "蛂": "别",
    "娔": "客",
    "梑": "敌",
    "抋": "亲",
    "赲": "利",
    "秶": "姿",
    "霡": "卖",
    "潌": "至",
    "幁": "需",
    "貣": "特",
    "蟱": "无",
    "鰷": "条",
    "綂": "统",
    "蜛": "居",
    "蒒": "诗",
    "遟": "持",
    "瘄": "促",
    "畁": "必",
    "馦": "先",
    "髴": "福",
    "塉": "极",
    "銋": "人",
    "旔": "见",
    "糓": "古",
    "蔩": "银",
    "罋": "瓮",
    "梮": "居",
    "纑": "芦",
    "恡": "吝",
    "鑩": "恶",
    "薎": "灭",
    "麬": "夫",
    "奯": "或",
    "蝑": "需",
    "崸": "羊",
    "愻": "训",
    "鱩": "雷",
    "螿": "将",
    "蠒": "减",
    "娨": "现",
    "籉": "台",
    "灳": "灰",
    "唨": "组",
    "媶": "容",
    "摨": "奶",
    "稏": "亚",
    "稦": "一",
    "醘": "科",
    "嵤": "容",
    "丳": "铲",
    "駜": "必",
    "犐": "科",
    "硜": "坑",
    "詚": "达",
    "搹": "恶",
    "鍈": "应",
    "蔃": "强",
    "悡": "离",
    "赹": "穷",
    "蒮": "玉",
    "兓": "金",
    "齅": "秀",
    "懅": "巨",
    "崵": "羊",
    "歖": "洗",
    "齼": "楚",
    "慞": "张",
    "忶": "魂",
    "鼁": "去",
    "礆": "减",
    "鵄": "吃",
    "鸘": "双",
    "瓡": "直",
    "簐": "年",
    "雦": "极",
    "嘋": "笑",
    "睻": "宣",
    "緐": "烦",
    "菤": "卷",
    "鸐": "敌",
    "鮏": "星",
    "岝": "做",
    "酻": "最",
    "汄": "责",
    "觟": "话",
    "謧": "离",
    "瀤": "怀",
    "屶": "会",
    "覴": "灯",
    "朆": "分",
    "詯": "会",
    "矔": "灌",
    "觡": "格",
    "莍": "求",
    "橗": "盟",
    "甋": "地",
    "隥": "凳",
    "篲": "会",
    "漌": "紧",
    "蘬": "亏",
    "獫": "显",
    "囆": "拆",
    "姡": "华",
    "橰": "高",
    "崜": "多",
    "鸏": "盟",
    "婝": "电",
    "砄": "觉",
    "菐": "葡",
    "麚": "家",
    "緰": "头",
    "黺": "粉",
    "鞂": "接",
    "刉": "机",
    "餻": "高",
    "婋": "消",
    "蠌": "则",
    "遬": "速",
    "鮿": "哲",
    "搻": "诺",
    "烻": "燕",
    "瑖": "断",
    "紿": "带",
    "澺": "意",
    "殟": "温",
    "舧": "烦",
    "夳": "太",
    "嬠": "参",
    "腜": "梅",
    "鯬": "离",
    "鏚": "七",
    "譃": "需",
    "鷩": "必",
    "嵷": "耸",
    "漜": "也",
    "篹": "赚",
    "盓": "迂",
    "摓": "逢",
    "仢": "博",
    "藮": "桥",
    "紼": "福",
    "錴": "路",
    "腀": "伦",
    "賩": "从",
    "縤": "速",
    "癓": "维",
    "篣": "朋",
    "坁": "指",
    "匢": "呼",
    "薘": "达",
    "輨": "管",
    "佅": "卖",
    "饁": "夜",
    "隑": "概",
    "壍": "欠",
    "鷴": "闲",
    "斣": "豆",
    "襥": "福",
    "鬌": "妥",
    "熚": "必",
    "孏": "懒",
    "咠": "气",
    "鶨": "串",
    "捵": "陈",
    "胉": "博",
    "娳": "利",
    "濴": "营",
    "秇": "意",
    "鵉": "鸾",
    "柪": "凹",
    "熞": "间",
    "磄": "唐",
    "剬": "端",
    "恾": "忙",
    "伌": "爱",
    "橮": "柳",
    "髺": "扩",
    "藗": "速",
    "儝": "穷",
    "軷": "拔",
    "鷿": "屁",
    "鯭": "猛",
    "灟": "竹",
    "懁": "宣",
    "譗": "炸",
    "嗃": "贺",
    "顈": "选",
    "棾": "情",
    "笐": "行",
    "陎": "书",
    "譆": "西",
    "爤": "烂",
    "遤": "马",
    "跥": "堕",
    "豩": "彬",
    "絧": "动",
    "榰": "知",
    "磃": "思",
    "黰": "枕",
    "鎜": "盘",
    "啠": "哲",
    "櫳": "龙",
    "拪": "前",
    "嫞": "庸",
    "鷬": "黄",
    "陚": "父",
    "毼": "和",
    "瑌": "软",
    "鞸": "必",
    "憅": "痛",
    "榩": "钱",
    "鰁": "全",
    "酼": "海",
    "袘": "宜",
    "輢": "以",
    "颽": "凯",
    "峅": "变",
    "佨": "包",
    "鲯": "其",
    "荕": "金",
    "覸": "间",
    "韔": "唱",
    "圱": "前",
    "縏": "盘",
    "黣": "美",
    "牭": "四",
    "霢": "卖",
    "澃": "窘",
    "鴙": "至",
    "紖": "镇",
    "菨": "接",
    "駻": "汉",
    "鯆": "铺",
    "痀": "居",
    "壿": "尊",
    "羳": "烦",
    "訬": "超",
    "朼": "比",
    "猍": "来",
    "硧": "永",
    "蒦": "或",
    "怘": "互",
    "灱": "消",
    "蠺": "残",
    "醊": "缀",
    "鰒": "父",
    "玀": "罗",
    "歈": "鱼",
    "綛": "忍",
    "媝": "秋",
    "胑": "知",
    "躿": "康",
    "埪": "空",
    "寠": "巨",
    "罻": "位",
    "樔": "朝",
    "釢": "奶",
    "逩": "笨",
    "悞": "物",
    "琙": "玉",
    "騐": "燕",
    "悐": "替",
    "厒": "妾",
    "矘": "躺",
    "嚘": "优",
    "醧": "玉",
    "宐": "宜",
    "碒": "银",
    "塺": "梅",
    "寁": "赞",
    "屗": "伟",
    "猣": "宗",
    "嵑": "可",
    "饝": "魔",
    "鴢": "咬",
    "睼": "天",
    "髞": "懆",
    "鮐": "台",
    "礣": "骂",
    "裺": "眼",
    "鲘": "后",
    "糦": "西",
    "趓": "朵",
    "萷": "消",
    "廗": "带",
    "鑅": "横",
    "矀": "梅",
    "鲬": "永",
    "鸁": "罗",
    "笝": "那",
    "劊": "贵",
    "龞": "憋",
    "渽": "灾",
    "乑": "银",
    "翢": "到",
    "蕿": "宣",
    "栣": "忍",
    "狾": "至",
    "鞐": "恰",
    "馼": "文",
    "壐": "洗",
    "攛": "窜",
    "簚": "密",
    "蟯": "脑",
    "佦": "诗",
    "淣": "尼",
    "桬": "沙",
    "毃": "敲",
    "狑": "零",
    "櫭": "节",
    "觠": "全",
    "嵲": "聂",
    "搼": "全",
    "蚐": "君",
    "薴": "凝",
    "旊": "访",
    "鸎": "应",
    "聁": "判",
    "胢": "科",
    "瘇": "种",
    "陹": "生",
    "檼": "印",
    "鏺": "坡",
    "躟": "嚷",
    "齚": "则",
    "軹": "指",
    "澠": "免",
    "豥": "该",
    "嫥": "专",
    "匰": "单",
    "烶": "挺",
    "泧": "萨",
    "疨": "虾",
    "匭": "鬼",
    "忩": "聪",
    "柮": "堕",
    "髩": "宾",
    "弣": "辅",
    "澯": "灿",
    "頟": "额",
    "貜": "觉",
    "躚": "先",
    "漊": "楼",
    "渒": "派",
    "澾": "踏",
    "盙": "辅",
    "蒤": "图",
    "鴥": "玉",
    "胿": "归",
    "褽": "位",
    "敥": "燕",
    "郖": "豆",
    "鷽": "学",
    "騍": "客",
    "鍴": "端",
    "樜": "这",
    "鼄": "朱",
    "霱": "玉",
    "乫": "家",
    "騕": "咬",
    "躽": "眼",
    "蹡": "枪",
    "尦": "料",
    "癕": "庸",
    "砤": "驮",
    "勽": "报",
    "碮": "提",
    "瓽": "荡",
    "凂": "美",
    "轑": "老",
    "臢": "匝",
    "煫": "岁",
    "蔇": "记",
    "諼": "宣",
    "菦": "琴",
    "僒": "窘",
    "倄": "摇",
    "羐": "有",
    "癋": "贺",
    "蚥": "父",
    "譾": "减",
    "糫": "环",
    "冔": "许",
    "飪": "任",
    "蚢": "行",
    "凬": "风",
    "蜫": "昆",
    "湽": "姿",
    "靰": "物",
    "翶": "敖",
    "騴": "燕",
    "葓": "红",
    "扄": "赏",
    "纼": "镇",
    "隮": "机",
    "榽": "西",
    "刟": "雕",
    "嶍": "习",
    "櫔": "利",
    "鼚": "昌",
    "鎟": "桑",
    "匫": "呼",
    "韱": "先",
    "翤": "赤",
    "髨": "昆",
    "稨": "扁",
    "浶": "劳",
    "癊": "印",
    "幮": "除",
    "繨": "哒",
    "靊": "风",
    "肦": "坟",
    "蟃": "万",
    "跇": "意",
    "悿": "舔",
    "墥": "懂",
    "疿": "费",
    "欛": "爸",
    "馾": "蛋",
    "轚": "极",
    "鄽": "缠",
    "藈": "奎",
    "睉": "搓",
    "鷀": "瓷",
    "庎": "借",
    "糪": "薄",
    "鯫": "邹",
    "裦": "否",
    "蕦": "需",
    "蝃": "地",
    "屫": "觉",
    "窹": "物",
    "恱": "月",
    "碆": "波",
    "宖": "红",
    "貾": "持",
    "匟": "抗",
    "譹": "豪",
    "鈨": "原",
    "鋞": "行",
    "魐": "干",
    "鷋": "图",
    "趹": "觉",
    "膔": "路",
    "匵": "毒",
    "痐": "回",
    "硨": "车",
    "娂": "红",
    "塪": "砍",
    "縰": "洗",
    "粴": "李",
    "冟": "是",
    "鏏": "位",
    "誳": "区",
    "鶊": "耕",
    "鵭": "琴",
    "浝": "忙",
    "愡": "总",
    "錣": "缀",
    "閐": "散",
    "孲": "呀",
    "簘": "消",
    "倃": "就",
    "磣": "尘",
    "弮": "圈",
    "摾": "降",
    "炃": "坟",
    "曣": "燕",
    "彠": "约",
    "擖": "咖",
    "藚": "续",
    "怓": "脑",
    "豏": "现",
    "赨": "同",
    "尮": "堕",
    "熑": "连",
    "覞": "要",
    "顇": "翠",
    "釶": "诗",
    "劮": "意",
    "鶁": "精",
    "鞗": "条",
    "籞": "玉",
    "暡": "瓮",
    "烄": "角",
    "跿": "图",
    "籎": "宜",
    "釓": "求",
    "堄": "逆",
    "靎": "贺",
    "餖": "豆",
    "圲": "前",
    "髐": "消",
    "虈": "消",
    "薭": "败",
    "璙": "聊",
    "辴": "铲",
    "毾": "踏",
    "漵": "续",
    "矆": "或",
    "轊": "位",
    "敟": "点",
    "裖": "枕",
    "熉": "云",
    "呞": "诗",
    "鏰": "甭",
    "婸": "荡",
    "忁": "报",
    "荴": "夫",
    "鰸": "区",
    "瞝": "吃",
    "蚷": "巨",
    "鸤": "诗",
    "糘": "家",
    "萀": "虎",
    "薼": "陈",
    "揯": "根",
    "椉": "成",
    "艢": "强",
    "凚": "进",
    "搃": "总",
    "炇": "铺",
    "磼": "杂",
    "癗": "垒",
    "岟": "养",
    "矐": "或",
    "譂": "铲",
    "軧": "底",
    "倹": "减",
    "涱": "帐",
    "醝": "搓",
    "姭": "现",
    "焋": "壮",
    "褷": "诗",
    "怲": "饼",
    "顟": "劳",
    "酨": "在",
    "鈟": "掉",
    "抾": "区",
    "捦": "琴",
    "餺": "博",
    "腁": "偏",
    "篳": "必",
    "乴": "学",
    "尲": "干",
    "弴": "雕",
    "柨": "不",
    "眓": "或",
    "繗": "林",
    "鬩": "系",
    "欇": "设",
    "蜰": "肥",
    "豣": "间",
    "騬": "成",
    "砶": "破",
    "醼": "燕",
    "毄": "机",
    "裲": "两",
    "蠠": "敏",
    "鄎": "西",
    "鵸": "其",
    "寣": "呼",
    "鑤": "报",
    "戼": "帽",
    "蛅": "占",
    "攩": "挡",
    "礥": "闲",
    "黂": "坟",
    "篕": "和",
    "杤": "万",
    "璌": "银",
    "媍": "父",
    "僋": "探",
    "岴": "区",
    "薥": "鼠",
    "麙": "闲",
    "鷮": "教",
    "埦": "碗",
    "湱": "或",
    "啳": "全",
    "騀": "饿",
    "禶": "赞",
    "鐈": "桥",
    "艫": "芦",
    "扝": "哭",
    "椨": "辅",
    "摤": "闯",
    "抸": "家",
    "溕": "盟",
    "遻": "恶",
    "蔪": "见",
    "皼": "古",
    "俖": "培",
    "禞": "告",
    "恄": "系",
    "蔱": "沙",
    "鞝": "上",
    "爈": "绿",
    "遱": "楼",
    "鞛": "崩",
    "疓": "奶",
    "篬": "枪",
    "櫁": "密",
    "鼛": "高",
    "腅": "蛋",
    "鶢": "原",
    "憻": "坦",
    "熎": "要",
    "萾": "营",
    "昗": "责",
    "饡": "赞",
    "蔤": "密",
    "諣": "话",
    "邷": "瓦",
    "鎇": "梅",
    "癚": "蛋",
    "衃": "培",
    "俲": "笑",
    "菣": "亲",
    "忨": "万",
    "駰": "因",
    "岉": "物",
    "窚": "成",
    "槥": "会",
    "僨": "愤",
    "猦": "风",
    "栜": "色",
    "萲": "宣",
    "孂": "角",
    "趰": "耳",
    "曋": "审",
    "酁": "缠",
    "霒": "因",
    "埣": "岁",
    "跁": "爸",
    "歕": "喷",
    "伂": "配",
    "遦": "灌",
    "馧": "晕",
    "刢": "零",
    "扗": "在",
    "鄟": "专",
    "鲝": "眨",
    "疜": "下",
    "鸙": "月",
    "勓": "开",
    "朖": "浪",
    "譛": "怎",
    "鶱": "先",
    "騯": "朋",
    "枽": "夜",
    "鐂": "刘",
    "鰜": "欠",
    "孁": "零",
    "鄝": "了",
    "鼅": "知",
    "耫": "炸",
    "閷": "筛",
    "嵠": "西",
    "弰": "烧",
    "鷕": "咬",
    "囒": "蓝",
    "罜": "主",
    "磩": "气",
    "瑘": "牙",
    "鳁": "温",
    "鱴": "灭",
    "鰀": "换",
    "摌": "铲",
    "瓪": "板",
    "睔": "滚",
    "訜": "分",
    "恲": "砰",
    "閜": "下",
    "瞲": "续",
    "牔": "博",
    "徣": "借",
    "岎": "坟",
    "騟": "鱼",
    "紽": "驮",
    "臡": "尼",
    "菮": "耕",
    "岪": "福",
    "繺": "筛",
    "趧": "提",
    "揢": "咳",
    "鯘": "内",
    "黇": "天",
    "乽": "者",
    "糽": "整",
    "屖": "西",
    "輏": "由",
    "輇": "全",
    "憽": "松",
    "寱": "意",
    "稸": "续",
    "檥": "以",
    "呹": "意",
    "亃": "吝",
    "雸": "岸",
    "鮆": "此",
    "渵": "毛",
    "媇": "亲",
    "駦": "腾",
    "睴": "滚",
    "趎": "除",
    "坙": "精",
    "郠": "梗",
    "魦": "沙",
    "簎": "册",
    "讈": "利",
    "郒": "狼",
    "犞": "桥",
    "鋎": "汉",
    "躠": "洒",
    "鼸": "现",
    "蘁": "物",
    "迃": "迂",
    "盷": "田",
    "霃": "陈",
    "齫": "允",
    "榢": "驾",
    "磍": "侠",
    "鼊": "必",
    "翈": "侠",
    "轙": "以",
    "蜭": "汉",
    "怇": "巨",
    "鉌": "和",
    "幭": "灭",
    "鯇": "换",
    "藘": "驴",
    "焆": "捐",
    "悂": "批",
    "熓": "五",
    "鷳": "闲",
    "薐": "棱",
    "萶": "春",
    "霵": "极",
    "懻": "记",
    "翷": "林",
    "濣": "卧",
    "溗": "成",
    "韼": "朋",
    "蕄": "盟",
    "乮": "帽",
    "鱖": "贵",
    "撀": "够",
    "鰅": "鱼",
    "峲": "李",
    "峑": "圈",
    "貃": "墨",
    "靻": "组",
    "峺": "梗",
    "螥": "仓",
    "鰊": "练",
    "妵": "偷",
    "侲": "镇",
    "穮": "标",
    "鰟": "房",
    "涹": "窝",
    "禭": "岁",
    "傰": "崩",
    "緧": "秋",
    "倇": "碗",
    "嵀": "助",
    "奦": "物",
    "槏": "浅",
    "圦": "快",
    "瀔": "古",
    "肻": "肯",
    "肬": "由",
    "嵍": "物",
    "獼": "迷",
    "牑": "编",
    "戫": "玉",
    "葐": "盆",
    "鶺": "极",
    "鏬": "下",
    "岯": "皮",
    "揓": "是",
    "揘": "庸",
    "坸": "够",
    "櫖": "绿",
    "驦": "双",
    "闟": "系",
    "懚": "印",
    "薧": "蒿",
    "崷": "求",
    "懪": "博",
    "瘣": "会",
    "嵻": "康",
    "茙": "容",
    "霠": "因",
    "齹": "刺",
    "僀": "地",
    "鷨": "华",
    "鸗": "龙",
    "軓": "饭",
    "欚": "李",
    "嶟": "尊",
    "燷": "蓝",
    "鮣": "印",
    "閯": "煞",
    "罊": "气",
    "涀": "现",
    "畂": "六",
    "鋫": "离",
    "讕": "蓝",
    "蓕": "贵",
    "蟖": "思",
    "鱰": "鼠",
    "傏": "唐",
    "怐": "巨",
    "釄": "迷",
    "褠": "勾",
    "狜": "苦",
    "璻": "嘴",
    "訅": "求",
    "垐": "瓷",
    "徸": "冲",
    "鏒": "三",
    "譡": "挡",
    "齩": "咬",
    "劶": "口",
    "堮": "恶",
    "肗": "乳",
    "餣": "夜",
    "膒": "欧",
    "睅": "汉",
    "蚽": "皮",
    "歍": "乌",
    "匎": "恶",
    "螜": "胡",
    "癪": "机",
    "坖": "记",
    "勈": "永",
    "稶": "玉",
    "臤": "前",
    "鋊": "玉",
    "劺": "谋",
    "濢": "翠",
    "贌": "葡",
    "靽": "办",
    "矹": "物",
    "篎": "秒",
    "雤": "学",
    "斀": "着",
    "犣": "裂",
    "蔏": "伤",
    "閇": "必",
    "駴": "害",
    "媰": "除",
    "揬": "图",
    "唘": "起",
    "韎": "妹",
    "孉": "全",
    "礕": "批",
    "鶅": "姿",
    "熡": "楼",
    "騢": "侠",
    "礃": "长",
    "艞": "要",
    "蒚": "利",
    "椧": "命",
    "鶝": "福",
    "矤": "审",
    "豠": "除",
    "虆": "雷",
    "蓩": "帽",
    "帩": "巧",
    "嚀": "凝",
    "迿": "训",
    "歫": "巨",
    "鵕": "俊",
    "擉": "绰",
    "傼": "汉",
    "嵸": "宗",
    "梚": "碗",
    "腖": "动",
    "畆": "母",
    "蚠": "坟",
    "溮": "诗",
    "嫅": "接",
    "锧": "至",
    "饏": "蛋",
    "鬕": "骂",
    "熐": "密",
    "骿": "偏",
    "殝": "真",
    "帠": "意",
    "奲": "朵",
    "誂": "挑",
    "葘": "姿",
    "阫": "培",
    "劎": "见",
    "蟔": "墨",
    "梍": "造",
    "攎": "芦",
    "軴": "助",
    "篟": "欠",
    "稵": "姿",
    "鈝": "银",
    "轛": "缀",
    "埉": "家",
    "樴": "直",
    "檧": "松",
    "蛬": "穷",
    "齳": "允",
    "銢": "匹",
    "趲": "赞",
    "鑯": "间",
    "嵢": "仓",
    "撍": "赞",
    "鶟": "图",
    "刞": "去",
    "椳": "微",
    "惵": "叠",
    "竨": "掉",
    "韹": "黄",
    "螤": "中",
    "鸔": "捕",
    "唺": "舔",
    "螌": "班",
    "繥": "西",
    "蝘": "眼",
    "烍": "显",
    "軱": "姑",
    "馣": "安",
    "鳈": "全",
    "髗": "芦",
    "耉": "狗",
    "緶": "变",
    "孇": "双",
    "囏": "间",
    "犑": "局",
    "幯": "节",
    "墹": "见",
    "鄇": "喉",
    "痎": "接",
    "覶": "罗",
    "憹": "脑",
    "嶫": "夜",
    "巏": "全",
    "嘃": "冲",
    "軺": "摇",
    "櫿": "营",
    "嚃": "他",
    "烑": "摇",
    "鮵": "夺",
    "濿": "利",
    "嶑": "向",
    "錺": "方",
    "翍": "批",
    "蘛": "鱼",
    "櫊": "格",
    "橤": "瑞",
    "篒": "诗",
    "鐴": "必",
    "刔": "觉",
    "鶞": "春",
    "炧": "谢",
    "臱": "眠",
    "嶻": "节",
    "肣": "含",
    "蒊": "花",
    "蘾": "坏",
    "蟸": "李",
    "飁": "习",
    "韸": "朋",
    "馛": "博",
    "睧": "昏",
    "擶": "见",
    "憛": "谈",
    "蓻": "姿",
    "鶗": "提",
    "菒": "搞",
    "摦": "话",
    "嶘": "战",
    "桍": "哭",
    "捤": "伟",
    "臲": "聂",
    "觼": "觉",
    "鶔": "柔",
    "蓞": "蛋",
    "耚": "批",
    "慪": "欧",
    "犔": "系",
    "譀": "汉",
    "蠑": "容",
    "觗": "至",
    "沠": "刘",
    "瞕": "帐",
    "蘮": "记",
    "鬸": "六",
    "鈄": "抖",
    "紣": "翠",
    "酑": "鱼",
    "稢": "玉",
    "鼺": "雷",
    "肔": "尺",
    "鎑": "夜",
    "爘": "参",
    "鱏": "寻",
    "蠨": "消",
    "趬": "敲",
    "憵": "批",
    "岾": "汉",
    "躕": "除",
    "帒": "带",
    "灎": "燕",
    "輀": "而",
    "憆": "称",
    "爒": "了",
    "塖": "成",
    "鳠": "互",
    "蛪": "妾",
    "鵔": "俊",
    "嵼": "铲",
    "踀": "处",
    "麜": "利",
    "驆": "必",
    "蜄": "肾",
    "帬": "群",
    "殐": "速",
    "袦": "那",
    "餯": "会",
    "傹": "静",
    "桰": "扩",
    "蛵": "星",
    "椕": "彬",
    "絍": "任",
    "捹": "笨",
    "遌": "恶",
    "噄": "吃",
    "禉": "有",
    "巤": "裂",
    "傽": "张",
    "鋾": "桃",
    "蔈": "标",
    "鐛": "影",
    "瓐": "芦",
    "罦": "福",
    "藶": "利",
    "逜": "物",
    "頺": "推",
    "孄": "懒",
    "箁": "剖",
    "蹎": "颠",
    "黕": "胆",
    "霮": "蛋",
    "錭": "桃",
    "邩": "火",
    "舏": "久",
    "黲": "惨",
    "懴": "颤",
    "欼": "尺",
    "霴": "带",
    "孮": "从",
    "賘": "脏",
    "鳰": "入",
    "灻": "赤",
    "陗": "巧",
    "帡": "平",
    "竲": "层",
    "駖": "零",
    "鵨": "书",
    "愑": "永",
    "玁": "显",
    "嬎": "饭",
    "馞": "博",
    "蓯": "聪",
    "梬": "影",
    "皹": "君",
    "鈚": "批",
    "餦": "张",
    "鯋": "沙",
    "礖": "玉",
    "匓": "就",
    "襸": "赞",
    "絊": "最",
    "澞": "鱼",
    "靗": "称",
    "纅": "要",
    "蔊": "罕",
    "黬": "颜",
    "甼": "挺",
    "蕂": "胜",
    "虸": "子",
    "鶜": "毛",
    "杊": "寻",
    "奝": "雕",
    "恑": "鬼",
    "腡": "罗",
    "罼": "必",
    "塃": "荒",
    "潷": "必",
    "灹": "咋",
    "躊": "愁",
    "癷": "波",
    "蹳": "波",
    "嶊": "嘴",
    "狕": "咬",
    "癅": "刘",
    "墄": "册",
    "縬": "促",
    "轕": "格",
    "誱": "节",
    "鶶": "唐",
    "軗": "书",
    "殨": "会",
    "鹻": "减",
    "辝": "瓷",
    "剭": "乌",
    "鞤": "帮",
    "攭": "利",
    "杔": "拖",
    "櫢": "搜",
    "瀥": "血",
    "罬": "着",
    "譍": "应",
    "涃": "困",
    "暩": "记",
    "俧": "至",
    "嵹": "降",
    "愇": "伟",
    "毨": "显",
    "黸": "芦",
    "郻": "敲",
    "塕": "瓮",
    "輬": "良",
    "鈆": "前",
    "晑": "想",
    "皗": "愁",
    "昁": "被",
    "艗": "意",
    "殌": "觉",
    "憰": "觉",
    "跈": "年",
    "硆": "恶",
    "柖": "勺",
    "鍸": "胡",
    "栕": "真",
    "覥": "舔",
    "炿": "周",
    "皏": "捧",
    "爡": "撤",
    "釃": "筛",
    "萟": "意",
    "梌": "图",
    "槯": "催",
    "鎙": "硕",
    "蟨": "觉",
    "蠷": "取",
    "焧": "聪",
    "轏": "战",
    "塶": "路",
    "譥": "教",
    "粭": "和",
    "賿": "聊",
    "瓆": "至",
    "鬵": "琴",
    "稕": "准",
    "罭": "玉",
    "陭": "意",
    "馯": "汉",
    "婽": "假",
    "嘐": "消",
    "餟": "缀",
    "恖": "思",
    "菚": "战",
    "稄": "训",
    "泏": "竹",
    "汷": "中",
    "襒": "别",
    "鰡": "刘",
    "鋔": "碗",
    "撌": "贵",
    "朡": "宗",
    "颪": "瓜",
    "雐": "呼",
    "傶": "族",
    "梕": "任",
    "毰": "培",
    "襣": "必",
    "閍": "崩",
    "騈": "偏",
    "釱": "地",
    "斍": "觉",
    "孎": "竹",
    "碄": "林",
    "檌": "最",
    "嬒": "会",
    "圛": "意",
    "歗": "笑",
    "燫": "连",
    "搎": "孙",
    "扅": "宜",
    "阥": "因",
    "鳂": "微",
    "褺": "跌",
    "檉": "称",
    "瞔": "则",
    "隬": "你",
    "騺": "至",
    "涍": "笑",
    "輄": "光",
    "貑": "家",
    "掝": "或",
    "傓": "善",
    "櫮": "恶",
    "麢": "零",
    "酄": "欢",
    "飶": "必",
    "鎎": "开",
    "峐": "该",
    "盀": "起",
    "廽": "回",
    "鞹": "扩",
    "鷏": "田",
    "驓": "层",
    "禲": "利",
    "蘽": "垒",
    "暥": "燕",
    "衋": "系",
    "鏶": "极",
    "翑": "取",
    "蚛": "重",
    "髸": "工",
    "蘴": "风",
    "蓈": "狼",
    "湈": "梅",
    "怈": "意",
    "鋺": "远",
    "鱐": "速",
    "痥": "夺",
    "譫": "占",
    "櫯": "苏",
    "籜": "拓",
    "庬": "忙",
    "婜": "前",
    "擆": "着",
    "誻": "踏",
    "癑": "弄",
    "骲": "报",
    "茿": "竹",
    "曯": "竹",
    "憘": "洗",
    "稓": "做",
    "媘": "接",
    "邜": "西",
    "氄": "容",
    "蟘": "特",
    "凒": "埃",
    "跀": "月",
    "鮚": "节",
    "轀": "温",
    "勴": "绿",
    "榪": "骂",
    "舥": "趴",
    "蛷": "求",
    "繣": "话",
    "膎": "鞋",
    "骫": "伟",
    "憪": "闲",
    "釽": "屁",
    "颒": "会",
    "窱": "挑",
    "翭": "喉",
    "誗": "缠",
    "灅": "垒",
    "翗": "咳",
    "矋": "垒",
    "箽": "懂",
    "鯟": "东",
    "楑": "奎",
    "墆": "至",
    "賐": "训",
    "蹝": "洗",
    "溣": "论",
    "嚔": "替",
    "襗": "则",
    "蹷": "觉",
    "翿": "到",
    "竩": "意",
    "諽": "格",
    "茦": "次",
    "雿": "掉",
    "瞘": "口",
    "孹": "薄",
    "熌": "闪",
    "剶": "穿",
    "硩": "撤",
    "瀖": "或",
    "妰": "着",
    "濔": "米",
    "芕": "虽",
    "蕍": "鱼",
    "蝹": "晕",
    "裶": "飞",
    "皍": "极",
    "炈": "意",
    "蛢": "平",
    "譄": "增",
    "羵": "坟",
    "穙": "葡",
    "齍": "姿",
    "椩": "耕",
    "軕": "山",
    "杚": "概",
    "匘": "脑",
    "鈛": "锅",
    "髝": "劳",
    "啢": "两",
    "绬": "应",
    "蟅": "这",
    "綨": "其",
    "襼": "意",
    "睭": "肘",
    "浺": "冲",
    "鼮": "停",
    "鏓": "总",
    "鋉": "速",
    "竔": "生",
    "賕": "求",
    "夰": "搞",
    "磢": "闯",
    "菃": "取",
    "轁": "掏",
    "逤": "所",
    "鳒": "间",
    "塸": "欧",
    "棥": "烦",
    "撟": "角",
    "碤": "应",
    "豙": "意",
    "耾": "红",
    "趞": "确",
    "甊": "楼",
    "鎈": "锁",
    "浾": "称",
    "鮜": "后",
    "皳": "求",
    "鱳": "利",
    "咞": "现",
    "轥": "吝",
    "懝": "爱",
    "譸": "周",
    "垼": "意",
    "磽": "敲",
    "鮕": "姑",
    "鳱": "干",
    "訔": "银",
    "鳷": "知",
    "桳": "笨",
    "缿": "向",
    "騽": "习",
    "孡": "胎",
    "齵": "偶",
    "硴": "花",
    "茥": "归",
    "焲": "意",
    "蒷": "云",
    "禷": "泪",
    "嶐": "龙",
    "阦": "羊",
    "鶌": "觉",
    "嬔": "父",
    "灔": "燕",
    "慩": "连",
    "捪": "民",
    "躼": "烙",
    "贐": "进",
    "疛": "肘",
    "璤": "会",
    "艓": "叠",
    "僙": "光",
    "獛": "葡",
    "伳": "谢",
    "哅": "胸",
    "蚏": "月",
    "鼝": "冤",
    "蕧": "父",
    "蚒": "同",
    "蘎": "记",
    "讆": "位",
    "譼": "见",
    "鄏": "乳",
    "籆": "月",
    "鄛": "朝",
    "齻": "颠",
    "滣": "纯",
    "雧": "极",
    "雴": "赤",
    "臗": "宽",
    "髛": "烤",
    "襐": "向",
    "螪": "伤",
    "鵧": "皮",
    "葝": "情",
    "篿": "团",
    "槒": "续",
    "鼥": "拔",
    "斖": "伟",
    "骳": "被",
    "讅": "审",
    "蘦": "零",
    "鉹": "尺",
    "袟": "至",
    "繴": "必",
    "輼": "温",
    "恞": "宜",
    "槴": "互",
    "厇": "哲",
    "炦": "拔",
    "媩": "胡",
    "媫": "节",
    "礯": "应",
    "頱": "罗",
    "栛": "利",
    "褄": "七",
    "飗": "刘",
    "陖": "俊",
    "麠": "精",
    "毝": "采",
    "踋": "角",
    "牃": "叠",
    "稇": "捆",
    "觙": "极",
    "鹝": "意",
    "豤": "肯",
    "邟": "抗",
    "鋑": "窜",
    "蘃": "瑞",
    "鏴": "路",
    "襰": "赖",
    "贘": "赏",
    "溩": "物",
    "蟉": "刘",
    "畭": "鱼",
    "庻": "树",
    "藬": "推",
    "鶿": "瓷",
    "庂": "责",
    "庉": "顿",
    "谽": "憨",
    "嬚": "脸",
    "嬼": "柳",
    "髜": "巧",
    "觺": "宜",
    "閊": "山",
    "蟤": "专",
    "蕆": "铲",
    "齥": "谢",
    "觷": "学",
    "豶": "坟",
    "螱": "位",
    "箚": "炸",
    "繠": "瑞",
    "憗": "印",
    "蜖": "回",
    "雽": "互",
    "糆": "面",
    "鑡": "绰",
    "躒": "利",
    "硱": "捆",
    "鬳": "燕",
    "遚": "臭",
    "錟": "谈",
    "銄": "想",
    "岓": "其",
    "徺": "角",
    "歅": "因",
    "椬": "宜",
    "浨": "懒",
    "梈": "砰",
    "旤": "或",
    "躮": "分",
    "庮": "有",
    "鵖": "逼",
    "糚": "装",
    "蚎": "月",
    "庩": "图",
    "鈬": "夺",
    "鬖": "三",
    "旜": "占",
    "柫": "福",
    "峹": "图",
    "璷": "芦",
    "軲": "姑",
    "陫": "费",
    "轈": "朝",
    "郙": "辅",
    "愗": "帽",
    "撊": "现",
    "駧": "动",
    "杒": "任",
    "揤": "极",
    "藛": "写",
    "鶮": "贺",
    "魾": "批",
    "閌": "抗",
    "荲": "离",
    "嗁": "提",
    "蟧": "劳",
    "齸": "意",
    "皫": "漂",
    "躳": "工",
    "楏": "奎",
    "鑃": "掉",
    "鼦": "雕",
    "筡": "图",
    "輆": "凯",
    "伵": "续",
    "葕": "燕",
    "躘": "龙",
    "赥": "西",
    "撡": "操",
    "庱": "成",
    "曻": "生",
    "袹": "博",
    "潀": "从",
    "靪": "丁",
    "旘": "至",
    "癠": "记",
    "癏": "关",
    "篫": "助",
    "鄊": "相",
    "愘": "恰",
    "莌": "拖",
    "綤": "绍",
    "撽": "巧",
    "醾": "迷",
    "焏": "极",
    "齇": "扎",
    "鶫": "东",
    "鸒": "玉",
    "橂": "电",
    "娽": "路",
    "狵": "忙",
    "椔": "姿",
    "欃": "缠",
    "耣": "伦",
    "暷": "传",
    "跙": "巨",
    "隌": "俺",
    "諔": "处",
    "勏": "不",
    "鴾": "谋",
    "齽": "进",
    "挔": "旅",
    "蝅": "残",
    "鋍": "博",
    "虶": "迂",
    "盚": "求",
    "頏": "行",
    "鋥": "赠",
    "庲": "来",
    "鰠": "骚",
    "葨": "微",
    "躢": "踏",
    "鋖": "思",
    "翨": "赤",
    "颮": "标",
    "曫": "鸾",
    "嬵": "眠",
    "槷": "聂",
    "憼": "井",
    "淁": "妾",
    "蓗": "总",
    "鮥": "落",
    "驔": "电",
    "闙": "起",
    "婛": "精",
    "矕": "满",
    "衈": "二",
    "檭": "银",
    "榝": "沙",
    "筤": "狼",
    "韲": "机",
    "鯣": "意",
    "恎": "叠",
    "櫍": "至",
    "豧": "夫",
    "陙": "纯",
    "谹": "红",
    "鳾": "诗",
    "磭": "绰",
    "黭": "眼",
    "雺": "物",
    "廏": "就",
    "邎": "摇",
    "秵": "因",
    "廥": "快",
    "氉": "懆",
    "鑜": "赏",
    "郬": "青",
    "敐": "陈",
    "幬": "愁",
    "劆": "连",
    "嵪": "敲",
    "嬯": "台",
    "雈": "环",
    "萒": "眼",
    "輁": "拱",
    "嵔": "伟",
    "鵂": "修",
    "諆": "七",
    "秜": "尼",
    "碐": "棱",
    "埾": "巨",
    "馰": "敌",
    "韐": "格",
    "鰙": "弱",
    "肰": "然",
    "謒": "枪",
    "頯": "奎",
    "噖": "银",
    "扊": "眼",
    "餛": "魂",
    "趏": "瓜",
    "鵀": "人",
    "菵": "网",
    "揊": "屁",
    "鈵": "饼",
    "鐆": "岁",
    "跭": "翔",
    "攈": "俊",
    "撏": "闲",
    "諥": "重",
    "宯": "消",
    "酠": "恰",
    "瓹": "捐",
    "攡": "吃",
    "絗": "胡",
    "毧": "容",
    "饛": "盟",
    "漛": "腾",
    "趃": "叠",
    "訄": "求",
    "鴚": "歌",
    "瞨": "葡",
    "涻": "设",
    "矌": "矿",
    "杹": "话",
    "砙": "瓦",
    "碯": "脑",
    "嬂": "直",
    "蝄": "网",
    "郀": "哭",
    "澿": "琴",
    "匧": "妾",
    "嬺": "逆",
    "鬉": "宗",
    "剆": "裸",
    "凗": "催",
    "煔": "闪",
    "橍": "润",
    "攓": "前",
    "髍": "魔",
    "忷": "胸",
    "訿": "子",
    "犈": "全",
    "塣": "挣",
    "噿": "嘴",
    "勂": "告",
    "怷": "树",
    "薕": "连",
    "綐": "对",
    "藭": "穷",
    "麡": "其",
    "鈓": "人",
    "鉪": "地",
    "墇": "帐",
    "韥": "毒",
    "帲": "平",
    "蚟": "王",
    "駇": "文",
    "顉": "亲",
    "鎐": "摇",
    "澢": "当",
    "茾": "前",
    "壏": "现",
    "喢": "煞",
    "匌": "格",
    "喖": "胡",
    "郮": "周",
    "岮": "驮",
    "鏪": "曹",
    "毜": "豪",
    "銂": "周",
    "閛": "砰",
    "齓": "衬",
    "捙": "夜",
    "渪": "如",
    "竌": "处",
    "虴": "哲",
    "鵏": "捕",
    "焭": "穷",
    "蕒": "买",
    "鐊": "羊",
    "礰": "利",
    "釼": "见",
    "摉": "搜",
    "漝": "习",
    "峌": "叠",
    "襱": "龙",
    "燛": "窘",
    "頮": "会",
    "擻": "搜",
    "邥": "审",
    "逪": "错",
    "濚": "营",
    "螡": "文",
    "噡": "占",
    "邖": "山",
    "鑈": "聂",
    "紻": "养",
    "寏": "环",
    "韅": "显",
    "韇": "毒",
    "贙": "炫",
    "饓": "称",
    "鑋": "青",
    "耝": "去",
    "炢": "竹",
    "晐": "该",
    "嗸": "敖",
    "嬐": "先",
    "膗": "揣",
    "稴": "闲",
    "隖": "物",
    "翖": "西",
    "腞": "赚",
    "痬": "意",
    "輲": "传",
    "梤": "坟",
    "駪": "深",
    "翇": "福",
    "臕": "标",
    "傟": "养",
    "嫶": "桥",
    "聀": "直",
    "觘": "吵",
    "馵": "助",
    "璛": "速",
    "灛": "铲",
    "徢": "谢",
    "趶": "裤",
    "螐": "乌",
    "硢": "鱼",
    "魪": "借",
    "鄼": "赞",
    "鮤": "裂",
    "焹": "杠",
    "譵": "对",
    "鋱": "特",
    "鹷": "零",
    "藀": "营",
    "靟": "飞",
    "哰": "劳",
    "廫": "聊",
    "袾": "朱",
    "楐": "借",
    "嬳": "月",
    "琽": "堵",
    "黅": "金",
    "葥": "见",
    "晪": "舔",
    "鑮": "博",
    "瓎": "蜡",
    "嗀": "互",
    "毥": "寻",
    "歒": "替",
    "詾": "胸",
    "夡": "气",
    "誴": "从",
    "埁": "琴",
    "澰": "练",
    "膱": "直",
    "鯚": "记",
    "鋜": "着",
    "尵": "推",
    "砇": "民",
    "揱": "消",
    "峉": "恶",
    "膭": "归",
    "躨": "奎",
    "宲": "宝",
    "蓾": "鲁",
    "瀢": "伟",
    "腉": "奶",
    "羒": "坟",
    "嵱": "永",
    "鑐": "需",
    "襎": "烦",
    "衉": "咖",
    "鳲": "诗",
    "抮": "枕",
    "椲": "伟",
    "踷": "眨",
    "懥": "至",
    "黗": "吞",
    "墧": "确",
    "肹": "西",
    "湂": "恶",
    "嘝": "胡",
    "雗": "汉",
    "蛌": "古",
    "帣": "卷",
    "勡": "票",
    "贕": "毒",
    "蒘": "如",
    "侺": "肾",
    "嬕": "是",
    "頉": "宜",
    "抙": "剖",
    "罤": "提",
    "悹": "灌",
    "鎋": "侠",
    "郶": "不",
    "蛜": "一",
    "耯": "或",
    "瀂": "鲁",
    "蘕": "朋",
    "駥": "容",
    "麧": "和",
    "貗": "巨",
    "迧": "陈",
    "鑝": "朋",
    "榵": "容",
    "蜹": "瑞",
    "瀪": "烦",
    "諻": "黄",
    "鮇": "位",
    "魬": "板",
    "苵": "叠",
    "鮍": "批",
    "墛": "位",
    "橭": "姑",
    "輵": "格",
    "雂": "琴",
    "藵": "宝",
    "錎": "现",
    "攊": "利",
    "馤": "爱",
    "勜": "瓮",
    "蔨": "倦",
    "峷": "深",
    "茒": "原",
    "嬻": "毒",
    "癉": "单",
    "艪": "鲁",
    "匉": "砰",
    "竆": "穷",
    "魛": "刀",
    "錃": "批",
    "齀": "物",
    "艵": "平",
    "瓸": "摆",
    "蕕": "由",
    "捳": "月",
    "佭": "翔",
    "燱": "意",
    "槉": "极",
    "櫾": "由",
    "傠": "罚",
    "扷": "奥",
    "蔙": "炫",
    "梥": "松",
    "螖": "华",
    "楾": "全",
    "痭": "崩",
    "摕": "地",
    "揋": "微",
    "焵": "杠",
    "蠽": "节",
    "谺": "虾",
    "鷾": "意",
    "贉": "蛋",
    "攳": "寻",
    "鏧": "龙",
    "覍": "变",
    "縍": "帮",
    "遳": "搓",
    "葾": "冤",
    "秚": "办",
    "楈": "需",
    "邼": "框",
    "傇": "容",
    "鰚": "宣",
    "蚾": "皮",
    "黤": "眼",
    "梞": "记",
    "懎": "色",
    "鉊": "招",
    "髼": "朋",
    "湒": "极",
    "翪": "宗",
    "妐": "中",
    "簛": "筛",
    "瀳": "见",
    "裷": "冤",
    "頀": "互",
    "糃": "唐",
    "娵": "居",
    "飉": "聊",
    "褯": "借",
    "鮔": "巨",
    "嶯": "极",
    "嵳": "搓",
    "嶀": "突",
    "枤": "地",
    "蘲": "雷",
    "潿": "维",
    "棞": "俊",
    "伜": "翠",
    "虊": "鸾",
    "桗": "堕",
    "曑": "深",
    "覙": "罗",
    "髱": "报",
    "鞖": "虽",
    "銡": "极",
    "氃": "同",
    "隟": "系",
    "隲": "至",
    "螒": "汉",
    "糩": "快",
    "鬤": "嚷",
    "怸": "西",
    "黀": "邹",
    "樄": "陈",
    "妅": "红",
    "蘈": "推",
    "泎": "则",
    "嫴": "姑",
    "鈠": "意",
    "秲": "至",
    "蘐": "宣",
    "貦": "完",
    "沊": "蛋",
    "鄁": "被",
    "閳": "铲",
    "瓭": "胆",
    "樖": "科",
    "軥": "取",
    "邭": "巨",
    "斴": "林",
    "鯺": "朱",
    "萞": "必",
    "嘽": "铲",
    "龓": "垄",
    "鷅": "利",
    "惽": "敏",
    "僁": "谢",
    "溔": "咬",
    "鲓": "烤",
    "鯕": "其",
    "翞": "将",
    "糔": "修",
    "躷": "矮",
    "墔": "催",
    "瓝": "博",
    "轣": "利",
    "橜": "觉",
    "駍": "培",
    "趩": "赤",
    "瘂": "哑",
    "詉": "脑",
    "筗": "重",
    "燩": "确",
    "愳": "巨",
    "悀": "永",
    "縖": "侠",
    "腝": "尼",
    "臅": "处",
    "馢": "间",
    "欮": "觉",
    "趀": "刺",
    "炠": "侠",
    "鼤": "文",
    "駬": "耳",
    "腣": "地",
    "鰿": "记",
    "齘": "谢",
    "崻": "至",
    "銁": "君",
    "堖": "脑",
    "嫷": "妥",
    "鮧": "体",
    "遪": "擦",
    "邌": "离",
    "桞": "柳",
    "縎": "古",
    "翉": "本",
    "靮": "敌",
    "皣": "夜",
    "敆": "和",
    "嵽": "叠",
    "聇": "睁",
    "猉": "其",
    "麭": "炮",
    "鄈": "奎",
    "轇": "教",
    "衇": "卖",
    "魳": "匝",
    "螔": "宜",
    "髊": "刺",
    "臰": "臭",
    "卶": "尺",
    "譠": "谈",
    "詥": "和",
    "籗": "着",
    "崨": "节",
    "烗": "开",
    "慺": "楼",
    "嶚": "聊",
    "恗": "呼",
    "睳": "灰",
    "蹃": "诺",
    "啔": "起",
    "轒": "坟",
    "鶂": "意",
    "愞": "诺",
    "埐": "金",
    "摿": "摇",
    "萕": "其",
    "菷": "肘",
    "棇": "聪",
    "搑": "容",
    "楎": "灰",
    "糛": "唐",
    "搝": "求",
    "蝪": "汤",
    "坕": "精",
    "揁": "睁",
    "苬": "修",
    "匑": "工",
    "胦": "养",
    "觰": "扎",
    "媅": "单",
    "嵭": "崩",
    "鱢": "骚",
    "蹸": "吝",
    "鮯": "格",
    "趭": "教",
    "踙": "聂",
    "鐋": "汤",
    "矲": "爸",
    "燘": "美",
    "蠳": "应",
    "騸": "善",
    "箿": "极",
    "霋": "七",
    "昲": "费",
    "鑟": "毒",
    "獱": "编",
    "錍": "批",
    "瓵": "宜",
    "敪": "多",
    "鮝": "想",
    "婙": "静",
    "珬": "续",
    "靾": "意",
    "巐": "吵",
    "蝒": "眠",
    "槆": "春",
    "澲": "夜",
    "鈌": "觉",
    "騗": "片",
    "刏": "机",
    "粣": "册",
    "鞷": "格",
    "郋": "习",
    "斒": "班",
    "瀱": "记",
    "鵚": "突",
    "麘": "相",
    "尳": "古",
    "漅": "朝",
    "駏": "巨",
    "搚": "拉",
    "鷈": "踢",
    "躗": "位",
    "栚": "镇",
    "沞": "匝",
    "祣": "旅",
    "鞙": "炫",
    "訩": "胸",
    "匶": "就",
    "氁": "木",
    "醥": "漂",
    "艝": "雪",
    "蜦": "伦",
    "紎": "姿",
    "狣": "照",
    "犡": "利",
    "撉": "蹲",
    "鵛": "精",
    "睵": "灾",
    "捾": "卧",
    "獖": "笨",
    "蓘": "滚",
    "烲": "谢",
    "峼": "告",
    "秢": "零",
    "蝊": "定",
    "鎪": "搜",
    "鹶": "金",
    "埩": "睁",
    "鶘": "胡",
    "痯": "管",
    "欈": "维",
    "狧": "踏",
    "郍": "挪",
    "繿": "蓝",
    "骔": "宗",
    "猅": "排",
    "孷": "离",
    "孠": "四",
    "龡": "吹",
    "蘵": "知",
    "蒣": "徐",
    "攟": "俊",
    "禃": "直",
    "蝫": "朱",
    "瓺": "长",
    "蠾": "竹",
    "龣": "觉",
    "釸": "西",
    "剈": "冤",
    "躻": "空",
    "麎": "陈",
    "摍": "缩",
    "葞": "米",
    "鄬": "维",
    "呝": "恶",
    "嵮": "颠",
    "攚": "瓮",
    "閚": "占",
    "搇": "亲",
    "鞊": "节",
    "齖": "牙",
    "攁": "养",
    "酭": "右",
    "鷑": "极",
    "轐": "哺",
    "錁": "果",
    "紴": "波",
    "豾": "批",
    "飌": "风",
    "饊": "三",
    "墶": "哒",
    "宺": "谎",
    "蜐": "节",
    "橶": "极",
    "罆": "灌",
    "倊": "宗",
    "壣": "林",
    "萖": "碗",
    "鮩": "病",
    "縅": "微",
    "腇": "内",
    "蒑": "因",
    "靆": "带",
    "疩": "翠",
    "嵰": "浅",
    "穯": "色",
    "稛": "捆",
    "揨": "陈",
    "宩": "使",
    "禯": "农",
    "頾": "姿",
    "螕": "逼",
    "鶹": "刘",
    "昿": "矿",
    "畘": "南",
    "稘": "机",
    "狫": "老",
    "騊": "桃",
    "溄": "逢",
    "鱹": "灌",
    "憴": "生",
    "縪": "必",
    "鼵": "突",
    "鵥": "判",
    "詴": "微",
    "晎": "红",
    "鵃": "周",
    "魶": "那",
    "鍷": "奎",
    "駺": "狼",
    "覻": "区",
    "餪": "暖",
    "腬": "柔",
    "軮": "养",
    "麲": "现",
    "蛼": "车",
    "鎄": "哀",
    "孈": "会",
    "搄": "根",
    "贚": "弄",
    "毇": "毁",
    "怟": "地",
    "隢": "扰",
    "謣": "鱼",
    "軝": "其",
    "腲": "伟",
    "砓": "哲",
    "崅": "确",
    "廭": "记",
    "溛": "挖",
    "膼": "抓",
    "寭": "会",
    "瑻": "昆",
    "欳": "溃",
    "煛": "窘",
    "輣": "朋",
    "毷": "帽",
    "氋": "盟",
    "硳": "赤",
    "舿": "夸",
    "灐": "营",
    "飥": "拖",
    "髚": "巧",
    "瞚": "顺",
    "炍": "判",
    "輱": "闲",
    "翝": "红",
    "縃": "需",
    "訋": "掉",
    "肵": "其",
    "蜠": "俊",
    "銴": "是",
    "埬": "东",
    "夦": "尘",
    "顐": "问",
    "鈲": "姑",
    "鰦": "姿",
    "嘊": "埃",
    "驈": "玉",
    "訤": "肖",
    "媨": "促",
    "斚": "假",
    "耟": "巨",
    "艍": "居",
    "霩": "扩",
    "轞": "见",
    "娦": "贫",
    "慏": "命",
    "抁": "眼",
    "梊": "地",
    "胅": "叠",
    "桘": "缀",
    "箵": "星",
    "剓": "离",
    "鋙": "雨",
    "洆": "成",
    "觓": "求",
    "胋": "田",
    "蚉": "文",
    "堲": "瓷",
    "鴬": "应",
    "銰": "哀",
    "媀": "玉",
    "猑": "昆",
    "痸": "赤",
    "駼": "图",
    "槝": "导",
    "訹": "续",
    "椝": "归",
    "絼": "镇",
    "嬏": "翻",
    "髿": "缩",
    "疅": "将",
    "捓": "爷",
    "蓛": "册",
    "鄵": "操",
    "鏎": "必",
    "輎": "烧",
    "鑘": "雷",
    "鋣": "爷",
    "懏": "俊",
    "蘟": "引",
    "俕": "散",
    "鄥": "敲",
    "飋": "色",
    "秿": "父",
    "胇": "费",
    "瞖": "意",
    "擏": "情",
    "亄": "意",
    "穳": "攒",
    "硡": "轰",
    "襋": "极",
    "檃": "引",
    "飦": "占",
    "爥": "竹",
    "欯": "系",
    "忰": "翠",
    "枾": "是",
    "鎃": "派",
    "姾": "全",
    "崓": "固",
    "炚": "光",
    "茟": "玉",
    "邒": "停",
    "汿": "续",
    "甇": "应",
    "歋": "夜",
    "闛": "唐",
    "萈": "环",
    "镼": "区",
    "纄": "朋",
    "厏": "眨",
    "蜶": "所",
    "岲": "矿",
    "鱑": "黄",
    "屔": "尼",
    "鎠": "刚",
    "隀": "虫",
    "衸": "借",
    "鳸": "互",
    "昒": "呼",
    "飰": "饭",
    "酀": "燕",
    "蘞": "脸",
    "嬹": "性",
    "趡": "脆",
    "釳": "系",
    "蘏": "窘",
    "蠫": "离",
    "隓": "灰",
    "牏": "鱼",
    "澅": "话",
    "駚": "养",
    "峛": "李",
    "癛": "吝",
    "巸": "宜",
    "欫": "气",
    "櫦": "庆",
    "鼞": "汤",
    "餁": "任",
    "葪": "记",
    "鰵": "敏",
    "恜": "赤",
    "秓": "知",
    "藑": "穷",
    "澵": "真",
    "歱": "种",
    "褹": "意",
    "碵": "田",
    "梷": "静",
    "撋": "软",
    "薚": "汤",
    "麉": "间",
    "攌": "缓",
    "螮": "地",
    "鵒": "玉",
    "蜬": "含",
    "槹": "高",
    "勎": "路",
    "釮": "其",
    "飳": "偷",
    "翸": "喷",
    "坓": "井",
    "躀": "灌",
    "諯": "专",
    "弞": "审",
    "瞮": "撤",
    "矨": "影",
    "蟚": "朋",
    "漡": "伤",
    "躤": "极",
    "濸": "仓",
    "瘱": "意",
    "竰": "离",
    "鷶": "买",
    "虲": "虾",
    "鎁": "爷",
    "挘": "裂",
    "舓": "是",
    "覝": "连",
    "傄": "虾",
    "拞": "底",
    "輰": "羊",
    "醿": "迷",
    "躑": "直",
    "礗": "拼",
    "爗": "夜",
    "躙": "吝",
    "噋": "吞",
    "墢": "拔",
    "鵫": "着",
    "眱": "地",
    "樎": "速",
    "茊": "姿",
    "晘": "汉",
    "諀": "匹",
    "鵴": "局",
    "艆": "狼",
    "赾": "寝",
    "舼": "穷",
    "梙": "换",
    "煂": "贺",
    "釲": "四",
    "嗋": "鞋",
    "熭": "位",
    "纝": "雷",
    "麱": "夫",
    "錗": "内",
    "闎": "全",
    "爋": "熏",
    "躴": "狼",
    "顄": "汉",
    "虇": "犬",
    "蛨": "墨",
    "韰": "谢",
    "濭": "矮",
    "僘": "场",
    "鳺": "夫",
    "溒": "原",
    "踑": "其",
    "鵅": "落",
    "餥": "飞",
    "隇": "微",
    "椣": "点",
    "誵": "肖",
    "裮": "昌",
    "晭": "肘",
    "壉": "巨",
    "顜": "讲",
    "梸": "离",
    "稖": "棒",
    "絉": "树",
    "齈": "弄",
    "媡": "练",
    "隿": "意",
    "蟂": "消",
    "諐": "前",
    "軂": "烙",
    "蠬": "龙",
    "篧": "着",
    "覹": "维",
    "惥": "永",
    "鬀": "替",
    "撨": "辅",
    "袉": "驮",
    "僌": "营",
    "誺": "吃",
    "鼼": "要",
    "槶": "贵",
    "韴": "杂",
    "韖": "柔",
    "鼶": "思",
    "刯": "耕",
    "炥": "福",
    "饻": "西",
    "砿": "矿",
    "鮽": "鱼",
    "葋": "取",
    "坥": "区",
    "醟": "用",
    "殱": "间",
    "鯠": "来",
    "妿": "阿",
    "驉": "需",
    "懩": "养",
    "鈗": "允",
    "貕": "西",
    "懛": "呆",
    "贂": "尘",
    "霌": "周",
    "莡": "错",
    "珕": "利",
    "蠝": "垒",
    "奃": "低",
    "朎": "零",
    "蠥": "聂",
    "孯": "前",
    "鼰": "局",
    "餙": "是",
    "嵦": "凯",
    "駽": "宣",
    "郌": "归",
    "瓄": "毒",
    "醔": "求",
    "髶": "容",
    "矵": "气",
    "軩": "带",
    "饳": "堕",
    "輺": "姿",
    "艬": "缠",
    "鷎": "高",
    "陦": "导",
    "珳": "文",
    "曭": "躺",
    "錖": "毒",
    "覐": "觉",
    "讄": "垒",
    "鰴": "灰",
    "峾": "银",
    "螦": "骚",
    "謼": "呼",
    "囐": "杂",
    "峀": "秀",
    "虄": "萨",
    "腒": "居",
    "鑏": "凝",
    "忀": "相",
    "獇": "枪",
    "瓋": "替",
    "殥": "银",
    "惼": "扁",
    "揟": "需",
    "嬦": "愁",
    "靕": "真",
    "雭": "色",
    "饚": "害",
    "匲": "连",
    "劷": "羊",
    "鰮": "温",
    "硻": "坑",
    "踕": "节",
    "瞗": "雕",
    "瀎": "墨",
    "葏": "精",
    "旞": "岁",
    "袸": "见",
    "斊": "其",
    "遉": "真",
    "梎": "凹",
    "渨": "微",
    "潝": "西",
    "庪": "鬼",
    "擜": "恶",
    "銒": "行",
    "憢": "消",
    "闦": "文",
    "翐": "至",
    "檕": "记",
    "騘": "聪",
    "歑": "呼",
    "陒": "鬼",
    "嘳": "溃",
    "艕": "棒",
    "黋": "矿",
    "肞": "插",
    "輚": "战",
    "餕": "俊",
    "鷢": "觉",
    "怬": "系",
    "逘": "以",
    "樭": "机",
    "猆": "飞",
    "螊": "连",
    "竗": "庙",
    "鈱": "民",
    "竧": "静",
    "鍹": "宣",
    "抍": "整",
    "蔋": "敌",
    "婒": "谈",
    "鶎": "尊",
    "犩": "维",
    "艐": "客",
    "焍": "地",
    "藡": "敌",
    "瀗": "现",
    "鑎": "溃",
    "躈": "巧",
    "鱎": "角",
    "詿": "挂",
    "矈": "眠",
    "謲": "灿",
    "砋": "指",
    "朑": "替",
    "誯": "唱",
    "戃": "躺",
    "睮": "鱼",
    "徱": "票",
    "袲": "尺",
    "蝵": "秋",
    "瘚": "觉",
    "嶭": "聂",
    "煰": "造",
    "銔": "批",
    "鯐": "走",
    "噞": "眼",
    "疀": "插",
    "樇": "修",
    "騝": "钱",
    "濐": "主",
    "覼": "罗",
    "璳": "田",
    "絈": "墨",
    "碀": "成",
    "爙": "嚷",
    "眪": "饼",
    "跮": "赤",
    "鉡": "办",
    "癨": "或",
    "厁": "三",
    "鵱": "路",
    "愲": "古",
    "鷥": "思",
    "薟": "先",
    "耓": "听",
    "棛": "玉",
    "峵": "容",
    "匔": "工",
    "烉": "换",
    "蔧": "会",
    "顭": "盟",
    "樢": "尿",
    "僲": "先",
    "魫": "审",
    "蹪": "推",
    "銌": "尊",
    "鏼": "色",
    "璭": "滚",
    "鞉": "桃",
    "鯴": "诗",
    "襇": "减",
    "炄": "纽",
    "暭": "号",
    "膢": "驴",
    "芌": "玉",
    "蹫": "局",
    "驜": "夜",
    "颴": "炫",
    "慻": "倦",
    "鬠": "扩",
    "燰": "微",
    "稒": "固",
    "鵊": "夹",
    "覕": "灭",
    "諹": "羊",
    "窙": "消",
    "瀈": "灰",
    "熮": "柳",
    "羦": "环",
    "譈": "对",
    "痮": "帐",
    "顅": "前",
    "軉": "玉",
    "藫": "谈",
    "斔": "雨",
    "壥": "缠",
    "鍁": "先",
    "歬": "钱",
    "殶": "助",
    "鱡": "贼",
    "粚": "吃",
    "懧": "诺",
    "攑": "前",
    "靍": "贺",
    "趻": "尘",
    "蜔": "电",
    "嗈": "庸",
    "榹": "思",
    "髉": "博",
    "蜌": "必",
    "氌": "鲁",
    "箳": "平",
    "鮅": "必",
    "鮳": "烤",
    "塛": "利",
    "袳": "尺",
    "薣": "古",
    "愅": "格",
    "飵": "做",
    "蠮": "椰",
    "稐": "伦",
    "藲": "欧",
    "橏": "展",
    "踲": "顿",
    "顋": "塞",
    "霟": "红",
    "鶓": "描",
    "擑": "接",
    "瑼": "专",
    "鵗": "西",
    "炂": "中",
    "盶": "远",
    "軁": "楼",
    "敽": "角",
    "駩": "圈",
    "濥": "引",
    "弫": "枕",
    "燆": "敲",
    "磠": "鲁",
    "駵": "刘",
    "郆": "极",
    "蟷": "当",
    "艃": "离",
    "庽": "玉",
    "谾": "轰",
    "趢": "路",
    "蟏": "消",
    "燍": "思",
    "隒": "眼",
    "攕": "先",
    "徟": "周",
    "曍": "号",
    "鮌": "滚",
    "掚": "两",
    "惖": "替",
    "裫": "院",
    "鄸": "盟",
    "劕": "至",
    "懱": "灭",
    "馲": "哲",
    "鴰": "瓜",
    "磶": "系",
    "娎": "谢",
    "憄": "至",
    "騪": "搜",
    "覉": "机",
    "蠩": "朱",
    "狿": "颜",
    "魵": "坟",
    "揧": "蜡",
    "踁": "静",
    "獤": "蹲",
    "魤": "额",
    "鸖": "贺",
    "窤": "关",
    "鰖": "妥",
    "跾": "书",
    "譢": "岁",
    "燽": "愁",
    "緽": "称",
    "騡": "全",
    "榲": "温",
    "蜏": "有",
    "鋿": "长",
    "蠯": "皮",
    "蘙": "意",
    "螲": "至",
    "蜤": "思",
    "掶": "节",
    "鑍": "应",
    "閝": "零",
    "葈": "洗",
    "毭": "豆",
    "袶": "降",
    "戵": "取",
    "螛": "和",
    "鶑": "应",
    "镽": "了",
    "陠": "铺",
    "榁": "使",
    "豴": "敌",
    "馟": "图",
    "綥": "其",
    "圽": "墨",
    "朇": "皮",
    "磇": "批",
    "趗": "促",
    "躖": "断",
    "豂": "聊",
    "蒕": "晕",
    "鶭": "访",
    "焳": "觉",
    "溳": "云",
    "縜": "云",
    "蠇": "利",
    "愌": "换",
    "溤": "马",
    "頢": "扩",
    "溊": "波",
    "溌": "坡",
    "椞": "系",
    "藰": "刘",
    "擨": "爷",
    "墂": "标",
    "醀": "维",
    "踸": "尘",
    "鞇": "因",
    "橯": "烙",
    "鸆": "鱼",
    "噒": "连",
    "俼": "玉",
    "鷧": "意",
    "檅": "会",
    "鑂": "训",
    "羷": "脸",
    "蔜": "敖",
    "梋": "宣",
    "囅": "铲",
    "垘": "福",
    "頨": "雨",
    "幊": "工",
    "逳": "玉",
    "溨": "灾",
    "莻": "共",
    "豅": "龙",
    "韷": "乐",
    "蕔": "包",
    "燤": "太",
    "硵": "鲁",
    "焥": "卧",
    "奞": "训",
    "釛": "八",
    "庰": "病",
    "蠜": "烦",
    "虉": "意",
    "瘬": "帐",
    "竱": "转",
    "镾": "迷",
    "穲": "离",
    "貖": "意",
    "瀊": "盘",
    "螰": "路",
    "瓗": "穷",
    "鴡": "居",
    "礈": "缀",
    "襽": "蓝",
    "筕": "行",
    "齂": "谢",
    "坾": "助",
    "豼": "皮",
    "霶": "旁",
    "桏": "穷",
    "馷": "配",
    "騆": "周",
    "睝": "离",
    "匩": "框",
    "柼": "咬",
    "螴": "陈",
    "褮": "应",
    "鉕": "坡",
    "闣": "荡",
    "軵": "容",
    "爦": "懒",
    "曥": "芦",
    "毠": "家",
    "釥": "巧",
    "瓳": "胡",
    "匥": "烦",
    "嶳": "地",
    "踓": "伟",
    "籢": "连",
    "蛥": "蛇",
    "蘉": "盟",
    "焝": "混",
    "逫": "觉",
    "鐰": "敲",
    "歝": "意",
    "镻": "叠",
    "竳": "灯",
    "麔": "就",
    "槣": "机",
    "歵": "则",
    "襉": "减",
    "矡": "觉",
    "雓": "鱼",
    "覄": "父",
    "赮": "侠",
    "蠗": "着",
    "臄": "觉",
    "廎": "请",
    "搈": "容",
    "嬊": "燕",
    "牄": "枪",
    "嬘": "岁",
    "礢": "养",
    "鸋": "凝",
    "娺": "着",
    "礏": "夜",
    "圸": "山",
    "崫": "哭",
    "贎": "万",
    "煘": "缠",
    "磤": "引",
    "燺": "贺",
    "顀": "垂",
    "釟": "八",
    "矎": "宣",
    "攗": "梅",
    "曊": "费",
    "垑": "尺",
    "鐷": "夜",
    "笽": "敏",
    "哛": "分",
    "鼿": "物",
    "蚭": "尼",
    "箃": "邹",
    "蜽": "两",
    "鑙": "机",
    "趘": "习",
    "癧": "利",
    "諅": "记",
    "耊": "叠",
    "懘": "赤",
    "黊": "话",
    "橁": "春",
    "豓": "燕",
    "跜": "尼",
    "騻": "双",
    "錓": "空",
    "鵣": "赖",
    "縘": "西",
    "榺": "胜",
    "疦": "觉",
    "覒": "帽",
    "襔": "满",
    "廜": "图",
    "擙": "奥",
    "獿": "脑",
    "瓾": "猛",
    "踃": "消",
    "耺": "云",
    "敡": "意",
    "鈯": "图",
    "闧": "他",
    "縙": "容",
    "祽": "最",
    "襺": "减",
    "橠": "挪",
    "餶": "古",
    "樐": "鲁",
    "錈": "卷",
    "庍": "败",
    "鵓": "博",
    "檱": "其",
    "鳭": "雕",
    "諃": "陈",
    "嚪": "蛋",
    "拀": "处",
    "輡": "砍",
    "毤": "拓",
    "陯": "伦",
    "駫": "窘",
    "鴶": "夹",
    "頿": "姿",
    "蠤": "秋",
    "燇": "俊",
    "齃": "恶",
    "菞": "离",
    "櫷": "归",
    "榏": "意",
    "鞽": "桥",
    "甉": "闲",
    "崣": "伟",
    "嘕": "先",
    "憏": "赤",
    "塧": "爱",
    "訰": "准",
    "睶": "春",
    "餄": "夹",
    "贀": "意",
    "騦": "思",
    "庌": "哑",
    "貄": "四",
    "墵": "谈",
    "檘": "平",
    "蛽": "被",
    "襳": "先",
    "嚝": "轰",
    "虝": "虎",
    "謵": "习",
    "塷": "鲁",
    "魧": "行",
    "糑": "诺",
    "翲": "飘",
    "裍": "捆",
    "趥": "秋",
    "蘨": "摇",
    "鬗": "瞒",
    "巀": "节",
    "烥": "巨",
    "櫇": "婆",
    "蘣": "偷",
    "澩": "学",
    "糄": "扁",
    "蔝": "米",
    "鸄": "机",
    "榸": "摘",
    "碖": "伦",
    "燲": "鞋",
    "籊": "替",
    "欆": "双",
    "遀": "随",
    "巕": "聂",
    "鈂": "陈",
    "餝": "是",
    "馪": "拼",
    "駋": "招",
    "鉇": "诗",
    "曎": "意",
    "魿": "零",
    "烅": "续",
    "稁": "搞",
    "弳": "静",
    "烡": "光",
    "蔖": "搓",
    "徿": "弄",
    "蛝": "闲",
    "鋛": "矿",
    "逧": "古",
    "豱": "温",
    "靲": "琴",
    "涄": "平",
    "縔": "双",
    "鼭": "石",
    "廅": "恶",
    "癄": "桥",
    "瓇": "柔",
    "嗂": "摇",
    "鳻": "班",
    "蛿": "汉",
    "珱": "应",
    "蕱": "烧",
    "屒": "枕",
    "蛡": "意",
    "餎": "了",
    "鰝": "号",
    "褿": "曹",
    "詋": "昼",
    "訲": "意",
    "轋": "魂",
    "戂": "迷",
    "霬": "意",
    "崺": "以",
    "麆": "助",
    "蛚": "裂",
    "藱": "会",
    "濗": "密",
    "壗": "进",
    "樚": "路",
    "藃": "消",
    "牫": "歌",
    "艁": "造",
    "覭": "明",
    "撛": "吝",
    "軯": "砰",
    "輫": "排",
    "蚙": "琴",
    "踗": "聂",
    "烢": "撤",
    "曃": "带",
    "鱍": "波",
    "驖": "铁",
    "夞": "外",
    "麍": "刘",
    "艜": "带",
    "崉": "踏",
    "鏿": "称",
    "庺": "松",
    "踜": "愣",
    "饖": "位",
    "蕛": "提",
    "絩": "跳",
    "濍": "松",
    "謻": "宜",
    "鬁": "利",
    "螑": "秀",
    "燢": "学",
    "虦": "战",
    "廀": "搜",
    "嬆": "西",
    "嵟": "堆",
    "竀": "称",
    "靃": "或",
    "瞈": "瓮",
    "蕮": "系",
    "櫅": "机",
    "奟": "崩",
    "嚉": "多",
    "鴑": "如",
    "欔": "觉",
    "烞": "破",
    "蜳": "蹲",
    "幧": "敲",
    "焩": "平",
    "鸃": "宜",
    "軇": "到",
    "擮": "节",
    "鬡": "凝",
    "萗": "册",
    "嚺": "踏",
    "穥": "玉",
    "灇": "从",
    "焨": "奉",
    "蔉": "滚",
    "爏": "利",
    "婎": "灰",
    "壾": "忙",
    "懫": "至",
    "鸉": "羊",
    "遃": "眼",
    "涋": "突",
    "鼨": "中",
    "觾": "燕",
    "鑧": "宽",
    "瞣": "万",
    "鑆": "缀",
    "磖": "啦",
    "訦": "陈",
    "噊": "玉",
    "灁": "冤",
    "懖": "扩",
    "陮": "对",
    "熽": "笑",
    "暽": "林",
    "攂": "泪",
    "浌": "罚",
    "磰": "善",
    "肑": "博",
    "銐": "赤",
    "殈": "续",
    "愹": "永",
    "賹": "意",
    "暬": "谢",
    "覟": "至",
    "鬫": "罕",
    "韗": "运",
    "鶷": "侠",
    "娹": "闲",
    "懬": "矿",
    "蟵": "除",
    "樳": "寻",
    "薞": "孙",
    "螼": "寝",
    "醏": "督",
    "牊": "朝",
    "魲": "芦",
    "鷷": "尊",
    "慂": "永",
    "魺": "和",
    "澷": "慢",
    "衑": "零",
    "駾": "退",
    "肳": "稳",
    "黆": "光",
    "濌": "踏",
    "灖": "米",
    "殬": "度",
    "翓": "鞋",
    "篖": "唐",
    "腪": "运",
    "蹨": "年",
    "懤": "愁",
    "銸": "哲",
    "漽": "提",
    "鈋": "额",
    "觨": "混",
    "醈": "谈",
    "鉠": "养",
    "馶": "知",
    "嶬": "宜",
    "暺": "坦",
    "懢": "蓝",
    "螸": "鱼",
    "跉": "零",
    "輽": "笨",
    "鄓": "夜",
    "譒": "薄",
    "貇": "昆",
    "憌": "穷",
    "娏": "忙",
    "毢": "塞",
    "籑": "赚",
    "艠": "灯",
    "槞": "龙",
    "嶛": "聊",
    "謭": "减",
    "顦": "桥",
    "腛": "卧",
    "鰪": "额",
    "軜": "那",
    "鸍": "迷",
    "灓": "鸾",
    "烵": "着",
    "鄻": "脸",
    "鶧": "应",
    "鶀": "其",
    "鎥": "条",
    "羻": "呛",
    "郹": "局",
    "贒": "闲",
    "艥": "极",
    "檴": "或",
    "鳵": "宝",
    "鏫": "离",
    "諁": "着",
    "憦": "烙",
    "嶩": "脑",
    "嶱": "可",
    "鏙": "催",
    "鉟": "批",
    "軬": "饭",
    "錰": "树",
    "鑬": "见",
    "惐": "玉",
    "欰": "续",
    "熪": "宜",
    "錪": "舔",
    "炨": "谢",
    "夓": "下",
    "瀶": "林",
    "藆": "减",
    "焈": "西",
    "譿": "会",
    "鬝": "前",
    "驧": "局",
    "軈": "应",
    "謶": "着",
    "顂": "赖",
    "濷": "费",
    "嚽": "绰",
    "脋": "鞋",
    "顝": "亏",
    "碊": "间",
    "焁": "西",
    "顩": "眼",
    "滜": "高",
    "燅": "寻",
    "焣": "吵",
    "壧": "颜",
    "肒": "换",
    "嶹": "导",
    "鴯": "而",
    "樈": "情",
    "漐": "直",
    "鉣": "节",
    "鍽": "编",
    "鯑": "西",
    "蜁": "旋",
    "燌": "坟",
    "魞": "八",
    "踾": "福",
    "跒": "恰",
    "唟": "去",
    "櫗": "灭",
    "萙": "枕",
    "跊": "妹",
    "硟": "颤",
    "飸": "掏",
    "賝": "陈",
    "韄": "互",
    "踿": "族",
    "鶋": "居",
    "裗": "刘",
    "愱": "极",
    "銿": "中",
    "嚍": "进",
    "溭": "则",
    "歽": "哲",
    "奊": "鞋",
    "熧": "宗",
    "髵": "而",
    "瀐": "间",
    "貥": "行",
    "銞": "君",
    "烌": "修",
    "鵌": "图",
    "閕": "虾",
    "谻": "极",
    "魱": "胡",
    "磮": "伦",
    "韒": "巧",
    "鉘": "福",
    "錔": "踏",
    "軤": "呼",
    "蘌": "雨",
    "鼧": "驮",
    "榌": "逼",
    "釯": "忙",
    "憠": "觉",
    "蟍": "离",
    "騥": "柔",
    "滰": "降",
    "顨": "训",
    "慸": "地",
    "駎": "昼",
    "焤": "辅",
    "茰": "鱼",
    "螷": "皮",
    "葤": "昼",
    "斪": "取",
    "幒": "中",
    "錷": "嘎",
    "矏": "眠",
    "濝": "其",
    "堫": "宗",
    "濻": "伟",
    "熍": "穷",
    "詸": "迷",
    "藒": "妾",
    "駷": "耸",
    "贆": "标",
    "颬": "虾",
    "嗩": "锁",
    "硛": "意",
    "諓": "见",
    "擳": "至",
    "詄": "叠",
    "瞁": "续",
    "怾": "指",
    "樤": "条",
    "讇": "铲",
    "筙": "来",
    "隭": "而",
    "爑": "觉",
    "鱌": "向",
    "骩": "伟",
    "欁": "农",
    "髰": "替",
    "釫": "华",
    "蓵": "节",
    "鎉": "达",
    "靅": "费",
    "尀": "坡",
    "碅": "君",
    "繱": "聪",
    "鮛": "书",
    "鍲": "民",
    "陱": "居",
    "讔": "引",
    "竐": "处",
    "籚": "芦",
    "褣": "容",
    "瀃": "四",
    "蜸": "浅",
    "謤": "标",
    "踥": "妾",
    "虠": "教",
    "蠞": "节",
    "謮": "则",
    "歶": "鱼",
    "鷪": "应",
    "耛": "宜",
    "鸈": "夜",
    "蘝": "脸",
    "慀": "系",
    "鞃": "红",
    "喡": "维",
    "頧": "堆",
    "覵": "见",
    "撪": "笨",
    "頕": "单",
    "聄": "枕",
    "趇": "系",
    "覛": "密",
    "飿": "堕",
    "磸": "定",
    "躵": "忍",
    "鯦": "就",
    "雵": "养",
    "藣": "杯",
    "聉": "袜",
    "袰": "波",
    "鋵": "突",
    "餩": "恶",
    "謱": "楼",
    "慗": "赤",
    "擛": "夜",
    "毺": "书",
    "鏥": "秀",
    "撎": "意",
    "譇": "扎",
    "橨": "坟",
    "鋓": "搀",
    "魩": "墨",
    "蹞": "魁",
    "頝": "敲",
    "釻": "求",
    "愵": "逆",
    "鴹": "羊",
    "遾": "是",
    "鰞": "乌",
    "釚": "求",
    "獔": "豪",
    "鷞": "双",
    "諑": "着",
    "瀄": "至",
    "軙": "陈",
    "謽": "降",
    "箲": "显",
    "踍": "敲",
    "灍": "觉",
    "螏": "极",
    "鶣": "偏",
    "筂": "持",
    "腗": "皮",
    "錊": "最",
    "燯": "零",
    "猈": "败",
    "顪": "会",
    "箺": "春",
    "坄": "意",
    "焷": "皮",
    "晇": "需",
    "謺": "哲",
    "軐": "信",
    "弲": "宣",
    "艔": "到",
    "熃": "物",
    "藖": "闲",
    "樦": "助",
    "蔐": "敌",
    "穁": "容",
    "駀": "由",
    "鱱": "利",
    "壛": "颜",
    "賟": "舔",
    "檹": "一",
    "鮾": "内",
    "襑": "信",
    "犥": "飘",
    "豍": "逼",
    "錌": "按",
    "錉": "民",
    "矃": "凝",
    "鈘": "以",
    "瞺": "会",
    "秛": "批",
    "鉙": "窄",
    "褬": "桑",
    "筟": "夫",
    "蓔": "咬",
    "瀩": "对",
    "霯": "腾",
    "硹": "松",
    "蔛": "胡",
    "暛": "锁",
    "螧": "其",
    "虥": "战",
    "鷌": "马",
    "詽": "颜",
    "檈": "旋",
    "鞺": "汤",
    "殔": "意",
    "蹮": "先",
    "馩": "坟",
    "浫": "罕",
    "欨": "需",
    "虃": "间",
    "餫": "运",
    "暞": "角",
    "毟": "裂",
    "熫": "至",
    "稪": "福",
    "嚋": "愁",
    "歚": "善",
    "睊": "倦",
    "蕽": "农",
    "豃": "罕",
    "鷭": "烦",
    "鯼": "宗",
    "幩": "坟",
    "碷": "顿",
    "籕": "昼",
    "鴭": "堆",
    "嶜": "金",
    "弡": "觉",
    "櫏": "前",
    "顤": "摇",
    "裐": "捐",
    "鋠": "肾",
    "廤": "裤",
    "篞": "聂",
    "貈": "和",
    "賌": "该",
    "踂": "聂",
    "鎀": "修",
    "諚": "偏",
    "駨": "熏",
    "塦": "镇",
    "旚": "飘",
    "鏔": "宜",
    "擈": "铺",
    "蜝": "其",
    "鮂": "求",
    "簢": "敏",
    "骪": "伟",
    "臹": "修",
    "浳": "意",
    "糐": "夫",
    "駣": "桃",
    "鱁": "竹",
    "燞": "角",
    "鴼": "落",
    "榒": "诺",
    "袎": "要",
    "鳨": "利",
    "鏯": "双",
    "沀": "续",
    "憁": "从",
    "鱃": "修",
    "艊": "博",
    "諩": "普",
    "覫": "旁",
    "頪": "泪",
    "顃": "谈",
    "鑉": "和",
    "饆": "必",
    "毩": "局",
    "騼": "路",
    "萔": "条",
    "鯄": "求",
    "毈": "断",
    "颹": "伟",
    "頛": "泪",
    "鶃": "意",
    "銺": "藏",
    "鴩": "铁",
    "顲": "懒",
    "塐": "速",
    "霘": "动",
    "駠": "刘",
    "騿": "张",
    "镺": "袄",
    "弤": "底",
    "焒": "旅",
    "魻": "侠",
    "贃": "万",
    "霕": "吞",
    "暳": "会",
    "駤": "至",
    "鎨": "损",
    "鶦": "胡",
    "槂": "孙",
    "頶": "胡",
    "襊": "翠",
    "嶎": "玉",
    "馸": "信",
    "樮": "烟",
    "誁": "病",
    "橩": "穷",
    "惾": "宗",
    "瘶": "嗽",
    "隵": "西",
    "鬇": "睁",
    "嶖": "烟",
    "隚": "唐",
    "橽": "踏",
    "鸅": "则",
    "橳": "胜",
    "銵": "坑",
    "騌": "宗",
    "蝷": "利",
    "毻": "拓",
    "蹗": "路",
    "鷤": "提",
    "嶥": "觉",
    "鬑": "连",
    "飷": "解",
    "蔅": "颜",
    "毲": "多",
    "猐": "枪",
    "肍": "求",
    "誝": "安",
    "稩": "记",
    "沎": "或",
    "袬": "玉",
    "蝆": "养",
    "鴮": "乌",
    "糋": "见",
    "鏩": "见",
    "飹": "宝",
    "鶳": "诗",
    "旫": "挑",
    "譑": "角",
    "錥": "玉",
    "鱛": "增",
    "韕": "扩",
    "誙": "坑",
    "錜": "聂",
    "薓": "深",
    "餢": "不",
    "穒": "贺",
    "鶙": "提",
    "誟": "笑",
    "謴": "滚",
    "驠": "燕",
    "誈": "乌",
    "鍻": "节",
    "鵋": "记",
    "擹": "贪",
    "硽": "烟",
    "襏": "博",
    "糣": "三",
    "祪": "鬼",
    "襨": "对",
    "隁": "燕",
    "餇": "同",
    "覣": "微",
    "靵": "纽",
    "鴺": "提",
    "鯏": "离",
    "鎫": "万",
    "軠": "狂",
    "軡": "钱",
    "錹": "肯",
    "鍿": "姿",
    "垥": "鞋",
    "烐": "周",
    "驏": "战",
    "賋": "角",
    "蔒": "熏",
    "覢": "闪",
    "鵘": "俊",
    "鴘": "扁",
    "溬": "枪",
    "趝": "见",
    "鴁": "邀",
    "渆": "冤",
    "跠": "宜",
    "籡": "妾",
    "毮": "沙",
    "訙": "训",
    "盕": "饭",
    "躎": "年",
    "鴱": "爱",
    "腵": "家",
    "愥": "应",
    "簅": "铲",
    "鮺": "眨",
    "鋧": "现",
    "瀒": "色",
    "迱": "驮",
    "鯅": "山",
    "瞸": "夜",
    "櫀": "其",
    "讍": "恶",
    "躸": "机",
    "彏": "觉",
    "窏": "乌",
    "鷱": "高",
    "磫": "宗",
    "貱": "必",
    "樬": "聪",
    "鬔": "朋",
    "鶾": "汉",
    "諊": "居",
    "鯜": "妾",
    "豒": "至",
    "嶾": "引",
    "攍": "营",
    "讑": "要",
    "骮": "意",
    "蠴": "鼠",
    "飻": "贴",
    "鉜": "福",
    "礍": "节",
    "鮷": "提",
    "橣": "凝",
    "鷠": "鱼",
    "顮": "彬",
    "銽": "瓜",
    "鋷": "最",
    "飅": "刘",
    "軣": "轰",
    "鯂": "苏",
    "魥": "恶",
    "鬎": "蜡",
    "鋴": "镇",
    "糮": "现",
    "鏭": "西",
    "爴": "觉",
    "諙": "话",
    "溑": "锁",
    "霼": "系",
    "霷": "羊",
    "燪": "总",
    "窲": "朝",
    "瞊": "荡",
    "謥": "从",
    "鰬": "钱",
    "愂": "被",
    "諿": "七",
    "壦": "熏",
    "驐": "蹲",
    "蹥": "连",
    "擌": "色",
    "餰": "间",
    "騚": "钱",
    "鯲": "鱼",
    "蹘": "聊",
    "聓": "续",
    "袻": "而",
    "濖": "树",
    "覮": "营",
    "軶": "恶",
    "鍯": "聪",
    "饇": "玉",
    "焎": "谢",
    "韯": "先",
    "攐": "前",
    "慃": "养",
    "噕": "灰",
    "縇": "宣",
    "隫": "坟",
    "踻": "瓜",
    "鵵": "兔",
    "橬": "钱",
    "薒": "灿",
    "橌": "现",
    "靀": "盟",
    "樥": "朋",
    "錅": "离",
    "駳": "蛋",
    "魝": "节",
    "稝": "朋",
    "瘷": "色",
    "鶈": "七",
    "襓": "扰",
    "隝": "导",
    "澖": "闲",
    "巈": "局",
    "蜟": "玉",
    "顊": "宜",
    "豟": "恶",
    "釠": "乱",
    "驝": "拖",
    "駯": "朱",
    "鮖": "石",
    "軭": "框",
    "蕵": "孙",
    "憥": "劳",
    "韀": "间",
    "猏": "间",
    "睤": "必",
    "踚": "伦",
    "銯": "思",
    "鞰": "温",
    "鮉": "雕",
    "鵆": "横",
    "鵎": "妥",
    "鉖": "同",
    "攇": "显",
    "鴲": "知",
    "訯": "洒",
    "睈": "成",
    "隦": "角",
    "殦": "雕",
    "鮬": "哭",
    "貋": "按",
    "誛": "亲",
    "謯": "接",
    "潂": "红",
    "鴪": "玉",
    "驨": "习",
    "愸": "整",
    "歄": "瓜",
    "驙": "占",
    "鯯": "至",
    "鞱": "掏",
    "籅": "鱼",
    "艀": "福",
    "鱜": "相",
    "躛": "位",
    "鮼": "亲",
    "鯌": "烤",
    "謰": "连",
    "賆": "偏",
    "讏": "位",
    "鵢": "深",
    "貵": "偏",
    "豛": "意",
    "銟": "插",
    "澻": "岁",
    "訵": "吃",
    "鰘": "是",
    "軖": "狂",
    "驞": "拼",
    "駲": "周",
    "詜": "掏",
    "徚": "东",
    "衻": "然",
    "韢": "岁",
    "駗": "枕",
    "鳹": "琴",
    "鵇": "年",
    "裚": "记",
    "鴤": "中",
    "懄": "琴",
    "訠": "审",
    "軪": "凹",
    "憉": "朋",
    "鮙": "踏",
    "鳫": "燕",
    "霻": "风",
    "韚": "格",
    "鶰": "原",
    "軅": "燕",
    "喸": "捕",
    "蘒": "秋",
    "騇": "设",
    "睓": "舔",
    "幥": "长",
    "騹": "其",
    "髬": "批",
    "鶍": "意",
    "鬜": "前",
    "鶐": "树",
    "誢": "现",
    "褼": "先",
    "鞩": "巧",
    "蕷": "玉",
    "譋": "蓝",
    "餆": "摇",
    "覱": "战",
    "鯞": "肘",
    "潉": "昆",
    "驑": "刘",
    "鯳": "底",
    "鵈": "饿",
    "隯": "导",
    "訍": "拆",
    "鮲": "福"
}


================================================
FILE: ChatTTS/res/sha256_map.json
================================================
{
	"sha256_asset_Decoder_safetensors": "77aa55e0a977949c4733df3c6f876fa85860d3298cba63295a7bc6901729d4e0",
	"sha256_asset_DVAE_safetensors"   : "1d0b044a8368c0513100a2eca98456b289e6be6a18b7a63be1bcaa315ea874d9",
	"sha256_asset_Embed_safetensors"  : "2ff0be7134934155741b643b74e32fb6bf3eec41257984459b2ed60cdb4c48b0",
	"sha256_asset_Vocos_safetensors"  : "07e5561491cce41f7f90cfdb94b2ff263ff5742c3d89339db99b17ad82cc3f44",

	"sha256_asset_gpt_config_json"         : "0aaa1ecd96c49ad4f473459eb1982fa7ad79fa5de08cde2781bf6ad1f9a0c236",
	"sha256_asset_gpt_model_safetensors"   : "cd0806fd971f52f6a22c923ec64982b305e817bcc41ca83417fcf9141b984a0f",

	"sha256_asset_tokenizer_special_tokens_map_json": "bd0ac9d9bb1657996b5c5fbcaa7d80f8de530d01a283da97f89deae5b1b8d011",
	"sha256_asset_tokenizer_tokenizer_config_json"  : "43e9d658b554fa5ee8d8e1d763349323bfef1ed7a89c0794220ab8861387d421",
	"sha256_asset_tokenizer_tokenizer_json"         : "843838a64e121e23e774cc75874c6fe862198d9f7dd43747914633a8fd89c20e"
}



================================================
FILE: ChatTTS/utils/__init__.py
================================================
from .dl import check_all_assets, download_all_assets
from .gpu import select_device
from .io import load_safetensors, get_latest_modified_file, del_all
from .log import logger



================================================
FILE: ChatTTS/utils/dl.py
================================================
import os
from pathlib import Path
import hashlib
import requests
from io import BytesIO
from typing import Dict, Tuple, Optional
from mmap import mmap, ACCESS_READ

from .log import logger


def sha256(fileno: int) -> str:
    data = mmap(fileno, 0, access=ACCESS_READ)
    h = hashlib.sha256(data).hexdigest()
    del data
    return h


def check_model(
    dir_name: Path, model_name: str, hash: str, remove_incorrect=False
) -> bool:
    target = dir_name / model_name
    relname = target.as_posix()
    logger.get_logger().debug(f"checking {relname}...")
    if not os.path.exists(target):
        logger.get_logger().info(f"{target} not exist.")
        return False
    with open(target, "rb") as f:
        digest = sha256(f.fileno())
        bakfile = f"{target}.bak"
        if digest != hash:
            logger.get_logger().warning(f"{target} sha256 hash mismatch.")
            logger.get_logger().info(f"expected: {hash}")
            logger.get_logger().info(f"real val: {digest}")
            if remove_incorrect:
                if not os.path.exists(bakfile):
                    os.rename(str(target), bakfile)
                else:
                    os.remove(str(target))
            return False
        if remove_incorrect and os.path.exists(bakfile):
            os.remove(bakfile)
    return True


def check_folder(
    base_dir: Path,
    *innder_dirs: str,
    names: Tuple[str],
    sha256_map: Dict[str, str],
    update=False,
) -> bool:
    key = "sha256_"
    current_dir = base_dir
    for d in innder_dirs:
        current_dir /= d
        key += f"{d}_"

    for model in names:
        menv = model.replace(".", "_")
        if not check_model(current_dir, model, sha256_map[f"{key}{menv}"], update):
            return False
    return True


def check_all_assets(base_dir: Path, sha256_map: Dict[str, str], update=False) -> bool:
    logger.get_logger().info("checking assets...")

    if not check_folder(
        base_dir,
        "asset",
        names=(
            "Decoder.safetensors",
            "DVAE.safetensors",
            "Embed.safetensors",
            "Vocos.safetensors",
        ),
        sha256_map=sha256_map,
        update=update,
    ):
        return False

    if not check_folder(
        base_dir,
        "asset",
        "gpt",
        names=(
            "config.json",
            "model.safetensors",
        ),
        sha256_map=sha256_map,
        update=update,
    ):
        return False

    if not check_folder(
        base_dir,
        "asset",
        "tokenizer",
        names=(
            "special_tokens_map.json",
            "tokenizer_config.json",
            "tokenizer.json",
        ),
        sha256_map=sha256_map,
        update=update,
    ):
        return False

    logger.get_logger().info("all assets are already latest.")
    return True


def download_and_extract_tar_gz(
    url: str, folder: str, headers: Optional[Dict[str, str]] = None
):
    import tarfile

    logger.get_logger().info(f"downloading {url}")
    response = requests.get(url, headers=headers, stream=True, timeout=(10, 3))
    with BytesIO() as out_file:
        out_file.write(response.content)
        out_file.seek(0)
        logger.get_logger().info(f"downloaded.")
        with tarfile.open(fileobj=out_file, mode="r:gz") as tar:
            tar.extractall(folder)
        logger.get_logger().info(f"extracted into {folder}")


def download_and_extract_zip(
    url: str, folder: str, headers: Optional[Dict[str, str]] = None
):
    import zipfile

    logger.get_logger().info(f"downloading {url}")
    response = requests.get(url, headers=headers, stream=True, timeout=(10, 3))
    with BytesIO() as out_file:
        out_file.write(response.content)
        out_file.seek(0)
        logger.get_logger().info(f"downloaded.")
        with zipfile.ZipFile(out_file) as zip_ref:
            zip_ref.extractall(folder)
        logger.get_logger().info(f"extracted into {folder}")


def download_dns_yaml(url: str, folder: str, headers: Dict[str, str]):
    logger.get_logger().info(f"downloading {url}")
    response = requests.get(url, headers=headers, stream=True, timeout=(100, 3))
    with open(os.path.join(folder, "dns.yaml"), "wb") as out_file:
        out_file.write(response.content)
        logger.get_logger().info(f"downloaded into {folder}")


def download_all_assets(tmpdir: str, homedir: str, version="0.2.10"):
    import subprocess
    import platform

    archs = {
        "aarch64": "arm64",
        "armv8l": "arm64",
        "arm64": "arm64",
        "x86": "386",
        "i386": "386",
        "i686": "386",
        "386": "386",
        "x86_64": "amd64",
        "x64": "amd64",
        "amd64": "amd64",
    }
    system_type = platform.system().lower()
    architecture = platform.machine().lower()
    is_win = system_type == "windows"

    architecture = archs.get(architecture, None)
    if not architecture:
        logger.get_logger().error(f"architecture {architecture} is not supported")
        exit(1)
    try:
        BASE_URL = "https://github.com/fumiama/RVC-Models-Downloader/releases/download/"
        suffix = "zip" if is_win else "tar.gz"
        RVCMD_URL = BASE_URL + f"v{version}/rvcmd_{system_type}_{architecture}.{suffix}"
        cmdfile = os.path.join(tmpdir, "rvcmd")
        if is_win:
            download_and_extract_zip(RVCMD_URL, tmpdir)
            cmdfile += ".exe"
        else:
            download_and_extract_tar_gz(RVCMD_URL, tmpdir)
            os.chmod(cmdfile, 0o755)
        subprocess.run([cmdfile, "-notui", "-w", "0", "-H", homedir, "assets/chtts"])
    except Exception:
        BASE_URL = (
            "https://gitea.seku.su/fumiama/RVC-Models-Downloader/releases/download/"
        )
        suffix = "zip" if is_win else "tar.gz"
        RVCMD_URL = BASE_URL + f"v{version}/rvcmd_{system_type}_{architecture}.{suffix}"
        download_dns_yaml(
            "https://gitea.seku.su/fumiama/RVC-Models-Downloader/raw/branch/main/dns.yaml",
            tmpdir,
            headers={
                "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0"
            },
        )
        cmdfile = os.path.join(tmpdir, "rvcmd")
        if is_win:
            download_and_extract_zip(RVCMD_URL, tmpdir)
            cmdfile += ".exe"
        else:
            download_and_extract_tar_gz(RVCMD_URL, tmpdir)
            os.chmod(cmdfile, 0o755)
        subprocess.run(
            [
                cmdfile,
                "-notui",
                "-w",
                "0",
                "-dns",
                os.path.join(tmpdir, "dns.yaml"),
                "-H",
                homedir,
                "assets/chtts",
            ]
        )



================================================
FILE: ChatTTS/utils/gpu.py
================================================
import torch

try:
    import torch_npu
except ImportError:
    pass

from .log import logger


def select_device(min_memory=2047, experimental=False):
    has_cuda = torch.cuda.is_available()
    if has_cuda or _is_torch_npu_available():
        provider = torch.cuda if has_cuda else torch.npu
        """
        Using Ascend NPU to accelerate the process of inferencing when GPU is not found.
        """
        dev_idx = 0
        max_free_memory = -1
        for i in range(provider.device_count()):
            props = provider.get_device_properties(i)
            free_memory = props.total_memory - provider.memory_reserved(i)
            if max_free_memory < free_memory:
                dev_idx = i
                max_free_memory = free_memory
        free_memory_mb = max_free_memory / (1024 * 1024)
        if free_memory_mb < min_memory:
            logger.get_logger().warning(
                f"{provider.device(dev_idx)} has {round(free_memory_mb, 2)} MB memory left. Switching to CPU."
            )
            device = torch.device("cpu")
        else:
            device = provider._get_device(dev_idx)
    elif torch.backends.mps.is_available():
        """
        Currently MPS is slower than CPU while needs more memory and core utility,
        so only enable this for experimental use.
        """
        if experimental:
            # For Apple M1/M2 chips with Metal Performance Shaders
            logger.get_logger().warning("experimantal: found apple GPU, using MPS.")
            device = torch.device("mps")
        else:
            logger.get_logger().info("found Apple GPU, but use CPU.")
            device = torch.device("cpu")
    else:
        logger.get_logger().warning("no GPU or NPU found, use CPU instead")
        device = torch.device("cpu")

    return device


def _is_torch_npu_available():
    try:
        # will raise a AttributeError if torch_npu is not imported or a RuntimeError if no NPU found
        _ = torch.npu.device_count()
        return torch.npu.is_available()
    except (AttributeError, RuntimeError):
        return False



================================================
FILE: ChatTTS/utils/io.py
================================================
import os
import logging
from typing import Union
from dataclasses import is_dataclass

from safetensors import safe_open
import torch

from .log import logger


@torch.inference_mode()
def load_safetensors(filename: str):
    state_dict_tensors = {}
    with safe_open(filename, framework="pt") as f:
        for k in f.keys():
            state_dict_tensors[k] = f.get_tensor(k)
    return state_dict_tensors


def get_latest_modified_file(directory):

    files = [os.path.join(directory, f) for f in os.listdir(directory)]
    if not files:
        logger.get_logger().log(
            logging.WARNING, f"no files found in the directory: {directory}"
        )
        return None
    latest_file = max(files, key=os.path.getmtime)

    return latest_file


def del_all(d: Union[dict, list]):
    if is_dataclass(d):
        for k in list(vars(d).keys()):
            x = getattr(d, k)
            if isinstance(x, dict) or isinstance(x, list) or is_dataclass(x):
                del_all(x)
            del x
            delattr(d, k)
    elif isinstance(d, dict):
        lst = list(d.keys())
        for k in lst:
            x = d.pop(k)
            if isinstance(x, dict) or isinstance(x, list) or is_dataclass(x):
                del_all(x)
            del x
    elif isinstance(d, list):
        while len(d):
            x = d.pop()
            if isinstance(x, dict) or isinstance(x, list) or is_dataclass(x):
                del_all(x)
            del x
    else:
        del d



================================================
FILE: ChatTTS/utils/log.py
================================================
import logging
from pathlib import Path


class Logger:
    def __init__(self, logger=logging.getLogger(Path(__file__).parent.name)):
        self.logger = logger

    def set_logger(self, logger: logging.Logger):
        self.logger = logger

    def get_logger(self) -> logging.Logger:
        return self.logger


logger = Logger()



================================================
FILE: docs/cn/README.md
================================================
<div align="center">

<a href="https://trendshift.io/repositories/10489" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10489" alt="2noise%2FChatTTS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

# ChatTTS
一款适用于日常对话的生成式语音模型。

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)

[![Huggingface](https://img.shields.io/badge/🤗%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)

[**English**](../../README.md) | **简体中文** | [**日本語**](../jp/README.md) | [**Русский**](../ru/README.md) | [**Español**](../es/README.md) | [**Français**](../fr/README.md) | [**한국어**](../kr/README.md)

</div>

> [!NOTE]
> 注意此版本可能不是最新版，所有内容请以英文版为准。

## 简介

> [!Note]
> 这个仓库包含算法架构和一些简单的示例。

> [!Tip]
> 由本仓库衍生出的用户端产品，请参见由社区维护的索引仓库  [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS)。

ChatTTS 是一款专门为对话场景（例如 LLM 助手）设计的文本转语音模型。

### 支持的语种

- [x] 英语
- [x] 中文
- [ ] 敬请期待...

### 亮点

> 你可以参考 **[Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)** 上的这个视频，了解本项目的详细情况。

1. **对话式 TTS**: ChatTTS 针对对话式任务进行了优化，能够实现自然且富有表现力的合成语音。它支持多个说话者，便于生成互动式对话。
2. **精细的控制**: 该模型可以预测和控制精细的韵律特征，包括笑声、停顿和插入语。
3. **更好的韵律**: ChatTTS 在韵律方面超越了大多数开源 TTS 模型。我们提供预训练模型以支持进一步的研究和开发。

### 数据集和模型

- 主模型使用了 100,000+ 小时的中文和英文音频数据进行训练。
- **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** 上的开源版本是一个在 40,000 小时数据上进行无监督微调的预训练模型。

### 路线图

- [x] 开源 4 万小时基础模型和 spk_stats 文件。
- [x] 支持流式语音输出。
- [x] 开源 DVAE 编码器和零样本推理代码
- [ ] 开源具有多情感控制功能的 4 万小时版本。
- [ ] ChatTTS.cpp (欢迎在 2noise 组织中新建仓库)。

### 免责声明

> [!Important]
> 此仓库仅供学术用途。

本项目旨在用于教育和研究目的，不适用于任何商业或法律目的。作者不保证信息的准确性、完整性和可靠性。此仓库中使用的信息和数据仅供学术和研究目的。数据来自公开来源，作者不声称对数据拥有任何所有权或版权。

ChatTTS 是一款强大的文本转语音系统。但是，负责任和道德地使用这项技术非常重要。为了限制 ChatTTS 的使用，我们在 40,000 小时模型的训练过程中添加了少量高频噪声，并使用 MP3 格式尽可能压缩音频质量，以防止恶意行为者将其用于犯罪目的。同时，我们内部训练了一个检测模型，并计划在未来开源它。

### 联系方式

> 欢迎随时提交 GitHub issues/PRs。

#### 合作洽谈

如需就模型和路线图进行合作洽谈，请发送邮件至 **open-source@2noise.com**。

#### 线上讨论

##### 1. 官方 QQ 群

- **群 1**, 808364215 (已满)
- **群 2**, 230696694 (已满)
- **群 3**, 933639842 (已满)
- **群 4**, 608667975

##### 2. Discord

点击加入 [Discord](https://discord.gg/Ud5Jxgx5yD)。

## 体验教程

### 克隆仓库

```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### 安装依赖

#### 1. 直接安装

```bash
pip install --upgrade -r requirements.txt
```

#### 2. 使用 conda 安装

```bash
conda create -n chattts
conda activate chattts
pip install -r requirements.txt
```

#### 可选 : 如果使用 NVIDIA GPU（仅限 Linux），可安装 TransformerEngine。

> [!Note]
> 安装过程可能耗时很长。

> [!Warning]
> TransformerEngine 的适配目前正在开发中，运行时可能会遇到较多问题。仅推荐出于开发目的安装。

```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable
```

#### 可选 : 安装 FlashAttention-2 (主要适用于 NVIDIA GPU)

> [!Note]
> 支持设备列表详见 [Hugging Face Doc](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2).

```bash
pip install flash-attn --no-build-isolation
```

### 快速启动

> 确保在执行以下命令时，处于项目根目录下。

#### 1. WebUI 可视化界面

```bash
python examples/web/webui.py
```

#### 2. 命令行交互

> 生成的音频将保存至 `./output_audio_n.mp3`

```bash
python examples/cmd/run.py "Your text 1." "Your text 2."
```

## 开发教程

### 安装 Python 包

1. 从 PyPI 安装稳定版

```bash
pip install ChatTTS
```

2. 从 GitHub 安装最新版

```bash
pip install git+https://github.com/2noise/ChatTTS
```

3. 从本地文件夹安装开发版

```bash
pip install -e .
```

### 基础用法

```python
import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) # Set to True for better performance

texts = ["PUT YOUR 1st TEXT HERE", "PUT YOUR 2nd TEXT HERE"]

wavs = chat.infer(texts)

torchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)
```

### 进阶用法

```python
###################################
# Sample a speaker from Gaussian.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # add sampled speaker 
    temperature = .3,   # using custom temperature
    top_P = 0.7,        # top P decode
    top_K = 20,         # top K decode
)

###################################
# For sentence level manual control.

# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# For word level manual control.

text = 'What is [uv_break]your favorite english food?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save("output2.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>示例: 自我介绍</h4></summary>

```python
inputs_en = """
chatTTS is a text to speech model designed for dialogue applications.
[uv_break]it supports mixed language input [uv_break]and offers multi speaker
capabilities with precise control over prosodic elements like
[uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation.
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
""".replace('\n', '') # English is still experimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save("output3.wav", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align="center">

**男性音色**

</td>
<td align="center">

**女性音色**

</td>
</tr>
<tr>
<td align="center">

[男性音色](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align="center">

[女性音色](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>

</details>

## 常见问题

#### 1. 我需要多少 VRAM？ 推理速度如何？

对于 30 秒的音频片段，至少需要 4GB 的 GPU 内存。 对于 4090 GPU，它可以每秒生成大约 7 个语义 token 对应的音频。实时因子 (RTF) 约为 0.3。

#### 2. 模型稳定性不够好，存在多个说话者或音频质量差等问题。

这是一个通常发生在自回归模型（例如 bark 和 valle）中的问题，通常很难避免。可以尝试多个样本以找到合适的结果。

#### 3. 除了笑声，我们还能控制其他东西吗？我们能控制其他情绪吗？

在当前发布的模型中，可用的 token 级控制单元是 `[laugh]`, `[uv_break]` 和 `[lbreak]`。未来的版本中，我们可能会开源具有更多情绪控制功能的模型。

## 致谢

- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) 和 [valle](https://arxiv.org/abs/2301.02111) 通过自回归式系统展示了非凡的 TTS 效果。
- [fish-speech](https://github.com/fishaudio/fish-speech) 揭示了 GVQ 作为 LLM 建模的音频分词器的能力。
- [vocos](https://github.com/gemelo-ai/vocos) vocos 被用作预训练声码器。

## 特别鸣谢

- [wlu-audio lab](https://audio.westlake.edu.cn/) 对于早期算法实验的支持。

## 贡献者列表

[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

## 项目浏览量

<div align="center">

![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>



================================================
FILE: docs/es/README.md
================================================
<div align="center">

<a href="https://trendshift.io/repositories/10489" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10489" alt="2noise%2FChatTTS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

# ChatTTS
Un modelo de generación de voz para la conversación diaria.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)

[![Huggingface](https://img.shields.io/badge/🤗%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)

[**English**](../../README.md) | [**简体中文**](../cn/README.md) | [**日本語**](../jp/README.md) | [**Русский**](../ru/README.md) | **Español**
 | [**Français**](../fr/README.md) | [**한국어**](../kr/README.md)
</div>

> [!NOTE]
> Atención, es posible que esta versión no sea la última. Por favor, consulte la versión en inglés para conocer todo el contenido.

## Introducción

ChatTTS es un modelo de texto a voz diseñado específicamente para escenarios conversacionales como LLM assistant.

### Idiomas Soportados

- [x] Inglés
- [x] Chino
- [ ] Manténganse al tanto...

### Aspectos Destacados

> Puede consultar **[este video en Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)** para obtener una descripción detallada.

1. **TTS Conversacional**: ChatTTS está optimizado para tareas conversacionales, logrando una síntesis de voz natural y expresiva. Soporta múltiples hablantes, lo que facilita la generación de diálogos interactivos.
2. **Control Finas**: Este modelo puede predecir y controlar características detalladas de la prosodia, incluyendo risas, pausas e interjecciones.
3. **Mejor Prosodia**: ChatTTS supera a la mayoría de los modelos TTS de código abierto en cuanto a prosodia. Ofrecemos modelos preentrenados para apoyar estudios y desarrollos adicionales.

### Conjunto de Datos & Modelo

- El modelo principal se entrena con más de 100.000 horas de datos de audio en chino e inglés.
- La versión de código abierto en **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** es un modelo preentrenado con 40.000 horas, sin SFT.

### Hoja de Ruta

- [x] Publicar el modelo base de 40k horas y el archivo spk_stats como código abierto
- [ ] Publicar los códigos de codificador VQ y entrenamiento de Lora como código abierto
- [ ] Generación de audio en streaming sin refinar el texto
- [ ] Publicar la versión de 40k horas con control de múltiples emociones como código abierto
- [ ] ¿ChatTTS.cpp? (Se aceptan PR o un nuevo repositorio)

### Descargo de Responsabilidad

> [!Important]
> Este repositorio es sólo para fines académicos.

Este proyecto está destinado a fines educativos y estudios, y no es adecuado para ningún propósito comercial o legal. El autor no garantiza la exactitud, integridad o fiabilidad de la información. La información y los datos utilizados en este repositorio son únicamente para fines académicos y de investigación. Los datos provienen de fuentes públicas, y el autor no reclama ningún derecho de propiedad o copyright sobre ellos.

ChatTTS es un potente sistema de conversión de texto a voz. Sin embargo, es crucial utilizar esta tecnología de manera responsable y ética. Para limitar el uso de ChatTTS, hemos añadido una pequeña cantidad de ruido de alta frecuencia durante el proceso de entrenamiento del modelo de 40.000 horas y hemos comprimido la calidad del audio en formato MP3 tanto como sea posible para evitar que actores malintencionados lo usen con fines delictivos. Además, hemos entrenado internamente un modelo de detección y planeamos hacerlo de código abierto en el futuro.

### Contacto

> No dudes en enviar issues/PRs de GitHub.

#### Consultas Formales

Si desea discutir la cooperación sobre modelos y hojas de ruta, envíe un correo electrónico a **open-source@2noise.com**.

#### Chat en Línea

##### 1. Grupo QQ (Aplicación Social China)

- **Grupo 1**, 808364215 (Lleno)
- **Grupo 2**, 230696694 (Lleno)
- **Grupo 3**, 933639842

## Instalación (En Proceso)

> Se cargará en pypi pronto según https://github.com/2noise/ChatTTS/issues/269.

```bash
pip install git+https://github.com/2noise/ChatTTS
```

## Inicio
### Clonar el repositorio
```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### Requerimientos de instalación
#### 1. Instalar directamente
```bash
pip install --upgrade -r requirements.txt
```

#### 2. Instalar desde conda
```bash
conda create -n chattts
conda activate chattts
pip install -r requirements.txt
```

### Inicio Rápido
#### 1. Iniciar la interfaz de usuario web (WebUI)
```bash
python examples/web/webui.py
```

#### 2. Inferir por línea de comando
> Guardará el audio en `./output_audio_xxx.wav`

```bash
python examples/cmd/run.py "Please input your text."
```

### Básico

```python
import ChatTTS
from IPython.display import Audio
import torchaudio
import torch

chat = ChatTTS.Chat()
chat.load(compile=False) # Set to True for better performance

texts = ["PUT YOUR TEXT HERE",]

wavs = chat.infer(texts)

torchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)
```

### Avanzado

```python
###################################
# Sample a speaker from Gaussian.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # add sampled speaker 
    temperature = .3,   # using custom temperature
    top_P = 0.7,        # top P decode
    top_K = 20,         # top K decode
)

###################################
# For sentence level manual control.

# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# For word level manual control.
text = 'What is [uv_break]your favorite english food?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save("output2.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Ejemplo: auto presentación</h4></summary>

```python
inputs_en = """
chat T T S is a text to speech model designed for dialogue applications. 
[uv_break]it supports mixed language input [uv_break]and offers multi speaker 
capabilities with precise control over prosodic elements [laugh]like like 
[uv_break]laughter[laugh], [uv_break]pauses, [uv_break]and intonation. 
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
""".replace('\n', '') # English is still experimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save("output3.wav", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align="center">

**altavoz masculino**

</td>
<td align="center">

**altavoz femenino**

</td>
</tr>
<tr>
<td align="center">

[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align="center">

[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>


</details>

## Preguntas y Respuestas

#### 1. ¿Cuánta memoria gráfica de acceso aleatorio necesito? ¿Qué tal inferir la velocidad?
Para un clip de audio de 30 segundos, se requieren al menos 4 GB de memoria de GPU. Para la GPU 4090, puede generar audio correspondiente a aproximadamente 7 tokens semánticos por segundo. El Factor en Tiempo Real (RTF) es aproximadamente 0,3.

#### 2. La estabilidad del modelo no es lo suficientemente buena y existen problemas como varios altavoces o mala calidad del sonido.

Este es un problema común en los modelos autorregresivos (para bark y valle). Generalmente es difícil de evitar. Puede probar varias muestras para encontrar resultados adecuados.

#### 3. ¿Podemos controlar algo más que la risa? ¿Podemos controlar otras emociones?

En el modelo lanzado actualmente, las únicas unidades de control a nivel de token son `[risa]`, `[uv_break]` y `[lbreak]`. En una versión futura, es posible que abramos el código fuente del modelo con capacidades adicionales de control de emociones.

## Agradecimientos
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) y [valle](https://arxiv.org/abs/2301.02111) demuestran un resultado TTS notable mediante un sistema de estilo autorregresivo.
- [fish-speech](https://github.com/fishaudio/fish-speech) revela las capacidades de GVQ como tokenizador de audio para el modelado LLM.
- [vocos](https://github.com/gemelo-ai/vocos) se utiliza como codificador de voz previamente entrenado.

## Agradecimiento Especial
- [wlu-audio lab](https://audio.westlake.edu.cn/) para experimentos iniciales del algoritmo.

## Recursos Relacionados
- [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS)

## Gracias a todos los contribuyentes por sus esfuerzos.
[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

<div align="center">

  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>



================================================
FILE: docs/fr/README.md
================================================
<div align="center">

<a href="https://trendshift.io/repositories/10489" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10489" alt="2noise%2FChatTTS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

# ChatTTS
Un modèle de parole génératif pour le dialogue quotidien.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)

[![Huggingface](https://img.shields.io/badge/🤗%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)

[**English**](../../README.md) | [**简体中文**](../cn/README.md) | [**日本語**](../jp/README.md) | [**Русский**](../ru/README.md) | [**Español**](../es/README.md)| **Français**  | [**한국어**](../kr/README.md)

</div>

## Introduction
> [!Note]
> Ce dépôt contient l'infrastructure de l'algorithme et quelques exemples simples.

> [!Tip]
> Pour les produits finaux étendus pour les utilisateurs, veuillez consulter le dépôt index [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS/tree/en) maintenu par la communauté.

ChatTTS est un modèle de synthèse vocale conçu spécifiquement pour les scénarios de dialogue tels que les assistants LLM.

### Langues prises en charge
- [x] Anglais
- [x] Chinois
- [ ] À venir...

### Points forts
> Vous pouvez vous référer à **[cette vidéo sur Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)** pour une description détaillée.

1. **Synthèse vocale conversationnelle**: ChatTTS est optimisé pour les tâches basées sur le dialogue, permettant une synthèse vocale naturelle et expressive. Il prend en charge plusieurs locuteurs, facilitant les conversations interactives.
2. **Contrôle granulaire**: Le modèle peut prédire et contrôler des caractéristiques prosodiques fines, y compris le rire, les pauses et les interjections.
3. **Meilleure prosodie**: ChatTTS dépasse la plupart des modèles TTS open-source en termes de prosodie. Nous fournissons des modèles pré-entraînés pour soutenir la recherche et le développement.

### Dataset & Modèle
- Le modèle principal est entraîné avec des données audio en chinois et en anglais de plus de 100 000 heures.
- La version open-source sur **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** est un modèle pré-entraîné de 40 000 heures sans SFT.

### Roadmap
- [x] Open-source du modèle de base de 40k heures et du fichier spk_stats.
- [x] Génération audio en streaming.
- [ ] Open-source de la version 40k heures avec contrôle multi-émotions.
- [ ] ChatTTS.cpp (nouveau dépôt dans l'organisation `2noise` est bienvenu)

### Avertissement
> [!Important]
> Ce dépôt est à des fins académiques uniquement.

Il est destiné à un usage éducatif et de recherche, et ne doit pas être utilisé à des fins commerciales ou légales. Les auteurs ne garantissent pas l'exactitude, l'exhaustivité ou la fiabilité des informations. Les informations et les données utilisées dans ce dépôt sont à des fins académiques et de recherche uniquement. Les données obtenues à partir de sources accessibles au public, et les auteurs ne revendiquent aucun droit de propriété ou de copyright sur les données.

ChatTTS est un système de synthèse vocale puissant. Cependant, il est très important d'utiliser cette technologie de manière responsable et éthique. Pour limiter l'utilisation de ChatTTS, nous avons ajouté une petite quantité de bruit haute fréquence pendant l'entraînement du modèle de 40 000 heures et compressé la qualité audio autant que possible en utilisant le format MP3, pour empêcher les acteurs malveillants de l'utiliser potentiellement à des fins criminelles. En même temps, nous avons entraîné en interne un modèle de détection et prévoyons de l'open-source à l'avenir.

### Contact
> Les issues/PRs sur GitHub sont toujours les bienvenus.

#### Demandes formelles
Pour les demandes formelles concernant le modèle et la feuille de route, veuillez nous contacter à **open-source@2noise.com**.

#### Discussion en ligne
##### 1. Groupe QQ (application sociale chinoise)
- **Groupe 1**, 808364215 (Complet)
- **Groupe 2**, 230696694 (Complet)
- **Groupe 3**, 933639842 (Complet)
- **Groupe 4**, 608667975

##### 2. Serveur Discord
Rejoignez en cliquant [ici](https://discord.gg/Ud5Jxgx5yD).

## Pour commencer
### Cloner le dépôt
```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### Installer les dépendances
#### 1. Installation directe
```bash
pip install --upgrade -r requirements.txt
```

#### 2. Installer depuis conda
```bash
conda create -n chattts
conda activate chattts
pip install -r requirements.txt
```

#### Optionnel : Installer TransformerEngine si vous utilisez un GPU NVIDIA (Linux uniquement)
> [!Note]
> Le processus d'installation est très lent.

> [!Warning]
> L'adaptation de TransformerEngine est actuellement en cours de développement et NE PEUT PAS fonctionner correctement pour le moment.
> Installez-le uniquement à des fins de développement.

```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable
```

#### Optionnel : Installer FlashAttention-2 (principalement GPU NVIDIA)
> [!Note]
> Voir les appareils pris en charge dans la [documentation Hugging Face](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2).

> [!Warning]
> Actuellement, FlashAttention-2 ralentira la vitesse de génération selon [ce problème](https://github.com/huggingface/transformers/issues/26990). 
> Installez-le uniquement à des fins de développement.

```bash
pip install flash-attn --no-build-isolation
```

### Démarrage rapide
> Assurez-vous que vous êtes dans le répertoire racine du projet lorsque vous exécutez ces commandes ci-dessous.

#### 1. Lancer WebUI
```bash
python examples/web/webui.py
```

#### 2. Inférence par ligne de commande
> Cela enregistrera l'audio sous ‘./output_audio_n.mp3’

```bash
python examples/cmd/run.py "Votre premier texte." "Votre deuxième texte."
```

## Installation

1. Installer la version stable depuis PyPI
```bash
pip install ChatTTS
```

2. Installer la dernière version depuis GitHub
```bash
pip install git+https://github.com/2noise/ChatTTS
```

3. Installer depuis le répertoire local en mode développement
```bash
pip install -e .
```

### Utilisation de base

```python
import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) # Définissez sur True pour de meilleures performances

texts = ["METTEZ VOTRE PREMIER TEXTE ICI", "METTEZ VOTRE DEUXIÈME TEXTE ICI"]

wavs = chat.infer(texts)

torchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)
```

### Utilisation avancée

```python
###################################
# Échantillonner un locuteur à partir d'une distribution gaussienne.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # sauvegardez-le pour une récupération ultérieure du timbre

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # ajouter le locuteur échantillonné 
    temperature = .3,   # en utilisant une température personnalisée
    top_P = 0.7,        # top P décode
    top_K = 20,         # top K décode
)

###################################
# Pour le contrôle manuel au niveau des phrases.

# utilisez oral_(0-9), laugh_(0-2), break_(0-7) 
# pour générer un token spécial dans le texte à synthétiser.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# Pour le contrôle manuel au niveau des mots.

text = 'Quel est [uv_break]votre plat anglais préféré?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save("output2.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Exemple : auto-présentation</h4></summary>

```python
inputs_en = """
chat T T S est un modèle de synthèse vocale conçu pour les applications de dialogue.
[uv_break]il prend en charge les entrées en langues mixtes [uv_break]et offre des capacités multi-locuteurs
avec un contrôle précis des éléments prosodiques comme 
[uv_break]le rire[uv_break][laugh], [uv_break]les pauses, [uv_break]et l'intonation.
[uv_break]il délivre une parole naturelle et expressive,[uv_break]donc veuillez
[uv_break]utiliser le projet de manière responsable à vos risques et périls.[uv_break]
""".replace('\n', '') # L'anglais est encore expérimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save("output3.wav", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align="center">

**locuteur masculin**

</td>
<td align="center">

**locutrice féminine**

</td>
</tr>
<tr>
<td align="center">

[locuteur masculin](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align="center">

[locutrice féminine](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>


</details>

## FAQ

#### 1. De combien de VRAM ai-je besoin ? Quelle est la vitesse d'inférence ?
Pour un clip audio de 30 secondes, au moins 4 Go de mémoire GPU sont nécessaires. Pour le GPU 4090, il peut générer de l'audio correspondant à environ 7 tokens sémantiques par seconde. Le Facteur Temps Réel (RTF) est d'environ 0.3.

#### 2. La stabilité du modèle n'est pas suffisante, avec des problèmes tels que des locuteurs multiples ou une mauvaise qualité audio.
C'est un problème qui se produit généralement avec les modèles autoregressifs (pour bark et valle). Il est généralement difficile à éviter. On peut essayer plusieurs échantillons pour trouver un résultat approprié.

#### 3. En plus du rire, pouvons-nous contrôler autre chose ? Pouvons-nous contrôler d'autres émotions ?
Dans le modèle actuellement publié, les seules unités de contrôle au niveau des tokens sont `[laugh]`, `[uv_break]`, et `[lbreak]`. Dans les futures versions, nous pourrions open-source des modèles avec des capacités de contrôle émotionnel supplémentaires.

## Remerciements
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) et [valle](https://arxiv.org/abs/2301.02111) démontrent un résultat TTS remarquable par un système de style autoregressif.
- [fish-speech](https://github.com/fishaudio/fish-speech) révèle la capacité de GVQ en tant que tokenizer audio pour la modélisation LLM.
- [vocos](https://github.com/gemelo-ai/vocos) qui est utilisé comme vocodeur pré-entraîné.

## Appréciation spéciale
- [wlu-audio lab](https://audio.westlake.edu.cn/) pour les expériences d'algorithme précoce.

## Merci à tous les contributeurs pour leurs efforts
[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

<div align="center">

  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>



================================================
FILE: docs/jp/README.md
================================================
# ChatTTS
> [!NOTE]
> 以下の内容は最新情報ではない可能性がありますのでご了承ください。全ての内容は英語版に基準することになります。

[![Huggingface](https://img.shields.io/badge/🤗%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)

[**English**](../../README.md) | [**简体中文**](../cn/README.md) | **日本語** | [**Русский**](../ru/README.md) | [**Español**](../es/README.md) | [**Français**](../fr/README.md) | [**한국어**](../kr/README.md)

ChatTTSは、LLMアシスタントなどの対話シナリオ用に特別に設計されたテキストから音声へのモデルです。英語と中国語の両方をサポートしています。私たちのモデルは、中国語と英語で構成される100,000時間以上でトレーニングされています。**[HuggingFace](https://huggingface.co/2Noise/ChatTTS)**でオープンソース化されているバージョンは、40,000時間の事前トレーニングモデルで、SFTは行われていません。

モデルやロードマップについての正式なお問い合わせは、**open-source@2noise.com**までご連絡ください。QQグループ：808364215に参加してディスカッションすることもできます。GitHubでの問題提起も歓迎します。

---
## ハイライト
1. **会話型TTS**: ChatTTSは対話ベースのタスクに最適化されており、自然で表現豊かな音声合成を実現します。複数の話者をサポートし、対話型の会話を容易にします。
2. **細かい制御**: このモデルは、笑い、一時停止、間投詞などの細かい韻律特徴を予測および制御することができます。
3. **より良い韻律**: ChatTTSは、韻律の面でほとんどのオープンソースTTSモデルを超えています。さらなる研究と開発をサポートするために、事前トレーニングされたモデルを提供しています。

モデルの詳細な説明については、**[Bilibiliのビデオ](https://www.bilibili.com/video/BV1zn4y1o7iV)**を参照してください。

---

## 免責事項

このリポジトリは学術目的のみのためです。教育および研究用途にのみ使用され、商業的または法的な目的には使用されません。著者は情報の正確性、完全性、または信頼性を保証しません。このリポジトリで使用される情報およびデータは、学術および研究目的のみのためのものです。データは公開されているソースから取得され、著者はデータに対する所有権または著作権を主張しません。

ChatTTSは強力なテキストから音声へのシステムです。しかし、この技術を責任を持って、倫理的に利用することが非常に重要です。ChatTTSの使用を制限するために、40,000時間のモデルのトレーニング中に少量の高周波ノイズを追加し、MP3形式を使用して音質を可能な限り圧縮しました。これは、悪意のあるアクターが潜在的に犯罪目的で使用することを防ぐためです。同時に、私たちは内部的に検出モデルをトレーニングしており、将来的にオープンソース化する予定です。

---
## 使用方法

<h4>基本的な使用方法</h4>

```python
import ChatTTS
from IPython.display import Audio
import torch

chat = ChatTTS.Chat()
chat.load(compile=False) # より良いパフォーマンスのためにTrueに設定

texts = ["ここにテキストを入力してください",]

wavs = chat.infer(texts, )

torchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)
```

<h4>高度な使用方法</h4>

```python
###################################
# ガウス分布から話者をサンプリングします。

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = {
  'spk_emb': rand_spk, # サンプリングされた話者を追加
  'temperature': .3, # カスタム温度を使用
  'top_P': 0.7, # トップPデコード
  'top_K': 20, # トップKデコード
}

###################################
# 文レベルの手動制御のために。

# 特別なトークンを生成するためにテキストにoral_(0-9)、laugh_(0-2)、break_(0-7)を使用します。
params_refine_text = {
  'prompt': '[oral_2][laugh_0][break_6]'
} 

wav = chat.infer(texts, params_refine_text=params_refine_text, params_infer_code=params_infer_code)

###################################
# 単語レベルの手動制御のために。
text = 'あなたの好きな英語の食べ物は何ですか？[uv_break][laugh][lbreak]'
wav = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save("output2.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>例：自己紹介</h4></summary>

```python
inputs_jp = """
ChatTTSは、対話アプリケーション用に設計されたテキストから音声へのモデルです。
[uv_break]混合言語入力をサポートし[uv_break]、韻律要素[laugh]の正確な制御を提供します
[uv_break]笑い[laugh]、[uv_break]一時停止、[uv_break]およびイントネーション。[uv_break]自然で表現豊かな音声を提供します
[uv_break]したがって、自己責任でプロジェクトを責任を持って使用してください。[uv_break]
""".replace('\n', '') # 英語はまだ実験的です。

params_refine_text = {
  'prompt': '[oral_2][laugh_0][break_4]'
} 
audio_array_jp = chat.infer(inputs_jp, params_refine_text=params_refine_text)
torchaudio.save("output3.wav", torch.from_numpy(audio_array_jp[0]), 24000)
```
[男性話者](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

[女性話者](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)
</details>

---
## ロードマップ
- [x] 40k時間のベースモデルとspk_statsファイルをオープンソース化
- [ ] VQエンコーダーとLoraトレーニングコードをオープンソース化
- [ ] テキストをリファインせずにストリーミングオーディオ生成*
- [ ] 複数の感情制御を備えた40k時間バージョンをオープンソース化
- [ ] ChatTTS.cppもしかしたら？（PRや新しいリポジトリが歓迎されます。）

----
## FAQ

##### VRAMはどれくらい必要ですか？推論速度はどうですか？
30秒のオーディオクリップには、少なくとも4GBのGPUメモリが必要です。4090 GPUの場合、約7つの意味トークンに対応するオーディオを1秒あたり生成できます。リアルタイムファクター（RTF）は約0.3です。

##### モデルの安定性が十分でなく、複数の話者や音質が悪いという問題があります。

これは、自己回帰モデル（barkおよびvalleの場合）で一般的に発生する問題です。一般的に避けるのは難しいです。複数のサンプルを試して、適切な結果を見つけることができます。

##### 笑い以外に何か制御できますか？他の感情を制御できますか？

現在リリースされているモデルでは、トークンレベルの制御ユニットは[laugh]、[uv_break]、および[lbreak]のみです。将来のバージョンでは、追加の感情制御機能を備えたモデルをオープンソース化する可能性があります。

---
## 謝辞
- [bark](https://github.com/suno-ai/bark)、[XTTSv2](https://github.com/coqui-ai/TTS)、および[valle](https://arxiv.org/abs/2301.02111)は、自己回帰型システムによる顕著なTTS結果を示しました。
- [fish-speech](https://github.com/fishaudio/fish-speech)は、LLMモデリングのためのオーディオトークナイザーとしてのGVQの能力を明らかにしました。
- 事前トレーニングされたボコーダーとして使用される[vocos](https://github.com/gemelo-ai/vocos)。

---
## 特別感謝
- 初期のアルゴリズム実験をサポートしてくれた[wlu-audio lab](https://audio.westlake.edu.cn/)。



================================================
FILE: docs/kr/README.md
================================================
<div align="center">

<a href="https://trendshift.io/repositories/10489" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10489" alt="2noise%2FChatTTS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

# ChatTTS
일상 대화를 위한 생성형 음성 모델입니다.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)

[![Huggingface](https://img.shields.io/badge/🤗%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)

[**English**](../../README.md) | [**简体中文**](../cn/README.md) | [**日本語**](../jp/README.md) | [**Русский**](../ru/README.md) | [**Español**](../es/README.md) | [**Français**](../fr/README.md) | **한국어**

</div>

> [!NOTE]
> 이 문서는 최신 버전이 아닐 수 있습니다. [영어 문서](../../README.md)를 기준으로 작업하는 것을 권장합니다.

## 프로젝트 소개

> [!Note]
> 이 저장소에는 알고리즘 구조와 간단한 예시들이 포함되어 있습니다.

> [!Tip]
> 이 프로젝트에서 파생된 프로젝트는 커뮤니티가 유지 관리하는 커뮤니티[Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS)를 참조하시길 바랍니다.

ChatTTS는 대화 기반 작업(예: LLM 어시스턴트)을 위해 설계된 텍스트-음성 변환(TTS) 모델입니다.

### 지원 언어

- [x] 영어
- [x] 중국어
- [ ] 계속 추가 예정...

### 프로젝트 특징

> 이 프로젝트의 내용은 **[Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)**에서 제공되는 비디오를 참조하시길 바랍니다.

1. **대화형 TTS**: ChatTTS는 대화 기반 작업에 최적화되어 자연스럽고 표현력 있는 음성 합성을 구현합니다. 다중 화자를 지원하여 상호작용적인 대화를 가능하게 합니다.
2. **세밀한 제어**: 이 모델은 웃음, 일시 정지, 삽입어 등 세밀한 운율적 특징을 예측하고 제어할 수 있습니다.
3. **향상된 운율**: ChatTTS는 운율 측면에서 대부분의 오픈 소스 TTS 모델을 능가하며, 추가 연구와 개발을 지원하기 위해 사전 훈련된 모델을 제공합니다.

### 데이터셋 및 모델
> [!Important]
> 공개된 모델은 학술 목적으로만 사용 가능합니다.

- 주요 모델은 100,000+ 시간의 중국어 및 영어 오디오 데이터를 사용하여 훈련되었습니다.
- **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)**에서 제공되는 오픈 소스 버전은 40,000시간의 사전 훈련된 모델로, SFT가 적용되지 않았습니다.

### 로드맵
- [x] 40,000시간 기반 모델과 spk_stats 파일 오픈 소스화.
- [x] 스트리밍 오디오 생성.
- [x] DVAE 인코더와 제로 샷 추론 코드 오픈 소스화.
- [ ] 다중 감정 제어 기능.
- [ ] ChatTTS.cpp (`2noise` 조직 내의 새로운 저장소를 환영합니다.)

### 라이선스

#### 코드
코드는 `AGPLv3+` 라이선스를 따릅니다.

#### 모델
모델은 `CC BY-NC 4.0` 라이선스로 공개되었습니다. 이 모델은 교육 및 연구 목적으로만 사용되며, 상업적 또는 불법적 목적으로 사용되어서는 안 됩니다. 저자들은 정보의 정확성, 완전성, 신뢰성을 보장하지 않습니다. 이 저장소에서 사용된 정보와 데이터는 학술 및 연구 목적으로만 사용되며, 공개적으로 이용 가능한 출처에서 얻은 데이터입니다. 저자들은 데이터에 대한 소유권 또는 저작권을 주장하지 않습니다.

### 면책 조항

ChatTTS는 강력한 텍스트-음성 변환 시스템입니다. 그렇기에 기술을 책임감 있고 윤리적으로 사용하는 것은 아주 중요합니다. ChatTTS의 악용을 방지하기 위해 40,000시간 모델의 훈련 중 소량의 고주파 노이즈를 추가하고, 오디오 품질을 최대한 압축하여 MP3 형식으로 제공했습니다. 또한, 우리는 내부적으로 탐지 모델을 훈련했으며, 추후 이를 오픈 소스화할 계획입니다.

### 연락처
> GitHub 이슈/PR은 언제든지 환영합니다.

#### 공식 문의
모델 및 로드맵에 대한 공식적인 문의는 **open-source@2noise.com**으로 연락해 주십시오.

#### 온라인 채팅
##### 1. QQ Group (Chinese Social APP)
- **Group 1**, 808364215
- **Group 2**, 230696694
- **Group 3**, 933639842
- **Group 4**, 608667975

##### 2. Discord 서버
[이곳](https://discord.gg/Ud5Jxgx5yD)를 클릭하여 참여하십시오.

## 시작하기
### 레포지토리 클론
```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### 의존성 설치
#### 1. 직접 설치
```bash
pip install --upgrade -r requirements.txt
```

#### 2. Conda에서 설치
```bash
conda create -n chattts
conda activate chattts
pip install -r requirements.txt
```

#### 선택사항: vLLM 설치 (Linux 전용)
```bash
pip install safetensors vllm==0.2.7 torchaudio
```

#### 권장되지 않는 선택사항: NVIDIA GPU 사용 시 TransformerEngine 설치 (Linux 전용)
> [!Warning]
> 설치하지 마십시오!
> TransformerEngine의 적응 작업은 현재 개발 중이며, 아직 제대로 작동하지 않습니다.
> 개발 목적으로만 설치하십시오. 자세한 내용은 #672 및 #676에서 확인할 수 있습니다.

> [!Note]
> 설치 과정은 매우 느립니다.

```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable
```

#### 권장되지 않는 선택사항: FlashAttention-2 설치 (주로 NVIDIA GPU)
> [!Warning]
> 설치하지 마십시오!
> 현재 FlashAttention-2는 [이 이슈](https://github.com/huggingface/transformers/issues/26990)에 따르면 생성 속도를 저하시킵니다.
> 개발 목적으로만 설치하십시오.

> [!Note]
> 지원되는 장치는 [Hugging Face 문서](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2)에서 확인할 수 있습니다.

```bash
pip install flash-attn --no-build-isolation
```

### 빠른 시작
> 아래 명령어를 실행할 때 반드시 프로젝트 루트 디렉토리에서 실행하십시오.

#### 1. WebUI 실행
```bash
python examples/web/webui.py
```

#### 2. 커맨드 라인에서 추론
> 오디오는 `./output_audio_n.mp3`에 저장됩니다.

```bash
python examples/cmd/run.py "Your text 1." "Your text 2."
```

## 설치 방법

1. PyPI에서 안정 버전 설치
```bash
pip install ChatTTS
```

2. GitHub에서 최신 버전 설치
```bash
pip install git+https://github.com/2noise/ChatTTS
```

3. 로컬 디렉토리에서 개발 모드로 설치
```bash
pip install -e .
```

### 기본 사용법

```python
import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) # 성능 향상을 위해 True로 설정 가능

texts = ["PUT YOUR 1st TEXT HERE", "PUT YOUR 2nd TEXT HERE"]

wavs = chat.infer(texts)

for i in range(len(wavs)):
    """
    torchaudio의 버전에 따라 첫 번째 줄이 작동할 수 있고, 다른 버전에서는 두 번째 줄이 작동할 수 있습니다.
    """
    try:
        torchaudio.save(f"basic_output{i}.wav", torch.from_numpy(wavs[i]).unsqueeze(0), 24000)
    except:
        torchaudio.save(f"basic_output{i}.wav", torch.from_numpy(wavs[i]), 24000)
```

### Advanced Usage

```python
###################################
# Sample a speaker from Gaussian.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # add sampled speaker 
    temperature = .3,   # using custom temperature
    top_P = 0.7,        # top P decode
    top_K = 20,         # top K decode
)

###################################
# For sentence level manual control.

# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# For word level manual control.

text = 'What is [uv_break]your favorite english food?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
"""
In some versions of torchaudio, the first line works but in other versions, so does the second line.
"""
try:
    torchaudio.save("word_level_output.wav", torch.from_numpy(wavs[0]).unsqueeze(0), 24000)
except:
    torchaudio.save("word_level_output.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Example: self introduction</h4></summary>

```python
inputs_en = """
chat T T S is a text to speech model designed for dialogue applications. 
[uv_break]it supports mixed language input [uv_break]and offers multi speaker 
capabilities with precise control over prosodic elements like 
[uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation. 
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
""".replace('\n', '') # English is still experimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save("self_introduction_output.wav", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align="center">

**male speaker**

</td>
<td align="center">

**female speaker**

</td>
</tr>
<tr>
<td align="center">

[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align="center">

[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>

</details>

## FAQ

#### 1. VRAM이 얼마나 필요한가요? 추론 속도는 어느 정도인가요?
30초 길이의 오디오 클립을 생성하려면 최소 4GB의 GPU 메모리가 필요합니다. 4090 GPU의 경우 초당 약 7개의 의미 토큰에 해당하는 오디오를 생성할 수 있습니다. 실시간 인자(RTF)는 약 0.3입니다.

#### 2. 모델의 안정성은 불안정하며, 화자가 많은 경우 및 오디오 품질이 저하되는 이슈 존재.

이는 일반적으로 autoregressive 모델(bark 및 valle 등)에서 발생하는 불가피한 문제입니다. 현재로선 여러 번 샘플링하여 적절한 결과를 찾는 것이 최선입니다.

#### 3. 웃음 뿐 아니라 다른 감정도 표현할 수 있나요?

현재 공개된 모델에서는 제어 가능한 토큰은 `[laugh]`, `[uv_break]`, `[lbreak]`입니다. 향후 버전의 모델에서는 추가적인 감정 제어 기능 포함하여 오픈 소스로 제공할 계획입니다.

## 감사의 인사
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS), [valle](https://arxiv.org/abs/2301.02111)는 autoregressive 방식의 시스템으로 뛰어난 TTS 성능을 보여주었습니다.
- [fish-speech](https://github.com/fishaudio/fish-speech)는 LLM 모델링을 위한 오디오 토크나이저로서 GVQ의 능력을 보여주었습니다.
- [vocos](https://github.com/gemelo-ai/vocos)는 사전 훈련된 vocoder로 사용되었습니다.

## 특별 감사
- 초기 알고리즘 실험을 위한 [wlu-audio lab](https://audio.westlake.edu.cn/)에 감사의 말씀을 전합니다.

## 모든 기여자들의 노고에 감사드립니다
[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

<div align="center">

  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>



================================================
FILE: docs/ru/README.md
================================================
# ChatTTS
> [!NOTE]
> Следующая информация может быть не самой последней, пожалуйста, смотрите английскую версию для актуальных данных.

[![Huggingface](https://img.shields.io/badge/🤗%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)

[**English**](../../README.md) | [**简体中文**](../cn/README.md) | [**日本語**](../jp/README.md) | **Русский** | [**Español**](../es/README.md) | [**Français**](../fr/README.md) | [**한국어**](../kr/README.md)

ChatTTS - это модель преобразования текста в речь, специально разработанная для диалоговых сценариев, таких как помощник LLM. Она поддерживает как английский, так и китайский языки. Наша модель обучена на более чем 100 000 часах английского и китайского языков. Открытая версия на **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** - это предварительно обученная модель с 40 000 часами без SFT.

Для официальных запросов о модели и плане развития, пожалуйста, свяжитесь с нами по адресу **open-source@2noise.com**. Вы можете присоединиться к нашей группе QQ: 808364215 для обсуждения. Добавление вопросов на GitHub также приветствуется.

---
## Особенности
1. **Диалоговый TTS**: ChatTTS оптимизирован для задач, основанных на диалогах, что позволяет создавать натуральную и выразительную речь. Он поддерживает несколько говорящих, облегчая интерактивные беседы.
2. **Тонкий контроль**: Модель может предсказывать и контролировать тонкие просодические особенности, включая смех, паузы и вставные слова.
3. **Лучшая просодия**: ChatTTS превосходит большинство открытых моделей TTS с точки зрения просодии. Мы предоставляем предварительно обученные модели для поддержки дальнейших исследований и разработок.

Для подробного описания модели вы можете обратиться к **[видео на Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)**

---

## Отказ от ответственности

Этот репозиторий предназначен только для академических целей. Он предназначен для образовательного и исследовательского использования и не должен использоваться в коммерческих или юридических целях. Авторы не гарантируют точность, полноту или надежность информации. Информация и данные, использованные в этом репозитории, предназначены только для академических и исследовательских целей. Данные получены из общедоступных источников, и авторы не заявляют о каких-либо правах собственности или авторских правах на данные.

ChatTTS - мощная система преобразования текста в речь. Однако очень важно использовать эту технологию ответственно и этично. Чтобы ограничить использование ChatTTS, мы добавили небольшое количество высокочастотного шума во время обучения модели на 40 000 часов и сжали качество аудио как можно больше с помощью формата MP3, чтобы предотвратить возможное использование злоумышленниками в преступных целях. В то же время мы внутренне обучили модель обнаружения и планируем открыть ее в будущем.

---
## Использование

<h4>Базовое использование</h4>

```python
import ChatTTS
from IPython.display import Audio
import torch

chat = ChatTTS.Chat()
chat.load(compile=False) # Установите значение True для лучшей производительности

texts = ["ВВЕДИТЕ ВАШ ТЕКСТ ЗДЕСЬ",]

wavs = chat.infer(texts)

torchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)
```

<h4>Продвинутое использование</h4>

```python
###################################
# Выборка говорящего из Гауссиана.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = {
  'spk_emb': rand_spk, # добавить выбранного говорящего
  'temperature': .3, # использовать пользовательскую температуру
  'top_P': 0.7, # декодирование top P
  'top_K': 20, # декодирование top K
}

###################################
# Для контроля на уровне предложений.

# используйте oral_(0-9), laugh_(0-2), break_(0-7)
# для генерации специального токена в тексте для синтеза.
params_refine_text = {
  'prompt': '[oral_2][laugh_0][break_6]'
} 

wav = chat.infer(texts, params_refine_text=params_refine_text, params_infer_code=params_infer_code)

###################################
# Для контроля на уровне слов.
text = 'Какая ваша любимая английская еда?[uv_break]your favorite english food?[laugh][lbreak]'
wav = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save("output2.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Пример: самопрезентация</h4></summary>

```python
inputs_ru = """
ChatTTS - это модель преобразования текста в речь, разработанная для диалоговых приложений. 
[uv_break]Она поддерживает смешанный языковой ввод [uv_break]и предлагает возможности множественных говорящих 
с точным контролем над просодическими элементами [laugh]как [uv_break]смех[laugh], [uv_break]паузы, [uv_break]и интонацию. 
[uv_break]Она обеспечивает натуральную и выразительную речь,[uv_break]поэтому, пожалуйста,
[uv_break] используйте проект ответственно и на свой страх и риск.[uv_break]
""".replace('\n', '') # Русский язык все еще находится в экспериментальной стадии.

params_refine_text = {
  'prompt': '[oral_2][laugh_0][break_4]'
} 
audio_array_ru = chat.infer(inputs_ru, params_refine_text=params_refine_text)
torchaudio.save("output3.wav", torch.from_numpy(audio_array_ru[0]), 24000)
```
[мужской говорящий](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

[женский говорящий](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)
</details>

---
## План развития
- [x] Открыть исходный код базовой модели на 40 тысяч часов и файла spk_stats
- [ ] Открыть исходный код кодировщика VQ и кода обучения Lora
- [ ] Потоковая генерация аудио без уточнения текста*
- [ ] Открыть исходный код версии на 40 тысяч часов с управлением множественными эмоциями
- [ ] ChatTTS.cpp возможно? (PR или новый репозиторий приветствуются.)

----
## Часто задаваемые вопросы

##### Сколько VRAM мне нужно? Как насчет скорости инференса?
Для 30-секундного аудиоклипа требуется как минимум 4 ГБ памяти GPU. Для GPU 4090, он может генерировать аудио, соответствующее примерно 7 семантическим токенам в секунду. Фактор реального времени (RTF) составляет около 0.3.

##### Стабильность модели кажется недостаточно хорошей, возникают проблемы с множественными говорящими или плохим качеством аудио.

Это проблема, которая обычно возникает с авторегрессивными моделями (для bark и valle). Это обычно трудно избежать. Можно попробовать несколько образцов, чтобы найти подходящий результат.

##### Помимо смеха, можем ли мы контролировать что-то еще? Можем ли мы контролировать другие эмоции?

В текущей выпущенной модели единственными элементами управления на уровне токенов являются [laugh], [uv_break] и [lbreak]. В будущих версиях мы можем открыть модели с дополнительными возможностями контроля эмоций.

---
## Благодарности
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) и [valle](https://arxiv.org/abs/2301.02111) демонстрируют замечательный результат TTS с помощью системы авторегрессивного стиля.
- [fish-speech](https://github.com/fishaudio/fish-speech) показывает возможности GVQ как аудио токенизатора для моделирования LLM.
- [vocos](https://github.com/gemelo-ai/vocos), который используется в качестве предварительно обученного вокодера.

---
## Особая благодарность
- [wlu-audio lab](https://audio.westlake.edu.cn/) за ранние эксперименты с алгоритмами.



================================================
FILE: examples/__init__.py
================================================



================================================
FILE: examples/api/README.md
================================================
# Generating voice with ChatTTS via API

## Install requirements

Install `FastAPI` and `requests`:

```
pip install -r examples/api/requirements.txt
```

## Run API server

```
fastapi dev examples/api/main.py --host 0.0.0.0 --port 8000
```

## Generate audio using requests

```
python examples/api/client.py
```

mp3 audio files will be saved to the `output` directory.



================================================
FILE: examples/api/client.py
================================================
import datetime
import os
import zipfile
from io import BytesIO

import requests

chattts_service_host = os.environ.get("CHATTTS_SERVICE_HOST", "localhost")
chattts_service_port = os.environ.get("CHATTTS_SERVICE_PORT", "8000")

CHATTTS_URL = f"http://{chattts_service_host}:{chattts_service_port}/generate_voice"


# main infer params
body = {
    "text": [
        "四川美食确实以辣闻名，但也有不辣的选择。",
        "比如甜水面、赖汤圆、蛋烘糕、叶儿粑等，这些小吃口味温和，甜而不腻，也很受欢迎。",
    ],
    "stream": False,
    "lang": None,
    "skip_refine_text": True,
    "refine_text_only": False,
    "use_decoder": True,
    "audio_seed": 12345678,
    "text_seed": 87654321,
    "do_text_normalization": True,
    "do_homophone_replacement": False,
}

# refine text params
params_refine_text = {
    "prompt": "",
    "top_P": 0.7,
    "top_K": 20,
    "temperature": 0.7,
    "repetition_penalty": 1,
    "max_new_token": 384,
    "min_new_token": 0,
    "show_tqdm": True,
    "ensure_non_empty": True,
    "stream_batch": 24,
}
body["params_refine_text"] = params_refine_text

# infer code params
params_infer_code = {
    "prompt": "[speed_5]",
    "top_P": 0.1,
    "top_K": 20,
    "temperature": 0.3,
    "repetition_penalty": 1.05,
    "max_new_token": 2048,
    "min_new_token": 0,
    "show_tqdm": True,
    "ensure_non_empty": True,
    "stream_batch": True,
    "spk_emb": None,
}
body["params_infer_code"] = params_infer_code


try:
    response = requests.post(CHATTTS_URL, json=body)
    response.raise_for_status()
    with zipfile.ZipFile(BytesIO(response.content), "r") as zip_ref:
        # save files for each request in a different folder
        dt = datetime.datetime.now()
        ts = int(dt.timestamp())
        tgt = f"./output/{ts}/"
        os.makedirs(tgt, 0o755)
        zip_ref.extractall(tgt)
        print("Extracted files into", tgt)

except requests.exceptions.RequestException as e:
    print(f"Request Error: {e}")



================================================
FILE: examples/api/main.py
================================================
import io
import os
import sys
import zipfile

from fastapi import FastAPI
from fastapi.responses import StreamingResponse


if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

from typing import Optional

import ChatTTS

from tools.audio import pcm_arr_to_mp3_view
from tools.logger import get_logger
import torch


from pydantic import BaseModel
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from tools.normalizer.en import normalizer_en_nemo_text
from tools.normalizer.zh import normalizer_zh_tn

logger = get_logger("Command")

app = FastAPI()


@app.on_event("startup")
async def startup_event():
    global chat

    chat = ChatTTS.Chat(get_logger("ChatTTS"))
    chat.normalizer.register("en", normalizer_en_nemo_text())
    chat.normalizer.register("zh", normalizer_zh_tn())

    logger.info("Initializing ChatTTS...")
    if chat.load(source="huggingface"):
        logger.info("Models loaded successfully.")
    else:
        logger.error("Models load failed.")
        sys.exit(1)


@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request, exc: RequestValidationError):
    logger.error(f"Validation error: {exc.errors()}")
    return JSONResponse(status_code=422, content={"detail": exc.errors()})


class ChatTTSParams(BaseModel):
    text: list[str]
    stream: bool = False
    lang: Optional[str] = None
    skip_refine_text: bool = False
    refine_text_only: bool = False
    use_decoder: bool = True
    do_text_normalization: bool = True
    do_homophone_replacement: bool = False
    params_refine_text: ChatTTS.Chat.RefineTextParams = None
    params_infer_code: ChatTTS.Chat.InferCodeParams


@app.post("/generate_voice")
async def generate_voice(params: ChatTTSParams):
    logger.info("Text input: %s", str(params.text))

    # audio seed
    if params.params_infer_code.manual_seed is not None:
        torch.manual_seed(params.params_infer_code.manual_seed)
        params.params_infer_code.spk_emb = chat.sample_random_speaker()

    # text seed for text refining
    if params.params_refine_text:
        text = chat.infer(
            text=params.text, skip_refine_text=False, refine_text_only=True
        )
        logger.info(f"Refined text: {text}")
    else:
        # no text refining
        text = params.text

    logger.info("Use speaker:")
    logger.info(params.params_infer_code.spk_emb)

    logger.info("Start voice inference.")
    wavs = chat.infer(
        text=text,
        stream=params.stream,
        lang=params.lang,
        skip_refine_text=params.skip_refine_text,
        use_decoder=params.use_decoder,
        do_text_normalization=params.do_text_normalization,
        do_homophone_replacement=params.do_homophone_replacement,
        params_infer_code=params.params_infer_code,
        params_refine_text=params.params_refine_text,
    )
    logger.info("Inference completed.")

    # zip all of the audio files together
    buf = io.BytesIO()
    with zipfile.ZipFile(
        buf, "a", compression=zipfile.ZIP_DEFLATED, allowZip64=False
    ) as f:
        for idx, wav in enumerate(wavs):
            f.writestr(f"{idx}.mp3", pcm_arr_to_mp3_view(wav))
    logger.info("Audio generation successful.")
    buf.seek(0)

    response = StreamingResponse(buf, media_type="application/zip")
    response.headers["Content-Disposition"] = "attachment; filename=audio_files.zip"
    return response



================================================
FILE: examples/api/postScript.py
================================================
import argparse
import datetime
import os
import zipfile
from io import BytesIO

import requests

chattts_service_host = os.environ.get("CHATTTS_SERVICE_HOST", "127.0.0.1")
chattts_service_port = os.environ.get("CHATTTS_SERVICE_PORT", "9900")

CHATTTS_URL = f"http://{chattts_service_host}:{chattts_service_port}/generate_voice"


def parse_arguments():
    parser = argparse.ArgumentParser(description="HTTP client for ChatTTS service")
    parser.add_argument(
        "--text", type=str, nargs="+", required=True, help="Text to synthesize"
    )
    parser.add_argument(
        "--audio_seed", type=int, required=True, help="Audio generation seed"
    )
    parser.add_argument(
        "--text_seed", type=int, required=True, help="Text generation seed"
    )
    parser.add_argument(
        "--stream", type=bool, default=False, help="Enable/disable streaming"
    )
    parser.add_argument("--lang", type=str, default=None, help="Language code for text")
    parser.add_argument(
        "--skip_refine_text", type=bool, default=True, help="Skip text refinement"
    )
    parser.add_argument(
        "--refine_text_only", type=bool, default=False, help="Only refine text"
    )
    parser.add_argument(
        "--use_decoder", type=bool, default=True, help="Use decoder during inference"
    )
    parser.add_argument(
        "--do_text_normalization",
        type=bool,
        default=True,
        help="Enable text normalization",
    )
    parser.add_argument(
        "--do_homophone_replacement",
        type=bool,
        default=False,
        help="Enable homophone replacement",
    )
    parser.add_argument(
        "--tgt",
        type=str,
        default="./output",
        help="Target directory to save output files",
    )
    parser.add_argument(
        "--filename",
        type=str,
        default="test.mp3",
        help="Target directory to save output files",
    )

    # Refinement text parameters
    parser.add_argument(
        "--refine_prompt", type=str, default="", help="Prompt for text refinement"
    )
    parser.add_argument(
        "--refine_top_P",
        type=float,
        default=0.7,
        help="Top P value for text refinement",
    )
    parser.add_argument(
        "--refine_top_K", type=int, default=20, help="Top K value for text refinement"
    )
    parser.add_argument(
        "--refine_temperature",
        type=float,
        default=0.7,
        help="Temperature for text refinement",
    )
    parser.add_argument(
        "--refine_repetition_penalty",
        type=float,
        default=1.0,
        help="Repetition penalty for text refinement",
    )
    parser.add_argument(
        "--refine_max_new_token",
        type=int,
        default=384,
        help="Max new tokens for text refinement",
    )
    parser.add_argument(
        "--refine_min_new_token",
        type=int,
        default=0,
        help="Min new tokens for text refinement",
    )
    parser.add_argument(
        "--refine_show_tqdm",
        type=bool,
        default=True,
        help="Show progress bar for text refinement",
    )
    parser.add_argument(
        "--refine_ensure_non_empty",
        type=bool,
        default=True,
        help="Ensure non-empty output",
    )
    parser.add_argument(
        "--refine_stream_batch",
        type=int,
        default=24,
        help="Stream batch size for refinement",
    )

    # Infer code parameters
    parser.add_argument(
        "--infer_prompt", type=str, default="[speed_5]", help="Prompt for inference"
    )
    parser.add_argument(
        "--infer_top_P", type=float, default=0.1, help="Top P value for inference"
    )
    parser.add_argument(
        "--infer_top_K", type=int, default=20, help="Top K value for inference"
    )
    parser.add_argument(
        "--infer_temperature", type=float, default=0.3, help="Temperature for inference"
    )
    parser.add_argument(
        "--infer_repetition_penalty",
        type=float,
        default=1.05,
        help="Repetition penalty for inference",
    )
    parser.add_argument(
        "--infer_max_new_token",
        type=int,
        default=2048,
        help="Max new tokens for inference",
    )
    parser.add_argument(
        "--infer_min_new_token",
        type=int,
        default=0,
        help="Min new tokens for inference",
    )
    parser.add_argument(
        "--infer_show_tqdm",
        type=bool,
        default=True,
        help="Show progress bar for inference",
    )
    parser.add_argument(
        "--infer_ensure_non_empty",
        type=bool,
        default=True,
        help="Ensure non-empty output",
    )
    parser.add_argument(
        "--infer_stream_batch",
        type=bool,
        default=True,
        help="Stream batch for inference",
    )
    parser.add_argument(
        "--infer_spk_emb",
        type=str,
        default=None,
        help="Speaker embedding for inference",
    )

    return parser.parse_args()


def main():
    args = parse_arguments()

    # Main infer params
    body = {
        "text": args.text,
        "stream": args.stream,
        "lang": args.lang,
        "filename": args.filename,
        "skip_refine_text": args.skip_refine_text,
        "refine_text_only": args.refine_text_only,
        "use_decoder": args.use_decoder,
        "audio_seed": args.audio_seed,
        "text_seed": args.text_seed,
        "do_text_normalization": args.do_text_normalization,
        "do_homophone_replacement": args.do_homophone_replacement,
    }
    # Refinement text parameters
    params_refine_text = {
        "prompt": args.refine_prompt,
        "top_P": args.refine_top_P,
        "top_K": args.refine_top_K,
        "temperature": args.refine_temperature,
        "repetition_penalty": args.refine_repetition_penalty,
        "max_new_token": args.refine_max_new_token,
        "min_new_token": args.refine_min_new_token,
        "show_tqdm": args.refine_show_tqdm,
        "ensure_non_empty": args.refine_ensure_non_empty,
        "stream_batch": args.refine_stream_batch,
    }
    body["params_refine_text"] = params_refine_text

    # Infer code parameters
    params_infer_code = {
        "prompt": args.infer_prompt,
        "top_P": args.infer_top_P,
        "top_K": args.infer_top_K,
        "temperature": args.infer_temperature,
        "repetition_penalty": args.infer_repetition_penalty,
        "max_new_token": args.infer_max_new_token,
        "min_new_token": args.infer_min_new_token,
        "show_tqdm": args.infer_show_tqdm,
        "ensure_non_empty": args.infer_ensure_non_empty,
        "stream_batch": args.infer_stream_batch,
        "spk_emb": args.infer_spk_emb,
    }
    body["params_infer_code"] = params_infer_code

    try:
        response = requests.post(CHATTTS_URL, json=body)
        response.raise_for_status()
        with zipfile.ZipFile(BytesIO(response.content), "r") as zip_ref:
            tgt = args.tgt
            # filename=args.filename
            os.makedirs(tgt, exist_ok=True)
            zip_ref.extractall(tgt)
            print(f"Extracted files:{tgt}/{filename}")
            # print(tgt)
    except requests.exceptions.RequestException as e:
        print(f"Request Error: {e}")


if __name__ == "__main__":
    main()



================================================
FILE: examples/api/requirements.txt
================================================
fastapi
requests



================================================
FILE: examples/cmd/run.py
================================================
import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

from typing import Optional, List
import argparse

import numpy as np

import ChatTTS

from tools.logger import get_logger
from tools.audio import pcm_arr_to_mp3_view
from tools.normalizer.en import normalizer_en_nemo_text
from tools.normalizer.zh import normalizer_zh_tn

logger = get_logger("Command")


def save_mp3_file(wav, index):
    data = pcm_arr_to_mp3_view(wav)
    mp3_filename = f"output_audio_{index}.mp3"
    with open(mp3_filename, "wb") as f:
        f.write(data)
    logger.info(f"Audio saved to {mp3_filename}")


def load_normalizer(chat: ChatTTS.Chat):
    # try to load normalizer
    try:
        chat.normalizer.register("en", normalizer_en_nemo_text())
    except ValueError as e:
        logger.error(e)
    except BaseException:
        logger.warning("Package nemo_text_processing not found!")
        logger.warning(
            "Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing",
        )
    try:
        chat.normalizer.register("zh", normalizer_zh_tn())
    except ValueError as e:
        logger.error(e)
    except BaseException:
        logger.warning("Package WeTextProcessing not found!")
        logger.warning(
            "Run: conda install -c conda-forge pynini=2.1.5 && pip install WeTextProcessing",
        )


def main(
    texts: List[str],
    spk: Optional[str] = None,
    stream: bool = False,
    source: str = "local",
    custom_path: str = "",
):
    logger.info("Text input: %s", str(texts))

    chat = ChatTTS.Chat(get_logger("ChatTTS"))
    logger.info("Initializing ChatTTS...")
    load_normalizer(chat)

    is_load = False
    if os.path.isdir(custom_path) and source == "custom":
        is_load = chat.load(source="custom", custom_path=custom_path)
    else:
        is_load = chat.load(source=source)

    if is_load:
        logger.info("Models loaded successfully.")
    else:
        logger.error("Models load failed.")
        sys.exit(1)

    if spk is None:
        spk = chat.sample_random_speaker()
    logger.info("Use speaker:")
    print(spk)

    logger.info("Start inference.")
    wavs = chat.infer(
        texts,
        stream,
        params_infer_code=ChatTTS.Chat.InferCodeParams(
            spk_emb=spk,
        ),
    )
    logger.info("Inference completed.")
    # Save each generated wav file to a local file
    if stream:
        wavs_list = []
    for index, wav in enumerate(wavs):
        if stream:
            for i, w in enumerate(wav):
                save_mp3_file(w, (i + 1) * 1000 + index)
            wavs_list.append(wav)
        else:
            save_mp3_file(wav, index)
    if stream:
        for index, wav in enumerate(np.concatenate(wavs_list, axis=1)):
            save_mp3_file(wav, index)
    logger.info("Audio generation successful.")


if __name__ == "__main__":
    r"""
    python -m examples.cmd.run \
        --source custom --custom_path ../../models/2Noise/ChatTTS 你好喲 ":)"
    """
    logger.info("Starting ChatTTS commandline demo...")
    parser = argparse.ArgumentParser(
        description="ChatTTS Command",
        usage='[--spk xxx] [--stream] [--source ***] [--custom_path XXX] "Your text 1." " Your text 2."',
    )
    parser.add_argument(
        "--spk",
        help="Speaker (empty to sample a random one)",
        type=Optional[str],
        default=None,
    )
    parser.add_argument(
        "--stream",
        help="Use stream mode",
        action="store_true",
    )
    parser.add_argument(
        "--source",
        help="source form [ huggingface(hf download), local(ckpt save to asset dir), custom(define) ]",
        type=str,
        default="local",
    )
    parser.add_argument(
        "--custom_path",
        help="custom defined model path(include asset ckpt dir)",
        type=str,
        default="",
    )
    parser.add_argument(
        "texts",
        help="Original text",
        default=["YOUR TEXT HERE"],
        nargs=argparse.REMAINDER,
    )
    args = parser.parse_args()
    logger.info(args)
    main(args.texts, args.spk, args.stream, args.source, args.custom_path)
    logger.info("ChatTTS process finished.")



================================================
FILE: examples/cmd/stream.py
================================================
import random

import numpy as np

from tools.audio import float_to_int16


# 流式推理数据获取器，支持流式获取音频编码字节流
class ChatStreamer:
    def __init__(self, base_block_size=8000):
        self.base_block_size = base_block_size

    # stream状态更新。数据量不足的stream，先存一段时间，直到拿到足够数据，监控小块数据情况
    @staticmethod
    def _update_stream(history_stream_wav, new_stream_wav, thre):
        if history_stream_wav is not None:
            result_stream = np.concatenate([history_stream_wav, new_stream_wav], axis=1)
            is_keep_next = result_stream.shape[0] * result_stream.shape[1] < thre
            if random.random() > 0.1:
                print(
                    "update_stream",
                    is_keep_next,
                    [i.shape if i is not None else None for i in result_stream],
                )
        else:
            result_stream = new_stream_wav
            is_keep_next = result_stream.shape[0] * result_stream.shape[1] < thre

        return result_stream, is_keep_next

    # 已推理batch数据保存
    @staticmethod
    def _accum(accum_wavs, stream_wav):
        if accum_wavs is None:
            accum_wavs = stream_wav
        else:
            accum_wavs = np.concatenate([accum_wavs, stream_wav], axis=1)
        return accum_wavs

    # batch stream数据格式转化
    @staticmethod
    def batch_stream_formatted(stream_wav, output_format="PCM16_byte"):
        if output_format in ("PCM16_byte", "PCM16"):
            format_data = float_to_int16(stream_wav)
        else:
            format_data = stream_wav
        return format_data

    # 数据格式转化
    @staticmethod
    def formatted(data, output_format="PCM16_byte"):
        if output_format == "PCM16_byte":
            format_data = data.astype("<i2").tobytes()
        else:
            format_data = data
        return format_data

    # 检查声音是否为空
    @staticmethod
    def checkvoice(data):
        if np.abs(data).max() < 1e-6:
            return False
        else:
            return True

    # 将声音进行适当拆分返回
    @staticmethod
    def _subgen(data, thre=12000):
        for stard_idx in range(0, data.shape[0], thre):
            end_idx = stard_idx + thre
            yield data[stard_idx:end_idx]

    # 流式数据获取，支持获取音频编码字节流
    def generate(self, streamchat, output_format=None):
        assert output_format in ("PCM16_byte", "PCM16", None)
        curr_sentence_index = 0
        history_stream_wav = None
        article_streamwavs = None
        for stream_wav in streamchat:
            print(np.abs(stream_wav).max(axis=1))
            n_texts = len(stream_wav)
            n_valid_texts = (np.abs(stream_wav).max(axis=1) > 1e-6).sum()
            if n_valid_texts == 0:
                continue
            else:
                block_thre = n_valid_texts * self.base_block_size
                stream_wav, is_keep_next = ChatStreamer._update_stream(
                    history_stream_wav, stream_wav, block_thre
                )
                # 数据量不足，先保存状态
                if is_keep_next:
                    history_stream_wav = stream_wav
                    continue
                # 数据量足够，执行写入操作
                else:
                    history_stream_wav = None
                    stream_wav = ChatStreamer.batch_stream_formatted(
                        stream_wav, output_format
                    )
                    article_streamwavs = ChatStreamer._accum(
                        article_streamwavs, stream_wav
                    )
                    # 写入当前句子
                    if ChatStreamer.checkvoice(stream_wav[curr_sentence_index]):
                        for sub_wav in ChatStreamer._subgen(
                            stream_wav[curr_sentence_index]
                        ):
                            if ChatStreamer.checkvoice(sub_wav):
                                yield ChatStreamer.formatted(sub_wav, output_format)
                    # 当前句子已写入完成，直接写下一个句子已经推理完成的部分
                    elif curr_sentence_index < n_texts - 1:
                        curr_sentence_index += 1
                        print("add next sentence")
                        finish_stream_wavs = article_streamwavs[curr_sentence_index]

                        for sub_wav in ChatStreamer._subgen(finish_stream_wavs):
                            if ChatStreamer.checkvoice(sub_wav):
                                yield ChatStreamer.formatted(sub_wav, output_format)

                    # streamchat遍历完毕，在外层把剩余结果写入
                    else:
                        break
        # 本轮剩余最后一点数据写入
        if is_keep_next:
            if len(list(filter(lambda x: x is not None, stream_wav))) > 0:
                stream_wav = ChatStreamer.batch_stream_formatted(
                    stream_wav, output_format
                )
                if ChatStreamer.checkvoice(stream_wav[curr_sentence_index]):

                    for sub_wav in ChatStreamer._subgen(
                        stream_wav[curr_sentence_index]
                    ):
                        if ChatStreamer.checkvoice(sub_wav):
                            yield ChatStreamer.formatted(sub_wav, output_format)
                    article_streamwavs = ChatStreamer._accum(
                        article_streamwavs, stream_wav
                    )
        # 把已经完成推理的下几轮剩余数据写入
        for i_text in range(curr_sentence_index + 1, n_texts):
            finish_stream_wavs = article_streamwavs[i_text]

            for sub_wav in ChatStreamer._subgen(finish_stream_wavs):
                if ChatStreamer.checkvoice(sub_wav):
                    yield ChatStreamer.formatted(sub_wav, output_format)

    # 流式播放接口
    def play(self, streamchat, wait=5):
        import pyaudio  # please install it manually

        p = pyaudio.PyAudio()
        print(p.get_device_count())
        # 设置音频流参数
        FORMAT = pyaudio.paInt16  # 16位深度
        CHANNELS = 1  # 单声道
        RATE = 24000  # 采样率
        CHUNK = 1024  # 每块音频数据大小

        # 打开输出流（扬声器）
        stream_out = p.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=RATE,
            output=True,
        )

        first_prefill_size = wait * RATE
        prefill_bytes = b""
        meet = False
        for i in self.generate(streamchat, output_format="PCM16_byte"):
            if not meet:
                prefill_bytes += i
                if len(prefill_bytes) > first_prefill_size:
                    meet = True
                    stream_out.write(prefill_bytes)
            else:
                stream_out.write(i)
        if not meet:
            stream_out.write(prefill_bytes)

        stream_out.stop_stream()
        stream_out.close()


if __name__ == "__main__":
    import ChatTTS

    # 加载 ChatTTS
    chat = ChatTTS.Chat()
    chat.load(compile=False)

    rand_spk = chat.sample_random_speaker()
    params_infer_code = ChatTTS.Chat.InferCodeParams(
        spk_emb=rand_spk,  # add sampled speaker
        temperature=0.3,  # using custom temperature
        top_P=0.7,  # top P decode
        top_K=20,  # top K decode
    )

    # 获取ChatTTS 流式推理generator
    streamchat = chat.infer(
        [
            "总结一下，AI Agent是大模型功能的扩展，让AI更接近于通用人工智能，也就是我们常说的AGI。",
            "你太聪明啦。",
            "举个例子，大模型可能可以写代码，但它不能独立完成一个完整的软件开发项目。这时候，AI Agent就根据大模型的智能，结合记忆和规划，一步步实现从需求分析到产品上线。",
        ],
        skip_refine_text=True,
        stream=True,
        params_infer_code=params_infer_code,
    )
    # 先存放一部分，存的差不多了再播放，适合生成速度比较慢的cpu玩家使用
    ChatStreamer().play(streamchat, wait=5)



================================================
FILE: examples/ipynb/colab.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## Clone Repo
"""

!cd /content
!rm -rf sample_data ChatTTS
!git clone https://github.com/2noise/ChatTTS.git

"""
## Install Requirements
"""

!pip install -r /content/ChatTTS/requirements.txt
!ldconfig /usr/lib64-nvidia

"""
## Import Packages
"""

import torch

torch._dynamo.config.cache_size_limit = 64
torch._dynamo.config.suppress_errors = True
torch.set_float32_matmul_precision("high")

from ChatTTS import ChatTTS
from ChatTTS.tools.logger import get_logger
from ChatTTS.tools.normalizer import normalizer_en_nemo_text, normalizer_zh_tn
from IPython.display import Audio

"""
## Load Models
"""

logger = get_logger("ChatTTS", format_root=True)
chat = ChatTTS.Chat(logger)

# try to load normalizer
try:
    chat.normalizer.register("en", normalizer_en_nemo_text())
except ValueError as e:
    logger.error(e)
except:
    logger.warning("Package nemo_text_processing not found!")
    logger.warning(
        "Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing",
    )
try:
    chat.normalizer.register("zh", normalizer_zh_tn())
except ValueError as e:
    logger.error(e)
except:
    logger.warning("Package WeTextProcessing not found!")
    logger.warning(
        "Run: conda install -c conda-forge pynini=2.1.5 && pip install WeTextProcessing",
    )

"""
### Here are three choices for loading models,
"""

"""
#### 1. Load models from Hugging Face (recommend)
"""

# use force_redownload=True if the weights have been updated.
chat.load(source="huggingface")

"""
#### 2. Load models from local directories 'asset' and 'config'
"""

chat.load()
# chat.load(source='local') same as above

"""
#### 3. Load models from a custom path
"""

# write the model path into custom_path
chat.load(source="custom", custom_path="YOUR CUSTOM PATH")

"""
### You can also unload models to save the memory
"""

chat.unload()

"""
## Inference
"""

"""
### Batch infer
"""

texts = [
    "So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.",
] * 3 + [
    "我觉得像我们这些写程序的人，他，我觉得多多少少可能会对开源有一种情怀在吧我觉得开源是一个很好的形式。现在其实最先进的技术掌握在一些公司的手里的话，就他们并不会轻易的开放给所有的人用。"
] * 3

wavs = chat.infer(texts)

Audio(wavs[0], rate=24_000, autoplay=True)

Audio(wavs[3], rate=24_000, autoplay=True)

"""
### Custom params
"""

params_infer_code = ChatTTS.Chat.InferCodeParams(
    prompt="[speed_5]",
    temperature=0.3,
)
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt="[oral_2][laugh_0][break_6]",
)

wav = chat.infer(
    "四川美食可多了，有麻辣火锅、宫保鸡丁、麻婆豆腐、担担面、回锅肉、夫妻肺片等，每样都让人垂涎三尺。",
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

Audio(wav[0], rate=24_000, autoplay=True)

"""
### fix random speaker
"""

rand_spk = chat.sample_random_speaker()
print(rand_spk)  # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb=rand_spk,
)

wav = chat.infer(
    "四川美食确实以辣闻名，但也有不辣的选择。比如甜水面、赖汤圆、蛋烘糕、叶儿粑等，这些小吃口味温和，甜而不腻，也很受欢迎。",
    params_infer_code=params_infer_code,
)

Audio(wav[0], rate=24_000, autoplay=True)

"""
### Zero shot (simulate speaker)
"""

from ChatTTS.tools.audio import load_audio

spk_smp = chat.sample_audio_speaker(load_audio("sample.mp3", 24000))
print(spk_smp)  # save it in order to load the speaker without sample audio next time

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_smp=spk_smp,
    txt_smp="与sample.mp3内容完全一致的文本转写。",
)

wav = chat.infer(
    "四川美食确实以辣闻名，但也有不辣的选择。比如甜水面、赖汤圆、蛋烘糕、叶儿粑等，这些小吃口味温和，甜而不腻，也很受欢迎。",
    params_infer_code=params_infer_code,
)

Audio(wav[0], rate=24_000, autoplay=True)

"""
### Two stage control
"""

text = "So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with."
refined_text = chat.infer(text, refine_text_only=True)
refined_text

wav = chat.infer(refined_text, skip_refine_text=True)

Audio(wav[0], rate=24_000, autoplay=True)

"""
## LLM Call
"""

from ChatTTS.tools.llm import ChatOpenAI

API_KEY = ""
client = ChatOpenAI(
    api_key=API_KEY, base_url="https://api.deepseek.com", model="deepseek-chat"
)

user_question = "四川有哪些好吃的美食呢?"

text = client.call(user_question, prompt_version="deepseek")
text

text = client.call(text, prompt_version="deepseek_TN")
text

wav = chat.infer(text)

Audio(wav[0], rate=24_000, autoplay=True)



================================================
FILE: examples/ipynb/example.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## Import packages
"""

import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

if not "root_dir" in globals():
    now_dir = os.getcwd()  # skip examples/ipynb
    root_dir = os.path.join(now_dir, "../../")
    sys.path.append(root_dir)
    print("init root dir to", root_dir)

import torch

torch._dynamo.config.cache_size_limit = 64
torch._dynamo.config.suppress_errors = True
torch.set_float32_matmul_precision("high")

import ChatTTS
from tools.logger import get_logger
from tools.normalizer import normalizer_en_nemo_text, normalizer_zh_tn
from IPython.display import Audio

"""
## Load Models
"""

os.chdir(root_dir)

logger = get_logger("ChatTTS")
chat = ChatTTS.Chat(logger)

# try to load normalizer
try:
    chat.normalizer.register("en", normalizer_en_nemo_text())
except ValueError as e:
    logger.error(e)
except:
    logger.warning("Package nemo_text_processing not found!")
    logger.warning(
        "Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing",
    )
try:
    chat.normalizer.register("zh", normalizer_zh_tn())
except ValueError as e:
    logger.error(e)
except:
    logger.warning("Package WeTextProcessing not found!")
    logger.warning(
        "Run: conda install -c conda-forge pynini=2.1.5 && pip install WeTextProcessing",
    )

"""
### Here are three choices for loading models,
"""

"""
#### 1. Load models from Hugging Face (not suitable in CN)
"""

# use force_redownload=True if the weights have been updated.
chat.load(source="huggingface", force_redownload=True)

"""
#### 2. Load models from local directories 'asset' and 'config' (recommend)
"""

chat.load()
# chat.load(source='local') same as above

"""
#### 3. Load models from a custom path
"""

# write the model path into custom_path
chat.load(source="custom", custom_path="YOUR CUSTOM PATH")

"""
### You can also unload models to save the memory
"""

chat.unload()

"""
## Inference
"""

"""
### Batch infer
"""

texts = [
    "So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.",
] * 3 + [
    "我觉得像我们这些写程序的人，他，我觉得多多少少可能会对开源有一种情怀在吧我觉得开源是一个很好的形式。现在其实最先进的技术掌握在一些公司的手里的话，就他们并不会轻易的开放给所有的人用。"
] * 3

wavs = chat.infer(texts)

Audio(wavs[0], rate=24_000, autoplay=True)

Audio(wavs[3], rate=24_000, autoplay=True)

"""
### Custom params
"""

params_infer_code = ChatTTS.Chat.InferCodeParams(
    prompt="[speed_5]",
    temperature=0.3,
)
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt="[oral_2][laugh_0][break_6]",
)

wav = chat.infer(
    "四川美食可多了，有麻辣火锅、宫保鸡丁、麻婆豆腐、担担面、回锅肉、夫妻肺片等，每样都让人垂涎三尺。",
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

Audio(wav[0], rate=24_000, autoplay=True)

"""
### Fix random speaker
"""

rand_spk = chat.sample_random_speaker()
print(rand_spk)  # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb=rand_spk,
)

wav = chat.infer(
    "四川美食确实以辣闻名，但也有不辣的选择。比如甜水面、赖汤圆、蛋烘糕、叶儿粑等，这些小吃口味温和，甜而不腻，也很受欢迎。",
    params_infer_code=params_infer_code,
)

Audio(wav[0], rate=24_000, autoplay=True)

"""
### Zero shot (simulate speaker)
"""

from tools.audio import load_audio

spk_smp = chat.sample_audio_speaker(load_audio("sample.mp3", 24000))
print(spk_smp)  # save it in order to load the speaker without sample audio next time

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_smp=spk_smp,
    txt_smp="与sample.mp3内容完全一致的文本转写。",
)

wav = chat.infer(
    "四川美食确实以辣闻名，但也有不辣的选择。比如甜水面、赖汤圆、蛋烘糕、叶儿粑等，这些小吃口味温和，甜而不腻，也很受欢迎。",
    params_infer_code=params_infer_code,
)

Audio(wav[0], rate=24_000, autoplay=True)

"""
### Two stage control
"""

text = "So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with."
refined_text = chat.infer(text, refine_text_only=True)
refined_text

wav = chat.infer(refined_text, skip_refine_text=True)

Audio(wav[0], rate=24_000, autoplay=True)

"""
## LLM Call
"""

from tools.llm import ChatOpenAI

API_KEY = ""
client = ChatOpenAI(
    api_key=API_KEY, base_url="https://api.deepseek.com", model="deepseek-chat"
)

user_question = "四川有哪些好吃的美食呢?"

text = client.call(user_question, prompt_version="deepseek")
text

text = client.call(text, prompt_version="deepseek_TN")
text

wav = chat.infer(text)

Audio(wav[0], rate=24_000, autoplay=True)



================================================
FILE: examples/onnx/README.md
================================================
# Export onnx or JIT models for deployment

## Run `pip install onnx -U`.

## Export GPT

3. Run `python examples/onnx/exporter.py --gpt`


## Export other models
Run `python examples/onnx/exporter.py --decoder --vocos`

## Reference
[Run LLMs on Sophon TPU](https://github.com/sophgo/LLM-TPU)


================================================
FILE: examples/onnx/exporter.py
================================================
import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

from dataclasses import asdict
import argparse
import torch
from tqdm import tqdm
from ChatTTS.model.dvae import DVAE
from ChatTTS.config import Config
from vocos import Vocos
from vocos.pretrained import instantiate_class
import torch.jit as jit

from gpt import GPT

# disable cuda
torch.cuda.is_available = lambda: False

# add args to control which modules to export
parser = argparse.ArgumentParser()
parser.add_argument("--gpt", action="store_true", help="trace gpt")
parser.add_argument("--decoder", action="store_true", help="trace decoder")
parser.add_argument("--vocos", action="store_true", help="trace vocos")
parser.add_argument(
    "--pth_dir", default="./assets", type=str, help="path to the pth model directory"
)
parser.add_argument(
    "--out_dir", default="./tmp", type=str, help="path to output directory"
)

args = parser.parse_args()
chattts_config = Config()


def export_gpt():
    gpt_model = GPT(gpt_config=asdict(chattts_config.gpt), use_flash_attn=False).eval()
    gpt_model.from_pretrained(asdict(chattts_config.path)["gpt_ckpt_path"])
    gpt_model = gpt_model.eval()
    for param in gpt_model.parameters():
        param.requires_grad = False

    config = gpt_model.gpt.config
    layers = gpt_model.gpt.layers
    model_norm = gpt_model.gpt.norm

    NUM_OF_LAYERS = config.num_hidden_layers
    HIDDEN_SIZE = config.hidden_size
    NUM_ATTENTION_HEADS = config.num_attention_heads
    NUM_KEY_VALUE_HEADS = config.num_key_value_heads
    HEAD_DIM = HIDDEN_SIZE // NUM_ATTENTION_HEADS  # 64
    TEXT_VOCAB_SIZE = gpt_model.emb_text.weight.shape[0]
    AUDIO_VOCAB_SIZE = gpt_model.emb_code[0].weight.shape[0]
    SEQ_LENGTH = 512

    folder = os.path.join(args.out_dir, "gpt")
    os.makedirs(folder, exist_ok=True)

    for param in gpt_model.emb_text.parameters():
        param.requires_grad = False

    for param in gpt_model.emb_code.parameters():
        param.requires_grad = False

    for param in gpt_model.head_code.parameters():
        param.requires_grad = False

    for param in gpt_model.head_text.parameters():
        param.requires_grad = False

    class EmbeddingText(torch.nn.Module):
        def __init__(self, *args, **kwargs) -> None:
            super().__init__(*args, **kwargs)

        def forward(self, input_ids):
            return gpt_model.emb_text(input_ids)

    def convert_embedding_text():
        model = EmbeddingText()
        input_ids = torch.tensor([range(SEQ_LENGTH)])

        torch.onnx.export(
            model,
            (input_ids),
            f"{folder}/embedding_text.onnx",
            verbose=False,
            input_names=["input_ids"],
            output_names=["input_embed"],
            do_constant_folding=True,
            opset_version=15,
        )

    class EmbeddingCode(torch.nn.Module):
        def __init__(self, *args, **kwargs) -> None:
            super().__init__(*args, **kwargs)

        def forward(self, input_ids):
            input_ids = input_ids.unsqueeze(2).expand(
                -1, -1, gpt_model.num_vq
            )  # for forward_first_code
            code_emb = [
                gpt_model.emb_code[i](input_ids[:, :, i])
                for i in range(gpt_model.num_vq)
            ]
            return torch.stack(code_emb, 2).sum(2)

    def convert_embedding_code():
        model = EmbeddingCode()
        input_ids = torch.tensor([range(SEQ_LENGTH)])

        torch.onnx.export(
            model,
            (input_ids),
            f"{folder}/embedding_code.onnx",
            verbose=False,
            input_names=["input_ids"],
            output_names=["input_embed"],
            do_constant_folding=True,
            opset_version=15,
        )

    class EmbeddingCodeCache(torch.nn.Module):  # for forward_next_code
        def __init__(self, *args, **kwargs) -> None:
            super().__init__(*args, **kwargs)

        def forward(self, input_ids):
            code_emb = [
                gpt_model.emb_code[i](input_ids[:, :, i])
                for i in range(gpt_model.num_vq)
            ]
            return torch.stack(code_emb, 2).sum(2)

    def convert_embedding_code_cache():
        model = EmbeddingCodeCache()
        input_ids = torch.tensor(
            [[[416, 290, 166, 212]]]
        )  # torch.tensor([[range(gpt_model.num_vq)]])
        torch.onnx.export(
            model,
            (input_ids),
            f"{folder}/embedding_code_cache.onnx",
            verbose=False,
            input_names=["input_ids"],
            output_names=["input_embed"],
            do_constant_folding=True,
            opset_version=15,
        )

    class Block(torch.nn.Module):
        def __init__(self, layer_id):
            super().__init__()
            self.layer_id = layer_id
            self.layer = layers[layer_id]  # LlamaDecoderLayer
            self.norm = model_norm

        def forward(self, hidden_states, position_ids, attention_mask):
            hidden_states, past_kv = self.layer(
                hidden_states=hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                use_cache=True,
            )
            present_k, present_v = past_kv
            if self.layer_id == NUM_OF_LAYERS - 1:
                hidden_states = self.norm(hidden_states)
            return hidden_states, present_k, present_v

    def convert_block(layer_id):
        model = Block(layer_id)
        hidden_states = torch.randn((1, SEQ_LENGTH, HIDDEN_SIZE))
        position_ids = torch.tensor([range(SEQ_LENGTH)], dtype=torch.long)
        attention_mask = -1000 * torch.ones(
            (1, 1, SEQ_LENGTH, SEQ_LENGTH), dtype=torch.float32
        ).triu(diagonal=1)
        model(hidden_states, position_ids, attention_mask)
        torch.onnx.export(
            model,
            (hidden_states, position_ids, attention_mask),
            f"{folder}/block_{layer_id}.onnx",
            verbose=False,
            input_names=["input_states", "position_ids", "attention_mask"],
            output_names=["hidden_states", "past_k", "past_v"],
            do_constant_folding=True,
            opset_version=15,
        )

    class BlockCache(torch.nn.Module):

        def __init__(self, layer_id):
            super().__init__()
            self.layer_id = layer_id
            self.layer = layers[layer_id]
            self.norm = model_norm

        def forward(self, hidden_states, position_ids, attention_mask, past_k, past_v):
            hidden_states, past_kv = self.layer(
                hidden_states,
                attention_mask,
                position_ids=position_ids,
                past_key_value=(past_k, past_v),
                use_cache=True,
            )
            present_k, present_v = past_kv
            if self.layer_id == NUM_OF_LAYERS - 1:
                hidden_states = self.norm(hidden_states)
            return hidden_states, present_k, present_v

    def convert_block_cache(layer_id):
        model = BlockCache(layer_id)
        hidden_states = torch.randn((1, 1, HIDDEN_SIZE))
        position_ids = torch.tensor([range(1)], dtype=torch.long)
        attention_mask = -1000 * torch.ones(
            (1, 1, 1, SEQ_LENGTH + 1), dtype=torch.float32
        ).triu(diagonal=1)
        past_k = torch.randn((1, SEQ_LENGTH, NUM_ATTENTION_HEADS, HEAD_DIM))
        past_v = torch.randn((1, SEQ_LENGTH, NUM_ATTENTION_HEADS, HEAD_DIM))

        torch.onnx.export(
            model,
            (hidden_states, position_ids, attention_mask, past_k, past_v),
            f"{folder}/block_cache_{layer_id}.onnx",
            verbose=False,
            input_names=[
                "input_states",
                "position_ids",
                "attention_mask",
                "history_k",
                "history_v",
            ],
            output_names=["hidden_states", "past_k", "past_v"],
            do_constant_folding=True,
            opset_version=15,
        )

    class GreedyHead(torch.nn.Module):

        def __init__(self):
            super().__init__()

        def forward(self, m_logits):
            _, token = torch.topk(m_logits.float(), 1)
            return token

    def convert_greedy_head_text():
        model = GreedyHead()
        m_logits = torch.randn(1, TEXT_VOCAB_SIZE)

        torch.onnx.export(
            model,
            (m_logits),
            f"{folder}/greedy_head_text.onnx",
            verbose=False,
            input_names=["m_logits"],
            output_names=["token"],
            do_constant_folding=True,
            opset_version=15,
        )

    def convert_greedy_head_code():
        model = GreedyHead()
        m_logits = torch.randn(1, AUDIO_VOCAB_SIZE, gpt_model.num_vq)

        torch.onnx.export(
            model,
            (m_logits),
            f"{folder}/greedy_head_code.onnx",
            verbose=False,
            input_names=["m_logits"],
            output_names=["token"],
            do_constant_folding=True,
            opset_version=15,
        )

    class LmHead_infer_text(torch.nn.Module):
        def __init__(self):
            super().__init__()

        def forward(self, hidden_states):
            m_logits = gpt_model.head_text(hidden_states)
            return m_logits

    class LmHead_infer_code(torch.nn.Module):
        def __init__(self):
            super().__init__()

        def forward(self, hidden_states):
            m_logits = torch.stack(
                [
                    gpt_model.head_code[i](hidden_states)
                    for i in range(gpt_model.num_vq)
                ],
                2,
            )
            return m_logits

    def convert_lm_head_text():
        model = LmHead_infer_text()
        input = torch.randn(1, HIDDEN_SIZE)

        torch.onnx.export(
            model,
            (input),
            f"{folder}/lm_head_text.onnx",
            verbose=False,
            input_names=["hidden_states"],
            output_names=["m_logits"],
            do_constant_folding=True,
            opset_version=15,
        )

    def convert_lm_head_code():
        model = LmHead_infer_code()
        input = torch.randn(1, HIDDEN_SIZE)
        torch.onnx.export(
            model,
            (input),
            f"{folder}/lm_head_code.onnx",
            verbose=False,
            input_names=["hidden_states"],
            output_names=["m_logits"],
            do_constant_folding=True,
            opset_version=15,
        )

    # export models
    print(f"Convert block & block_cache")
    for i in tqdm(range(NUM_OF_LAYERS)):
        convert_block(i)
        convert_block_cache(i)

    print(f"Convert embedding")
    convert_embedding_text()
    convert_embedding_code()
    convert_embedding_code_cache()

    print(f"Convert lm_head")
    convert_lm_head_code()
    convert_lm_head_text()

    print(f"Convert greedy_head")
    convert_greedy_head_text()
    convert_greedy_head_code()


def export_decoder():
    decoder = DVAE(
        decoder_config=asdict(chattts_config.decoder),
        dim=chattts_config.decoder.idim,
    ).eval()
    decoder.load_state_dict(
        torch.load(
            asdict(chattts_config.path)["decoder_ckpt_path"],
            weights_only=True,
            mmap=True,
        )
    )

    for param in decoder.parameters():
        param.requires_grad = False
    rand_input = torch.rand([1, 768, 1024], requires_grad=False)

    def mydec(_inp):
        return decoder(_inp, mode="decode")

    jitmodel = jit.trace(mydec, [rand_input])
    jit.save(jitmodel, f"{args.out_dir}/decoder_jit.pt")


def export_vocos():
    feature_extractor = instantiate_class(
        args=(), init=asdict(chattts_config.vocos.feature_extractor)
    )
    backbone = instantiate_class(args=(), init=asdict(chattts_config.vocos.backbone))
    head = instantiate_class(args=(), init=asdict(chattts_config.vocos.head))
    vocos = Vocos(
        feature_extractor=feature_extractor, backbone=backbone, head=head
    ).eval()
    vocos.load_state_dict(
        torch.load(
            asdict(chattts_config.path)["vocos_ckpt_path"], weights_only=True, mmap=True
        )
    )

    for param in vocos.parameters():
        param.requires_grad = False
    rand_input = torch.rand([1, 100, 2048], requires_grad=False)

    def myvocos(_inp):
        # return chat.vocos.decode(_inp) # TPU cannot support the istft OP, thus it has to be moved to postprocessing
        # reference: https://github.com/gemelo-ai/vocos.git
        x = vocos.backbone(_inp)
        x = vocos.head.out(x).transpose(1, 2)
        mag, p = x.chunk(2, dim=1)
        mag = torch.exp(mag)
        mag = torch.clip(
            mag, max=1e2
        )  # safeguard to prevent excessively large magnitudes
        # wrapping happens here. These two lines produce real and imaginary value
        x = torch.cos(p)
        y = torch.sin(p)
        return mag, x, y

    jitmodel = jit.trace(myvocos, [rand_input])
    torch.onnx.export(
        jitmodel,
        [rand_input],
        f"{args.out_dir}/vocos_1-100-2048.onnx",
        opset_version=12,
        do_constant_folding=True,
    )


if args.gpt:
    export_gpt()

if args.decoder:
    export_decoder()

if args.vocos:
    export_vocos()

print("Done. Please check the files in", args.out_dir)



================================================
FILE: examples/onnx/gpt.py
================================================
import logging
from typing import Tuple

import torch
import torch.nn as nn
from torch.nn.utils.parametrizations import weight_norm

from modeling_llama import LlamaModel, LlamaConfig


class GPT(nn.Module):
    def __init__(
        self,
        gpt_config: dict,
        num_audio_tokens: int = 626,
        num_text_tokens: int = 21178,
        num_vq=4,
        use_flash_attn=False,
        device=torch.device("cpu"),
        logger=logging.getLogger(__name__),
    ):
        super().__init__()

        self.logger = logger

        self.device = device
        self.device_gpt = device if "mps" not in str(device) else torch.device("cpu")

        self.num_vq = num_vq
        self.num_audio_tokens = num_audio_tokens

        self.use_flash_attn = use_flash_attn

        self.gpt, self.llama_config = self._build_llama(gpt_config, self.device_gpt)
        self.is_te_llama = False
        self.model_dim = int(self.gpt.config.hidden_size)
        self.emb_code = nn.ModuleList(
            [
                nn.Embedding(
                    num_audio_tokens,
                    self.model_dim,
                    device=self.device_gpt,
                )
                for _ in range(num_vq)
            ],
        )
        self.emb_text = nn.Embedding(
            num_text_tokens, self.model_dim, device=self.device_gpt
        )

        self.head_text = weight_norm(
            nn.Linear(
                self.model_dim,
                num_text_tokens,
                bias=False,
                device=device,
            ),
            name="weight",
        )
        self.head_code = nn.ModuleList(
            [
                weight_norm(
                    nn.Linear(
                        self.model_dim,
                        num_audio_tokens,
                        bias=False,
                        device=device,
                    ),
                    name="weight",
                )
                for _ in range(self.num_vq)
            ],
        )

    def from_pretrained(self, file_path: str):
        self.load_state_dict(
            torch.load(file_path, weights_only=True, mmap=True), strict=False
        )

    def _build_llama(
        self,
        config: dict,
        device: torch.device,
    ) -> Tuple[LlamaModel, LlamaConfig]:

        llama_config = LlamaConfig(**config)
        model = LlamaModel(llama_config)
        del model.embed_tokens
        return model.to(device), llama_config



================================================
FILE: examples/onnx/modeling_llama.py
================================================
# coding=utf-8
# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
PyTorch LLaMA model.
Copied from https://github.com/sophgo/LLM-TPU/blob/main/models/Llama2/compile/files/llama-2-7b-chat-hf/modeling_llama.py
"""
import math
from typing import List, Optional, Tuple, Union

import torch
import torch.nn.functional as F
import torch.utils.checkpoint
from torch import nn
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss

from transformers.activations import ACT2FN
from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
    SequenceClassifierOutputWithPast,
)
from transformers.modeling_utils import PreTrainedModel
from transformers.utils import (
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    logging,
    replace_return_docstrings,
)
from transformers.models.llama.configuration_llama import LlamaConfig


logger = logging.get_logger(__name__)

_CONFIG_FOR_DOC = "LlamaConfig"


# Copied from transformers.models.bart.modeling_bart._make_causal_mask
def _make_causal_mask(
    input_ids_shape: torch.Size,
    dtype: torch.dtype,
    device: torch.device,
    past_key_values_length: int = 0,
):
    """
    Make causal mask used for bi-directional self-attention.
    """
    bsz, tgt_len = input_ids_shape
    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)
    mask_cond = torch.arange(mask.size(-1), device=device)
    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)
    mask = mask.to(dtype)

    if past_key_values_length > 0:
        mask = torch.cat(
            [
                torch.zeros(
                    tgt_len, past_key_values_length, dtype=dtype, device=device
                ),
                mask,
            ],
            dim=-1,
        )
    return mask[None, None, :, :].expand(
        bsz, 1, tgt_len, tgt_len + past_key_values_length
    )


# Copied from transformers.models.bart.modeling_bart._expand_mask
def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):
    """
    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.
    """
    bsz, src_len = mask.size()
    tgt_len = tgt_len if tgt_len is not None else src_len

    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)

    inverted_mask = 1.0 - expanded_mask

    return inverted_mask.masked_fill(
        inverted_mask.to(torch.bool), torch.finfo(dtype).min
    )


class LlamaRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        LlamaRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)


class LlamaRotaryEmbedding(torch.nn.Module):
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()

        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base
        inv_freq = 1.0 / (
            self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim)
        )
        self.register_buffer("inv_freq", inv_freq)

        # Build here to make `torch.jit.trace` work.
        self._set_cos_sin_cache(
            seq_len=max_position_embeddings,
            device=self.inv_freq.device,
            dtype=torch.get_default_dtype(),
        )

    def _set_cos_sin_cache(self, seq_len, device, dtype):
        self.max_seq_len_cached = seq_len
        t = torch.arange(
            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype
        )

        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer(
            "cos_cached", emb.cos()[None, None, :, :].to(dtype), persistent=False
        )
        self.register_buffer(
            "sin_cached", emb.sin()[None, None, :, :].to(dtype), persistent=False
        )

    def forward(self, x, seq_len=None):
        # x: [bs, num_attention_heads, seq_len, head_size]
        if seq_len > self.max_seq_len_cached:
            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)

        return (
            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
        )


class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
    """LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev"""

    def __init__(
        self,
        dim,
        max_position_embeddings=2048,
        base=10000,
        device=None,
        scaling_factor=1.0,
    ):
        self.scaling_factor = scaling_factor
        super().__init__(dim, max_position_embeddings, base, device)

    def _set_cos_sin_cache(self, seq_len, device, dtype):
        self.max_seq_len_cached = seq_len
        t = torch.arange(
            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype
        )
        t = t / self.scaling_factor

        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer(
            "cos_cached", emb.cos()[None, None, :, :].to(dtype), persistent=False
        )
        self.register_buffer(
            "sin_cached", emb.sin()[None, None, :, :].to(dtype), persistent=False
        )


class LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):
    """LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla"""

    def __init__(
        self,
        dim,
        max_position_embeddings=2048,
        base=10000,
        device=None,
        scaling_factor=1.0,
    ):
        self.scaling_factor = scaling_factor
        super().__init__(dim, max_position_embeddings, base, device)

    def _set_cos_sin_cache(self, seq_len, device, dtype):
        self.max_seq_len_cached = seq_len

        if seq_len > self.max_position_embeddings:
            base = self.base * (
                (self.scaling_factor * seq_len / self.max_position_embeddings)
                - (self.scaling_factor - 1)
            ) ** (self.dim / (self.dim - 2))
            inv_freq = 1.0 / (
                base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim)
            )
            self.register_buffer("inv_freq", inv_freq)

        t = torch.arange(
            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype
        )

        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer(
            "cos_cached", emb.cos()[None, None, :, :].to(dtype), persistent=False
        )
        self.register_buffer(
            "sin_cached", emb.sin()[None, None, :, :].to(dtype), persistent=False
        )


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.
    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]
    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]
    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
    cos = cos.transpose(1, 2)
    sin = sin.transpose(1, 2)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


class LlamaMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.pretraining_tp = config.pretraining_tp
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        if self.pretraining_tp > 1:
            slice = self.intermediate_size // self.pretraining_tp
            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)
            up_proj_slices = self.up_proj.weight.split(slice, dim=0)
            down_proj_slices = self.down_proj.weight.split(slice, dim=1)

            gate_proj = torch.cat(
                [F.linear(x, gate_proj_slices[i]) for i in range(self.pretraining_tp)],
                dim=-1,
            )
            up_proj = torch.cat(
                [F.linear(x, up_proj_slices[i]) for i in range(self.pretraining_tp)],
                dim=-1,
            )

            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)
            down_proj = [
                F.linear(intermediate_states[i], down_proj_slices[i])
                for i in range(self.pretraining_tp)
            ]
            down_proj = sum(down_proj)
        else:
            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

        return down_proj


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(
        batch, num_key_value_heads, n_rep, slen, head_dim
    )
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


class LlamaAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: LlamaConfig):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.pretraining_tp = config.pretraining_tp
        self.max_position_embeddings = config.max_position_embeddings

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )
        self.q_proj = nn.Linear(
            self.hidden_size, self.num_heads * self.head_dim, bias=False
        )
        self.k_proj = nn.Linear(
            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
        )
        self.v_proj = nn.Linear(
            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
        )
        self.o_proj = nn.Linear(
            self.num_heads * self.head_dim, self.hidden_size, bias=False
        )
        self._init_rope()

    def _init_rope(self):
        if self.config.rope_scaling is None:
            self.rotary_emb = LlamaRotaryEmbedding(
                self.head_dim, max_position_embeddings=self.max_position_embeddings
            )
        else:
            scaling_type = self.config.rope_scaling["type"]
            scaling_factor = self.config.rope_scaling["factor"]
            if scaling_type == "linear":
                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(
                    self.head_dim,
                    max_position_embeddings=self.max_position_embeddings,
                    scaling_factor=scaling_factor,
                )
            elif scaling_type == "dynamic":
                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(
                    self.head_dim,
                    max_position_embeddings=self.max_position_embeddings,
                    scaling_factor=scaling_factor,
                )
            else:
                raise ValueError(f"Unknown RoPE scaling type {scaling_type}")

    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
        return (
            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)
            .transpose(1, 2)
            .contiguous()
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()

        if self.pretraining_tp > 1:
            key_value_slicing = (
                self.num_key_value_heads * self.head_dim
            ) // self.pretraining_tp
            query_slices = self.q_proj.weight.split(
                (self.num_heads * self.head_dim) // self.pretraining_tp, dim=0
            )
            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)
            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)

            query_states = [
                F.linear(hidden_states, query_slices[i])
                for i in range(self.pretraining_tp)
            ]
            query_states = torch.cat(query_states, dim=-1)

            key_states = [
                F.linear(hidden_states, key_slices[i])
                for i in range(self.pretraining_tp)
            ]
            key_states = torch.cat(key_states, dim=-1)

            value_states = [
                F.linear(hidden_states, value_slices[i])
                for i in range(self.pretraining_tp)
            ]
            value_states = torch.cat(value_states, dim=-1)

        else:
            query_states = self.q_proj(hidden_states)
            key_states = self.k_proj(hidden_states)
            value_states = self.v_proj(hidden_states)
        # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        # value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)
        key_states = key_states.view(
            bsz, q_len, self.num_key_value_heads, self.head_dim
        )
        value_states = value_states.view(
            bsz, q_len, self.num_key_value_heads, self.head_dim
        )

        # kv_seq_len = key_states.shape[-2]
        kv_seq_len = key_states.shape[-3]
        if past_key_value is not None:
            # kv_seq_len += past_key_value[0].shape[-2]
            kv_seq_len += past_key_value[0].shape[-3]
        if past_key_value is not None:
            cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len - 1)
        else:
            cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
        query_states, key_states = apply_rotary_pos_emb(
            query_states, key_states, cos, sin, position_ids
        )
        past_kv = (key_states, value_states) if use_cache else None
        if past_key_value is not None:
            # reuse k, v, self_attention
            # key_states = torch.cat([past_key_value[0], key_states], dim=2)
            # value_states = torch.cat([past_key_value[1], value_states], dim=2)
            key_states = torch.cat([past_key_value[0], key_states], dim=1)
            value_states = torch.cat([past_key_value[1], value_states], dim=1)
        # past_key_value = (key_states, value_states) if use_cache else None

        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        # attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
        attn_weights = torch.matmul(
            query_states.transpose(1, 2), key_states.transpose(1, 2).transpose(2, 3)
        ) / math.sqrt(self.head_dim)
        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
                f" {attn_weights.size()}"
            )

        if attention_mask is not None:
            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
                raise ValueError(
                    f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
                )
            attn_weights = attn_weights + attention_mask

        # upcast attention to fp32
        attn_weights = nn.functional.softmax(
            attn_weights, dim=-1, dtype=torch.float32
        ).to(query_states.dtype)
        attn_output = torch.matmul(attn_weights, value_states.transpose(1, 2))

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)

        if self.pretraining_tp > 1:
            attn_output = attn_output.split(
                self.hidden_size // self.pretraining_tp, dim=2
            )
            o_proj_slices = self.o_proj.weight.split(
                self.hidden_size // self.pretraining_tp, dim=1
            )
            attn_output = sum(
                [
                    F.linear(attn_output[i], o_proj_slices[i])
                    for i in range(self.pretraining_tp)
                ]
            )
        else:
            attn_output = self.o_proj(attn_output)

        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_kv


class LlamaDecoderLayer(nn.Module):
    def __init__(self, config: LlamaConfig):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.self_attn = LlamaAttention(config=config)
        self.mlp = LlamaMLP(config)
        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = LlamaRMSNorm(
            config.hidden_size, eps=config.rms_norm_eps
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
    ) -> Tuple[
        torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]
    ]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            use_cache (`bool`, *optional*):
                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
                (see `past_key_values`).
            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
        """

        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights,)

        if use_cache:
            outputs += (present_key_value,)

        return outputs


LLAMA_START_DOCSTRING = r"""
    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Parameters:
        config ([`LlamaConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
"""


@add_start_docstrings(
    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
    LLAMA_START_DOCSTRING,
)
class LlamaPreTrainedModel(PreTrainedModel):
    config_class = LlamaConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["LlamaDecoderLayer"]
    _skip_keys_device_placement = "past_key_values"

    def _init_weights(self, module):
        std = self.config.initializer_range
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()

    def _set_gradient_checkpointing(self, module, value=False):
        if isinstance(module, LlamaModel):
            module.gradient_checkpointing = value


LLAMA_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
            it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
            `past_key_values`).

            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
            config.n_positions - 1]`.

            [What are position IDs?](../glossary#position-ids)
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
            model's internal embedding lookup matrix.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
            `past_key_values`).
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
            tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
            more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
"""


@add_start_docstrings(
    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
    LLAMA_START_DOCSTRING,
)
class LlamaModel(LlamaPreTrainedModel):
    """
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]

    Args:
        config: LlamaConfig
    """

    def __init__(self, config: LlamaConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(
            config.vocab_size, config.hidden_size, self.padding_idx
        )
        self.layers = nn.ModuleList(
            [LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)]
        )
        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        self.gradient_checkpointing = False
        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value

    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask
    def _prepare_decoder_attention_mask(
        self, attention_mask, input_shape, inputs_embeds, past_key_values_length
    ):
        # create causal mask
        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
        combined_attention_mask = None
        if input_shape[-1] > 1:
            combined_attention_mask = _make_causal_mask(
                input_shape,
                inputs_embeds.dtype,
                device=inputs_embeds.device,
                past_key_values_length=past_key_values_length,
            )

        if attention_mask is not None:
            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
            expanded_attn_mask = _expand_mask(
                attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]
            ).to(inputs_embeds.device)
            combined_attention_mask = (
                expanded_attn_mask
                if combined_attention_mask is None
                else expanded_attn_mask + combined_attention_mask
            )

        return combined_attention_mask

    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        # retrieve input_ids and inputs_embeds
        if input_ids is not None and inputs_embeds is not None:
            raise ValueError(
                "You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time"
            )
        elif input_ids is not None:
            batch_size, seq_length = input_ids.shape
        elif inputs_embeds is not None:
            batch_size, seq_length, _ = inputs_embeds.shape
        else:
            raise ValueError(
                "You have to specify either decoder_input_ids or decoder_inputs_embeds"
            )

        seq_length_with_past = seq_length
        past_key_values_length = 0

        if past_key_values is not None:
            past_key_values_length = past_key_values[0][0].shape[2]
            seq_length_with_past = seq_length_with_past + past_key_values_length

        if position_ids is None:
            device = input_ids.device if input_ids is not None else inputs_embeds.device
            position_ids = torch.arange(
                past_key_values_length,
                seq_length + past_key_values_length,
                dtype=torch.long,
                device=device,
            )
            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
        else:
            position_ids = position_ids.view(-1, seq_length).long()

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)
        # embed positions
        if attention_mask is None:
            attention_mask = torch.ones(
                (batch_size, seq_length_with_past),
                dtype=torch.bool,
                device=inputs_embeds.device,
            )
        attention_mask = self._prepare_decoder_attention_mask(
            attention_mask,
            (batch_size, seq_length),
            inputs_embeds,
            past_key_values_length,
        )

        hidden_states = inputs_embeds

        if self.gradient_checkpointing and self.training:
            if use_cache:
                logger.warning_once(
                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                )
                use_cache = False

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        next_decoder_cache = () if use_cache else None

        for idx, decoder_layer in enumerate(self.layers):
            if output_hidden_states:
                all_hidden_states += (hidden_states,)

            past_key_value = (
                past_key_values[idx] if past_key_values is not None else None
            )

            if self.gradient_checkpointing and self.training:

                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        # None for past_key_value
                        return module(*inputs, output_attentions, None)

                    return custom_forward

                layer_outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(decoder_layer),
                    hidden_states,
                    attention_mask,
                    position_ids,
                    None,
                )
            else:
                layer_outputs = decoder_layer(
                    hidden_states,
                    attention_mask=attention_mask,
                    position_ids=position_ids,
                    past_key_value=past_key_value,
                    output_attentions=output_attentions,
                    use_cache=use_cache,
                )
            hidden_states = layer_outputs[0]

            if use_cache:
                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        # add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        next_cache = next_decoder_cache if use_cache else None
        if not return_dict:
            return tuple(
                v
                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns]
                if v is not None
            )
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=next_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )


class LlamaForCausalLM(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.pretraining_tp = config.pretraining_tp
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)
    @replace_return_docstrings(
        output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC
    )
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        r"""
        Args:
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

        Returns:

        Example:

        ```python
        >>> from transformers import AutoTokenizer, LlamaForCausalLM

        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""

        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        hidden_states = outputs[0]
        if self.pretraining_tp > 1:
            lm_head_slices = self.lm_head.weight.split(
                self.vocab_size // self.pretraining_tp, dim=0
            )
            logits = [
                F.linear(hidden_states, lm_head_slices[i])
                for i in range(self.pretraining_tp)
            ]
            logits = torch.cat(logits, dim=-1)
        else:
            logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)
            loss = loss_fct(shift_logits, shift_labels)

        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        attention_mask=None,
        inputs_embeds=None,
        **kwargs,
    ):
        if past_key_values:
            input_ids = input_ids[:, -1:]

        position_ids = kwargs.get("position_ids", None)
        if attention_mask is not None and position_ids is None:
            # create position_ids on the fly for batch generation
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
            if past_key_values:
                position_ids = position_ids[:, -1].unsqueeze(-1)

        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step
        if inputs_embeds is not None and past_key_values is None:
            model_inputs = {"inputs_embeds": inputs_embeds}
        else:
            model_inputs = {"input_ids": input_ids}

        model_inputs.update(
            {
                "position_ids": position_ids,
                "past_key_values": past_key_values,
                "use_cache": kwargs.get("use_cache"),
                "attention_mask": attention_mask,
            }
        )
        return model_inputs

    @staticmethod
    def _reorder_cache(past_key_values, beam_idx):
        reordered_past = ()
        for layer_past in past_key_values:
            reordered_past += (
                tuple(
                    past_state.index_select(0, beam_idx.to(past_state.device))
                    for past_state in layer_past
                ),
            )
        return reordered_past


@add_start_docstrings(
    """
    The LLaMa Model transformer with a sequence classification head on top (linear layer).

    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do.

    Since it does classification on the last token, it requires to know the position of the last token. If a
    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
    each row of the batch).
    """,
    LLAMA_START_DOCSTRING,
)
class LlamaForSequenceClassification(LlamaPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.model = LlamaModel(config)
        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        transformer_outputs = self.model(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        hidden_states = transformer_outputs[0]
        logits = self.score(hidden_states)

        if input_ids is not None:
            batch_size = input_ids.shape[0]
        else:
            batch_size = inputs_embeds.shape[0]

        if self.config.pad_token_id is None and batch_size != 1:
            raise ValueError(
                "Cannot handle batch sizes > 1 if no padding token is defined."
            )
        if self.config.pad_token_id is None:
            sequence_lengths = -1
        else:
            if input_ids is not None:
                sequence_lengths = (
                    torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1
                ).to(logits.device)
            else:
                sequence_lengths = -1

        pooled_logits = logits[
            torch.arange(batch_size, device=logits.device), sequence_lengths
        ]

        loss = None
        if labels is not None:
            labels = labels.to(logits.device)
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (
                    labels.dtype == torch.long or labels.dtype == torch.int
                ):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "regression":
                loss_fct = MSELoss()
                if self.num_labels == 1:
                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())
                else:
                    loss = loss_fct(pooled_logits, labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(
                    pooled_logits.view(-1, self.num_labels), labels.view(-1)
                )
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(pooled_logits, labels)
        if not return_dict:
            output = (pooled_logits,) + transformer_outputs[1:]
            return ((loss,) + output) if loss is not None else output

        return SequenceClassifierOutputWithPast(
            loss=loss,
            logits=pooled_logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
        )



================================================
FILE: examples/web/__init__.py
================================================



================================================
FILE: examples/web/ex.py
================================================
ex = [
    [
        "四川美食确实以辣闻名，但也有不辣的选择。比如甜水面、赖汤圆、蛋烘糕、叶儿粑等，这些小吃口味温和，甜而不腻，也很受欢迎。",
        0.3,
        0.7,
        20,
        2,
        42,
        True,
    ],
    [
        "What is your favorite english food?",
        0.5,
        0.5,
        10,
        245,
        531,
        True,
    ],
    [
        "chat T T S is a text to speech model designed for dialogue applications. [uv_break]it supports mixed language input [uv_break]and offers multi speaker capabilities with precise control over prosodic elements like [uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation. [uv_break]it delivers natural and expressive speech,[uv_break]so please[uv_break] use the project responsibly at your own risk.[uv_break]",
        0.8,
        0.4,
        7,
        70,
        165,
        False,
    ],
]



================================================
FILE: examples/web/funcs.py
================================================
import random
from typing import Optional
from time import sleep

import gradio as gr

import sys

sys.path.append("..")
sys.path.append("../..")
from tools.audio import float_to_int16, has_ffmpeg_installed, load_audio
from tools.logger import get_logger

logger = get_logger(" WebUI ")

from tools.seeder import TorchSeedContext
from tools.normalizer import normalizer_en_nemo_text, normalizer_zh_tn

import ChatTTS

chat = ChatTTS.Chat(get_logger("ChatTTS"))

custom_path: Optional[str] = None

has_interrupted = False
is_in_generate = False

seed_min = 1
seed_max = 4294967295

use_mp3 = has_ffmpeg_installed()
if not use_mp3:
    logger.warning("no ffmpeg installed, use wav file output")

# 音色选项：用于预置合适的音色
voices = {
    "Default": {"seed": 2},
    "Timbre1": {"seed": 1111},
    "Timbre2": {"seed": 2222},
    "Timbre3": {"seed": 3333},
    "Timbre4": {"seed": 4444},
    "Timbre5": {"seed": 5555},
    "Timbre6": {"seed": 6666},
    "Timbre7": {"seed": 7777},
    "Timbre8": {"seed": 8888},
    "Timbre9": {"seed": 9999},
}


def generate_seed():
    return gr.update(value=random.randint(seed_min, seed_max))


# 返回选择音色对应的seed
def on_voice_change(vocie_selection):
    return voices.get(vocie_selection)["seed"]


def on_audio_seed_change(audio_seed_input):
    with TorchSeedContext(audio_seed_input):
        rand_spk = chat.sample_random_speaker()
    return rand_spk


def load_chat(cust_path: Optional[str], coef: Optional[str]) -> bool:
    if cust_path == None:
        ret = chat.load(coef=coef)
    else:
        logger.info("local model path: %s", cust_path)
        ret = chat.load("custom", custom_path=cust_path, coef=coef)
        global custom_path
        custom_path = cust_path
    if ret:
        try:
            chat.normalizer.register("en", normalizer_en_nemo_text())
        except ValueError as e:
            logger.error(e)
        except:
            logger.warning("Package nemo_text_processing not found!")
            logger.warning(
                "Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing",
            )
        try:
            chat.normalizer.register("zh", normalizer_zh_tn())
        except ValueError as e:
            logger.error(e)
        except:
            logger.warning("Package WeTextProcessing not found!")
            logger.warning(
                "Run: conda install -c conda-forge pynini=2.1.5 && pip install WeTextProcessing",
            )
    return ret


def reload_chat(coef: Optional[str]) -> str:
    global is_in_generate

    if is_in_generate:
        gr.Warning("Cannot reload when generating!")
        return coef

    chat.unload()
    gr.Info("Model unloaded.")
    if len(coef) != 230:
        gr.Warning("Ingore invalid DVAE coefficient.")
        coef = None
    try:
        global custom_path
        ret = load_chat(custom_path, coef)
    except Exception as e:
        raise gr.Error(str(e))
    if not ret:
        raise gr.Error("Unable to load model.")
    gr.Info("Reload succeess.")
    return chat.coef


def on_upload_sample_audio(sample_audio_input: Optional[str]) -> str:
    if sample_audio_input is None:
        return ""
    sample_audio = load_audio(sample_audio_input, 24000)
    spk_smp = chat.sample_audio_speaker(sample_audio)
    del sample_audio
    return spk_smp


def _set_generate_buttons(generate_button, interrupt_button, is_reset=False):
    return gr.update(
        value=generate_button, visible=is_reset, interactive=is_reset
    ), gr.update(value=interrupt_button, visible=not is_reset, interactive=not is_reset)


def refine_text(
    text,
    text_seed_input,
    refine_text_flag,
    temperature,
    top_P,
    top_K,
    split_batch,
):
    global chat

    if not refine_text_flag:
        sleep(1)  # to skip fast answer of loading mark
        return text

    text = chat.infer(
        text,
        skip_refine_text=False,
        refine_text_only=True,
        params_refine_text=ChatTTS.Chat.RefineTextParams(
            temperature=temperature,
            top_P=top_P,
            top_K=top_K,
            manual_seed=text_seed_input,
        ),
        split_text=split_batch > 0,
    )

    return text[0] if isinstance(text, list) else text


def generate_audio(
    text,
    temperature,
    top_P,
    top_K,
    spk_emb_text: str,
    stream,
    audio_seed_input,
    sample_text_input,
    sample_audio_code_input,
    split_batch,
):
    global chat, has_interrupted

    if not text or has_interrupted or not spk_emb_text.startswith("蘁淰"):
        return None

    params_infer_code = ChatTTS.Chat.InferCodeParams(
        spk_emb=spk_emb_text,
        temperature=temperature,
        top_P=top_P,
        top_K=top_K,
        manual_seed=audio_seed_input,
    )

    if sample_text_input and sample_audio_code_input:
        params_infer_code.txt_smp = sample_text_input
        params_infer_code.spk_smp = sample_audio_code_input
        params_infer_code.spk_emb = None

    wav = chat.infer(
        text,
        skip_refine_text=True,
        params_infer_code=params_infer_code,
        stream=stream,
        split_text=split_batch > 0,
        max_split_batch=split_batch,
    )
    if stream:
        for gen in wav:
            audio = gen[0]
            if audio is not None and len(audio) > 0:
                yield 24000, float_to_int16(audio).T
            del audio
    else:
        yield 24000, float_to_int16(wav[0]).T


def interrupt_generate():
    global chat, has_interrupted

    has_interrupted = True
    chat.interrupt()


def set_buttons_before_generate(generate_button, interrupt_button):
    global has_interrupted, is_in_generate

    has_interrupted = False
    is_in_generate = True

    return _set_generate_buttons(
        generate_button,
        interrupt_button,
    )


def set_buttons_after_generate(generate_button, interrupt_button, audio_output):
    global has_interrupted, is_in_generate

    is_in_generate = False

    return _set_generate_buttons(
        generate_button,
        interrupt_button,
        audio_output is not None or has_interrupted,
    )



================================================
FILE: examples/web/webui.py
================================================
import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

import argparse

import gradio as gr

from funcs import *
from ex import ex


def main():

    with gr.Blocks() as demo:
        gr.Markdown("# ChatTTS WebUI")
        gr.Markdown("- **GitHub Repo**: https://github.com/2noise/ChatTTS")
        gr.Markdown("- **HuggingFace Repo**: https://huggingface.co/2Noise/ChatTTS")

        with gr.Row():
            with gr.Column(scale=2):
                text_input = gr.Textbox(
                    label="Input Text",
                    lines=4,
                    max_lines=4,
                    placeholder="Please Input Text...",
                    value=ex[0][0],
                    interactive=True,
                )
                sample_text_input = gr.Textbox(
                    label="Sample Text",
                    lines=4,
                    max_lines=4,
                    placeholder="If Sample Audio and Sample Text are available, the Speaker Embedding will be disabled.",
                    interactive=True,
                )
            with gr.Column():
                with gr.Tab(label="Sample Audio"):
                    sample_audio_input = gr.Audio(
                        value=None,
                        type="filepath",
                        interactive=True,
                        show_label=False,
                        waveform_options=gr.WaveformOptions(
                            sample_rate=24000,
                        ),
                        scale=1,
                    )
                with gr.Tab(label="Sample Audio Code"):
                    sample_audio_code_input = gr.Textbox(
                        lines=12,
                        max_lines=12,
                        show_label=False,
                        placeholder="Paste the Code copied before after uploading Sample Audio.",
                        interactive=True,
                    )

        with gr.Row():
            refine_text_checkbox = gr.Checkbox(
                label="Refine text", value=ex[0][6], interactive=True
            )
            temperature_slider = gr.Slider(
                minimum=0.00001,
                maximum=1.0,
                step=0.00001,
                value=ex[0][1],
                label="Audio Temperature",
                interactive=True,
            )
            top_p_slider = gr.Slider(
                minimum=0.1,
                maximum=0.9,
                step=0.05,
                value=ex[0][2],
                label="top_P",
                interactive=True,
            )
            top_k_slider = gr.Slider(
                minimum=1,
                maximum=20,
                step=1,
                value=ex[0][3],
                label="top_K",
                interactive=True,
            )

        with gr.Row():
            voice_selection = gr.Dropdown(
                label="Timbre",
                choices=voices.keys(),
                value="Default",
                interactive=True,
            )
            audio_seed_input = gr.Number(
                value=ex[0][4],
                label="Audio Seed",
                interactive=True,
                minimum=seed_min,
                maximum=seed_max,
            )
            generate_audio_seed = gr.Button("\U0001f3b2", interactive=True)
            text_seed_input = gr.Number(
                value=ex[0][5],
                label="Text Seed",
                interactive=True,
                minimum=seed_min,
                maximum=seed_max,
            )
            generate_text_seed = gr.Button("\U0001f3b2", interactive=True)

        with gr.Row():
            spk_emb_text = gr.Textbox(
                label="Speaker Embedding",
                max_lines=3,
                show_copy_button=True,
                interactive=True,
                scale=2,
            )
            dvae_coef_text = gr.Textbox(
                label="DVAE Coefficient",
                max_lines=3,
                show_copy_button=True,
                interactive=True,
                scale=2,
            )
            reload_chat_button = gr.Button("Reload", scale=1, interactive=True)

        with gr.Row():
            auto_play_checkbox = gr.Checkbox(
                label="Auto Play", value=False, scale=1, interactive=True
            )
            stream_mode_checkbox = gr.Checkbox(
                label="Stream Mode",
                value=False,
                scale=1,
                interactive=True,
            )
            split_batch_slider = gr.Slider(
                minimum=0,
                maximum=100,
                step=1,
                value=4,
                label="Split Batch",
                interactive=True,
            )
            generate_button = gr.Button(
                "Generate", scale=2, variant="primary", interactive=True
            )
            interrupt_button = gr.Button(
                "Interrupt",
                scale=2,
                variant="stop",
                visible=False,
                interactive=False,
            )

        text_output = gr.Textbox(
            label="Output Text",
            interactive=False,
            show_copy_button=True,
        )

        sample_audio_input.change(
            fn=on_upload_sample_audio,
            inputs=sample_audio_input,
            outputs=sample_audio_code_input,
        ).then(fn=lambda: gr.Info("Sampled Audio Code generated at another Tab."))

        # 使用Gradio的回调功能来更新数值输入框
        voice_selection.change(
            fn=on_voice_change, inputs=voice_selection, outputs=audio_seed_input
        )

        generate_audio_seed.click(generate_seed, outputs=audio_seed_input)

        generate_text_seed.click(generate_seed, outputs=text_seed_input)

        audio_seed_input.change(
            on_audio_seed_change, inputs=audio_seed_input, outputs=spk_emb_text
        )

        reload_chat_button.click(
            reload_chat, inputs=dvae_coef_text, outputs=dvae_coef_text
        )

        interrupt_button.click(interrupt_generate)

        @gr.render(inputs=[auto_play_checkbox, stream_mode_checkbox])
        def make_audio(autoplay, stream):
            audio_output = gr.Audio(
                label="Output Audio",
                value=None,
                format="mp3" if use_mp3 and not stream else "wav",
                autoplay=autoplay,
                streaming=stream,
                interactive=False,
                show_label=True,
                waveform_options=gr.WaveformOptions(
                    sample_rate=24000,
                ),
            )
            generate_button.click(
                fn=set_buttons_before_generate,
                inputs=[generate_button, interrupt_button],
                outputs=[generate_button, interrupt_button],
            ).then(
                refine_text,
                inputs=[
                    text_input,
                    text_seed_input,
                    refine_text_checkbox,
                    temperature_slider,
                    top_p_slider,
                    top_k_slider,
                    split_batch_slider,
                ],
                outputs=text_output,
            ).then(
                generate_audio,
                inputs=[
                    text_output,
                    temperature_slider,
                    top_p_slider,
                    top_k_slider,
                    spk_emb_text,
                    stream_mode_checkbox,
                    audio_seed_input,
                    sample_text_input,
                    sample_audio_code_input,
                    split_batch_slider,
                ],
                outputs=audio_output,
            ).then(
                fn=set_buttons_after_generate,
                inputs=[generate_button, interrupt_button, audio_output],
                outputs=[generate_button, interrupt_button],
            )

        gr.Examples(
            examples=ex,
            inputs=[
                text_input,
                temperature_slider,
                top_p_slider,
                top_k_slider,
                audio_seed_input,
                text_seed_input,
                refine_text_checkbox,
            ],
        )

    parser = argparse.ArgumentParser(description="ChatTTS demo Launch")
    parser.add_argument(
        "--server_name", type=str, default="0.0.0.0", help="server name"
    )
    parser.add_argument("--server_port", type=int, default=8080, help="server port")
    parser.add_argument("--root_path", type=str, help="root path")
    parser.add_argument("--custom_path", type=str, help="custom model path")
    parser.add_argument("--coef", type=str, help="custom dvae coefficient")
    args = parser.parse_args()

    logger.info("loading ChatTTS model...")

    if load_chat(args.custom_path, args.coef):
        logger.info("Models loaded successfully.")
    else:
        logger.error("Models load failed.")
        sys.exit(1)

    spk_emb_text.value = on_audio_seed_change(audio_seed_input.value)
    dvae_coef_text.value = chat.coef

    demo.launch(
        server_name=args.server_name,
        server_port=args.server_port,
        root_path=args.root_path,
        inbrowser=True,
        show_api=False,
    )


if __name__ == "__main__":
    main()



================================================
FILE: tests/#511.py
================================================
import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

import logging

import ChatTTS

from tools.logger import get_logger

logger = get_logger("Test", lv=logging.WARN)

chat = ChatTTS.Chat(logger)
chat.load(compile=False, source="huggingface")  # Set to True for better performance

texts = [
    "的 话 语 音 太 短 了 会 造 成 生 成 音 频 错 误 ， 这 是 占 位 占 位 ， 老 大 爷 觉 得 车 夫 的 想 法 很 有 道 理 [uv_break]",
    "的 话 评 分 只 是 衡 量 音 色 的 稳 定 性 ， 不 代 表 音 色 的 好 坏 ， 可 以 根 据 自 己 的 需 求 选 择 [uv_break] 合 适 的 音 色",
    "然 后 举 个 简 单 的 例 子 ， 如 果 一 个 [uv_break] 沙 哑 且 结 巴 的 音 色 一 直 很 稳 定 ， 那 么 它 的 评 分 就 会 很 高 。",
    "语 音 太 短 了 会 造 成 生 成 音 频 错 误 ， 这 是 占 位 [uv_break] 占 位 。 我 使 用 seed id 去 生 成 音 频 ， 但 是 生 成 的 音 频 不 稳 定",
    "在d id 只 是 一 个 参 考 id [uv_break] 不 同 的 环 境 下 音 色 不 一 定 一 致 。 还 是 推 荐 使 用 。 pt 文 件 载 入 音 色",
    "的 话 语 音 太 短 了 会 造 成 生 成 音 频 错 误 ， 这 是 占 位 占 位 。 音 色 标 的 男 女 [uv_break] 准 确 吗",
    "， 当 前 第 一 批 测 试 的 音 色 有 两 千 条 [uv_break] ， 根 据 声 纹 相 似 性 简 单 打 标 ， 准 确 度 不 高 ， 特 别 是 特 征 一 项",
    "语 音 太 短 了 会 造 成 生 成 音 频 错 误 ， 这 是 占 位 占 位 。 仅 供 参 考 。 如 果 大 家 有 更 好 的 标 注 方 法 ， 欢 迎 pr [uv_break] 。",
]

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb=chat.sample_random_speaker(),
    temperature=0.3,
    top_P=0.005,
    top_K=1,
    show_tqdm=False,
)

fail = False

wavs = chat.infer(
    texts,
    skip_refine_text=True,
    split_text=False,
    params_infer_code=params_infer_code,
)

for k, wav in enumerate(wavs):
    if wav is None:
        logger.warning("index", k, "is None")
        fail = True

if fail:
    import sys

    sys.exit(1)



================================================
FILE: tests/#588.py
================================================
import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

import logging
import re

import ChatTTS

from tools.logger import get_logger

logger = get_logger("Test", lv=logging.WARN)

chat = ChatTTS.Chat(logger)
chat.load(compile=False, source="huggingface")  # Set to True for better performance

texts = [
    "总结一下，AI Agent是大模型功能的扩展，让AI更接近于通用人工智能，也就是我们常说的AGI。",
    "你真是太聪明啦。",
]

fail = False

refined = chat.infer(
    texts,
    refine_text_only=True,
    stream=False,
    split_text=False,
    params_refine_text=ChatTTS.Chat.RefineTextParams(show_tqdm=False),
)

trimre = re.compile("\\[[\w_]+\\]")


def trim_tags(txt: str) -> str:
    global trimre
    return trimre.sub("", txt)


for i, t in enumerate(refined):
    if len(trim_tags(t)) > 4 * len(texts[i]):
        fail = True
        logger.warning("in: %s, out: %s", texts[i], t)

if fail:
    import sys

    sys.exit(1)



================================================
FILE: tests/#655.py
================================================
import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

import logging

import torch

import ChatTTS

from tools.logger import get_logger
from tools.normalizer import normalizer_en_nemo_text

logger = get_logger("Test", lv=logging.WARN)

chat = ChatTTS.Chat(logger)
chat.load(compile=False, source="huggingface")  # Set to True for better performance
try:
    chat.normalizer.register("en", normalizer_en_nemo_text())
except:
    logger.warning("Package nemo_text_processing not found!")

rand_spk = chat.sample_random_speaker()


text = ["What is [uv_break]your favorite english food?[laugh][lbreak]"]

fail = False

refined_text = chat.infer(
    text,
    refine_text_only=True,
    params_refine_text=ChatTTS.Chat.RefineTextParams(
        prompt="[oral_2][laugh_0][break_6]",
        manual_seed=12345,
    ),
    split_text=False,
)
if (
    refined_text[0]
    != "what is [uv_break] your favorite english [uv_break] food [laugh] like [lbreak]"
):
    fail = True
    logger.warning("refined text is '%s'", refined_text[0])

params = ChatTTS.Chat.InferCodeParams(
    spk_emb=rand_spk,  # add sampled speaker
    temperature=0.3,  # using custom temperature
    top_P=0.7,  # top P decode
    top_K=20,  # top K decode
)
input_ids, attention_mask, text_mask = chat.tokenizer.encode(
    chat.speaker.decorate_code_prompts(
        text,
        params.prompt,
        params.txt_smp,
        params.spk_emb,
    ),
    chat.config.gpt.num_vq,
    prompt=(
        chat.speaker.decode_prompt(params.spk_smp)
        if params.spk_smp is not None
        else None
    ),
    device=chat.device_gpt,
)
with torch.inference_mode():
    start_idx, end_idx = 0, torch.zeros(
        input_ids.shape[0], device=input_ids.device, dtype=torch.long
    ).fill_(input_ids.shape[1])

    recoded_text = chat.tokenizer.decode(
        chat.gpt._prepare_generation_outputs(
            input_ids,
            start_idx,
            end_idx,
            [],
            [],
            True,
        ).ids
    )

if (
    recoded_text[0]
    != "[Stts] [spk_emb] [speed_5] what is [uv_break] your favorite english food? [laugh] [lbreak] [Ptts]"
):
    fail = True
    logger.warning("recoded text is '%s'", refined_text)

if fail:
    import sys

    sys.exit(1)



================================================
FILE: tests/testall.sh
================================================
#!/bin/sh

exitcode=0

for file in tests/*.py
do
    echo "Testing $file..."
    python "$file"
    if [ $? -ne 0 ]
    then
        echo "Error: $file exited with a non-zero status."
        exitcode=1
    fi
    echo "Test $file success"
done

exit $exitcode



================================================
FILE: tools/__init__.py
================================================



================================================
FILE: tools/audio/__init__.py
================================================
from .av import load_audio
from .pcm import pcm_arr_to_mp3_view
from .ffmpeg import has_ffmpeg_installed
from .np import float_to_int16



================================================
FILE: tools/audio/av.py
================================================
from io import BufferedWriter, BytesIO
from pathlib import Path
from typing import Dict, Tuple, Optional, Union, List

import av
from av.audio.frame import AudioFrame
from av.audio.resampler import AudioResampler
import numpy as np


video_format_dict: Dict[str, str] = {
    "m4a": "mp4",
}

audio_format_dict: Dict[str, str] = {
    "ogg": "libvorbis",
    "mp4": "aac",
}


def wav2(i: BytesIO, o: BufferedWriter, format: str):
    """
    https://github.com/fumiama/Retrieval-based-Voice-Conversion-WebUI/blob/412a9950a1e371a018c381d1bfb8579c4b0de329/infer/lib/audio.py#L20
    """
    inp = av.open(i, "r")
    format = video_format_dict.get(format, format)
    out = av.open(o, "w", format=format)
    format = audio_format_dict.get(format, format)

    ostream = out.add_stream(format)

    for frame in inp.decode(audio=0):
        for p in ostream.encode(frame):
            out.mux(p)

    for p in ostream.encode(None):
        out.mux(p)

    out.close()
    inp.close()


def load_audio(
    file: Union[str, BytesIO, Path],
    sr: Optional[int] = None,
    format: Optional[str] = None,
    mono=True,
) -> Union[np.ndarray, Tuple[np.ndarray, int]]:
    """
    https://github.com/fumiama/Retrieval-based-Voice-Conversion-WebUI/blob/412a9950a1e371a018c381d1bfb8579c4b0de329/infer/lib/audio.py#L39
    """
    if (isinstance(file, str) and not Path(file).exists()) or (
        isinstance(file, Path) and not file.exists()
    ):
        raise FileNotFoundError(f"File not found: {file}")
    rate = 0

    container = av.open(file, format=format)
    audio_stream = next(s for s in container.streams if s.type == "audio")
    channels = 1 if audio_stream.layout == "mono" else 2
    container.seek(0)
    resampler = (
        AudioResampler(format="fltp", layout=audio_stream.layout, rate=sr)
        if sr is not None
        else None
    )

    # Estimated maximum total number of samples to pre-allocate the array
    # AV stores length in microseconds by default
    estimated_total_samples = (
        int(container.duration * sr // 1_000_000) if sr is not None else 48000
    )
    decoded_audio = np.zeros(
        (
            estimated_total_samples + 1
            if channels == 1
            else (channels, estimated_total_samples + 1)
        ),
        dtype=np.float32,
    )

    offset = 0

    def process_packet(packet: List[AudioFrame]):
        frames_data = []
        rate = 0
        for frame in packet:
            # frame.pts = None  # 清除时间戳，避免重新采样问题
            resampled_frames = (
                resampler.resample(frame) if resampler is not None else [frame]
            )
            for resampled_frame in resampled_frames:
                frame_data = resampled_frame.to_ndarray()
                rate = resampled_frame.rate
                frames_data.append(frame_data)
        return (rate, frames_data)

    def frame_iter(container):
        for p in container.demux(container.streams.audio[0]):
            yield p.decode()

    for r, frames_data in map(process_packet, frame_iter(container)):
        if not rate:
            rate = r
        for frame_data in frames_data:
            end_index = offset + len(frame_data[0])

            # 检查 decoded_audio 是否有足够的空间，并在必要时调整大小
            if end_index > decoded_audio.shape[1]:
                decoded_audio = np.resize(
                    decoded_audio, (decoded_audio.shape[0], end_index * 4)
                )

            np.copyto(decoded_audio[..., offset:end_index], frame_data)
            offset += len(frame_data[0])

    container.close()

    # Truncate the array to the actual size
    decoded_audio = decoded_audio[..., :offset]

    if mono and decoded_audio.shape[0] > 1:
        decoded_audio = decoded_audio.mean(0)

    if sr is not None:
        return decoded_audio
    return decoded_audio, rate



================================================
FILE: tools/audio/ffmpeg.py
================================================
from pydub.utils import which


def has_ffmpeg_installed() -> bool:
    return which("ffmpeg") and which("ffprobe")



================================================
FILE: tools/audio/np.py
================================================
import math

import numpy as np
from numba import jit


@jit(nopython=True)
def float_to_int16(audio: np.ndarray) -> np.ndarray:
    am = int(math.ceil(float(np.abs(audio).max())) * 32768)
    am = 32767 * 32768 // am
    return np.multiply(audio, am).astype(np.int16)



================================================
FILE: tools/audio/pcm.py
================================================
import wave
from io import BytesIO

import numpy as np

from .np import float_to_int16
from .av import wav2


def pcm_arr_to_mp3_view(wav: np.ndarray):
    buf = BytesIO()
    with wave.open(buf, "wb") as wf:
        wf.setnchannels(1)  # Mono channel
        wf.setsampwidth(2)  # Sample width in bytes
        wf.setframerate(24000)  # Sample rate in Hz
        wf.writeframes(float_to_int16(wav))
    buf.seek(0, 0)
    buf2 = BytesIO()
    wav2(buf, buf2, "mp3")
    buf.seek(0, 0)
    return buf2.getbuffer()



================================================
FILE: tools/checksum/main.go
================================================
package main

import (
	"crypto/sha256"
	"encoding/hex"
	"fmt"
	"io"
	"os"
)

func main() {
	var buf [32]byte
	h := sha256.New()
	lst := make([]any, 0, 64)
	for _, fname := range files {
		f, err := os.Open(fname)
		if err != nil {
			panic(err)
		}
		_, err = io.Copy(h, f)
		if err != nil {
			panic(err)
		}
		s := hex.EncodeToString(h.Sum(buf[:0]))
		fmt.Println("sha256 of", fname, "=", s)
		lst = append(lst, s)
		h.Reset()
		f.Close()
	}
	f, err := os.Create("ChatTTS/res/sha256_map.json")
	if err != nil {
		panic(err)
	}
	_, err = fmt.Fprintf(f, jsontmpl, lst...)
	if err != nil {
		panic(err)
	}
}



================================================
FILE: tools/checksum/tmpl.go
================================================
package main

var files = [...]string{
	"asset/Decoder.safetensors",
	"asset/DVAE.safetensors",
	"asset/Embed.safetensors",
	"asset/Vocos.safetensors",

	"asset/gpt/config.json",
	"asset/gpt/model.safetensors",

	"asset/tokenizer/special_tokens_map.json",
	"asset/tokenizer/tokenizer_config.json",
	"asset/tokenizer/tokenizer.json",
}

const jsontmpl = `{
	"sha256_asset_Decoder_safetensors": "%s",
	"sha256_asset_DVAE_safetensors"   : "%s",
	"sha256_asset_Embed_safetensors"  : "%s",
	"sha256_asset_Vocos_safetensors"  : "%s",

	"sha256_asset_gpt_config_json"         : "%s",
	"sha256_asset_gpt_model_safetensors"   : "%s",

	"sha256_asset_tokenizer_special_tokens_map_json": "%s",
	"sha256_asset_tokenizer_tokenizer_config_json"  : "%s",
	"sha256_asset_tokenizer_tokenizer_json"         : "%s"
}
`



================================================
FILE: tools/llm/__init__.py
================================================
from .llm import ChatOpenAI



================================================
FILE: tools/llm/llm.py
================================================
from openai import OpenAI

prompt_dict = {
    "kimi": [
        {
            "role": "system",
            "content": "你是 Kimi，由 Moonshot AI 提供的人工智能助手，你更擅长中文和英文的对话。",
        },
        {
            "role": "user",
            "content": "你好，请注意你现在生成的文字要按照人日常生活的口吻，你的回复将会后续用TTS模型转为语音，并且请把回答控制在100字以内。并且标点符号仅包含逗号和句号，将数字等转为文字回答。",
        },
        {
            "role": "assistant",
            "content": "好的，我现在生成的文字将按照人日常生活的口吻， 并且我会把回答控制在一百字以内, 标点符号仅包含逗号和句号，将阿拉伯数字等转为中文文字回答。下面请开始对话。",
        },
    ],
    "deepseek": [
        {"role": "system", "content": "You are a helpful assistant"},
        {
            "role": "user",
            "content": "你好，请注意你现在生成的文字要按照人日常生活的口吻，你的回复将会后续用TTS模型转为语音，并且请把回答控制在100字以内。并且标点符号仅包含逗号和句号，将数字等转为文字回答。",
        },
        {
            "role": "assistant",
            "content": "好的，我现在生成的文字将按照人日常生活的口吻， 并且我会把回答控制在一百字以内, 标点符号仅包含逗号和句号，将阿拉伯数字等转为中文文字回答。下面请开始对话。",
        },
    ],
    "deepseek_TN": [
        {"role": "system", "content": "You are a helpful assistant"},
        {
            "role": "user",
            "content": "你好，现在我们在处理TTS的文本输入，下面将会给你输入一段文本，请你将其中的阿拉伯数字等等转为文字表达，并且输出的文本里仅包含逗号和句号这两个标点符号",
        },
        {
            "role": "assistant",
            "content": "好的，我现在对TTS的文本输入进行处理。这一般叫做text normalization。下面请输入",
        },
        {"role": "user", "content": "We paid $123 for this desk."},
        {
            "role": "assistant",
            "content": "We paid one hundred and twenty three dollars for this desk.",
        },
        {"role": "user", "content": "详询请拨打010-724654"},
        {"role": "assistant", "content": "详询请拨打零幺零，七二四六五四"},
        {"role": "user", "content": "罗森宣布将于7月24日退市，在华门店超6000家！"},
        {
            "role": "assistant",
            "content": "罗森宣布将于七月二十四日退市，在华门店超过六千家。",
        },
    ],
}


class ChatOpenAI:
    def __init__(self, api_key, base_url, model):
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )
        self.model = model

    def call(self, user_question, temperature=0.3, prompt_version="kimi", **kwargs):

        completion = self.client.chat.completions.create(
            model=self.model,
            messages=prompt_dict[prompt_version]
            + [
                {"role": "user", "content": user_question},
            ],
            temperature=temperature,
            **kwargs
        )
        return completion.choices[0].message.content



================================================
FILE: tools/logger/__init__.py
================================================
from .log import get_logger



================================================
FILE: tools/logger/log.py
================================================
import platform, sys
import logging
from datetime import datetime, timezone

logging.getLogger("numba").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("wetext-zh_normalizer").setLevel(logging.WARNING)
logging.getLogger("NeMo-text-processing").setLevel(logging.WARNING)

# from https://github.com/FloatTech/ZeroBot-Plugin/blob/c70766a989698452e60e5e48fb2f802a2444330d/console/console_windows.go#L89-L96
colorCodePanic = "\x1b[1;31m"
colorCodeFatal = "\x1b[1;31m"
colorCodeError = "\x1b[31m"
colorCodeWarn = "\x1b[33m"
colorCodeInfo = "\x1b[37m"
colorCodeDebug = "\x1b[32m"
colorCodeTrace = "\x1b[36m"
colorReset = "\x1b[0m"

log_level_color_code = {
    logging.DEBUG: colorCodeDebug,
    logging.INFO: colorCodeInfo,
    logging.WARN: colorCodeWarn,
    logging.ERROR: colorCodeError,
    logging.FATAL: colorCodeFatal,
}

log_level_msg_str = {
    logging.DEBUG: "DEBU",
    logging.INFO: "INFO",
    logging.WARN: "WARN",
    logging.ERROR: "ERRO",
    logging.FATAL: "FATL",
}


class Formatter(logging.Formatter):
    def __init__(self, color=platform.system().lower() != "windows"):
        # https://stackoverflow.com/questions/2720319/python-figure-out-local-timezone
        self.tz = datetime.now(timezone.utc).astimezone().tzinfo
        self.color = color

    def format(self, record: logging.LogRecord):
        logstr = "[" + datetime.now(self.tz).strftime("%z %Y%m%d %H:%M:%S") + "] ["
        if self.color:
            logstr += log_level_color_code.get(record.levelno, colorCodeInfo)
        logstr += log_level_msg_str.get(record.levelno, record.levelname)
        if self.color:
            logstr += colorReset
        if sys.version_info >= (3, 9):
            fn = record.filename.removesuffix(".py")
        elif record.filename.endswith(".py"):
            fn = record.filename[:-3]
        logstr += f"] {str(record.name)} | {fn} | {str(record.msg)%record.args}"
        return logstr


def get_logger(name: str, lv=logging.INFO, remove_exist=False, format_root=False):
    logger = logging.getLogger(name)
    logger.setLevel(lv)
    if remove_exist and logger.hasHandlers():
        logger.handlers.clear()
    if not logger.hasHandlers():
        syslog = logging.StreamHandler()
        syslog.setFormatter(Formatter())
        logger.addHandler(syslog)
    else:
        for h in logger.handlers:
            h.setFormatter(Formatter())
    if format_root:
        for h in logger.root.handlers:
            h.setFormatter(Formatter())
    return logger



================================================
FILE: tools/normalizer/__init__.py
================================================
from .en import normalizer_en_nemo_text
from .zh import normalizer_zh_tn



================================================
FILE: tools/normalizer/en.py
================================================
from typing import Callable
from functools import partial


def normalizer_en_nemo_text() -> Callable[[str], str]:
    from nemo_text_processing.text_normalization.normalize import Normalizer

    return partial(
        Normalizer(input_case="cased", lang="en").normalize,
        verbose=False,
        punct_post_process=True,
    )



================================================
FILE: tools/normalizer/zh.py
================================================
from typing import Callable


def normalizer_zh_tn() -> Callable[[str], str]:
    from tn.chinese.normalizer import Normalizer

    return Normalizer(remove_interjections=False).normalize



================================================
FILE: tools/seeder/__init__.py
================================================
from .ctx import TorchSeedContext



================================================
FILE: tools/seeder/ctx.py
================================================
import torch


class TorchSeedContext:
    def __init__(self, seed):
        self.seed = seed
        self.state = None

    def __enter__(self):
        self.state = torch.random.get_rng_state()
        torch.manual_seed(self.seed)

    def __exit__(self, type, value, traceback):
        torch.random.set_rng_state(self.state)



================================================
FILE: .github/workflows/checksum.yml
================================================
name: Calculate and Sync SHA256
on:
  workflow_dispatch:

jobs:
  checksum:
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@v4

      - name: Setup Go Environment
        uses: actions/setup-go@v5

      - name: Run RVC-Models-Downloader
        run: |
          wget https://github.com/fumiama/RVC-Models-Downloader/releases/download/v0.2.10/rvcmd_linux_amd64.deb
          sudo apt -y install ./rvcmd_linux_amd64.deb
          rm -f ./rvcmd_linux_amd64.deb
          rvcmd -notrs -w 1 -notui assets/chtts

      - name: Calculate all Checksums
        run: go run tools/checksum/*.go

      - name: Commit back
        if: ${{ !github.head_ref }}
        id: commitback
        continue-on-error: true
        run: |
          git config --local user.name 'github-actions[bot]'
          git config --local user.email 'github-actions[bot]@users.noreply.github.com'
          git add --all
          git commit -m "chore(env): sync checksum on ${{github.ref_name}}"

      - name: Create Pull Request
        if: steps.commitback.outcome == 'success'
        continue-on-error: true
        uses: peter-evans/create-pull-request@v5
        with:
          delete-branch: true
          body: "Automatically sync checksum in .env"
          title: "chore(env): sync checksum on ${{github.ref_name}}"
          commit-message: "chore(env): sync checksum on ${{github.ref_name}}"
          branch: checksum-${{github.ref_name}}



================================================
FILE: .github/workflows/close-issue.yml
================================================
name: Close Inactive Issues
on:
  schedule:
    - cron: "0 4 * * *"

jobs:
  close-issues:
    runs-on: ubuntu-24.04
    permissions:
      issues: write
      pull-requests: write
    steps:
      - uses: actions/stale@v5
        with:
          exempt-issue-labels: "help wanted,following up,todo list,enhancement,algorithm,delayed,performance"
          days-before-issue-stale: 30
          days-before-issue-close: 15
          stale-issue-label: "stale"
          close-issue-message: "This issue was closed because it has been inactive for 15 days since being marked as stale."
          days-before-pr-stale: -1
          days-before-pr-close: -1
          operations-per-run: 10000
          repo-token: ${{ secrets.GITHUB_TOKEN }}



================================================
FILE: .github/workflows/pull-format.yml
================================================
name: Check Pull Request Format

on:
  pull_request_target:
    types: [opened, reopened, synchronize]

jobs:
  # This workflow closes invalid PR
  change-or-close-pr:
    # The type of runner that the job will run on
    runs-on: ubuntu-24.04
    permissions: write-all

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      - name: Change Base Branch
        if: github.event.pull_request.base.ref != 'dev'
        uses: actions/github-script@v4
        id: change-base
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const { owner, repo, number } = context.issue;
            const newBase = 'dev';
            try {
              const result = await github.pulls.update({
                owner,
                repo,
                pull_number: number,
                base: newBase
              });
              console.log(result);
              return 'success';
            } catch (error) {
              console.log(error);
              return 'failed';
            }

      - name: Close PR if it is not pointed to dev Branch
        if: "github.event.pull_request.base.ref != 'dev' && steps.change-base.outputs.result == 'failed'"
        uses: superbrothers/close-pull-request@v3
        with:
          # Optional. Post a issue comment just before closing a pull request.
          comment: "Invalid PR to `non-dev` branch `${{ github.event.pull_request.base.ref }}`."

  pull-format:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    continue-on-error: true

    steps:
      - name: Checkout Repo
        continue-on-error: true
        uses: actions/checkout@v4

      - name: Checkout PR # see https://github.com/orgs/community/discussions/24945
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh pr checkout ${{ github.event.pull_request.number }}

      - name: Set up Python
        uses: actions/setup-python@v5

      - name: Create venv
        run: python3 -m venv .venv

      - name: Activate venv
        run: |
          . .venv/bin/activate
          echo PATH=$PATH >> $GITHUB_ENV

      - name: Install Black
        run: pip install "black[jupyter]"

      - name: Run Black
        # run: black $(git ls-files '*.py')
        run: black .

      - name: Commit back
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true
        run: |
          git config --local user.name 'github-actions[bot]'
          git config --local user.email 'github-actions[bot]@users.noreply.github.com'
          git add --all
          git commit -m "chore(format): run black on ${{github.ref_name}}"
          git push



================================================
FILE: .github/workflows/push-format.yml
================================================
name: Standardize Code Format

on:
  push:
    branches:
      - main
      - dev

jobs:
  push-format:
    runs-on: ubuntu-latest

    if: "!contains(github.event.head_commit.message, 'chore(format): ') && !contains(github.event.head_commit.message, 'chore(env): ')"
  
    permissions:
      contents: write
      pull-requests: write

    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{github.ref_name}}

      - name: Set up Python
        uses: actions/setup-python@v5

      - name: Create venv
        run: python3 -m venv .venv

      - name: Activate venv
        run: |
          . .venv/bin/activate
          echo PATH=$PATH >> $GITHUB_ENV

      - name: Install Black
        run: pip install "black[jupyter]"

      - name: Run Black
        # run: black $(git ls-files '*.py')
        run: black .

      - name: Commit Back
        continue-on-error: true
        id: commitback
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add --all
          git commit -m "chore(format): run black on ${{github.ref_name}}"

      - name: Create Pull Request
        if: steps.commitback.outcome == 'success'
        continue-on-error: true
        uses: peter-evans/create-pull-request@v5
        with:
          delete-branch: true
          body: "Automatically apply code formatter change"
          title: "chore(format): run black on ${{github.ref_name}}"
          commit-message: "chore(format): run black on ${{github.ref_name}}"
          branch: formatter-${{github.ref_name}}



================================================
FILE: .github/workflows/unitest.yml
================================================
name: Unit Test
on: [ push, pull_request ]
jobs:
  build:
    runs-on: ${{ matrix.os }}

    if: "!contains(github.event.head_commit.message, 'chore(format): ') && !contains(github.event.head_commit.message, 'chore(env): ')"

    strategy:
      matrix:
        python-version: ["3.8", "3.9", "3.10"]
        os: [ubuntu-latest]
      fail-fast: true

    steps:

      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install Dependents
        run: |
          sudo apt-get install -y portaudio19-dev python3-pyaudio

      - name: Create venv
        run: python3 -m venv .venv

      - name: Activate venv
        run: |
          . .venv/bin/activate
          echo PATH=$PATH >> $GITHUB_ENV

      - name: Test Install
        run: pip install .

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: Run Test
        run: tests/testall.sh



================================================
FILE: .github/workflows/upload-pypi.yml
================================================
name: Upload to PyPI

on:
  push:
    tags:
      - 'v*'

jobs:
  build:
    runs-on: ubuntu-22.04

    steps:

      - uses: actions/checkout@v4
        with:
          ref: ${{github.ref_name}}

      - name: Set up Python
        uses: actions/setup-python@v5

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install --upgrade setuptools
          python -m pip install --upgrade wheel
          pip install twine

      - name: Build Package
        env:
          CHTTS_VER: ${{ github.ref_name }}
        run: |
          echo "Release Tag: ${{ github.ref_name }}"
          sed -i 's/v0.0.0/${{ github.ref_name }}/g' setup.py
          python setup.py sdist

      - name: Upload Package
        run: |
          twine upload dist/* -u "__token__" -p ${{ secrets.PYPI_TOKEN }}


