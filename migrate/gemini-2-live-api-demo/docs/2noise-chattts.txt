Directory structure:
â””â”€â”€ 2noise-chattts/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ setup.py
    â”œâ”€â”€ ChatTTS/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ core.py
    â”‚   â”œâ”€â”€ norm.py
    â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ config.py
    â”‚   â”œâ”€â”€ model/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ dvae.py
    â”‚   â”‚   â”œâ”€â”€ embed.py
    â”‚   â”‚   â”œâ”€â”€ gpt.py
    â”‚   â”‚   â”œâ”€â”€ processors.py
    â”‚   â”‚   â”œâ”€â”€ speaker.py
    â”‚   â”‚   â”œâ”€â”€ tokenizer.py
    â”‚   â”‚   â”œâ”€â”€ cuda/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ patch.py
    â”‚   â”‚   â”‚   â””â”€â”€ te_llama.py
    â”‚   â”‚   â””â”€â”€ velocity/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ block_manager.py
    â”‚   â”‚       â”œâ”€â”€ configs.py
    â”‚   â”‚       â”œâ”€â”€ llama.py
    â”‚   â”‚       â”œâ”€â”€ llm.py
    â”‚   â”‚       â”œâ”€â”€ llm_engine.py
    â”‚   â”‚       â”œâ”€â”€ model_loader.py
    â”‚   â”‚       â”œâ”€â”€ model_runner.py
    â”‚   â”‚       â”œâ”€â”€ output.py
    â”‚   â”‚       â”œâ”€â”€ sampler.py
    â”‚   â”‚       â”œâ”€â”€ sampling_params.py
    â”‚   â”‚       â”œâ”€â”€ scheduler.py
    â”‚   â”‚       â”œâ”€â”€ sequence.py
    â”‚   â”‚       â””â”€â”€ worker.py
    â”‚   â”œâ”€â”€ res/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ homophones_map.json
    â”‚   â”‚   â””â”€â”€ sha256_map.json
    â”‚   â””â”€â”€ utils/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ dl.py
    â”‚       â”œâ”€â”€ gpu.py
    â”‚       â”œâ”€â”€ io.py
    â”‚       â””â”€â”€ log.py
    â”œâ”€â”€ docs/
    â”‚   â”œâ”€â”€ cn/
    â”‚   â”‚   â””â”€â”€ README.md
    â”‚   â”œâ”€â”€ es/
    â”‚   â”‚   â””â”€â”€ README.md
    â”‚   â”œâ”€â”€ fr/
    â”‚   â”‚   â””â”€â”€ README.md
    â”‚   â”œâ”€â”€ jp/
    â”‚   â”‚   â””â”€â”€ README.md
    â”‚   â”œâ”€â”€ kr/
    â”‚   â”‚   â””â”€â”€ README.md
    â”‚   â””â”€â”€ ru/
    â”‚       â””â”€â”€ README.md
    â”œâ”€â”€ examples/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ api/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ client.py
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ postScript.py
    â”‚   â”‚   â””â”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ cmd/
    â”‚   â”‚   â”œâ”€â”€ run.py
    â”‚   â”‚   â””â”€â”€ stream.py
    â”‚   â”œâ”€â”€ ipynb/
    â”‚   â”‚   â”œâ”€â”€ colab.ipynb
    â”‚   â”‚   â””â”€â”€ example.ipynb
    â”‚   â”œâ”€â”€ onnx/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ exporter.py
    â”‚   â”‚   â”œâ”€â”€ gpt.py
    â”‚   â”‚   â””â”€â”€ modeling_llama.py
    â”‚   â””â”€â”€ web/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ ex.py
    â”‚       â”œâ”€â”€ funcs.py
    â”‚       â””â”€â”€ webui.py
    â”œâ”€â”€ tests/
    â”‚   â”œâ”€â”€ #511.py
    â”‚   â”œâ”€â”€ #588.py
    â”‚   â”œâ”€â”€ #655.py
    â”‚   â””â”€â”€ testall.sh
    â”œâ”€â”€ tools/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ audio/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ av.py
    â”‚   â”‚   â”œâ”€â”€ ffmpeg.py
    â”‚   â”‚   â”œâ”€â”€ np.py
    â”‚   â”‚   â””â”€â”€ pcm.py
    â”‚   â”œâ”€â”€ checksum/
    â”‚   â”‚   â”œâ”€â”€ main.go
    â”‚   â”‚   â””â”€â”€ tmpl.go
    â”‚   â”œâ”€â”€ llm/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ llm.py
    â”‚   â”œâ”€â”€ logger/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ log.py
    â”‚   â”œâ”€â”€ normalizer/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ en.py
    â”‚   â”‚   â””â”€â”€ zh.py
    â”‚   â””â”€â”€ seeder/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â””â”€â”€ ctx.py
    â””â”€â”€ .github/
        â””â”€â”€ workflows/
            â”œâ”€â”€ checksum.yml
            â”œâ”€â”€ close-issue.yml
            â”œâ”€â”€ pull-format.yml
            â”œâ”€â”€ push-format.yml
            â”œâ”€â”€ unitest.yml
            â””â”€â”€ upload-pypi.yml

================================================
FILE: README.md
================================================
<div align="center">

<a href="https://trendshift.io/repositories/10489" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10489" alt="2noise%2FChatTTS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

# ChatTTS
A generative speech model for daily dialogue.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)

[![Huggingface](https://img.shields.io/badge/ğŸ¤—%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)

**English** | [**ç®€ä½“ä¸­æ–‡**](docs/cn/README.md) | [**æ—¥æœ¬èª**](docs/jp/README.md) | [**Ğ ÑƒÑÑĞºĞ¸Ğ¹**](docs/ru/README.md) | [**EspaÃ±ol**](docs/es/README.md) | [**FranÃ§ais**](docs/fr/README.md) | [**í•œêµ­ì–´**](docs/kr/README.md)

</div>

## Introduction
> [!Note]
> This repo contains the algorithm infrastructure and some simple examples.

> [!Tip]
> For the extended end-user products, please refer to the index repo [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS/tree/en) maintained by the community.

ChatTTS is a text-to-speech model designed specifically for dialogue scenarios such as LLM assistant.

### Supported Languages
- [x] English
- [x] Chinese
- [ ] Coming Soon...

### Highlights
> You can refer to **[this video on Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)** for the detailed description.

1. **Conversational TTS**: ChatTTS is optimized for dialogue-based tasks, enabling natural and expressive speech synthesis. It supports multiple speakers, facilitating interactive conversations.
2. **Fine-grained Control**: The model could predict and control fine-grained prosodic features, including laughter, pauses, and interjections. 
3. **Better Prosody**: ChatTTS surpasses most of open-source TTS models in terms of prosody. We provide pretrained models to support further research and development.

### Dataset & Model
> [!Important]
> The released model is for academic purposes only.

- The main model is trained with Chinese and English audio data of 100,000+ hours.
- The open-source version on **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** is a 40,000 hours pre-trained model without SFT.

### Roadmap
- [x] Open-source the 40k-hours-base model and spk_stats file.
- [x] Streaming audio generation.
- [x] Open-source DVAE encoder and zero shot inferring code.
- [ ] Multi-emotion controlling.
- [ ] ChatTTS.cpp (new repo in `2noise` org is welcomed)

### Licenses

#### The Code

The code is published under `AGPLv3+` license.

#### The model

The model is published under `CC BY-NC 4.0` license. It is intended for educational and research use, and should not be used for any commercial or illegal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information. The information and data used in this repo, are for academic and research purposes only. The data obtained from publicly available sources, and the authors do not claim any ownership or copyright over the data.

### Disclaimer

ChatTTS is a powerful text-to-speech system. However, it is very important to utilize this technology responsibly and ethically. To limit the use of ChatTTS, we added a small amount of high-frequency noise during the training of the 40,000-hour model, and compressed the audio quality as much as possible using MP3 format, to prevent malicious actors from potentially using it for criminal purposes. At the same time, we have internally trained a detection model and plan to open-source it in the future.

### Contact
> GitHub issues/PRs are always welcomed.

#### Formal Inquiries
For formal inquiries about the model and roadmap, please contact us at **open-source@2noise.com**.

#### Online Chat
##### 1. QQ Group (Chinese Social APP)
- **Group 1**, 808364215
- **Group 2**, 230696694
- **Group 3**, 933639842
- **Group 4**, 608667975

##### 2. Discord Server
Join by clicking [here](https://discord.gg/Ud5Jxgx5yD).

## Get Started
### Clone Repo
```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### Install requirements
#### 1. Install Directly
```bash
pip install --upgrade -r requirements.txt
```

#### 2. Install from conda
```bash
conda create -n chattts python=3.11
conda activate chattts
pip install -r requirements.txt
```

#### Optional: Install vLLM (Linux only)
```bash
pip install safetensors vllm==0.2.7 torchaudio
```

#### Unrecommended Optional: Install TransformerEngine if using NVIDIA GPU (Linux only)
> [!Warning]
> DO NOT INSTALL! 
> The adaptation of TransformerEngine is currently under development and CANNOT run properly now. 
> Only install it on developing purpose. See more details on at #672 #676

> [!Note]
> The installation process is very slow.

```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable
```

#### Unrecommended Optional: Install FlashAttention-2 (mainly NVIDIA GPU)
> [!Warning]
> DO NOT INSTALL! 
> Currently the FlashAttention-2 will slow down the generating speed according to [this issue](https://github.com/huggingface/transformers/issues/26990). 
> Only install it on developing purpose.

> [!Note]
> See supported devices at the [Hugging Face Doc](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2).


```bash
pip install flash-attn --no-build-isolation
```

### Quick Start
> Make sure you are under the project root directory when you execute these commands below.

#### 1. Launch WebUI
```bash
python examples/web/webui.py
```

#### 2. Infer by Command Line
> It will save audio to `./output_audio_n.mp3`

```bash
python examples/cmd/run.py "Your text 1." "Your text 2."
```

## Installation

1. Install the stable version from PyPI
```bash
pip install ChatTTS
```

2. Install the latest version from GitHub
```bash
pip install git+https://github.com/2noise/ChatTTS
```

3. Install from local directory in dev mode
```bash
pip install -e .
```

### Basic Usage

```python
import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) # Set to True for better performance

texts = ["PUT YOUR 1st TEXT HERE", "PUT YOUR 2nd TEXT HERE"]

wavs = chat.infer(texts)

for i in range(len(wavs)):
    """
    In some versions of torchaudio, the first line works but in other versions, so does the second line.
    """
    try:
        torchaudio.save(f"basic_output{i}.wav", torch.from_numpy(wavs[i]).unsqueeze(0), 24000)
    except:
        torchaudio.save(f"basic_output{i}.wav", torch.from_numpy(wavs[i]), 24000)
```

### Advanced Usage

```python
###################################
# Sample a speaker from Gaussian.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # add sampled speaker 
    temperature = .3,   # using custom temperature
    top_P = 0.7,        # top P decode
    top_K = 20,         # top K decode
)

###################################
# For sentence level manual control.

# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# For word level manual control.

text = 'What is [uv_break]your favorite english food?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
"""
In some versions of torchaudio, the first line works but in other versions, so does the second line.
"""
try:
    torchaudio.save("word_level_output.wav", torch.from_numpy(wavs[0]).unsqueeze(0), 24000)
except:
    torchaudio.save("word_level_output.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Example: self introduction</h4></summary>

```python
inputs_en = """
chat T T S is a text to speech model designed for dialogue applications. 
[uv_break]it supports mixed language input [uv_break]and offers multi speaker 
capabilities with precise control over prosodic elements like 
[uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation. 
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
""".replace('\n', '') # English is still experimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save("self_introduction_output.wav", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align="center">

**male speaker**

</td>
<td align="center">

**female speaker**

</td>
</tr>
<tr>
<td align="center">

[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align="center">

[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>


</details>

## FAQ

#### 1. How much VRAM do I need? How about infer speed?
For a 30-second audio clip, at least 4GB of GPU memory is required. For the 4090 GPU, it can generate audio corresponding to approximately 7 semantic tokens per second. The Real-Time Factor (RTF) is around 0.3.

#### 2. Model stability is not good enough, with issues such as multi speakers or poor audio quality.

This is a problem that typically occurs with autoregressive models (for bark and valle). It's generally difficult to avoid. One can try multiple samples to find a suitable result.

#### 3. Besides laughter, can we control anything else? Can we control other emotions?

In the current released model, the only token-level control units are `[laugh]`, `[uv_break]`, and `[lbreak]`. In future versions, we may open-source models with additional emotional control capabilities.

## Acknowledgements
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) and [valle](https://arxiv.org/abs/2301.02111) demonstrate a remarkable TTS result by an autoregressive-style system.
- [fish-speech](https://github.com/fishaudio/fish-speech) reveals capability of GVQ as audio tokenizer for LLM modeling.
- [vocos](https://github.com/gemelo-ai/vocos) which is used as a pretrained vocoder.

## Special Appreciation
- [wlu-audio lab](https://audio.westlake.edu.cn/) for early algorithm experiments.

## Thanks to all contributors for their efforts
[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

<div align="center">

  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>



================================================
FILE: LICENSE
================================================
                    GNU AFFERO GENERAL PUBLIC LICENSE
                       Version 3, 19 November 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU Affero General Public License is a free, copyleft license for
software and other kinds of works, specifically designed to ensure
cooperation with the community in the case of network server software.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
our General Public Licenses are intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  Developers that use our General Public Licenses protect your rights
with two steps: (1) assert copyright on the software, and (2) offer
you this License which gives you legal permission to copy, distribute
and/or modify the software.

  A secondary benefit of defending all users' freedom is that
improvements made in alternate versions of the program, if they
receive widespread use, become available for other developers to
incorporate.  Many developers of free software are heartened and
encouraged by the resulting cooperation.  However, in the case of
software used on network servers, this result may fail to come about.
The GNU General Public License permits making a modified version and
letting the public access it on a server without ever releasing its
source code to the public.

  The GNU Affero General Public License is designed specifically to
ensure that, in such cases, the modified source code becomes available
to the community.  It requires the operator of a network server to
provide the source code of the modified version running there to the
users of that server.  Therefore, public use of a modified version, on
a publicly accessible server, gives the public access to the source
code of the modified version.

  An older license, called the Affero General Public License and
published by Affero, was designed to accomplish similar goals.  This is
a different license, not a version of the Affero GPL, but Affero has
released a new version of the Affero GPL which permits relicensing under
this license.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU Affero General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Remote Network Interaction; Use with the GNU General Public License.

  Notwithstanding any other provision of this License, if you modify the
Program, your modified version must prominently offer all users
interacting with it remotely through a computer network (if your version
supports such interaction) an opportunity to receive the Corresponding
Source of your version by providing access to the Corresponding Source
from a network server at no charge, through some standard or customary
means of facilitating copying of software.  This Corresponding Source
shall include the Corresponding Source for any work covered by version 3
of the GNU General Public License that is incorporated pursuant to the
following paragraph.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the work with which it is combined will remain governed by version
3 of the GNU General Public License.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU Affero General Public License from time to time.  Such new versions
will be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU Affero General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU Affero General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU Affero General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If your software can interact with users remotely through a computer
network, you should also make sure that it provides a way for users to
get its source.  For example, if your program is a web application, its
interface could display a "Source" link that leads users to an archive
of the code.  There are many ways you could offer source, and different
solutions will be better for different programs; see section 13 for the
specific requirements.

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU AGPL, see
<https://www.gnu.org/licenses/>.



================================================
FILE: requirements.txt
================================================
numpy<2.0.0
numba
torch>=2.1.0
torchaudio
tqdm
vector_quantize_pytorch
transformers>=4.41.1
vocos
IPython
gradio
pybase16384
pynini==2.1.5; sys_platform == 'linux'
WeTextProcessing; sys_platform == 'linux'
nemo_text_processing; sys_platform == 'linux'
av
pydub



================================================
FILE: setup.py
================================================
import os
from setuptools import setup, find_packages

version = "v0.0.0"

setup(
    name="chattts",
    version=os.environ.get("CHTTS_VER", version).lstrip("v"),
    description="A generative speech model for daily dialogue",
    long_description=open("README.md", encoding="utf8").read(),
    long_description_content_type="text/markdown",
    author="2noise",
    author_email="open-source@2noise.com",
    maintainer="fumiama",
    url="https://github.com/2noise/ChatTTS",
    packages=find_packages(include=["ChatTTS", "ChatTTS.*"]),
    package_data={
        "ChatTTS.res": ["homophones_map.json", "sha256_map.json"],
    },
    license="AGPLv3+",
    install_requires=[
        "numba",
        "numpy<2.0.0",
        "pybase16384",
        "torch>=2.1.0",
        "torchaudio",
        "tqdm",
        "transformers>=4.41.1",
        "vector_quantize_pytorch",
        "vocos",
    ],
    platforms="any",
    classifiers=[
        "Programming Language :: Python :: 3",
        "Operating System :: OS Independent",
        "License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)",
    ],
)



================================================
FILE: ChatTTS/__init__.py
================================================
from .core import Chat



================================================
FILE: ChatTTS/core.py
================================================
import os
import re
import logging
import tempfile
from dataclasses import dataclass, asdict
from typing import Literal, Optional, List, Tuple, Dict, Union
from json import load
from pathlib import Path

import numpy as np
import torch
from vocos import Vocos
from vocos.pretrained import instantiate_class
from huggingface_hub import snapshot_download

from .config import Config
from .model import DVAE, Embed, GPT, gen_logits, Tokenizer, Speaker
from .utils import (
    load_safetensors,
    check_all_assets,
    download_all_assets,
    select_device,
    get_latest_modified_file,
    del_all,
)
from .utils import logger as utils_logger

from .norm import Normalizer


class Chat:
    def __init__(self, logger=logging.getLogger(__name__)):
        self.logger = logger
        utils_logger.set_logger(logger)

        self.config = Config()

        self.normalizer = Normalizer(
            os.path.join(os.path.dirname(__file__), "res", "homophones_map.json"),
            logger,
        )
        with open(
            os.path.join(os.path.dirname(__file__), "res", "sha256_map.json")
        ) as f:
            self.sha256_map: Dict[str, str] = load(f)

        self.context = GPT.Context()

    def has_loaded(self, use_decoder=False):
        not_finish = False
        check_list = ["vocos", "gpt", "tokenizer", "embed"]

        if use_decoder:
            check_list.append("decoder")
        else:
            check_list.append("dvae")

        for module in check_list:
            if not hasattr(self, module):
                self.logger.warning(f"{module} not initialized.")
                not_finish = True

        return not not_finish

    def download_models(
        self,
        source: Literal["huggingface", "local", "custom"] = "local",
        force_redownload=False,
        custom_path: Optional[torch.serialization.FILE_LIKE] = None,
    ) -> Optional[str]:
        if source == "local":
            download_path = custom_path if custom_path is not None else os.getcwd()
            if (
                not check_all_assets(Path(download_path), self.sha256_map, update=True)
                or force_redownload
            ):
                with tempfile.TemporaryDirectory() as tmp:
                    download_all_assets(tmpdir=tmp, homedir=download_path)
                if not check_all_assets(
                    Path(download_path), self.sha256_map, update=False
                ):
                    self.logger.error(
                        "download to local path %s failed.", download_path
                    )
                    return None
        elif source == "huggingface":
            try:
                download_path = (
                    get_latest_modified_file(
                        os.path.join(
                            os.getenv(
                                "HF_HOME", os.path.expanduser("~/.cache/huggingface")
                            ),
                            "hub/models--2Noise--ChatTTS/snapshots",
                        )
                    )
                    if custom_path is None
                    else get_latest_modified_file(
                        os.path.join(custom_path, "models--2Noise--ChatTTS/snapshots")
                    )
                )
            except:
                download_path = None
            if download_path is None or force_redownload:
                self.logger.log(
                    logging.INFO,
                    f"download from HF: https://huggingface.co/2Noise/ChatTTS",
                )
                try:
                    download_path = snapshot_download(
                        repo_id="2Noise/ChatTTS",
                        allow_patterns=["*.yaml", "*.json", "*.safetensors"],
                        cache_dir=custom_path,
                        force_download=force_redownload,
                    )
                except:
                    download_path = None
                else:
                    self.logger.log(
                        logging.INFO,
                        f"load latest snapshot from cache: {download_path}",
                    )
        elif source == "custom":
            self.logger.log(logging.INFO, f"try to load from local: {custom_path}")
            if not check_all_assets(Path(custom_path), self.sha256_map, update=False):
                self.logger.error("check models in custom path %s failed.", custom_path)
                return None
            download_path = custom_path

        if download_path is None:
            self.logger.error("Model download failed")
            return None

        return download_path

    def load(
        self,
        source: Literal["huggingface", "local", "custom"] = "local",
        force_redownload=False,
        compile: bool = False,
        custom_path: Optional[torch.serialization.FILE_LIKE] = None,
        device: Optional[torch.device] = None,
        coef: Optional[torch.Tensor] = None,
        use_flash_attn=False,
        use_vllm=False,
        experimental: bool = False,
    ) -> bool:
        download_path = self.download_models(source, force_redownload, custom_path)
        if download_path is None:
            return False
        return self._load(
            device=device,
            compile=compile,
            coef=coef,
            use_flash_attn=use_flash_attn,
            use_vllm=use_vllm,
            experimental=experimental,
            **{
                k: os.path.join(download_path, v)
                for k, v in asdict(self.config.path).items()
            },
        )

    def unload(self):
        logger = self.logger
        self.normalizer.destroy()
        del self.normalizer
        del self.sha256_map
        del_list = ["vocos", "gpt", "decoder", "dvae", "tokenizer", "embed"]
        for module in del_list:
            if hasattr(self, module):
                delattr(self, module)
        self.__init__(logger)

    def sample_random_speaker(self) -> str:
        return self.speaker.sample_random()

    def sample_audio_speaker(self, wav: Union[np.ndarray, torch.Tensor]) -> str:
        return self.speaker.encode_prompt(self.dvae.sample_audio(wav))

    @dataclass(repr=False, eq=False)
    class RefineTextParams:
        prompt: str = ""
        top_P: float = 0.7
        top_K: int = 20
        temperature: float = 0.7
        repetition_penalty: float = 1.0
        max_new_token: int = 384
        min_new_token: int = 0
        show_tqdm: bool = True
        ensure_non_empty: bool = True
        manual_seed: Optional[int] = None

    @dataclass(repr=False, eq=False)
    class InferCodeParams(RefineTextParams):
        prompt: str = "[speed_5]"
        spk_emb: Optional[str] = None
        spk_smp: Optional[str] = None
        txt_smp: Optional[str] = None
        temperature: float = 0.3
        repetition_penalty: float = 1.05
        max_new_token: int = 2048
        stream_batch: int = 24
        stream_speed: int = 12000
        pass_first_n_batches: int = 2

    def infer(
        self,
        text,
        stream=False,
        lang=None,
        skip_refine_text=False,
        refine_text_only=False,
        use_decoder=True,
        do_text_normalization=True,
        do_homophone_replacement=True,
        split_text=True,
        max_split_batch=4,
        params_refine_text=RefineTextParams(),
        params_infer_code=InferCodeParams(),
    ):
        self.context.set(False)

        if split_text and isinstance(text, str):
            if "\n" in text:
                text = text.split("\n")
            else:
                text = re.split(r"(?<=ã€‚)|(?<=\.\s)", text)
                nt = []
                if isinstance(text, list):
                    for t in text:
                        if t:
                            nt.append(t)
                    text = nt
                else:
                    text = [text]
            self.logger.info("split text into %d parts", len(text))
            self.logger.debug("%s", str(text))

        if len(text) == 0:
            return []

        res_gen = self._infer(
            text,
            stream,
            lang,
            skip_refine_text,
            refine_text_only,
            use_decoder,
            do_text_normalization,
            do_homophone_replacement,
            split_text,
            max_split_batch,
            params_refine_text,
            params_infer_code,
        )
        if stream:
            return res_gen
        elif not refine_text_only:
            stripped_wavs = []
            for wavs in res_gen:
                for wav in wavs:
                    stripped_wavs.append(wav[np.abs(wav) > 1e-5])
            if split_text:
                return [np.concatenate(stripped_wavs)]
            return stripped_wavs
        else:
            return next(res_gen)

    def interrupt(self):
        self.context.set(True)

    @torch.no_grad()
    def _load(
        self,
        vocos_ckpt_path: str = None,
        dvae_ckpt_path: str = None,
        gpt_ckpt_path: str = None,
        embed_path: str = None,
        decoder_ckpt_path: str = None,
        tokenizer_path: str = None,
        device: Optional[torch.device] = None,
        compile: bool = False,
        coef: Optional[str] = None,
        use_flash_attn=False,
        use_vllm=False,
        experimental: bool = False,
    ):
        if device is None:
            device = select_device(experimental=experimental)
            self.logger.info("use device %s", str(device))
        self.device = device
        self.device_gpt = device if "mps" not in str(device) else torch.device("cpu")
        self.compile = compile

        feature_extractor = instantiate_class(
            args=(), init=asdict(self.config.vocos.feature_extractor)
        )
        backbone = instantiate_class(args=(), init=asdict(self.config.vocos.backbone))
        head = instantiate_class(args=(), init=asdict(self.config.vocos.head))
        vocos = (
            Vocos(feature_extractor=feature_extractor, backbone=backbone, head=head)
            .to(
                # Vocos on mps will crash, use cpu fallback.
                # Plus, complex dtype used in the decode process of Vocos is not supported in torch_npu now,
                # so we put this calculation of data on CPU instead of NPU.
                "cpu"
                if "mps" in str(device) or "npu" in str(device)
                else device
            )
            .eval()
        )
        assert vocos_ckpt_path, "vocos_ckpt_path should not be None"
        vocos.load_state_dict(load_safetensors(vocos_ckpt_path))
        self.vocos = vocos
        self.logger.log(logging.INFO, "vocos loaded.")

        # computation of MelSpectrogram on npu is not support now, use cpu fallback.
        dvae_device = torch.device("cpu") if "npu" in str(self.device) else device
        dvae = DVAE(
            decoder_config=asdict(self.config.dvae.decoder),
            encoder_config=asdict(self.config.dvae.encoder),
            vq_config=asdict(self.config.dvae.vq),
            dim=self.config.dvae.decoder.idim,
            coef=coef,
            device=dvae_device,
        )
        coef = str(dvae)
        assert dvae_ckpt_path, "dvae_ckpt_path should not be None"
        dvae.load_pretrained(dvae_ckpt_path, dvae_device)
        self.dvae = dvae.eval()
        self.logger.log(logging.INFO, "dvae loaded.")

        embed = Embed(
            self.config.embed.hidden_size,
            self.config.embed.num_audio_tokens,
            self.config.embed.num_text_tokens,
            self.config.embed.num_vq,
        )
        embed.load_pretrained(embed_path, device=device)
        self.embed = embed.to(device)
        self.logger.log(logging.INFO, "embed loaded.")

        gpt = GPT(
            gpt_config=asdict(self.config.gpt),
            embed=self.embed,
            use_flash_attn=use_flash_attn,
            use_vllm=use_vllm,
            device=device,
            device_gpt=self.device_gpt,
            logger=self.logger,
        ).eval()
        assert gpt_ckpt_path, "gpt_ckpt_path should not be None"
        gpt.load_pretrained(gpt_ckpt_path, embed_path, experimental=experimental)
        gpt.prepare(compile=compile and "cuda" in str(device))
        self.gpt = gpt
        self.logger.log(logging.INFO, "gpt loaded.")

        self.speaker = Speaker(
            self.config.gpt.hidden_size, self.config.spk_stat, device
        )
        self.logger.log(logging.INFO, "speaker loaded.")

        decoder = DVAE(
            decoder_config=asdict(self.config.decoder),
            dim=self.config.decoder.idim,
            coef=coef,
            device=device,
        )
        coef = str(decoder)
        assert decoder_ckpt_path, "decoder_ckpt_path should not be None"
        decoder.load_pretrained(decoder_ckpt_path, device)
        self.decoder = decoder.eval()
        self.logger.log(logging.INFO, "decoder loaded.")

        if tokenizer_path:
            self.tokenizer = Tokenizer(tokenizer_path)
            self.logger.log(logging.INFO, "tokenizer loaded.")

        self.coef = coef

        return self.has_loaded()

    def _infer(
        self,
        text: Union[List[str], str],
        stream=False,
        lang=None,
        skip_refine_text=False,
        refine_text_only=False,
        use_decoder=True,
        do_text_normalization=True,
        do_homophone_replacement=True,
        split_text=True,
        max_split_batch=4,
        params_refine_text=RefineTextParams(),
        params_infer_code=InferCodeParams(),
    ):

        assert self.has_loaded(use_decoder=use_decoder)

        if not isinstance(text, list):
            text = [text]

        text = [
            self.normalizer(
                t,
                do_text_normalization,
                do_homophone_replacement,
                lang,
            )
            for t in text
        ]

        self.logger.debug("normed texts %s", str(text))

        if not skip_refine_text:
            refined = self._refine_text(
                text,
                self.device,
                params_refine_text,
            )
            text_tokens = refined.ids
            text_tokens = [i[i.less(self.tokenizer.break_0_ids)] for i in text_tokens]
            text = self.tokenizer.decode(text_tokens)
            refined.destroy()
            if refine_text_only:
                if split_text and isinstance(text, list):
                    text = "\n".join(text)
                yield text
                return

        if split_text and len(text) > 1 and params_infer_code.spk_smp is None:
            refer_text = text[0]
            result = next(
                self._infer_code(
                    refer_text,
                    False,
                    self.device,
                    use_decoder,
                    params_infer_code,
                )
            )
            wavs = self._decode_to_wavs(
                result.hiddens if use_decoder else result.ids,
                use_decoder,
            )
            result.destroy()
            assert len(wavs), 1
            params_infer_code.spk_smp = self.sample_audio_speaker(wavs[0])
            params_infer_code.txt_smp = refer_text

        if stream:
            length = 0
            pass_batch_count = 0
        if split_text:
            n = len(text) // max_split_batch
            if len(text) % max_split_batch:
                n += 1
        else:
            n = 1
            max_split_batch = len(text)
        for i in range(n):
            text_remain = text[i * max_split_batch :]
            if len(text_remain) > max_split_batch:
                text_remain = text_remain[:max_split_batch]
            if split_text:
                self.logger.info(
                    "infer split %d~%d",
                    i * max_split_batch,
                    i * max_split_batch + len(text_remain),
                )
            for result in self._infer_code(
                text_remain,
                stream,
                self.device,
                use_decoder,
                params_infer_code,
            ):
                wavs = self._decode_to_wavs(
                    result.hiddens if use_decoder else result.ids,
                    use_decoder,
                )
                result.destroy()
                if stream:
                    pass_batch_count += 1
                    if pass_batch_count <= params_infer_code.pass_first_n_batches:
                        continue
                    a = length
                    b = a + params_infer_code.stream_speed
                    if b > wavs.shape[1]:
                        b = wavs.shape[1]
                    new_wavs = wavs[:, a:b]
                    length = b
                    yield new_wavs
                else:
                    yield wavs
            if stream:
                new_wavs = wavs[:, length:]
                keep_cols = np.sum(np.abs(new_wavs) > 1e-5, axis=0) > 0
                yield new_wavs[:][:, keep_cols]

    @torch.inference_mode()
    def _vocos_decode(self, spec: torch.Tensor) -> np.ndarray:
        if "mps" in str(self.device) or "npu" in str(self.device):
            return self.vocos.decode(spec.cpu()).cpu().numpy()
        else:
            return self.vocos.decode(spec).cpu().numpy()

    @torch.inference_mode()
    def _decode_to_wavs(
        self,
        result_list: List[torch.Tensor],
        use_decoder: bool,
    ):
        decoder = self.decoder if use_decoder else self.dvae
        max_x_len = -1
        if len(result_list) == 0:
            return np.array([], dtype=np.float32)
        for result in result_list:
            if result.size(0) > max_x_len:
                max_x_len = result.size(0)
        batch_result = torch.zeros(
            (len(result_list), result_list[0].size(1), max_x_len),
            dtype=result_list[0].dtype,
            device=result_list[0].device,
        )
        for i in range(len(result_list)):
            src = result_list[i]
            batch_result[i].narrow(1, 0, src.size(0)).copy_(src.permute(1, 0))
            del src
        del_all(result_list)
        mel_specs = decoder(batch_result)
        del batch_result
        wavs = self._vocos_decode(mel_specs)
        del mel_specs
        return wavs

    @torch.no_grad()
    def _infer_code(
        self,
        text: Tuple[List[str], str],
        stream: bool,
        device: torch.device,
        return_hidden: bool,
        params: InferCodeParams,
    ):

        gpt = self.gpt

        if not isinstance(text, list):
            text = [text]

        assert len(text), "text should not be empty"

        if not isinstance(params.temperature, list):
            temperature = [params.temperature] * self.config.gpt.num_vq
        else:
            temperature = params.temperature

        input_ids, attention_mask, text_mask = self.tokenizer.encode(
            self.speaker.decorate_code_prompts(
                text,
                params.prompt,
                params.txt_smp,
                params.spk_emb,
            ),
            self.config.gpt.num_vq,
            prompt=(
                self.speaker.decode_prompt(params.spk_smp)
                if params.spk_smp is not None
                else None
            ),
            device=self.device_gpt,
        )
        start_idx = input_ids.shape[-2]

        num_code = self.config.gpt.num_audio_tokens - 1

        logits_warpers, logits_processors = gen_logits(
            num_code=num_code,
            top_P=params.top_P,
            top_K=params.top_K,
            repetition_penalty=params.repetition_penalty,
        )

        if gpt.is_vllm:
            from .model.velocity import SamplingParams

            sample_params = SamplingParams(
                temperature=temperature,
                max_new_token=params.max_new_token,
                max_tokens=8192,
                min_new_token=params.min_new_token,
                logits_processors=(logits_processors, logits_warpers),
                eos_token=num_code,
                infer_text=False,
                start_idx=start_idx,
            )
            input_ids = [i.tolist() for i in input_ids]

            result = gpt.llm.generate(
                None,
                sample_params,
                input_ids,
            )

            token_ids = []
            hidden_states = []
            for i in result:
                token_ids.append(torch.tensor(i.outputs[0].token_ids))
                hidden_states.append(
                    i.outputs[0].hidden_states.to(torch.float32).to(self.device)
                )

            del text_mask, input_ids

            return [
                GPT.GenerationOutputs(
                    ids=token_ids,
                    hiddens=hidden_states,
                    attentions=[],
                ),
            ]

        emb = self.embed(input_ids, text_mask)

        del text_mask

        if params.spk_emb is not None:
            self.speaker.apply(
                emb,
                params.spk_emb,
                input_ids,
                self.tokenizer.spk_emb_ids,
                self.gpt.device_gpt,
            )

        result = gpt.generate(
            emb,
            input_ids,
            temperature=torch.tensor(temperature, device=device),
            eos_token=num_code,
            attention_mask=attention_mask,
            max_new_token=params.max_new_token,
            min_new_token=params.min_new_token,
            logits_processors=(*logits_processors, *logits_warpers),
            infer_text=False,
            return_hidden=return_hidden,
            stream=stream,
            show_tqdm=params.show_tqdm,
            ensure_non_empty=params.ensure_non_empty,
            stream_batch=params.stream_batch,
            manual_seed=params.manual_seed,
            context=self.context,
        )

        del emb, input_ids

        return result

    @torch.no_grad()
    def _refine_text(
        self,
        text: str,
        device: torch.device,
        params: RefineTextParams,
    ):

        gpt = self.gpt

        if not isinstance(text, list):
            text = [text]

        input_ids, attention_mask, text_mask = self.tokenizer.encode(
            self.speaker.decorate_text_prompts(text, params.prompt),
            self.config.gpt.num_vq,
            device=self.device_gpt,
        )

        logits_warpers, logits_processors = gen_logits(
            num_code=self.tokenizer.len,
            top_P=params.top_P,
            top_K=params.top_K,
            repetition_penalty=params.repetition_penalty,
        )

        if gpt.is_vllm:
            from .model.velocity import SamplingParams

            sample_params = SamplingParams(
                repetition_penalty=params.repetition_penalty,
                temperature=params.temperature,
                top_p=params.top_P,
                top_k=params.top_K,
                max_new_token=params.max_new_token,
                max_tokens=8192,
                min_new_token=params.min_new_token,
                logits_processors=(logits_processors, logits_warpers),
                eos_token=self.tokenizer.eos_token,
                infer_text=True,
                start_idx=input_ids.shape[-2],
            )
            input_ids_list = [i.tolist() for i in input_ids]
            del input_ids

            result = gpt.llm.generate(
                None, sample_params, input_ids_list, params.show_tqdm
            )
            token_ids = []
            hidden_states = []
            for i in result:
                token_ids.append(torch.tensor(i.outputs[0].token_ids))
                hidden_states.append(i.outputs[0].hidden_states)

            del text_mask, input_ids_list, result

            return GPT.GenerationOutputs(
                ids=token_ids,
                hiddens=hidden_states,
                attentions=[],
            )

        emb = self.embed(input_ids, text_mask)

        del text_mask

        result = next(
            gpt.generate(
                emb,
                input_ids,
                temperature=torch.tensor([params.temperature], device=device),
                eos_token=self.tokenizer.eos_token,
                attention_mask=attention_mask,
                max_new_token=params.max_new_token,
                min_new_token=params.min_new_token,
                logits_processors=(*logits_processors, *logits_warpers),
                infer_text=True,
                stream=False,
                show_tqdm=params.show_tqdm,
                ensure_non_empty=params.ensure_non_empty,
                manual_seed=params.manual_seed,
                context=self.context,
            )
        )

        del emb, input_ids

        return result



================================================
FILE: ChatTTS/norm.py
================================================
import json
import logging
import re
from typing import Dict, Tuple, List, Literal, Callable, Optional
import sys

from numba import jit
import numpy as np

from .utils import del_all


@jit(nopython=True)
def _find_index(table: np.ndarray, val: np.uint16):
    for i in range(table.size):
        if table[i] == val:
            return i
    return -1


@jit(nopython=True)
def _fast_replace(
    table: np.ndarray, text: bytes
) -> Tuple[np.ndarray, List[Tuple[str, str]]]:
    result = np.frombuffer(text, dtype=np.uint16).copy()
    replaced_words = []
    for i in range(result.size):
        ch = result[i]
        p = _find_index(table[0], ch)
        if p >= 0:
            repl_char = table[1][p]
            result[i] = repl_char
            replaced_words.append((chr(ch), chr(repl_char)))
    return result, replaced_words


@jit(nopython=True)
def _split_tags(text: str) -> Tuple[List[str], List[str]]:
    texts: List[str] = []
    tags: List[str] = []
    current_text = ""
    current_tag = ""
    for c in text:
        if c == "[":
            texts.append(current_text)
            current_text = ""
            current_tag = c
        elif current_tag != "":
            current_tag += c
        else:
            current_text += c
        if c == "]":
            tags.append(current_tag)
            current_tag = ""
    if current_text != "":
        texts.append(current_text)
    return texts, tags


@jit(nopython=True)
def _combine_tags(texts: List[str], tags: List[str]) -> str:
    text = ""
    for t in texts:
        tg = ""
        if len(tags) > 0:
            tg = tags.pop(0)
        text += t + tg
    return text


class Normalizer:
    def __init__(self, map_file_path: str, logger=logging.getLogger(__name__)):
        self.logger = logger
        self.normalizers: Dict[str, Callable[[str], str]] = {}
        self.homophones_map = self._load_homophones_map(map_file_path)
        """
        homophones_map

        Replace the mispronounced characters with correctly pronounced ones.

        Creation process of homophones_map.json:

        1. Establish a word corpus using the [Tencent AI Lab Embedding Corpora v0.2.0 large] with 12 million entries. After cleaning, approximately 1.8 million entries remain. Use ChatTTS to infer the text.
        2. Record discrepancies between the inferred and input text, identifying about 180,000 misread words.
        3. Create a pinyin to common characters mapping using correctly read characters by ChatTTS.
        4. For each discrepancy, extract the correct pinyin using [python-pinyin] and find homophones with the correct pronunciation from the mapping.

        Thanks to:
        [Tencent AI Lab Embedding Corpora for Chinese and English Words and Phrases](https://ai.tencent.com/ailab/nlp/en/embedding.html)
        [python-pinyin](https://github.com/mozillazg/python-pinyin)

        """
        self.coding = "utf-16-le" if sys.byteorder == "little" else "utf-16-be"
        self.reject_pattern = re.compile(r"[^\u4e00-\u9fffA-Za-zï¼Œã€‚ã€,\. ]")
        self.sub_pattern = re.compile(r"\[[\w_]+\]")
        self.chinese_char_pattern = re.compile(r"[\u4e00-\u9fff]")
        self.english_word_pattern = re.compile(r"\b[A-Za-z]+\b")
        self.character_simplifier = str.maketrans(
            {
                "ï¼š": "ï¼Œ",
                "ï¼›": "ï¼Œ",
                "ï¼": "ã€‚",
                "ï¼ˆ": "ï¼Œ",
                "ï¼‰": "ï¼Œ",
                "ã€": "ï¼Œ",
                "ã€‘": "ï¼Œ",
                "ã€": "ï¼Œ",
                "ã€": "ï¼Œ",
                "ã€Œ": "ï¼Œ",
                "ã€": "ï¼Œ",
                "ã€Š": "ï¼Œ",
                "ã€‹": "ï¼Œ",
                "ï¼": "ï¼Œ",
                ":": ",",
                ";": ",",
                "!": ".",
                "(": ",",
                ")": ",",
                # "[": ",",
                # "]": ",",
                ">": ",",
                "<": ",",
                "-": ",",
            }
        )
        self.halfwidth_2_fullwidth = str.maketrans(
            {
                "!": "ï¼",
                '"': "â€œ",
                "'": "â€˜",
                "#": "ï¼ƒ",
                "$": "ï¼„",
                "%": "ï¼…",
                "&": "ï¼†",
                "(": "ï¼ˆ",
                ")": "ï¼‰",
                ",": "ï¼Œ",
                "-": "ï¼",
                "*": "ï¼Š",
                "+": "ï¼‹",
                ".": "ã€‚",
                "/": "ï¼",
                ":": "ï¼š",
                ";": "ï¼›",
                "<": "ï¼œ",
                "=": "ï¼",
                ">": "ï¼",
                "?": "ï¼Ÿ",
                "@": "ï¼ ",
                # '[': 'ï¼»',
                "\\": "ï¼¼",
                # ']': 'ï¼½',
                "^": "ï¼¾",
                # '_': 'ï¼¿',
                "`": "ï½€",
                "{": "ï½›",
                "|": "ï½œ",
                "}": "ï½",
                "~": "ï½",
            }
        )

    def __call__(
        self,
        text: str,
        do_text_normalization=True,
        do_homophone_replacement=True,
        lang: Optional[Literal["zh", "en"]] = None,
    ) -> str:
        if do_text_normalization:
            _lang = self._detect_language(text) if lang is None else lang
            if _lang in self.normalizers:
                texts, tags = _split_tags(text)
                self.logger.debug("split texts %s, tags %s", str(texts), str(tags))
                texts = [self.normalizers[_lang](t) for t in texts]
                self.logger.debug("normed texts %s", str(texts))
                text = _combine_tags(texts, tags) if len(tags) > 0 else texts[0]
                self.logger.debug("combined text %s", text)
            if _lang == "zh":
                text = self._apply_half2full_map(text)
        invalid_characters = self._count_invalid_characters(text)
        if len(invalid_characters):
            self.logger.warning(f"found invalid characters: {invalid_characters}")
            text = self._apply_character_map(text)
        if do_homophone_replacement:
            arr, replaced_words = _fast_replace(
                self.homophones_map,
                text.encode(self.coding),
            )
            if replaced_words:
                text = arr.tobytes().decode(self.coding)
                repl_res = ", ".join([f"{_[0]}->{_[1]}" for _ in replaced_words])
                self.logger.info(f"replace homophones: {repl_res}")
        if len(invalid_characters):
            texts, tags = _split_tags(text)
            self.logger.debug("split texts %s, tags %s", str(texts), str(tags))
            texts = [self.reject_pattern.sub("", t) for t in texts]
            self.logger.debug("normed texts %s", str(texts))
            text = _combine_tags(texts, tags) if len(tags) > 0 else texts[0]
            self.logger.debug("combined text %s", text)
        return text

    def register(self, name: str, normalizer: Callable[[str], str]) -> bool:
        if name in self.normalizers:
            self.logger.warning(f"name {name} has been registered")
            return False
        try:
            val = normalizer("test string æµ‹è¯•å­—ç¬¦ä¸²")
            if not isinstance(val, str):
                self.logger.warning("normalizer must have caller type (str) -> str")
                return False
        except Exception as e:
            self.logger.warning(e)
            return False
        self.normalizers[name] = normalizer
        return True

    def unregister(self, name: str):
        if name in self.normalizers:
            del self.normalizers[name]

    def destroy(self):
        del_all(self.normalizers)
        del self.homophones_map

    def _load_homophones_map(self, map_file_path: str) -> np.ndarray:
        with open(map_file_path, "r", encoding="utf-8") as f:
            homophones_map: Dict[str, str] = json.load(f)
        map = np.empty((2, len(homophones_map)), dtype=np.uint32)
        for i, k in enumerate(homophones_map.keys()):
            map[:, i] = (ord(k), ord(homophones_map[k]))
        del homophones_map
        return map

    def _count_invalid_characters(self, s: str):
        s = self.sub_pattern.sub("", s)
        non_alphabetic_chinese_chars = self.reject_pattern.findall(s)
        return set(non_alphabetic_chinese_chars)

    def _apply_half2full_map(self, text: str) -> str:
        return text.translate(self.halfwidth_2_fullwidth)

    def _apply_character_map(self, text: str) -> str:
        return text.translate(self.character_simplifier)

    def _detect_language(self, sentence: str) -> Literal["zh", "en"]:
        chinese_chars = self.chinese_char_pattern.findall(sentence)
        english_words = self.english_word_pattern.findall(sentence)

        if len(chinese_chars) > len(english_words):
            return "zh"
        else:
            return "en"



================================================
FILE: ChatTTS/config/__init__.py
================================================
from .config import Config



================================================
FILE: ChatTTS/config/config.py
================================================
from dataclasses import dataclass


@dataclass(repr=False, eq=False)
class Path:
    vocos_ckpt_path: str = "asset/Vocos.safetensors"
    dvae_ckpt_path: str = "asset/DVAE.safetensors"
    gpt_ckpt_path: str = "asset/gpt"
    decoder_ckpt_path: str = "asset/Decoder.safetensors"
    tokenizer_path: str = "asset/tokenizer"
    embed_path: str = "asset/Embed.safetensors"


@dataclass(repr=False, eq=False)
class Decoder:
    idim: int = 384
    odim: int = 384
    hidden: int = 512
    n_layer: int = 12
    bn_dim: int = 128


@dataclass(repr=False, eq=False)
class VQ:
    dim: int = 1024
    levels: tuple = (5, 5, 5, 5)
    G: int = 2
    R: int = 2


@dataclass(repr=False, eq=False)
class DVAE:
    encoder: Decoder = Decoder(
        idim=512,
        odim=1024,
        hidden=256,
        n_layer=12,
        bn_dim=128,
    )
    decoder: Decoder = Decoder(
        idim=512,
        odim=512,
        hidden=256,
        n_layer=12,
        bn_dim=128,
    )
    vq: VQ = VQ()


@dataclass(repr=False, eq=False)
class GPT:
    hidden_size: int = 768
    intermediate_size: int = 3072
    num_attention_heads: int = 12
    num_hidden_layers: int = 20
    use_cache: bool = False
    max_position_embeddings: int = 4096

    spk_emb_dim: int = 192
    spk_KL: bool = False
    num_audio_tokens: int = 626
    num_text_tokens: int = 21178
    num_vq: int = 4


@dataclass(repr=False, eq=False)
class Embed:
    hidden_size: int = 768
    num_audio_tokens: int = 626
    num_text_tokens: int = 21178
    num_vq: int = 4


@dataclass(repr=False, eq=False)
class FeatureExtractorInitArgs:
    sample_rate: int = 24000
    n_fft: int = 1024
    hop_length: int = 256
    n_mels: int = 100
    padding: str = "center"


@dataclass(repr=False, eq=False)
class FeatureExtractor:
    class_path: str = "vocos.feature_extractors.MelSpectrogramFeatures"
    init_args: FeatureExtractorInitArgs = FeatureExtractorInitArgs()


@dataclass(repr=False, eq=False)
class BackboneInitArgs:
    input_channels: int = 100
    dim: int = 512
    intermediate_dim: int = 1536
    num_layers: int = 8


@dataclass(repr=False, eq=False)
class Backbone:
    class_path: str = "vocos.models.VocosBackbone"
    init_args: BackboneInitArgs = BackboneInitArgs()


@dataclass(repr=False, eq=False)
class FourierHeadInitArgs:
    dim: int = 512
    n_fft: int = 1024
    hop_length: int = 256
    padding: str = "center"


@dataclass(repr=False, eq=False)
class FourierHead:
    class_path: str = "vocos.heads.ISTFTHead"
    init_args: FourierHeadInitArgs = FourierHeadInitArgs()


@dataclass(repr=False, eq=False)
class Vocos:
    feature_extractor: FeatureExtractor = FeatureExtractor()
    backbone: Backbone = Backbone()
    head: FourierHead = FourierHead()


@dataclass(repr=False, eq=False)
class Config:
    path: Path = Path()
    decoder: Decoder = Decoder()
    dvae: DVAE = DVAE()
    gpt: GPT = GPT()
    embed: Embed = Embed()
    vocos: Vocos = Vocos()
    spk_stat: str = (
        "æ„ç©¤å·©å™…å»·æˆ‡ç¬‰å±ˆç™åª„å¹å§å¸¶çˆ²æ¼ˆå¡€æ®æ…„äº…å€´åº²èˆ´çŒ‚ç‘ˆåœç‹´å¤¥åœ“å¸æˆ›æŒ è…‰è€åŠ¤å½å–³å¹¾æˆ˜è¬‡è€å´’æ „å‘¥å€¸åº­ç‡¡æ¬ˆæè¥è¤„ä¹­åŸ—å¹ºçˆƒå¼”æ‘æ–æ”å…•ä½–å»èˆç«¾è±ƒç£å§“è¶¡ä½„å¹’çˆšæ¬„è±„è®çš³è¨µä»©å¸†æŠ•è¬Œèƒèå„åœä¼†å¹¦æŠ‚èŒå‘„æ‘æ–ƒè®¹å‚®åºçˆ£èœ€æ©åç¥„äº¥å…¡å¸¸çˆ‚æ¬æ‰‰ä¸æµ”ä½±åƒˆå¼·æ‰•ä¼…æ‰‚è›å¾´æ†å‚å·€æˆºæ¬€è‰‚çå—´å•¥å€¤å½·åˆ‚æ¬Šç©ˆæ‰’å¤ä¿”è´²åº›åˆç¬‚å„è´æ´ä»­äºåº›å‰çŒ¢æ‰ƒç¼è¶¤åˆåµå¹ªèˆä¼Œç…å©æ½¤æ™ä½å¼¾èˆ™èŒ¥ç©è‘è £è¨‘ä¼åº¤åˆŠç¬æ©æº‘åƒ”äº‘ååº¯æˆšä¼æ½‰è†è„´åƒµå™”å»ƒè‰…åŒŠç¥‚å”æ†´å£å—™å¸­çˆ¥æ¬è™è°ç‰´å¸½åŠ¿å¼¿ç‰³èœå…€è›å‚„å–©ä¸¿å¸”åˆ”åœ†è¡å»ç½¤åºä¿ƒå¸™åŠ¢ä¼ˆæ±„æ¨æª„å‹µä¼´å¼èˆ‘æ¬ç½…è™æ˜´åŠ­å‹…å¸œåˆ¼æœŠè•è™è“´æ¨‘ä¼«å¹¨æ‰‘è¬ªå‰€å ç¨´ä¸µä¼±å¼èˆ®è«¸èµç¿’ä¿”å®¹å±å¹«ç‰¶è¬ƒå­„ç³ç­”å—åƒŠå¸œç‡²ç¬„çµ‚ç€’åˆ¤ä¹…åƒ¤å¸˜çˆ´èŒ‡åƒå­‘å†„å‡•ä½³å¼•æ‰èœæ­ç¼è£„å‰½å„ºæ˜çˆ‹æœçœ¿å»å‘„å¡å˜‡å¹»çˆ±èŒ è©è¨å‰´å”­ä¿å¹¾æˆŠæ¬€ç¡èè´„æ¥•å’å·¡çˆ€å¼å±„èç³è³™å‡¶å½åˆ…æ¼„å€å”æº´å‰‘åŠ‹åº½èˆ½çŒ„ç…ƒè·å¤”æƒ¥ä¼¾åº®èˆä¼ˆç½å‘å„æ€…ä¸šæ€¯åˆæœ‡çå¶è¦”å©ä¿³å·¶çˆœæœæ½å´è„ä¿¹å‡›å¸¸çˆºç¬Œç©€èæ­¤å¤¡å€›å¸¡åˆ€åŒ‰çµ‚çªèˆ£è²©ä¾½æ€¿æ‰‰ä¼¥è´¿æ†å¿“è¬©å§†å¹ŒçŠŠæ¼‚æ…†ç™’å´ç”å…å¸¼æˆæ¬…è©‚æµæœ”ä»¹å£­å¸°è‡·å¼æ‡èç¤å¸¡å–å¸˜çˆä¼…è…‚çšçº¤å›…å……å¹“æˆ ä¼¥ç‚ä¸è¨¤æˆ±å€±å¼‹çˆ®å¬Œç™æå­„ä¾¥åŠ¬å¿¶åˆ“åœ‹è©€æ¡’å¤å©å˜„åº¬æˆšèŒèµ‚ç›‘ç‡¤å˜‘å‹Œå¹¦èˆ½æŒå‘‚è«æ£¤å§‘å†åº•èˆ¡ç¬è‰ƒç€å­´å€‰å‚”å¼‹çˆ”çŒ ä¹æ¿‘å¡„å½å˜§æ‚èˆ›ç¼‡è¥ƒåçª´ä»¡åˆ±å¿•åˆ¥æ¼‡ç©å²ç¼´å»½ä»·åºŒçˆŠè¬ˆç¡„è®‘æƒ¤å€å„‚åº­çˆ‹ä¼‡è‚å¶è”æ‘å‚ åº“åˆèŒ„æ­ƒæˆè–¤ä¼ä¼¯å»®åˆ›ç¬ å¡„ç†å…´å‹½ä¿„å¸…å‰‰æœ€è…€ç æ•¤åä¾å¼†æˆºæœ’è™ƒæ—èš„æ¢•äº–å¹”ç‰»æœ£æ‰…è´ç”å å™…å¸¡å‰Œåœ…æ‘€å´å½¤æµåƒ³åº™çˆ–å¬‡å•æ¸æ‚¤å ä¸›å¹†åˆ§æŒœå½ƒæ‚å¹¤åˆ¹åšŸæ•èŠçœ‹è€æ‘ç„”å‘ä¹å¸–çˆ­æ¬ç™ƒç³’åœ„å¼™ä½±å»œæˆ¤è¬å©€å’æ˜´ç„äº©å»¦è‰æ‹¼è¬¿èŠç™¤æ€¹å…½å¹¸èˆ³æœ‡ç•å–ç¨”æ¯ä¸¼å¼ˆæ‡²æŒ€è­‚å‹‘å“´å•ä¼å¸¸èˆ­ç¬¯æ™å ‘ä¿„å©å‰”å»Ÿçˆæ¬¦çµå¤’ä¼¤ä¼‘å‚‘å»³æˆŒèœ…æ½†ç™å½´æ‘‘å‹¯åºŠåˆ½æ¬…è‰ç å¿„æ‰ä»å»¡èˆŠçŒ¥æ½‚å”å§”ä»±åƒœå»¼çˆ¤æœ„å‘ƒå¼ç¤”æ»µå“å¹©çˆ„æŒ‚ç­ä¹ç±¤åˆ•å‡Ÿå¹µçˆ å¼‰ç™…ä¹‘å´å‹¥ä¼–å¸ªèˆ©èŒ†å©ç¢å¹¤å­ä¹¢å·œè‰³çŒæ¡€æ¡å•„å”©ä¿Šå¹èˆ®çŒ€è‰…ç„è”ç½äº€å¸‹çˆœç¼…å™ƒå’æ–¤å–©äºˆå¹©çˆ›ç¬†æ‘€æµçŒ´ä¾ä¾¹å¹ƒåˆ•åœ’æ…„è›æ ¤æ¾¹ä»‘åº§çˆ¼è¬‰æ¡ƒæ…æµ”æ–•å»å¹›æ‡°å¬“è¡æ„æ°„æ‚…ä»¿åº”èŠ”æ¼„è¡ƒæ•è¬¤å‚åŒ©å¹¹æŠƒåœ‰ç™„å»è£„å±µå™‰å¹åˆ©è¬è‚æè›”åš™åæ€—èˆåœç•ƒè†æ „åˆµä¸œå·†æˆ¤è«¾å‘ƒå‘åª¤å—¨è·å¿¶çˆçœ„ç¥‚æœ’å¶”åƒ­åŠ‰å¿¾åˆåŒ‹ç™„è¢ç¿´ç…åƒ·å»²èŠ„èŒˆæˆçšæ“„å´‘ä¼„å»‰ç‰åŒƒå‰ƒçŠæ¾¤å”‘ä¸„åººæˆƒä¼ƒç…€æŸæ„å™äº½å¸´åˆ‡ç¼Œç½„æŒå°´å™™å€°å¸¦èˆæ¼„æ©„å¡ç³´ä¿©åƒ¯å¸€èˆ¬æ¼€å‚æ æ›´ä¸¡ä¿‡å»±èˆŒçŒæ…‚æ‹å¤å¶±å¶åº”åˆªçœ‰çèŒä¼”å˜…åºå¸ŸèˆŠæ¼‚æ€æ æš„å–¡ä¹åº™èˆ†åŒ‚æ•€æ½‘æ”åŠ‘ä¾–å»¶æˆ¦ç›½æ€¶å”¯æ…³è˜èŸƒå­«å¨ç›Šè¢°çå±ƒç—¶ç¿®ç¬ªå„šè£€å€¹æ¤Œç»ç¿€è©µç­½èˆ˜æƒ¯å ¿æŸä¾°æ™ˆè—ç¼®è©—å»¦å¤¸å¦ç‘»ç€’è£”åª€æ†å”ƒå†¶ç’­ç‹»æ¸ è‘å¥¬ç†¹èŒ…æ„ºæ°°è£æ» ç¿¦å²“è¤Œæ³£å´²åš­æ¬“æ¹’è™å®ºçˆ„è›…æ„¸åºåŒƒå¸†èª”ç©®æ‡Œè“ªç·æ¾Œæ°‹æŠŒè¨™å±Œè‡å»›ç¸å¬å±ºå¸Œç–­å­å‡‚ç´‹æ–°ç…å½ƒè†²è·±å°ªæ‡çœ†çª´çå“æ¨è¸ç´­æ¦‚å›¥æ˜¾å£Œæ¦„å«å˜®å¬­è¦¤åª¸ä¾µä½®çƒ’è€¸è§Œå©€ç§‹ç‹ƒå¸¹è‘¯è¨¤æ¡œç³¨ç¬¾è…¢ä¼€è‚¶æ‚ç‚‚è‰¤ç¦–å²…è‡ºæƒ˜æ¢·çå‹ç›ä½¨å²§æ†³ç“§å˜´æ±¬è—Šæ„Œè˜¤å¶ ç¡´ç»¤èœ²è¥æ‹¬å‹¾è°‚ç¸¨å¦¥è“ªæ¾­ç«­è¢è—œçºç³²ç…®æ„†ç€¯å­¯ç“ç½‚è«ºå¡¿ç‡—ç‹Ÿå¼™è¡¯æ»ç¸·ä¸±ç³…è‡„æ¢±ç€®æ°å·³çŒ™äºŠç¬¦èƒ åŒƒæ³€å»åœƒè†‚è’ƒç±ç¤©å²ˆç°¹ç¼ŒåŠºç‡²è¤¡å­“è†œæ‹”è ¿è§®å‘‹ç…£åŒå°·ç†œè«–å¼²ç‰­ç´«å¯Šèªƒç´€æ©´è³¬å‚¸ç®å¼šçªƒä¾«ç°²æ…¯çƒ£æ¸½ç¥Œå£“åª¥å™œå¤½å¤›è«›ç¹ç–®ç¦„å†ªè¬‡åª½è¡¤ç›°ç¼ºç¹‘è–«å…¾è§åµ±æ‰“æ»½ç®ºåš¯å‡£ç‹¢è œå´¼è¦½çƒ¸ç°¶ç›¯ç±“æ‘€è‹¶å³¸æ‡—æ³²æ¶»å‡®æ„³ç·—å‰‹ç¬”æ‡†å»¡ç¿æ¤ç¤¤æƒè—¥å´è…ˆçƒ„ä¼¹äº¯æ˜£ç¿¬è¤çµ‹æ¡«åƒ¨å¨èŒä¸›çŸ„èœå¨ˆæ†Šè‹†å¡è“åš¢å«¼ç»»å´±å©‹å›±è ¸ç¯¯æ™£èŠ€ç¹¼ç´¢å…“åƒ–èª¹å²¯åœªè¤°è ‡å”“å¦·èƒ…å·æ¸®ç ›å‚ˆè·åµšå†ƒè³¼èµå³è£‹è‚èˆ¾ç¬¦ç†»å²³å¢©å¯®ç²ƒå‡²è¢‘å½šå¤ªç»²å¤´æ‘¯ç¹³ç‹ä¿¥ç±Œå†è«è¨»åå¹«æ“¤è©’å®’å‡•è³å”¶æ¢å™”å¼¼èª²å±¿è¦å›¨ç„¬æ«±æ’ªè®è¬ç°¸æ‡°æ««æ¶ºåµç»å±ªç¿”å³æ…˜æ»Ÿç†²æ˜±å†›çƒŠèˆ¿å°¦èˆ„ç³–å¥æºå‡‚å½†è²ç³´ç¦å›°çš»çç‰‹ç’è¯™å¶±è‡€å¼€è“ˆçœè…¼ä¸¢çº»å»æ†¤å«–æš­è¢­å´²è‚¸è›å¦’æ¦—ç´‰è°¨çª®è¢ƒç‘ èç»Šè…†äº¿å†²è‘å–‹ç¸”è©–å²‘å…¾ç»™å ¸èµæ—»æ¡€è›¨åª†è¨‚å³¦ç´·æ•¯å›¬åç­¨å²¸ç„¸æ‹­ç¬µæ®’å“œå¢’èå±“å¨“è«™æ¢°è‡®æœ›æ‘°èŠ‘å¯­å‡†åƒè°¹æ°æ—‹æ†¢è®å±ƒåˆ’æ¬£ç˜«è°è˜»å“ç¹ç±¥ç¦¦åƒ¿èªµçš¯å¢“ç‡€ç¸¿ç¬ç†¦ç»—ç¨¹æ¦çŸ»ç¶è““å¸¡æˆ“æ²ºåŒºæ‰ç•ƒæ´Šè©ªç³è£¶ç›°çª¶è€åŒåŠ‚èªåº©æƒæ»œæ²ºå“®å‘ƒç…è­ å´„æ§€çŒ„è‚¼è”æ“‹æ¹Œè ºç¯ƒæ¥è«Œç¦å®å «æŒªè£•å´‘æ…©ç‹²æ‚ ç…‹ä»›æ„ç ˆç²µå…«æ£å®³æ¥å¦‹è”è²¨å°µå¥‚è‹°æ€«èªå‚«å²†è•¯å±‡è„‰å¤ˆä»†èŒåˆ“ç¹¸èŠºå£¸ç¢—æ››æ±æˆ­ç‚»ç»å‡‰åªå…ç‹œçˆ´æ€°è³ƒçºè¢å¨·ç¦ƒè“¥è†¹è–ªæ¸»ç½¸çª¿ç²«å‡¾è¤„èˆºçª®å¢«å¹²è‹Šç¹å†åƒ®è¨¸å¤¯ç»›è“ªè™›ç¾½æ…²çƒæ†·è¶çŠè °èå¡æˆå»ç›æ¬å–“èœ®è­¤å´†æ¥å›˜çŸ‡è–­ä¼£è‰˜è™å¸´å¥®è‹¢æ¸¶è™æš£ç¿èƒå°¾ç¨ˆç³¶ç€´ç½åµšæ°®è‘¯ç¬«æ…æ£Œæ‚¶ç‚¯ç«»çˆ…ä»¬åª¡å§¢å«ºçª·åˆ®æ­«åŠˆè£©å±¬æ¤•è³‘èœ¹è–Šåˆ²ç¾©å“¯å°—è¤¦ç“€ç¨¾ç¤‹æ£çª¼èˆ«å°‹å§æ¤„ä¾¸å—«çºä¿®çº˜åªƒè…½è››ç¨¹æ¢­å‘›ç€ˆè˜Ÿç¸€ç¤‰è«–å¤µå”®ä¸»æ¢®è ‰å¨…å¨­è£€èª¼å¶­è¦³æ³å€Šç°ˆè¤ƒæ“ç¶¿å‚¬çƒæº¶è‹Šç¬›è¥¹æ«²ç›…å…­å›«ç©ä½ƒç²¨æ…¯ç“¢çœ¸æ—±èƒå©¨è”å²‹ç¥—å¢¼ç„»ç½‘ç‰»ç–è©†å³‹ç§‰èƒ³åª´è¢­æ¾“è³¢çµŒç¨Ÿå£©èƒ«ç¢¯åå›«å¶çº†çªˆæ§Šè³æ’¹ç’¬èƒç¼˜èª¾å®­æ„Šçœ—å–·ç›‘åŠ‹è˜è¨¯ç¸½æ§¿æ£­æˆ¾å¢®çŠ„æŒç¸ˆç°æ¨¥è›”æè¢­å«›æ†«å€†ç¯å¢µè³ˆç¾¯èŒè§³è’œè‡´å¨¢æ…„å‹’è¦¸è˜æ›²æ ‚è‘­å®†å¦‹çš½ç¼½å…ç›³çŒ¼è”‚ç³¥è§§çƒ³æª¸ä½¯æ†“ç…¶è”ç­¼ç§ç¹·ç²è†Œå¡„å‰°è®å¯¾è…•æ£¥æ¸½å¿²ä¿›æµªè­¬ç§›æƒ›å£’å˜¸æ·«å†»æ›„ç»ç ƒå¥«è²¯åº´çˆ…ç²“è„®è„¡å¨å¦–å³µè˜²è¨æƒ‹æ³Šè €ã´†"
    )



================================================
FILE: ChatTTS/model/__init__.py
================================================
from .dvae import DVAE
from .embed import Embed
from .gpt import GPT
from .processors import gen_logits
from .speaker import Speaker
from .tokenizer import Tokenizer



================================================
FILE: ChatTTS/model/dvae.py
================================================
import math
from typing import List, Optional, Literal, Union

import numpy as np
import pybase16384 as b14
import torch
import torch.nn as nn
import torchaudio
from vector_quantize_pytorch import GroupedResidualFSQ

from ..utils import load_safetensors


class ConvNeXtBlock(nn.Module):
    def __init__(
        self,
        dim: int,
        intermediate_dim: int,
        kernel: int,
        dilation: int,
        layer_scale_init_value: float = 1e-6,
    ):
        # ConvNeXt Block copied from Vocos.
        super().__init__()
        self.dwconv = nn.Conv1d(
            dim,
            dim,
            kernel_size=kernel,
            padding=dilation * (kernel // 2),
            dilation=dilation,
            groups=dim,
        )  # depthwise conv

        self.norm = nn.LayerNorm(dim, eps=1e-6)
        self.pwconv1 = nn.Linear(
            dim, intermediate_dim
        )  # pointwise/1x1 convs, implemented with linear layers
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(intermediate_dim, dim)
        self.weight = (
            nn.Parameter(layer_scale_init_value * torch.ones(dim), requires_grad=True)
            if layer_scale_init_value > 0
            else None
        )

    def forward(self, x: torch.Tensor, cond=None) -> torch.Tensor:
        residual = x

        y = self.dwconv(x)
        y.transpose_(1, 2)  # (B, C, T) -> (B, T, C)
        x = self.norm(y)
        del y
        y = self.pwconv1(x)
        del x
        x = self.act(y)
        del y
        y = self.pwconv2(x)
        del x
        if self.weight is not None:
            y *= self.weight
        y.transpose_(1, 2)  # (B, T, C) -> (B, C, T)

        x = y + residual
        del y

        return x


class GFSQ(nn.Module):

    def __init__(
        self, dim: int, levels: List[int], G: int, R: int, eps=1e-5, transpose=True
    ):
        super(GFSQ, self).__init__()
        self.quantizer = GroupedResidualFSQ(
            dim=dim,
            levels=list(levels),
            num_quantizers=R,
            groups=G,
        )
        self.n_ind = math.prod(levels)
        self.eps = eps
        self.transpose = transpose
        self.G = G
        self.R = R

    def _embed(self, x: torch.Tensor):
        if self.transpose:
            x = x.transpose(1, 2)
        """
        x = rearrange(
            x, "b t (g r) -> g b t r", g = self.G, r = self.R,
        )
        """
        x = x.view(x.size(0), x.size(1), self.G, self.R).permute(2, 0, 1, 3)
        feat = self.quantizer.get_output_from_indices(x)
        return feat.transpose_(1, 2) if self.transpose else feat

    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        return super().__call__(x)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.transpose:
            x.transpose_(1, 2)
        # feat, ind = self.quantizer(x)
        _, ind = self.quantizer(x)
        """
        ind = rearrange(
            ind, "g b t r ->b t (g r)",
        )
        """
        ind = ind.permute(1, 2, 0, 3).contiguous()
        ind = ind.view(ind.size(0), ind.size(1), -1)
        """
        embed_onehot_tmp = F.one_hot(ind.long(), self.n_ind)
        embed_onehot = embed_onehot_tmp.to(x.dtype)
        del embed_onehot_tmp
        e_mean = torch.mean(embed_onehot, dim=[0, 1])
        # e_mean = e_mean / (e_mean.sum(dim=1) + self.eps).unsqueeze(1)
        torch.div(e_mean, (e_mean.sum(dim=1) + self.eps).unsqueeze(1), out=e_mean)
        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + self.eps), dim=1))

        return 
            torch.zeros(perplexity.shape, dtype=x.dtype, device=x.device),
            feat.transpose_(1, 2) if self.transpose else feat,
            perplexity,
        """
        return ind.transpose_(1, 2) if self.transpose else ind


class DVAEDecoder(nn.Module):
    def __init__(
        self,
        idim: int,
        odim: int,
        n_layer=12,
        bn_dim=64,
        hidden=256,
        kernel=7,
        dilation=2,
        up=False,
    ):
        super().__init__()
        self.up = up
        self.conv_in = nn.Sequential(
            nn.Conv1d(idim, bn_dim, 3, 1, 1),
            nn.GELU(),
            nn.Conv1d(bn_dim, hidden, 3, 1, 1),
        )
        self.decoder_block = nn.ModuleList(
            [
                ConvNeXtBlock(
                    hidden,
                    hidden * 4,
                    kernel,
                    dilation,
                )
                for _ in range(n_layer)
            ]
        )
        self.conv_out = nn.Conv1d(hidden, odim, kernel_size=1, bias=False)

    def forward(self, x: torch.Tensor, conditioning=None) -> torch.Tensor:
        # B, C, T
        y = self.conv_in(x)
        del x
        for f in self.decoder_block:
            y = f(y, conditioning)

        x = self.conv_out(y)
        del y
        return x


class MelSpectrogramFeatures(torch.nn.Module):
    def __init__(
        self,
        sample_rate=24000,
        n_fft=1024,
        hop_length=256,
        n_mels=100,
        padding: Literal["center", "same"] = "center",
        device: torch.device = torch.device("cpu"),
    ):
        super().__init__()
        self.device = device
        if padding not in ["center", "same"]:
            raise ValueError("Padding must be 'center' or 'same'.")
        self.padding = padding
        self.mel_spec = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            n_mels=n_mels,
            center=padding == "center",
            power=1,
        )

    def __call__(self, audio: torch.Tensor) -> torch.Tensor:
        return super().__call__(audio)

    def forward(self, audio: torch.Tensor) -> torch.Tensor:
        audio = audio.to(self.device)
        mel: torch.Tensor = self.mel_spec(audio)
        features = torch.log(torch.clip(mel, min=1e-5))
        return features


class DVAE(nn.Module):
    def __init__(
        self,
        decoder_config: dict,
        encoder_config: Optional[dict] = None,
        vq_config: Optional[dict] = None,
        dim=512,
        coef: Optional[str] = None,
        device: torch.device = torch.device("cpu"),
    ):
        super().__init__()
        if coef is None:
            coef = torch.rand(100)
        else:
            coef = torch.from_numpy(
                np.frombuffer(b14.decode_from_string(coef), dtype=np.float32).copy()
            )
        self.register_buffer("coef", coef.unsqueeze(0).unsqueeze_(2))

        if encoder_config is not None:
            self.downsample_conv = nn.Sequential(
                nn.Conv1d(100, dim, 3, 1, 1),
                nn.GELU(),
                nn.Conv1d(dim, dim, 4, 2, 1),
                nn.GELU(),
            )
            self.preprocessor_mel = MelSpectrogramFeatures(device=device)
            self.encoder: Optional[DVAEDecoder] = DVAEDecoder(**encoder_config)

        self.decoder = DVAEDecoder(**decoder_config)
        self.out_conv = nn.Conv1d(dim, 100, 3, 1, 1, bias=False)
        if vq_config is not None:
            self.vq_layer = GFSQ(**vq_config)
        else:
            self.vq_layer = None

    def __repr__(self) -> str:
        return b14.encode_to_string(
            self.coef.cpu().numpy().astype(np.float32).tobytes()
        )

    def __call__(
        self, inp: torch.Tensor, mode: Literal["encode", "decode"] = "decode"
    ) -> torch.Tensor:
        return super().__call__(inp, mode)

    @torch.inference_mode()
    def load_pretrained(self, filename: str, device: torch.device):
        state_dict_tensors = load_safetensors(filename)
        self.load_state_dict(state_dict_tensors)
        self.to(device)

    @torch.inference_mode()
    def forward(
        self, inp: torch.Tensor, mode: Literal["encode", "decode"] = "decode"
    ) -> torch.Tensor:
        if mode == "encode" and hasattr(self, "encoder") and self.vq_layer is not None:
            mel = self.preprocessor_mel(inp)
            x: torch.Tensor = self.downsample_conv(
                torch.div(mel, self.coef.view(100, 1).expand(mel.shape), out=mel),
            ).unsqueeze_(0)
            del mel
            x = self.encoder(x)
            ind = self.vq_layer(x)
            del x
            return ind

        if self.vq_layer is not None:
            vq_feats = self.vq_layer._embed(inp)
        else:
            vq_feats = inp

        vq_feats = (
            vq_feats.view(
                (vq_feats.size(0), 2, vq_feats.size(1) // 2, vq_feats.size(2)),
            )
            .permute(0, 2, 3, 1)
            .flatten(2)
        )

        dec_out = self.out_conv(
            self.decoder(
                x=vq_feats,
            ),
        )

        del vq_feats

        return torch.mul(dec_out, self.coef, out=dec_out)

    @torch.inference_mode()
    def sample_audio(self, wav: Union[np.ndarray, torch.Tensor]) -> torch.Tensor:
        if isinstance(wav, np.ndarray):
            wav = torch.from_numpy(wav)
        return self(wav, "encode").squeeze_(0)



================================================
FILE: ChatTTS/model/embed.py
================================================
import torch
import torch.nn as nn
from torch.nn.utils.parametrizations import weight_norm

from ..utils import load_safetensors


class Embed(nn.Module):
    def __init__(
        self, hidden_size: int, num_audio_tokens: int, num_text_tokens: int, num_vq=4
    ):
        super().__init__()

        self.num_vq = num_vq
        self.num_audio_tokens = num_audio_tokens

        self.model_dim = hidden_size
        self.emb_code = nn.ModuleList(
            [nn.Embedding(num_audio_tokens, self.model_dim) for _ in range(num_vq)],
        )
        self.emb_text = nn.Embedding(num_text_tokens, self.model_dim)

        self.head_text = weight_norm(
            nn.Linear(self.model_dim, num_text_tokens, bias=False),
            name="weight",
        )
        self.head_code = nn.ModuleList(
            [
                weight_norm(
                    nn.Linear(self.model_dim, num_audio_tokens, bias=False),
                    name="weight",
                )
                for _ in range(self.num_vq)
            ],
        )

    @torch.inference_mode()
    def load_pretrained(self, filename: str, device: torch.device):
        state_dict_tensors = load_safetensors(filename)
        self.load_state_dict(state_dict_tensors)
        self.to(device)

    def __call__(
        self, input_ids: torch.Tensor, text_mask: torch.Tensor
    ) -> torch.Tensor:
        """
        get_emb
        """
        return super().__call__(input_ids, text_mask)

    @torch.inference_mode()
    def forward(self, input_ids: torch.Tensor, text_mask: torch.Tensor) -> torch.Tensor:
        """
        get_emb
        """
        device = next(self.parameters()).device
        emb_text: torch.Tensor = self.emb_text(
            input_ids[text_mask].narrow(1, 0, 1).squeeze_(1).to(device)
        )

        text_mask_inv = text_mask.logical_not().to(device)
        masked_input_ids: torch.Tensor = input_ids[text_mask_inv].to(device)

        emb_code = [
            self.emb_code[i](masked_input_ids[:, i]) for i in range(self.num_vq)
        ]
        emb_code = torch.stack(emb_code, 2).sum(2)

        emb = torch.zeros(
            (input_ids.shape[:-1]) + (emb_text.shape[-1],),
            device=emb_text.device,
            dtype=emb_text.dtype,
        )
        emb[text_mask] = emb_text
        emb[text_mask_inv] = emb_code.to(emb.dtype)

        del emb_text, emb_code, text_mask_inv

        return emb



================================================
FILE: ChatTTS/model/gpt.py
================================================
import platform
from dataclasses import dataclass
import logging
from typing import Union, List, Optional, Tuple, Callable
import gc

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.parametrize as P
from tqdm import tqdm
from transformers import LlamaModel, LlamaConfig
from transformers.cache_utils import Cache
from transformers.modeling_outputs import BaseModelOutputWithPast
from transformers.utils import is_flash_attn_2_available

from ..utils import del_all
from .embed import Embed


class GPT(nn.Module):
    def __init__(
        self,
        gpt_config: dict,
        embed: Embed,
        use_flash_attn=False,
        use_vllm=False,
        device=torch.device("cpu"),
        device_gpt=torch.device("cpu"),
        logger=logging.getLogger(__name__),
    ):
        super().__init__()

        self.logger = logger

        self.device = device
        self.device_gpt = device_gpt

        self.generator = torch.Generator(device=device)

        self.num_vq = int(gpt_config["num_vq"])
        self.num_audio_tokens = int(gpt_config["num_audio_tokens"])
        self.num_text_tokens = int(gpt_config["num_text_tokens"])

        self.use_flash_attn = use_flash_attn
        self.is_te_llama = False
        self.is_vllm = use_vllm

        if self.is_vllm:
            return

        self.llama_config = self._build_llama_config(gpt_config)

        self.emb_code = [ec.__call__ for ec in embed.emb_code]
        self.emb_text = embed.emb_text.__call__
        self.head_text = embed.head_text.__call__
        self.head_code = [hc.__call__ for hc in embed.head_code]

    def load_pretrained(
        self, gpt_folder: str, embed_file_path: str, experimental=False
    ):
        if self.is_vllm and platform.system().lower() == "linux":

            from .velocity import LLM

            self.llm = LLM(
                model=gpt_folder,
                num_audio_tokens=self.num_audio_tokens,
                num_text_tokens=self.num_text_tokens,
                post_model_path=embed_file_path,
            )
            self.logger.info("vLLM model loaded")
            return

        self.gpt: LlamaModel = LlamaModel.from_pretrained(gpt_folder).to(
            self.device_gpt
        )
        del self.gpt.embed_tokens

        if (
            experimental
            and "cuda" in str(self.device_gpt)
            and platform.system().lower() == "linux"
        ):  # is TELlamaModel
            try:
                from .cuda import TELlamaModel

                self.logger.warning(
                    "Linux with CUDA, try NVIDIA accelerated TELlamaModel because experimental is enabled"
                )
                state_dict = self.gpt.state_dict()
                vanilla = TELlamaModel.from_state_dict(state_dict, self.llama_config)
                # Force mem release. Taken from huggingface code
                del state_dict, self.gpt
                gc.collect()
                self.gpt = vanilla
                self.is_te_llama = True
            except Exception as e:
                self.logger.warning(
                    f"use default LlamaModel for importing TELlamaModel error: {e}"
                )

    class Context:
        def __init__(self):
            self._interrupt = False

        def set(self, v: bool):
            self._interrupt = v

        def get(self) -> bool:
            return self._interrupt

    def _build_llama_config(
        self,
        config: dict,
    ) -> Tuple[LlamaModel, LlamaConfig]:

        if self.use_flash_attn and is_flash_attn_2_available():
            llama_config = LlamaConfig(
                **config,
                attn_implementation="flash_attention_2",
            )
            self.logger.warning(
                "enabling flash_attention_2 may make gpt be even slower"
            )
        else:
            llama_config = LlamaConfig(**config)

        return llama_config

    def prepare(self, compile=False):
        if self.use_flash_attn and is_flash_attn_2_available():
            self.gpt = self.gpt.to(dtype=torch.float16)
        if compile and not self.is_te_llama and not self.is_vllm:
            try:
                self.compile(backend="inductor", dynamic=True)
                self.gpt.compile(backend="inductor", dynamic=True)
            except RuntimeError as e:
                self.logger.warning(f"compile failed: {e}. fallback to normal mode.")

    @dataclass(repr=False, eq=False)
    class _GenerationInputs:
        position_ids: torch.Tensor
        cache_position: torch.Tensor
        use_cache: bool
        input_ids: Optional[torch.Tensor] = None
        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
        attention_mask: Optional[torch.Tensor] = None
        inputs_embeds: Optional[torch.Tensor] = None

        def to(self, device: torch.device, dtype: torch.dtype):
            if self.attention_mask is not None:
                self.attention_mask = self.attention_mask.to(device, dtype=dtype)
            if self.position_ids is not None:
                self.position_ids = self.position_ids.to(device, dtype=dtype)
            if self.inputs_embeds is not None:
                self.inputs_embeds = self.inputs_embeds.to(device, dtype=dtype)
            if self.cache_position is not None:
                self.cache_position = self.cache_position.to(device, dtype=dtype)

    @torch.no_grad()
    def _prepare_generation_inputs(
        self,
        input_ids: torch.Tensor,
        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
        cache_position: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        use_cache=True,
    ) -> _GenerationInputs:
        # With static cache, the `past_key_values` is None
        # TODO joao: standardize interface for the different Cache classes and remove of this if
        has_static_cache = False
        if past_key_values is None:
            if hasattr(self.gpt.layers[0], "self_attn"):
                past_key_values = getattr(
                    self.gpt.layers[0].self_attn, "past_key_value", None
                )
            has_static_cache = past_key_values is not None

        past_length = 0
        if past_key_values is not None:
            if isinstance(past_key_values, Cache):
                past_length = (
                    int(cache_position[0])
                    if cache_position is not None
                    else past_key_values.get_seq_length()
                )
                try:
                    max_cache_length = past_key_values.get_max_cache_shape()
                except:
                    max_cache_length = (
                        past_key_values.get_max_length()
                    )  # deprecated in transformers 4.48
                cache_length = (
                    past_length
                    if max_cache_length is None
                    else min(max_cache_length, past_length)
                )
            # TODO joao: remove this `else` after `generate` prioritizes `Cache` objects
            else:
                cache_length = past_length = past_key_values[0][0].shape[2]
                max_cache_length = None

            # Keep only the unprocessed tokens:
            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where
            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as
            # input)
            if (
                attention_mask is not None
                and attention_mask.shape[1] > input_ids.shape[1]
            ):
                start = attention_mask.shape[1] - past_length
                input_ids = input_ids.narrow(1, -start, start)
            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard
            # input_ids based on the past_length.
            elif past_length < input_ids.shape[1]:
                input_ids = input_ids.narrow(
                    1, past_length, input_ids.size(1) - past_length
                )
            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.

            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.
            if (
                max_cache_length is not None
                and attention_mask is not None
                and cache_length + input_ids.shape[1] > max_cache_length
            ):
                attention_mask = attention_mask.narrow(
                    1, -max_cache_length, max_cache_length
                )

        if attention_mask is not None and position_ids is None:
            # create position_ids on the fly for batch generation
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask.eq(0), 1)
            if past_key_values:
                position_ids = position_ids.narrow(
                    1, -input_ids.shape[1], input_ids.shape[1]
                )

        input_length = (
            position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]
        )
        if cache_position is None:
            cache_position = torch.arange(
                past_length, past_length + input_length, device=input_ids.device
            )
        else:
            cache_position = cache_position.narrow(0, -input_length, input_length)

        if has_static_cache:
            past_key_values = None

        model_inputs = self._GenerationInputs(
            position_ids=position_ids,
            cache_position=cache_position,
            use_cache=use_cache,
        )

        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step
        if inputs_embeds is not None and past_key_values is None:
            model_inputs.inputs_embeds = inputs_embeds
        else:
            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise
            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114
            # TODO: use `next_tokens` directly instead.
            model_inputs.input_ids = input_ids.contiguous()

        model_inputs.past_key_values = past_key_values
        model_inputs.attention_mask = attention_mask

        return model_inputs

    @dataclass(repr=False, eq=False)
    class GenerationOutputs:
        ids: List[torch.Tensor]
        attentions: List[Optional[Tuple[torch.FloatTensor, ...]]]
        hiddens: List[torch.Tensor]

        def destroy(self):
            del_all(self.ids)
            del_all(self.attentions)
            del_all(self.hiddens)

    @torch.no_grad()
    def _prepare_generation_outputs(
        self,
        inputs_ids: torch.Tensor,
        start_idx: int,
        end_idx: torch.Tensor,
        attentions: List[Optional[Tuple[torch.FloatTensor, ...]]],
        hiddens: List[torch.Tensor],
        infer_text: bool,
    ) -> GenerationOutputs:
        inputs_ids = [
            inputs_ids[idx].narrow(0, start_idx, i) for idx, i in enumerate(end_idx)
        ]
        if infer_text:
            inputs_ids = [i.narrow(1, 0, 1).squeeze_(1) for i in inputs_ids]

        if len(hiddens) > 0:
            hiddens = torch.stack(hiddens, 1)
            hiddens = [
                hiddens[idx].narrow(0, 0, i) for idx, i in enumerate(end_idx.int())
            ]

        return self.GenerationOutputs(
            ids=inputs_ids,
            attentions=attentions,
            hiddens=hiddens,
        )

    @torch.no_grad()
    def generate(
        self,
        emb: torch.Tensor,
        inputs_ids: torch.Tensor,
        temperature: torch.Tensor,
        eos_token: Union[int, torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        max_new_token=2048,
        min_new_token=0,
        logits_processors: Tuple[
            Callable[[torch.LongTensor, torch.FloatTensor], torch.FloatTensor]
        ] = (),
        infer_text=False,
        return_attn=False,
        return_hidden=False,
        stream=False,
        show_tqdm=True,
        ensure_non_empty=True,
        stream_batch=24,
        manual_seed: Optional[int] = None,
        context=Context(),
    ):

        attentions: List[Optional[Tuple[torch.FloatTensor, ...]]] = []
        hiddens = []
        stream_iter = 0

        start_idx, end_idx = inputs_ids.shape[1], torch.zeros(
            inputs_ids.shape[0], device=inputs_ids.device, dtype=torch.long
        )
        finish = torch.zeros(inputs_ids.shape[0], device=inputs_ids.device).bool()

        old_temperature = temperature

        temperature = (
            temperature.unsqueeze(0)
            .expand(inputs_ids.shape[0], -1)
            .contiguous()
            .view(-1, 1)
        )

        attention_mask_cache = torch.ones(
            (
                inputs_ids.shape[0],
                inputs_ids.shape[1] + max_new_token,
            ),
            dtype=torch.bool,
            device=inputs_ids.device,
        )
        if attention_mask is not None:
            attention_mask_cache.narrow(1, 0, attention_mask.shape[1]).copy_(
                attention_mask
            )

        progress = inputs_ids.size(1)
        # pre-allocate inputs_ids
        inputs_ids_buf = torch.zeros(
            inputs_ids.size(0),
            progress + max_new_token,
            inputs_ids.size(2),
            dtype=inputs_ids.dtype,
            device=inputs_ids.device,
        )
        inputs_ids_buf.narrow(1, 0, progress).copy_(inputs_ids)
        del inputs_ids
        inputs_ids = inputs_ids_buf.narrow(1, 0, progress)

        pbar: Optional[tqdm] = None

        if show_tqdm:
            pbar = tqdm(
                total=max_new_token,
                desc="text" if infer_text else "code",
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}(max) [{elapsed}, {rate_fmt}{postfix}]",
            )

        past_key_values = None

        for i in range(max_new_token):

            model_input = self._prepare_generation_inputs(
                inputs_ids,
                past_key_values,
                attention_mask_cache.narrow(1, 0, inputs_ids.shape[1]),
                use_cache=not self.is_te_llama,
            )

            if i > 0:
                del emb
                inputs_ids_emb = model_input.input_ids.to(self.device_gpt)
                if infer_text:
                    emb: torch.Tensor = self.emb_text(inputs_ids_emb[:, :, 0])
                else:
                    code_emb = [
                        self.emb_code[i](inputs_ids_emb[:, :, i])
                        for i in range(self.num_vq)
                    ]
                    emb = torch.stack(code_emb, 3).sum(3)
                del inputs_ids_emb, model_input.input_ids
            model_input.inputs_embeds = emb

            model_input.to(self.device_gpt, self.gpt.dtype)

            outputs: BaseModelOutputWithPast = self.gpt(
                attention_mask=model_input.attention_mask,
                position_ids=model_input.position_ids,
                past_key_values=model_input.past_key_values,
                inputs_embeds=model_input.inputs_embeds,
                use_cache=model_input.use_cache,
                output_attentions=return_attn,
                cache_position=model_input.cache_position,
            )
            del_all(model_input)
            attentions.append(outputs.attentions)
            hidden_states = outputs.last_hidden_state.to(
                self.device, dtype=torch.float
            )  # ğŸ»
            past_key_values = outputs.past_key_values
            del_all(outputs)
            if return_hidden:
                hiddens.append(hidden_states.narrow(1, -1, 1).squeeze_(1))

            with P.cached():
                if infer_text:
                    logits: torch.Tensor = self.head_text(hidden_states)
                else:
                    # logits = torch.stack([self.head_code[i](hidden_states) for i in range(self.num_vq)], 3)
                    logits = torch.empty(
                        hidden_states.size(0),
                        hidden_states.size(1),
                        self.num_audio_tokens,
                        self.num_vq,
                        dtype=torch.float,
                        device=self.device,
                    )
                    for num_vq_iter in range(self.num_vq):
                        x: torch.Tensor = self.head_code[num_vq_iter](hidden_states)
                        logits[..., num_vq_iter] = x
                        del x

            del hidden_states

            # logits = logits[:, -1].float()
            logits = logits.narrow(1, -1, 1).squeeze_(1).float()

            if not infer_text:
                # logits = rearrange(logits, "b c n -> (b n) c")
                logits = logits.permute(0, 2, 1)
                logits = logits.reshape(-1, logits.size(2))
                # logits_token = rearrange(inputs_ids[:, start_idx:], "b c n -> (b n) c")
                inputs_ids_sliced = inputs_ids.narrow(
                    1,
                    start_idx,
                    inputs_ids.size(1) - start_idx,
                ).permute(0, 2, 1)
                logits_token = inputs_ids_sliced.reshape(
                    inputs_ids_sliced.size(0) * inputs_ids_sliced.size(1),
                    -1,
                ).to(self.device)
                del inputs_ids_sliced
            else:
                logits_token = (
                    inputs_ids.narrow(
                        1,
                        start_idx,
                        inputs_ids.size(1) - start_idx,
                    )
                    .narrow(2, 0, 1)
                    .to(self.device)
                )

            logits /= temperature

            for logitsProcessors in logits_processors:
                logits = logitsProcessors(logits_token, logits)

            del logits_token

            if i < min_new_token:
                logits[:, eos_token] = -torch.inf

            scores = F.softmax(logits, dim=-1)

            del logits

            if manual_seed is None:
                idx_next = torch.multinomial(scores, num_samples=1).to(finish.device)
            else:
                idx_next = torch.multinomial(
                    scores,
                    num_samples=1,
                    generator=self.generator.manual_seed(manual_seed),
                ).to(finish.device)

            del scores

            if not infer_text:
                # idx_next = rearrange(idx_next, "(b n) 1 -> b n", n=self.num_vq)
                idx_next = idx_next.view(-1, self.num_vq)
                finish_or = idx_next.eq(eos_token).any(1)
                finish.logical_or_(finish_or)
                del finish_or
                inputs_ids_buf.narrow(1, progress, 1).copy_(idx_next.unsqueeze_(1))
            else:
                finish_or = idx_next.eq(eos_token).any(1)
                finish.logical_or_(finish_or)
                del finish_or
                inputs_ids_buf.narrow(1, progress, 1).copy_(
                    idx_next.unsqueeze_(-1).expand(-1, -1, self.num_vq),
                )

            if i == 0 and finish.any():
                self.logger.warning(
                    "unexpected end at index %s",
                    str([unexpected_idx.item() for unexpected_idx in finish.nonzero()]),
                )
                if ensure_non_empty and manual_seed is None:
                    if show_tqdm:
                        pbar.close()
                    self.logger.warning("regenerate in order to ensure non-empty")
                    del_all(attentions)
                    del_all(hiddens)
                    del (
                        start_idx,
                        end_idx,
                        finish,
                        temperature,
                        attention_mask_cache,
                        past_key_values,
                        idx_next,
                        inputs_ids_buf,
                    )
                    new_gen = self.generate(
                        emb,
                        inputs_ids,
                        old_temperature,
                        eos_token,
                        attention_mask,
                        max_new_token,
                        min_new_token,
                        logits_processors,
                        infer_text,
                        return_attn,
                        return_hidden,
                        stream,
                        show_tqdm,
                        ensure_non_empty,
                        stream_batch,
                        manual_seed,
                        context,
                    )
                    for result in new_gen:
                        yield result
                    del inputs_ids
                return

            del idx_next
            progress += 1
            inputs_ids = inputs_ids_buf.narrow(1, 0, progress)

            not_finished = finish.logical_not().to(end_idx.device)
            end_idx.add_(not_finished.int())
            stream_iter += not_finished.any().int()
            if stream:
                if stream_iter > 0 and stream_iter % stream_batch == 0:
                    self.logger.debug("yield stream result, end: %d", end_idx)
                    yield self._prepare_generation_outputs(
                        inputs_ids,
                        start_idx,
                        end_idx,
                        attentions,
                        hiddens,
                        infer_text,
                    )
            del not_finished

            if finish.all() or context.get():
                break

            if pbar is not None:
                pbar.update(1)

        if pbar is not None:
            pbar.close()

        if not finish.all():
            if context.get():
                self.logger.warning("generation is interrupted")
            else:
                self.logger.warning(
                    f"incomplete result. hit max_new_token: {max_new_token}"
                )

        del finish, inputs_ids_buf

        yield self._prepare_generation_outputs(
            inputs_ids,
            start_idx,
            end_idx,
            attentions,
            hiddens,
            infer_text,
        )



================================================
FILE: ChatTTS/model/processors.py
================================================
import torch
import torch.nn.functional as F
from transformers.generation import TopKLogitsWarper, TopPLogitsWarper


class CustomRepetitionPenaltyLogitsProcessorRepeat:

    def __init__(self, penalty: float, max_input_ids: int, past_window: int):
        if not isinstance(penalty, float) or not (penalty > 0):
            raise ValueError(
                f"`penalty` has to be a strictly positive float, but is {penalty}"
            )

        self.penalty = penalty
        self.max_input_ids = max_input_ids
        self.past_window = past_window

    def __call__(
        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
    ) -> torch.FloatTensor:
        if input_ids.size(1) > self.past_window:
            input_ids = input_ids.narrow(1, -self.past_window, self.past_window)
        freq = F.one_hot(input_ids, scores.size(1)).sum(1)
        if freq.size(0) > self.max_input_ids:
            freq.narrow(
                0, self.max_input_ids, freq.size(0) - self.max_input_ids
            ).zero_()
        alpha = torch.pow(self.penalty, freq)
        scores = scores.contiguous()
        inp = scores.multiply(alpha)
        oth = scores.divide(alpha)
        con = scores < 0
        out = torch.where(con, inp, oth)
        del inp, oth, scores, con, alpha
        return out


def gen_logits(
    num_code: int,
    top_P=0.7,
    top_K=20,
    repetition_penalty=1.0,
):
    logits_warpers = []
    if top_P is not None:
        logits_warpers.append(TopPLogitsWarper(top_P, min_tokens_to_keep=3))
    if top_K is not None:
        logits_warpers.append(TopKLogitsWarper(top_K, min_tokens_to_keep=3))

    logits_processors = []
    if repetition_penalty is not None and repetition_penalty != 1:
        logits_processors.append(
            CustomRepetitionPenaltyLogitsProcessorRepeat(
                repetition_penalty, num_code, 16
            )
        )

    return logits_warpers, logits_processors



================================================
FILE: ChatTTS/model/speaker.py
================================================
import lzma
from typing import List, Optional, Union

import pybase16384 as b14
import numpy as np
import torch
import torch.nn.functional as F


class Speaker:
    def __init__(self, dim: int, spk_cfg: str, device=torch.device("cpu")) -> None:
        spk_stat = torch.from_numpy(
            np.frombuffer(b14.decode_from_string(spk_cfg), dtype=np.float16).copy()
        ).to(device=device)
        self.std, self.mean = spk_stat.requires_grad_(False).chunk(2)
        self.dim = dim

    def sample_random(self) -> str:
        return self._encode(self._sample_random())

    @torch.inference_mode()
    def apply(
        self,
        emb: torch.Tensor,
        spk_emb: Union[str, torch.Tensor],
        input_ids: torch.Tensor,
        spk_emb_ids: int,
        device: torch.device,
        inplace: bool = True,
    ) -> torch.Tensor:
        if isinstance(spk_emb, str):
            spk_emb_tensor = torch.from_numpy(self._decode(spk_emb))
        else:
            spk_emb_tensor = spk_emb
        n = (
            F.normalize(
                spk_emb_tensor,
                p=2.0,
                dim=0,
                eps=1e-12,
            )
            .to(device)
            .unsqueeze_(0)
            .expand(emb.size(0), -1)
            .unsqueeze_(1)
            .expand(emb.shape)
        )
        cond = input_ids.narrow(-1, 0, 1).eq(spk_emb_ids).expand(emb.shape)
        out = torch.where(cond, n, emb, out=emb if inplace else None)
        if inplace:
            del cond, n
        return out

    @staticmethod
    @torch.no_grad()
    def decorate_code_prompts(
        text: List[str],
        prompt: str,
        txt_smp: Optional[str],
        spk_emb: Optional[str],
    ) -> List[str]:
        for i, t in enumerate(text):
            text[i] = (
                t.replace("[Stts]", "")
                .replace("[spk_emb]", "")
                .replace("[empty_spk]", "")
                .strip()
            )
            """
            see https://github.com/2noise/ChatTTS/issues/459
            """

        if prompt:
            text = [prompt + i for i in text]

        txt_smp = "" if txt_smp is None else txt_smp
        if spk_emb is not None:
            text = [f"[Stts][spk_emb]{txt_smp}{i}[Ptts]" for i in text]
        else:
            text = [f"[Stts][empty_spk]{txt_smp}{i}[Ptts]" for i in text]

        return text

    @staticmethod
    @torch.no_grad()
    def decorate_text_prompts(text: List[str], prompt: str) -> List[str]:
        return [f"[Sbreak]{i}[Pbreak]{prompt}" for i in text]

    @staticmethod
    @torch.no_grad()
    def encode_prompt(prompt: torch.Tensor) -> str:
        arr: np.ndarray = prompt.cpu().numpy().astype(np.uint16)
        shp = arr.shape
        assert len(shp) == 2, "prompt must be a 2D tensor"
        s = b14.encode_to_string(
            np.array(shp, dtype="<u2").tobytes()
            + lzma.compress(
                arr.astype("<u2").tobytes(),
                format=lzma.FORMAT_RAW,
                filters=[{"id": lzma.FILTER_LZMA2, "preset": 9 | lzma.PRESET_EXTREME}],
            ),
        )
        del arr
        return s

    @staticmethod
    @torch.no_grad()
    def decode_prompt(prompt: str) -> torch.Tensor:
        dec = b14.decode_from_string(prompt)
        shp = np.frombuffer(dec[:4], dtype="<u2")
        p = np.frombuffer(
            lzma.decompress(
                dec[4:],
                format=lzma.FORMAT_RAW,
                filters=[{"id": lzma.FILTER_LZMA2, "preset": 9 | lzma.PRESET_EXTREME}],
            ),
            dtype="<u2",
        ).copy()
        del dec
        return torch.from_numpy(p.astype(np.int32)).view(*shp)

    @torch.no_grad()
    def _sample_random(self) -> torch.Tensor:
        spk = (
            torch.randn(self.dim, device=self.std.device, dtype=self.std.dtype)
            .mul_(self.std)
            .add_(self.mean)
        )
        return spk

    @staticmethod
    @torch.no_grad()
    def _encode(spk_emb: torch.Tensor) -> str:
        arr: np.ndarray = spk_emb.to(dtype=torch.float16, device="cpu").numpy()
        s = b14.encode_to_string(
            lzma.compress(
                arr.tobytes(),
                format=lzma.FORMAT_RAW,
                filters=[{"id": lzma.FILTER_LZMA2, "preset": 9 | lzma.PRESET_EXTREME}],
            ),
        )
        del arr
        return s

    @staticmethod
    def _decode(spk_emb: str) -> np.ndarray:
        return np.frombuffer(
            lzma.decompress(
                b14.decode_from_string(spk_emb),
                format=lzma.FORMAT_RAW,
                filters=[{"id": lzma.FILTER_LZMA2, "preset": 9 | lzma.PRESET_EXTREME}],
            ),
            dtype=np.float16,
        ).copy()



================================================
FILE: ChatTTS/model/tokenizer.py
================================================
import os

os.environ["TOKENIZERS_PARALLELISM"] = "false"
"""
https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning
"""

from typing import List, Tuple, Optional, Union

import torch
from transformers import BertTokenizerFast

from ..utils import del_all


class Tokenizer:
    def __init__(
        self,
        tokenizer_path: torch.serialization.FILE_LIKE,
    ):
        """
        tokenizer: BertTokenizerFast = torch.load(
            tokenizer_path, map_location=device, mmap=True
        )
        # tokenizer.save_pretrained("asset/tokenizer", legacy_format=False)
        """
        tokenizer: BertTokenizerFast = BertTokenizerFast.from_pretrained(tokenizer_path)
        self._tokenizer = tokenizer

        self.len = len(tokenizer)
        self.spk_emb_ids = tokenizer.convert_tokens_to_ids("[spk_emb]")
        self.break_0_ids = tokenizer.convert_tokens_to_ids("[break_0]")
        self.eos_token = tokenizer.convert_tokens_to_ids("[Ebreak]")

    @torch.inference_mode()
    def encode(
        self,
        text: List[str],
        num_vq: int,
        prompt: Optional[torch.Tensor] = None,
        device="cpu",
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:

        input_ids_lst = []
        attention_mask_lst = []
        max_input_ids_len = -1
        max_attention_mask_len = -1
        prompt_size = 0

        if prompt is not None:
            assert prompt.size(0) == num_vq, "prompt dim 0 must equal to num_vq"
            prompt_size = prompt.size(1)

        # avoid random speaker embedding of tokenizer in the other dims
        for t in text:
            x = self._tokenizer.encode_plus(
                t, return_tensors="pt", add_special_tokens=False, padding=True
            )
            input_ids_lst.append(x["input_ids"].squeeze_(0))
            attention_mask_lst.append(x["attention_mask"].squeeze_(0))
            del_all(x)
            ids_sz = input_ids_lst[-1].size(0)
            if ids_sz > max_input_ids_len:
                max_input_ids_len = ids_sz
            attn_sz = attention_mask_lst[-1].size(0)
            if attn_sz > max_attention_mask_len:
                max_attention_mask_len = attn_sz

        if prompt is not None:
            max_input_ids_len += prompt_size
            max_attention_mask_len += prompt_size

        input_ids = torch.zeros(
            len(input_ids_lst),
            max_input_ids_len,
            device=device,
            dtype=input_ids_lst[0].dtype,
        )
        for i in range(len(input_ids_lst)):
            input_ids.narrow(0, i, 1).narrow(
                1,
                max_input_ids_len - prompt_size - input_ids_lst[i].size(0),
                input_ids_lst[i].size(0),
            ).copy_(
                input_ids_lst[i]
            )  # left padding
        del_all(input_ids_lst)

        attention_mask = torch.zeros(
            len(attention_mask_lst),
            max_attention_mask_len,
            device=device,
            dtype=attention_mask_lst[0].dtype,
        )
        for i in range(len(attention_mask_lst)):
            attn = attention_mask.narrow(0, i, 1)
            attn.narrow(
                1,
                max_attention_mask_len - prompt_size - attention_mask_lst[i].size(0),
                attention_mask_lst[i].size(0),
            ).copy_(
                attention_mask_lst[i]
            )  # left padding
            if prompt_size > 0:
                attn.narrow(
                    1,
                    max_attention_mask_len - prompt_size,
                    prompt_size,
                ).fill_(1)
        del_all(attention_mask_lst)

        text_mask = attention_mask.bool()
        new_input_ids = input_ids.unsqueeze_(-1).expand(-1, -1, num_vq).clone()
        del input_ids

        if prompt_size > 0:
            text_mask.narrow(1, max_input_ids_len - prompt_size, prompt_size).fill_(0)
            prompt_t = prompt.t().unsqueeze_(0).expand(new_input_ids.size(0), -1, -1)
            new_input_ids.narrow(
                1,
                max_input_ids_len - prompt_size,
                prompt_size,
            ).copy_(prompt_t)
            del prompt_t

        return new_input_ids, attention_mask, text_mask

    @torch.inference_mode
    def decode(
        self,
        sequences: Union[List[int], List[List[int]]],
        skip_special_tokens: bool = False,
        clean_up_tokenization_spaces: bool = None,
        **kwargs,
    ):
        return self._tokenizer.batch_decode(
            sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs
        )



================================================
FILE: ChatTTS/model/cuda/__init__.py
================================================
from .te_llama import TELlamaModel



================================================
FILE: ChatTTS/model/cuda/patch.py
================================================
import torch


class LlamaRMSNorm(torch.nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        LlamaRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = torch.nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states: torch.Tensor):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight.to(hidden_states.device) * hidden_states.to(input_dtype)



================================================
FILE: ChatTTS/model/cuda/te_llama.py
================================================
# Copyright (c) 2022-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# See LICENSE for license information.
#
# From https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/te_llama/te_llama.py
#
# Edited by fumiama.

import re
from contextlib import contextmanager
from typing import Dict

import transformer_engine as te
from transformer_engine.pytorch.attention import RotaryPositionEmbedding

import torch

import transformers
from transformers.models.llama.modeling_llama import (
    LlamaModel,
    LlamaConfig,
)
from transformers.modeling_utils import _load_state_dict_into_model

from .patch import LlamaRMSNorm


@contextmanager
def replace_decoder(te_decoder_cls, llama_rms_norm_cls):
    """
    Replace `LlamaDecoderLayer` with custom `TELlamaDecoderLayer`.
    """
    original_llama_decoder_cls = (
        transformers.models.llama.modeling_llama.LlamaDecoderLayer
    )
    transformers.models.llama.modeling_llama.LlamaDecoderLayer = te_decoder_cls
    original_llama_rms_norm_cls = transformers.models.llama.modeling_llama.LlamaRMSNorm
    transformers.models.llama.modeling_llama.LlamaRMSNorm = llama_rms_norm_cls
    try:
        yield
    finally:
        transformers.models.llama.modeling_llama.LlamaDecoderLayer = (
            original_llama_decoder_cls
        )
        transformers.models.llama.modeling_llama.LlamaRMSNorm = (
            original_llama_rms_norm_cls
        )


class TELlamaDecoderLayer(te.pytorch.TransformerLayer):
    """
    Wrapper class over TE's `TransformerLayer`. This makes the wrapper very
    similar to HF's `LlamaDecoderLayer` and easier to replace it in the code.

    Args:
        config: LlamaConfig
        args: positional args (for compatibility with `LlamaDecoderLayer`)
        kwargs: keyword args (for compatibility with `LlamaDecoderLayer`)
    """

    def __init__(self, config, *args, **kwargs):
        super().__init__(
            hidden_size=config.hidden_size,
            ffn_hidden_size=config.intermediate_size,
            num_attention_heads=config.num_attention_heads,
            bias=False,
            layernorm_epsilon=config.rms_norm_eps,
            hidden_dropout=0,
            attention_dropout=0,
            fuse_qkv_params=False,
            normalization="RMSNorm",
            activation="swiglu",
            attn_input_format="bshd",
            num_gqa_groups=config.num_key_value_heads,
        )
        te_rope = RotaryPositionEmbedding(
            config.hidden_size // config.num_attention_heads
        )
        self.te_rope_emb = te_rope(max_seq_len=config.max_position_embeddings).cuda()

    def forward(self, hidden_states, *args, attention_mask, **kwargs):
        """
        Custom forward to make sure we only pass relevant arguments to the
        forward pass of the `TransformerLayer`. Also, make sure the output
        format matches the output of the HF's `LlamaDecoderLayer`.
        """
        return (
            super().forward(
                hidden_states,
                attention_mask=attention_mask,
                rotary_pos_emb=self.te_rope_emb,
            ),
        )


class TELlamaModel:
    """
    LM created with `LlamaModel`. The underlying `LlamaDecoderLayer`
    class is monkey-patched with `TELlamaDecoderLayer` class before
    initializing the causal LM with `LlamaModel`.

    Args:
        config: LlamaConfig
    """

    def __new__(cls, config: LlamaConfig):
        with replace_decoder(
            te_decoder_cls=TELlamaDecoderLayer, llama_rms_norm_cls=LlamaRMSNorm
        ):
            model = LlamaModel(config)
        return model

    @classmethod
    def from_state_dict(
        cls,
        state_dict: Dict[str, torch.Tensor],
        config: LlamaConfig,
    ):
        """
        Custom method adapted from `from_pretrained` method in HuggingFace
        Transformers repo: https://github.com/huggingface/transformers/blob/f497f564bb76697edab09184a252fc1b1a326d1e/src/transformers/modeling_utils.py#L2579
        """

        vanilla_model = cls(config)

        # replace_params copies parameters relevant only to TransformerEngine
        _replace_params(state_dict, vanilla_model.state_dict(), config)
        # _load_state_dict_into_model copies parameters other than those in TransformerEngine
        _load_state_dict_into_model(vanilla_model, state_dict, start_prefix="")

        return vanilla_model


def _replace_params(hf_state_dict, te_state_dict, config):
    # collect all layer prefixes to update
    all_layer_prefixes = set()
    for param_key in hf_state_dict.keys():
        layer_prefix_pat = "model.layers.\d+."
        m = re.match(layer_prefix_pat, param_key)
        if m is not None:
            all_layer_prefixes.add(m.group())

    for layer_prefix in all_layer_prefixes:
        # When loading weights into models with less number of layers, skip the
        # copy if the corresponding layer doesn't exist in HF model
        if layer_prefix + "input_layernorm.weight" in hf_state_dict:
            te_state_dict[
                layer_prefix + "self_attention.layernorm_qkv.layer_norm_weight"
            ].data[:] = hf_state_dict[layer_prefix + "input_layernorm.weight"].data[:]

        if layer_prefix + "self_attn.q_proj.weight" in hf_state_dict:
            te_state_dict[
                layer_prefix + "self_attention.layernorm_qkv.query_weight"
            ].data[:] = hf_state_dict[layer_prefix + "self_attn.q_proj.weight"].data[:]

        if layer_prefix + "self_attn.k_proj.weight" in hf_state_dict:
            te_state_dict[
                layer_prefix + "self_attention.layernorm_qkv.key_weight"
            ].data[:] = hf_state_dict[layer_prefix + "self_attn.k_proj.weight"].data[:]

        if layer_prefix + "self_attn.v_proj.weight" in hf_state_dict:
            te_state_dict[
                layer_prefix + "self_attention.layernorm_qkv.value_weight"
            ].data[:] = hf_state_dict[layer_prefix + "self_attn.v_proj.weight"].data[:]

        if layer_prefix + "self_attn.o_proj.weight" in hf_state_dict:
            te_state_dict[layer_prefix + "self_attention.proj.weight"].data[:] = (
                hf_state_dict[layer_prefix + "self_attn.o_proj.weight"].data[:]
            )

        if layer_prefix + "post_attention_layernorm.weight" in hf_state_dict:
            te_state_dict[layer_prefix + "layernorm_mlp.layer_norm_weight"].data[:] = (
                hf_state_dict[layer_prefix + "post_attention_layernorm.weight"].data[:]
            )

        # It may happen that gate_proj.weight and up_proj.weight will be in the different files, so we need to
        # load them separately.
        if layer_prefix + "mlp.gate_proj.weight" in hf_state_dict:
            te_state_dict[layer_prefix + "layernorm_mlp.fc1_weight"].data[
                : config.intermediate_size
            ] = hf_state_dict[layer_prefix + "mlp.gate_proj.weight"].data

        if layer_prefix + "mlp.up_proj.weight" in hf_state_dict:
            te_state_dict[layer_prefix + "layernorm_mlp.fc1_weight"].data[
                config.intermediate_size :
            ] = hf_state_dict[layer_prefix + "mlp.up_proj.weight"].data

        if layer_prefix + "mlp.down_proj.weight" in hf_state_dict:
            te_state_dict[layer_prefix + "layernorm_mlp.fc2_weight"].data[:] = (
                hf_state_dict[layer_prefix + "mlp.down_proj.weight"].data[:]
            )
    return all_layer_prefixes



================================================
FILE: ChatTTS/model/velocity/__init__.py
================================================
from .llm import LLM
from .sampling_params import SamplingParams



================================================
FILE: ChatTTS/model/velocity/block_manager.py
================================================
"""A block manager that manages token blocks."""

import enum
from typing import Dict, List, Optional, Set, Tuple

from vllm.block import PhysicalTokenBlock
from .sequence import Sequence, SequenceGroup, SequenceStatus
from vllm.utils import Device

# Mapping: logical block number -> physical block.
BlockTable = List[PhysicalTokenBlock]


class BlockAllocator:
    """Manages free physical token blocks for a device.

    The allocator maintains a list of free blocks and allocates a block when
    requested. When a block is freed, its reference count is decremented. If
    the reference count becomes zero, the block is added back to the free list.
    """

    def __init__(
        self,
        device: Device,
        block_size: int,
        num_blocks: int,
    ) -> None:
        self.device = device
        self.block_size = block_size
        self.num_blocks = num_blocks

        # Initialize the free blocks.
        self.free_blocks: BlockTable = []
        for i in range(num_blocks):
            block = PhysicalTokenBlock(
                device=device, block_number=i, block_size=block_size
            )
            self.free_blocks.append(block)

    def allocate(self) -> PhysicalTokenBlock:
        if not self.free_blocks:
            raise ValueError("Out of memory! No free blocks are available.")
        block = self.free_blocks.pop()
        block.ref_count = 1
        return block

    def free(self, block: PhysicalTokenBlock) -> None:
        if block.ref_count == 0:
            raise ValueError(f"Double free! {block} is already freed.")
        block.ref_count -= 1
        if block.ref_count == 0:
            self.free_blocks.append(block)

    def get_num_free_blocks(self) -> int:
        return len(self.free_blocks)


class AllocStatus(enum.Enum):
    """Result for BlockSpaceManager.can_allocate

    1. Ok: seq_group can be allocated now.
    2. Later: seq_group cannot be allocated.
      The capacity of allocator is larger than seq_group required.
    3. Never: seq_group can never be allocated.
      The seq_group is too large to allocated in GPU.
    """

    OK = enum.auto()
    LATER = enum.auto()
    NEVER = enum.auto()


class BlockSpaceManager:
    """Manages the mapping between logical and physical token blocks."""

    def __init__(
        self,
        block_size: int,
        num_gpu_blocks: int,
        num_cpu_blocks: int,
        watermark: float = 0.01,
        sliding_window: Optional[int] = None,
    ) -> None:
        self.block_size = block_size
        self.num_total_gpu_blocks = num_gpu_blocks
        self.num_total_cpu_blocks = num_cpu_blocks

        self.block_sliding_window = None
        if sliding_window is not None:
            assert sliding_window % block_size == 0, (sliding_window, block_size)
            self.block_sliding_window = sliding_window // block_size

        self.watermark = watermark
        assert watermark >= 0.0

        self.watermark_blocks = int(watermark * num_gpu_blocks)
        self.gpu_allocator = BlockAllocator(Device.GPU, block_size, num_gpu_blocks)
        self.cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)
        # Mapping: seq_id -> BlockTable.
        self.block_tables: Dict[int, BlockTable] = {}

    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:
        # FIXME(woosuk): Here we assume that all sequences in the group share
        # the same prompt. This may not be true for preempted sequences.
        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]
        num_required_blocks = len(seq.logical_token_blocks)
        if self.block_sliding_window is not None:
            num_required_blocks = min(num_required_blocks, self.block_sliding_window)
        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()

        # Use watermark to avoid frequent cache eviction.
        if self.num_total_gpu_blocks - num_required_blocks < self.watermark_blocks:
            return AllocStatus.NEVER
        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:
            return AllocStatus.OK
        else:
            return AllocStatus.LATER

    def allocate(self, seq_group: SequenceGroup) -> None:
        # NOTE: Here we assume that all sequences in the group have the same
        # prompt.
        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]

        # Allocate new physical token blocks that will store the prompt tokens.
        block_table: BlockTable = []
        for logical_idx in range(len(seq.logical_token_blocks)):
            if (
                self.block_sliding_window is not None
                and logical_idx >= self.block_sliding_window
            ):
                block = block_table[logical_idx % self.block_sliding_window]
            else:
                block = self.gpu_allocator.allocate()
            # Set the reference counts of the token blocks.
            block.ref_count = seq_group.num_seqs()
            block_table.append(block)

        # Assign the block table for each sequence.
        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):
            self.block_tables[seq.seq_id] = block_table.copy()

    def can_append_slot(self, seq_group: SequenceGroup) -> bool:
        # Simple heuristic: If there is at least one free block
        # for each sequence, we can append.
        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()
        num_seqs = seq_group.num_seqs(status=SequenceStatus.RUNNING)
        return num_seqs <= num_free_gpu_blocks

    def append_slot(self, seq: Sequence) -> Optional[Tuple[int, int]]:
        """Allocate a physical slot for a new token."""
        logical_blocks = seq.logical_token_blocks
        block_table = self.block_tables[seq.seq_id]

        if len(block_table) < len(logical_blocks):
            if (
                self.block_sliding_window
                and len(block_table) >= self.block_sliding_window
            ):
                # re-use a block
                block_table.append(
                    block_table[len(block_table) % self.block_sliding_window]
                )
            else:
                # The sequence has a new logical block.
                # Allocate a new physical block.
                block = self.gpu_allocator.allocate()
                block_table.append(block)
                return None

        # We want to append the token to the last physical block.
        last_block = block_table[-1]
        assert last_block.device == Device.GPU
        if last_block.ref_count == 1:
            # Not shared with other sequences. Appendable.
            return None
        else:
            # The last block is shared with other sequences.
            # Copy on Write: Allocate a new block and copy the tokens.
            new_block = self.gpu_allocator.allocate()
            block_table[-1] = new_block
            self.gpu_allocator.free(last_block)
            return last_block.block_number, new_block.block_number

    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:
        # NOTE: fork does not allocate a new physical block.
        # Thus, it is always safe from OOM.
        src_block_table = self.block_tables[parent_seq.seq_id]
        self.block_tables[child_seq.seq_id] = src_block_table.copy()
        for block in src_block_table:
            block.ref_count += 1

    def _get_physical_blocks(
        self, seq_group: SequenceGroup
    ) -> List[PhysicalTokenBlock]:
        # NOTE: Here, we assume that the physical blocks are only shared by
        # the sequences in the same group.
        blocks: Set[PhysicalTokenBlock] = set()
        for seq in seq_group.get_seqs():
            if seq.is_finished():
                continue
            blocks.update(self.block_tables[seq.seq_id])
        return list(blocks)

    def can_swap_in(self, seq_group: SequenceGroup) -> bool:
        blocks = self._get_physical_blocks(seq_group)
        num_swapped_seqs = seq_group.num_seqs(status=SequenceStatus.SWAPPED)
        num_free_blocks = self.gpu_allocator.get_num_free_blocks()
        # NOTE: Conservatively, we assume that every sequence will allocate
        # at least one free block right after the swap-in.
        # NOTE: This should match the logic in can_append_slot().
        num_required_blocks = len(blocks) + num_swapped_seqs
        return num_free_blocks - num_required_blocks >= self.watermark_blocks

    def swap_in(self, seq_group: SequenceGroup) -> Dict[int, int]:
        # CPU block -> GPU block.
        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}
        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):
            new_block_table: BlockTable = []
            block_table = self.block_tables[seq.seq_id]

            for cpu_block in block_table:
                if cpu_block in mapping:
                    gpu_block = mapping[cpu_block]
                    gpu_block.ref_count += 1
                else:
                    gpu_block = self.gpu_allocator.allocate()
                    mapping[cpu_block] = gpu_block
                new_block_table.append(gpu_block)
                # Free the CPU block swapped in to GPU.
                self.cpu_allocator.free(cpu_block)
            self.block_tables[seq.seq_id] = new_block_table

        block_number_mapping = {
            cpu_block.block_number: gpu_block.block_number
            for cpu_block, gpu_block in mapping.items()
        }
        return block_number_mapping

    def can_swap_out(self, seq_group: SequenceGroup) -> bool:
        blocks = self._get_physical_blocks(seq_group)
        return len(blocks) <= self.cpu_allocator.get_num_free_blocks()

    def swap_out(self, seq_group: SequenceGroup) -> Dict[int, int]:
        # GPU block -> CPU block.
        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}
        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
            new_block_table: BlockTable = []
            block_table = self.block_tables[seq.seq_id]

            for gpu_block in block_table:
                if gpu_block in mapping:
                    cpu_block = mapping[gpu_block]
                    cpu_block.ref_count += 1
                else:
                    cpu_block = self.cpu_allocator.allocate()
                    mapping[gpu_block] = cpu_block
                new_block_table.append(cpu_block)
                # Free the GPU block swapped out to CPU.
                self.gpu_allocator.free(gpu_block)
            self.block_tables[seq.seq_id] = new_block_table

        block_number_mapping = {
            gpu_block.block_number: cpu_block.block_number
            for gpu_block, cpu_block in mapping.items()
        }
        return block_number_mapping

    def _free_block_table(self, block_table: BlockTable) -> None:
        for block in set(block_table):
            if block.device == Device.GPU:
                self.gpu_allocator.free(block)
            else:
                self.cpu_allocator.free(block)

    def free(self, seq: Sequence) -> None:
        if seq.seq_id not in self.block_tables:
            # Already freed or haven't been scheduled yet.
            return
        block_table = self.block_tables[seq.seq_id]
        self._free_block_table(block_table)
        del self.block_tables[seq.seq_id]

    def reset(self) -> None:
        for block_table in self.block_tables.values():
            self._free_block_table(block_table)
        self.block_tables.clear()

    def get_block_table(self, seq: Sequence) -> List[int]:
        block_table = self.block_tables[seq.seq_id]
        return [block.block_number for block in block_table]

    def get_num_free_gpu_blocks(self) -> int:
        return self.gpu_allocator.get_num_free_blocks()

    def get_num_free_cpu_blocks(self) -> int:
        return self.cpu_allocator.get_num_free_blocks()



================================================
FILE: ChatTTS/model/velocity/configs.py
================================================
from typing import Optional, Union, Tuple
import os

import torch
from transformers import PretrainedConfig

from vllm.logger import init_logger
from vllm.transformers_utils.config import get_config
from vllm.utils import get_cpu_memory, is_hip

import argparse
import dataclasses
from dataclasses import dataclass


logger = init_logger(__name__)

_GB = 1 << 30


class ModelConfig:
    """Configuration for the model.

    Args:
        model: Name or path of the huggingface model to use.
        tokenizer: Name or path of the huggingface tokenizer to use.
        tokenizer_mode: Tokenizer mode. "auto" will use the fast tokenizer if
            available, and "slow" will always use the slow tokenizer.
        trust_remote_code: Trust remote code (e.g., from HuggingFace) when
            downloading the model and tokenizer.
        download_dir: Directory to download and load the weights, default to the
            default cache directory of huggingface.
        load_format: The format of the model weights to load:
            "auto" will try to load the weights in the safetensors format and
                fall back to the pytorch bin format if safetensors format is
                not available.
            "pt" will load the weights in the pytorch bin format.
            "safetensors" will load the weights in the safetensors format.
            "npcache" will load the weights in pytorch format and store
                a numpy cache to speed up the loading.
            "dummy" will initialize the weights with random values, which is
                mainly for profiling.
        dtype: Data type for model weights and activations. The "auto" option
            will use FP16 precision for FP32 and FP16 models, and BF16 precision
            for BF16 models.
        seed: Random seed for reproducibility.
        revision: The specific model version to use. It can be a branch name,
            a tag name, or a commit id. If unspecified, will use the default
            version.
        tokenizer_revision: The specific tokenizer version to use. It can be a
            branch name, a tag name, or a commit id. If unspecified, will use
            the default version.
        max_model_len: Maximum length of a sequence (including prompt and
            output). If None, will be derived from the model.
        quantization: Quantization method that was used to quantize the model
            weights. If None, we assume the model weights are not quantized.
        enforce_eager: Whether to enforce eager execution. If True, we will
            disable CUDA graph and always execute the model in eager mode.
            If False, we will use CUDA graph and eager execution in hybrid.
        max_context_len_to_capture: Maximum context len covered by CUDA graphs.
            When a sequence has context length larger than this, we fall back
            to eager mode.
    """

    def __init__(
        self,
        model: str,
        tokenizer: str,
        tokenizer_mode: str,
        trust_remote_code: bool,
        download_dir: Optional[str],
        load_format: str,
        dtype: Union[str, torch.dtype],
        seed: int,
        revision: Optional[str] = None,
        tokenizer_revision: Optional[str] = None,
        max_model_len: Optional[int] = None,
        quantization: Optional[str] = None,
        enforce_eager: bool = False,
        max_context_len_to_capture: Optional[int] = None,
        num_audio_tokens: int = 1024,
        num_text_tokens: int = 80,
    ) -> None:
        self.model = model
        self.tokenizer = tokenizer
        self.tokenizer_mode = tokenizer_mode
        self.trust_remote_code = trust_remote_code
        self.download_dir = download_dir
        self.load_format = load_format
        self.seed = seed
        self.revision = revision
        self.tokenizer_revision = tokenizer_revision
        self.quantization = quantization
        self.enforce_eager = enforce_eager
        self.max_context_len_to_capture = max_context_len_to_capture
        self.num_audio_tokens = num_audio_tokens
        self.num_text_tokens = num_text_tokens

        if os.environ.get("VLLM_USE_MODELSCOPE", "False").lower() == "true":
            # download model from ModelScope hub,
            # lazy import so that modelscope is not required for normal use.
            from modelscope.hub.snapshot_download import (
                snapshot_download,
            )  # pylint: disable=C

            model_path = snapshot_download(
                model_id=model, cache_dir=download_dir, revision=revision
            )
            self.model = model_path
            self.download_dir = model_path
            self.tokenizer = model_path

        self.hf_config = get_config(self.model, trust_remote_code, revision)
        self.dtype = _get_and_verify_dtype(self.hf_config, dtype)
        self.max_model_len = _get_and_verify_max_len(self.hf_config, max_model_len)
        self._verify_load_format()
        self._verify_tokenizer_mode()
        self._verify_quantization()
        self._verify_cuda_graph()

    def _verify_load_format(self) -> None:
        load_format = self.load_format.lower()
        supported_load_format = ["auto", "pt", "safetensors", "npcache", "dummy"]
        rocm_not_supported_load_format = []
        if load_format not in supported_load_format:
            raise ValueError(
                f"Unknown load format: {self.load_format}. Must be one of "
                "'auto', 'pt', 'safetensors', 'npcache', or 'dummy'."
            )
        if is_hip() and load_format in rocm_not_supported_load_format:
            rocm_supported_load_format = [
                f
                for f in supported_load_format
                if (f not in rocm_not_supported_load_format)
            ]
            raise ValueError(
                f"load format '{load_format}' is not supported in ROCm. "
                f"Supported load format are "
                f"{rocm_supported_load_format}"
            )

        # TODO: Remove this check once HF updates the pt weights of Mixtral.
        architectures = getattr(self.hf_config, "architectures", [])
        if "MixtralForCausalLM" in architectures and load_format == "pt":
            raise ValueError(
                "Currently, the 'pt' format is not supported for Mixtral. "
                "Please use the 'safetensors' format instead. "
            )
        self.load_format = load_format

    def _verify_tokenizer_mode(self) -> None:
        tokenizer_mode = self.tokenizer_mode.lower()
        if tokenizer_mode not in ["auto", "slow"]:
            raise ValueError(
                f"Unknown tokenizer mode: {self.tokenizer_mode}. Must be "
                "either 'auto' or 'slow'."
            )
        self.tokenizer_mode = tokenizer_mode

    def _verify_quantization(self) -> None:
        supported_quantization = ["awq", "gptq", "squeezellm"]
        rocm_not_supported_quantization = ["awq"]
        if self.quantization is not None:
            self.quantization = self.quantization.lower()

        # Parse quantization method from the HF model config, if available.
        hf_quant_config = getattr(self.hf_config, "quantization_config", None)
        if hf_quant_config is not None:
            hf_quant_method = str(hf_quant_config["quant_method"]).lower()
            if self.quantization is None:
                self.quantization = hf_quant_method
            elif self.quantization != hf_quant_method:
                raise ValueError(
                    "Quantization method specified in the model config "
                    f"({hf_quant_method}) does not match the quantization "
                    f"method specified in the `quantization` argument "
                    f"({self.quantization})."
                )

        if self.quantization is not None:
            if self.quantization not in supported_quantization:
                raise ValueError(
                    f"Unknown quantization method: {self.quantization}. Must "
                    f"be one of {supported_quantization}."
                )
            if is_hip() and self.quantization in rocm_not_supported_quantization:
                raise ValueError(
                    f"{self.quantization} quantization is currently not supported "
                    f"in ROCm."
                )
            logger.warning(
                f"{self.quantization} quantization is not fully "
                "optimized yet. The speed can be slower than "
                "non-quantized models."
            )

    def _verify_cuda_graph(self) -> None:
        if self.max_context_len_to_capture is None:
            self.max_context_len_to_capture = self.max_model_len
        self.max_context_len_to_capture = min(
            self.max_context_len_to_capture, self.max_model_len
        )

    def verify_with_parallel_config(
        self,
        parallel_config: "ParallelConfig",
    ) -> None:
        total_num_attention_heads = self.hf_config.num_attention_heads
        tensor_parallel_size = parallel_config.tensor_parallel_size
        if total_num_attention_heads % tensor_parallel_size != 0:
            raise ValueError(
                f"Total number of attention heads ({total_num_attention_heads})"
                " must be divisible by tensor parallel size "
                f"({tensor_parallel_size})."
            )

        total_num_hidden_layers = self.hf_config.num_hidden_layers
        pipeline_parallel_size = parallel_config.pipeline_parallel_size
        if total_num_hidden_layers % pipeline_parallel_size != 0:
            raise ValueError(
                f"Total number of hidden layers ({total_num_hidden_layers}) "
                "must be divisible by pipeline parallel size "
                f"({pipeline_parallel_size})."
            )

    def get_sliding_window(self) -> Optional[int]:
        return getattr(self.hf_config, "sliding_window", None)

    def get_vocab_size(self) -> int:
        return self.hf_config.vocab_size

    def get_hidden_size(self) -> int:
        return self.hf_config.hidden_size

    def get_head_size(self) -> int:
        # FIXME(woosuk): This may not be true for all models.
        return self.hf_config.hidden_size // self.hf_config.num_attention_heads

    def get_total_num_kv_heads(self) -> int:
        """Returns the total number of KV heads."""
        # For GPTBigCode & Falcon:
        # NOTE: for falcon, when new_decoder_architecture is True, the
        # multi_query flag is ignored and we use n_head_kv for the number of
        # KV heads.
        falcon_model_types = ["falcon", "RefinedWeb", "RefinedWebModel"]
        new_decoder_arch_falcon = (
            self.hf_config.model_type in falcon_model_types
            and getattr(self.hf_config, "new_decoder_architecture", False)
        )
        if not new_decoder_arch_falcon and getattr(
            self.hf_config, "multi_query", False
        ):
            # Multi-query attention, only one KV head.
            # Currently, tensor parallelism is not supported in this case.
            return 1

        attributes = [
            # For Falcon:
            "n_head_kv",
            "num_kv_heads",
            # For LLaMA-2:
            "num_key_value_heads",
            # For ChatGLM:
            "multi_query_group_num",
        ]
        for attr in attributes:
            num_kv_heads = getattr(self.hf_config, attr, None)
            if num_kv_heads is not None:
                return num_kv_heads

        # For non-grouped-query attention models, the number of KV heads is
        # equal to the number of attention heads.
        return self.hf_config.num_attention_heads

    def get_num_kv_heads(self, parallel_config: "ParallelConfig") -> int:
        """Returns the number of KV heads per GPU."""
        total_num_kv_heads = self.get_total_num_kv_heads()
        # If tensor parallelism is used, we divide the number of KV heads by
        # the tensor parallel size. We will replicate the KV heads in the
        # case where the number of KV heads is smaller than the tensor
        # parallel size so each GPU has at least one KV head.
        return max(1, total_num_kv_heads // parallel_config.tensor_parallel_size)

    def get_num_layers(self, parallel_config: "ParallelConfig") -> int:
        total_num_hidden_layers = self.hf_config.num_hidden_layers
        return total_num_hidden_layers // parallel_config.pipeline_parallel_size


class CacheConfig:
    """Configuration for the KV cache.

    Args:
        block_size: Size of a cache block in number of tokens.
        gpu_memory_utilization: Fraction of GPU memory to use for the
            vLLM execution.
        swap_space: Size of the CPU swap space per GPU (in GiB).
    """

    def __init__(
        self,
        block_size: int,
        gpu_memory_utilization: float,
        swap_space: int,
        sliding_window: Optional[int] = None,
    ) -> None:
        self.block_size = block_size
        self.gpu_memory_utilization = gpu_memory_utilization
        self.swap_space_bytes = swap_space * _GB
        self.sliding_window = sliding_window
        self._verify_args()

        # Will be set after profiling.
        self.num_gpu_blocks = None
        self.num_cpu_blocks = None

    def _verify_args(self) -> None:
        if self.gpu_memory_utilization > 1.0:
            raise ValueError(
                "GPU memory utilization must be less than 1.0. Got "
                f"{self.gpu_memory_utilization}."
            )

    def verify_with_parallel_config(
        self,
        parallel_config: "ParallelConfig",
    ) -> None:
        total_cpu_memory = get_cpu_memory()
        # FIXME(woosuk): Here, it is assumed that the GPUs in a tensor parallel
        # group are in the same node. However, the GPUs may span multiple nodes.
        num_gpus_per_node = parallel_config.tensor_parallel_size
        cpu_memory_usage = self.swap_space_bytes * num_gpus_per_node

        msg = (
            f"{cpu_memory_usage / _GB:.2f} GiB out of "
            f"the {total_cpu_memory / _GB:.2f} GiB total CPU memory is "
            "allocated for the swap space."
        )
        if cpu_memory_usage > 0.7 * total_cpu_memory:
            raise ValueError("Too large swap space. " + msg)
        elif cpu_memory_usage > 0.4 * total_cpu_memory:
            logger.warning("Possibly too large swap space. " + msg)


class ParallelConfig:
    """Configuration for the distributed execution.

    Args:
        pipeline_parallel_size: Number of pipeline parallel groups.
        tensor_parallel_size: Number of tensor parallel groups.
        worker_use_ray: Whether to use Ray for model workers. Will be set to
            True if either pipeline_parallel_size or tensor_parallel_size is
            greater than 1.
    """

    def __init__(
        self,
        pipeline_parallel_size: int,
        tensor_parallel_size: int,
        worker_use_ray: bool,
        max_parallel_loading_workers: Optional[int] = None,
    ) -> None:
        self.pipeline_parallel_size = pipeline_parallel_size
        self.tensor_parallel_size = tensor_parallel_size
        self.worker_use_ray = worker_use_ray
        self.max_parallel_loading_workers = max_parallel_loading_workers

        self.world_size = pipeline_parallel_size * tensor_parallel_size
        if self.world_size > 1:
            self.worker_use_ray = True
        self._verify_args()

    def _verify_args(self) -> None:
        if self.pipeline_parallel_size > 1:
            raise NotImplementedError("Pipeline parallelism is not supported yet.")


class SchedulerConfig:
    """Scheduler configuration.

    Args:
        max_num_batched_tokens: Maximum number of tokens to be processed in
            a single iteration.
        max_num_seqs: Maximum number of sequences to be processed in a single
            iteration.
        max_model_len: Maximum length of a sequence (including prompt
            and generated text).
        max_paddings: Maximum number of paddings to be added to a batch.
    """

    def __init__(
        self,
        max_num_batched_tokens: Optional[int],
        max_num_seqs: int,
        max_model_len: int,
        max_paddings: int,
    ) -> None:
        if max_num_batched_tokens is not None:
            self.max_num_batched_tokens = max_num_batched_tokens
        else:
            # If max_model_len is too short, use 2048 as the default value for
            # higher throughput.
            self.max_num_batched_tokens = max(max_model_len, 2048)
        self.max_num_seqs = max_num_seqs
        self.max_model_len = max_model_len
        self.max_paddings = max_paddings
        self._verify_args()

    def _verify_args(self) -> None:
        if self.max_num_batched_tokens < self.max_model_len:
            raise ValueError(
                f"max_num_batched_tokens ({self.max_num_batched_tokens}) is "
                f"smaller than max_model_len ({self.max_model_len}). "
                "This effectively limits the maximum sequence length to "
                "max_num_batched_tokens and makes vLLM reject longer "
                "sequences. Please increase max_num_batched_tokens or "
                "decrease max_model_len."
            )
        if self.max_num_batched_tokens < self.max_num_seqs:
            raise ValueError(
                f"max_num_batched_tokens ({self.max_num_batched_tokens}) must "
                "be greater than or equal to max_num_seqs "
                f"({self.max_num_seqs})."
            )


_STR_DTYPE_TO_TORCH_DTYPE = {
    "half": torch.float16,
    "float16": torch.float16,
    "float": torch.float32,
    "float32": torch.float32,
    "bfloat16": torch.bfloat16,
}

_ROCM_NOT_SUPPORTED_DTYPE = ["float", "float32"]


def _get_and_verify_dtype(
    config: PretrainedConfig,
    dtype: Union[str, torch.dtype],
) -> torch.dtype:
    # NOTE: getattr(config, "torch_dtype", torch.float32) is not correct
    # because config.torch_dtype can be None.
    config_dtype = getattr(config, "torch_dtype", None)
    if config_dtype is None:
        config_dtype = torch.float32

    if isinstance(dtype, str):
        dtype = dtype.lower()
        if dtype == "auto":
            if config_dtype == torch.float32:
                # Following the common practice, we use float16 for float32
                # models.
                torch_dtype = torch.float16
            else:
                torch_dtype = config_dtype
        else:
            if dtype not in _STR_DTYPE_TO_TORCH_DTYPE:
                raise ValueError(f"Unknown dtype: {dtype}")
            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]
    elif isinstance(dtype, torch.dtype):
        torch_dtype = dtype
    else:
        raise ValueError(f"Unknown dtype: {dtype}")

    if is_hip() and torch_dtype == torch.float32:
        rocm_supported_dtypes = [
            k
            for k, v in _STR_DTYPE_TO_TORCH_DTYPE.items()
            if (k not in _ROCM_NOT_SUPPORTED_DTYPE)
        ]
        raise ValueError(
            f"dtype '{dtype}' is not supported in ROCm. "
            f"Supported dtypes are {rocm_supported_dtypes}"
        )

    # Verify the dtype.
    if torch_dtype != config_dtype:
        if torch_dtype == torch.float32:
            # Upcasting to float32 is allowed.
            pass
        elif config_dtype == torch.float32:
            # Downcasting from float32 to float16 or bfloat16 is allowed.
            pass
        else:
            # Casting between float16 and bfloat16 is allowed with a warning.
            logger.warning(f"Casting {config_dtype} to {torch_dtype}.")

    return torch_dtype


def _get_and_verify_max_len(
    hf_config: PretrainedConfig,
    max_model_len: Optional[int],
) -> int:
    """Get and verify the model's maximum length."""
    derived_max_model_len = float("inf")
    possible_keys = [
        # OPT
        "max_position_embeddings",
        # GPT-2
        "n_positions",
        # MPT
        "max_seq_len",
        # ChatGLM2
        "seq_length",
        # Others
        "max_sequence_length",
        "max_seq_length",
        "seq_len",
    ]
    for key in possible_keys:
        max_len_key = getattr(hf_config, key, None)
        if max_len_key is not None:
            derived_max_model_len = min(derived_max_model_len, max_len_key)
    if derived_max_model_len == float("inf"):
        if max_model_len is not None:
            # If max_model_len is specified, we use it.
            return max_model_len

        default_max_len = 2048
        logger.warning(
            "The model's config.json does not contain any of the following "
            "keys to determine the original maximum length of the model: "
            f"{possible_keys}. Assuming the model's maximum length is "
            f"{default_max_len}."
        )
        derived_max_model_len = default_max_len

    rope_scaling = getattr(hf_config, "rope_scaling", None)
    if rope_scaling is not None:
        assert "factor" in rope_scaling
        scaling_factor = rope_scaling["factor"]
        if rope_scaling["type"] == "yarn":
            derived_max_model_len = rope_scaling["original_max_position_embeddings"]
        derived_max_model_len *= scaling_factor

    if max_model_len is None:
        max_model_len = derived_max_model_len
    elif max_model_len > derived_max_model_len:
        raise ValueError(
            f"User-specified max_model_len ({max_model_len}) is greater than "
            f"the derived max_model_len ({max_len_key}={derived_max_model_len}"
            " in model's config.json). This may lead to incorrect model "
            "outputs or CUDA errors. Make sure the value is correct and "
            "within the model context size."
        )
    return int(max_model_len)


@dataclass
class EngineArgs:
    """Arguments for vLLM engine."""

    model: str
    tokenizer: Optional[str] = None
    tokenizer_mode: str = "auto"
    trust_remote_code: bool = False
    download_dir: Optional[str] = None
    load_format: str = "auto"
    dtype: str = "auto"
    seed: int = 0
    max_model_len: Optional[int] = None
    worker_use_ray: bool = False
    pipeline_parallel_size: int = 1
    tensor_parallel_size: int = 1
    max_parallel_loading_workers: Optional[int] = None
    block_size: int = 16
    swap_space: int = 4  # GiB
    gpu_memory_utilization: float = 0.90
    max_num_batched_tokens: Optional[int] = None
    max_num_seqs: int = 256
    max_paddings: int = 256
    disable_log_stats: bool = False
    revision: Optional[str] = None
    tokenizer_revision: Optional[str] = None
    quantization: Optional[str] = None
    enforce_eager: bool = False
    max_context_len_to_capture: int = 8192
    num_audio_tokens: int = 1024
    num_text_tokens: int = 80

    def __post_init__(self):
        if self.tokenizer is None:
            self.tokenizer = self.model

    @staticmethod
    def add_cli_args(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:
        """Shared CLI arguments for vLLM engine."""

        # NOTE: If you update any of the arguments below, please also
        # make sure to update docs/source/models/engine_args.rst

        # Model arguments
        parser.add_argument(
            "--model",
            type=str,
            default="facebook/opt-125m",
            help="name or path of the huggingface model to use",
        )
        parser.add_argument(
            "--tokenizer",
            type=str,
            default=EngineArgs.tokenizer,
            help="name or path of the huggingface tokenizer to use",
        )
        parser.add_argument(
            "--revision",
            type=str,
            default=None,
            help="the specific model version to use. It can be a branch "
            "name, a tag name, or a commit id. If unspecified, will use "
            "the default version.",
        )
        parser.add_argument(
            "--tokenizer-revision",
            type=str,
            default=None,
            help="the specific tokenizer version to use. It can be a branch "
            "name, a tag name, or a commit id. If unspecified, will use "
            "the default version.",
        )
        parser.add_argument(
            "--tokenizer-mode",
            type=str,
            default=EngineArgs.tokenizer_mode,
            choices=["auto", "slow"],
            help='tokenizer mode. "auto" will use the fast '
            'tokenizer if available, and "slow" will '
            "always use the slow tokenizer.",
        )
        parser.add_argument(
            "--trust-remote-code",
            action="store_true",
            help="trust remote code from huggingface",
        )
        parser.add_argument(
            "--download-dir",
            type=str,
            default=EngineArgs.download_dir,
            help="directory to download and load the weights, "
            "default to the default cache dir of "
            "huggingface",
        )
        parser.add_argument(
            "--load-format",
            type=str,
            default=EngineArgs.load_format,
            choices=["auto", "pt", "safetensors", "npcache", "dummy"],
            help="The format of the model weights to load. "
            '"auto" will try to load the weights in the safetensors format '
            "and fall back to the pytorch bin format if safetensors format "
            "is not available. "
            '"pt" will load the weights in the pytorch bin format. '
            '"safetensors" will load the weights in the safetensors format. '
            '"npcache" will load the weights in pytorch format and store '
            "a numpy cache to speed up the loading. "
            '"dummy" will initialize the weights with random values, '
            "which is mainly for profiling.",
        )
        parser.add_argument(
            "--dtype",
            type=str,
            default=EngineArgs.dtype,
            choices=["auto", "half", "float16", "bfloat16", "float", "float32"],
            help="data type for model weights and activations. "
            'The "auto" option will use FP16 precision '
            "for FP32 and FP16 models, and BF16 precision "
            "for BF16 models.",
        )
        parser.add_argument(
            "--max-model-len",
            type=int,
            default=None,
            help="model context length. If unspecified, "
            "will be automatically derived from the model.",
        )
        # Parallel arguments
        parser.add_argument(
            "--worker-use-ray",
            action="store_true",
            help="use Ray for distributed serving, will be "
            "automatically set when using more than 1 GPU",
        )
        parser.add_argument(
            "--pipeline-parallel-size",
            "-pp",
            type=int,
            default=EngineArgs.pipeline_parallel_size,
            help="number of pipeline stages",
        )
        parser.add_argument(
            "--tensor-parallel-size",
            "-tp",
            type=int,
            default=EngineArgs.tensor_parallel_size,
            help="number of tensor parallel replicas",
        )
        parser.add_argument(
            "--max-parallel-loading-workers",
            type=int,
            help="load model sequentially in multiple batches, "
            "to avoid RAM OOM when using tensor "
            "parallel and large models",
        )
        # KV cache arguments
        parser.add_argument(
            "--block-size",
            type=int,
            default=EngineArgs.block_size,
            choices=[8, 16, 32],
            help="token block size",
        )
        # TODO(woosuk): Support fine-grained seeds (e.g., seed per request).
        parser.add_argument(
            "--seed", type=int, default=EngineArgs.seed, help="random seed"
        )
        parser.add_argument(
            "--swap-space",
            type=int,
            default=EngineArgs.swap_space,
            help="CPU swap space size (GiB) per GPU",
        )
        parser.add_argument(
            "--gpu-memory-utilization",
            type=float,
            default=EngineArgs.gpu_memory_utilization,
            help="the fraction of GPU memory to be used for "
            "the model executor, which can range from 0 to 1."
            "If unspecified, will use the default value of 0.9.",
        )
        parser.add_argument(
            "--max-num-batched-tokens",
            type=int,
            default=EngineArgs.max_num_batched_tokens,
            help="maximum number of batched tokens per " "iteration",
        )
        parser.add_argument(
            "--max-num-seqs",
            type=int,
            default=EngineArgs.max_num_seqs,
            help="maximum number of sequences per iteration",
        )
        parser.add_argument(
            "--max-paddings",
            type=int,
            default=EngineArgs.max_paddings,
            help="maximum number of paddings in a batch",
        )
        parser.add_argument(
            "--disable-log-stats",
            action="store_true",
            help="disable logging statistics",
        )
        # Quantization settings.
        parser.add_argument(
            "--quantization",
            "-q",
            type=str,
            choices=["awq", "gptq", "squeezellm", None],
            default=None,
            help="Method used to quantize the weights. If "
            "None, we first check the `quantization_config` "
            "attribute in the model config file. If that is "
            "None, we assume the model weights are not "
            "quantized and use `dtype` to determine the data "
            "type of the weights.",
        )
        parser.add_argument(
            "--enforce-eager",
            action="store_true",
            help="Always use eager-mode PyTorch. If False, "
            "will use eager mode and CUDA graph in hybrid "
            "for maximal performance and flexibility.",
        )
        parser.add_argument(
            "--max-context-len-to-capture",
            type=int,
            default=EngineArgs.max_context_len_to_capture,
            help="maximum context length covered by CUDA "
            "graphs. When a sequence has context length "
            "larger than this, we fall back to eager mode.",
        )
        return parser

    @classmethod
    def from_cli_args(cls, args: argparse.Namespace) -> "EngineArgs":
        # Get the list of attributes of this dataclass.
        attrs = [attr.name for attr in dataclasses.fields(cls)]
        # Set the attributes from the parsed arguments.
        engine_args = cls(**{attr: getattr(args, attr) for attr in attrs})
        return engine_args

    def create_engine_configs(
        self,
    ) -> Tuple[ModelConfig, CacheConfig, ParallelConfig, SchedulerConfig]:
        model_config = ModelConfig(
            self.model,
            self.tokenizer,
            self.tokenizer_mode,
            self.trust_remote_code,
            self.download_dir,
            self.load_format,
            self.dtype,
            self.seed,
            self.revision,
            self.tokenizer_revision,
            self.max_model_len,
            self.quantization,
            self.enforce_eager,
            self.max_context_len_to_capture,
            self.num_audio_tokens,
            self.num_text_tokens,
        )
        cache_config = CacheConfig(
            self.block_size,
            self.gpu_memory_utilization,
            self.swap_space,
            model_config.get_sliding_window(),
        )
        parallel_config = ParallelConfig(
            self.pipeline_parallel_size,
            self.tensor_parallel_size,
            self.worker_use_ray,
            self.max_parallel_loading_workers,
        )
        scheduler_config = SchedulerConfig(
            self.max_num_batched_tokens,
            self.max_num_seqs,
            model_config.max_model_len,
            self.max_paddings,
        )
        return model_config, cache_config, parallel_config, scheduler_config


@dataclass
class AsyncEngineArgs(EngineArgs):
    """Arguments for asynchronous vLLM engine."""

    engine_use_ray: bool = False
    disable_log_requests: bool = False
    max_log_len: Optional[int] = None

    @staticmethod
    def add_cli_args(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:
        parser = EngineArgs.add_cli_args(parser)
        parser.add_argument(
            "--engine-use-ray",
            action="store_true",
            help="use Ray to start the LLM engine in a "
            "separate process as the server process.",
        )
        parser.add_argument(
            "--disable-log-requests",
            action="store_true",
            help="disable logging requests",
        )
        parser.add_argument(
            "--max-log-len",
            type=int,
            default=None,
            help="max number of prompt characters or prompt "
            "ID numbers being printed in log. "
            "Default: unlimited.",
        )
        return parser



================================================
FILE: ChatTTS/model/velocity/llama.py
================================================
# coding=utf-8
# Adapted from
# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py
# Copyright 2023 The vLLM team.
# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Inference-only LLaMA model compatible with HuggingFace weights."""
from typing import Any, Dict, List, Optional, Tuple

import torch
from torch import nn
from transformers import LlamaConfig

from vllm.model_executor.input_metadata import InputMetadata
from vllm.model_executor.layers.activation import SiluAndMul
from vllm.model_executor.layers.attention import PagedAttention
from vllm.model_executor.layers.layernorm import RMSNorm
from vllm.model_executor.layers.linear import (
    LinearMethodBase,
    MergedColumnParallelLinear,
    QKVParallelLinear,
    RowParallelLinear,
)
from vllm.model_executor.layers.rotary_embedding import get_rope
from vllm.model_executor.layers.sampler import Sampler
from vllm.model_executor.layers.vocab_parallel_embedding import (
    VocabParallelEmbedding,
    ParallelLMHead,
)
from vllm.model_executor.parallel_utils.parallel_state import (
    get_tensor_model_parallel_world_size,
)
from vllm.model_executor.sampling_metadata import SamplingMetadata
from vllm.model_executor.weight_utils import (
    default_weight_loader,
    hf_model_weights_iterator,
)
from vllm.sequence import SamplerOutput

KVCache = Tuple[torch.Tensor, torch.Tensor]


class LlamaMLP(nn.Module):

    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int,
        hidden_act: str,
        linear_method: Optional[LinearMethodBase] = None,
    ) -> None:
        super().__init__()
        self.gate_up_proj = MergedColumnParallelLinear(
            hidden_size,
            [intermediate_size] * 2,
            bias=False,
            linear_method=linear_method,
        )
        self.down_proj = RowParallelLinear(
            intermediate_size, hidden_size, bias=False, linear_method=linear_method
        )
        if hidden_act != "silu":
            raise ValueError(
                f"Unsupported activation: {hidden_act}. "
                "Only silu is supported for now."
            )
        self.act_fn = SiluAndMul()

    def forward(self, x):
        gate_up, _ = self.gate_up_proj(x)
        x = self.act_fn(gate_up)
        x, _ = self.down_proj(x)
        return x


class LlamaAttention(nn.Module):

    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        num_kv_heads: int,
        rope_theta: float = 10000,
        rope_scaling: Optional[Dict[str, Any]] = None,
        max_position_embeddings: int = 8192,
        linear_method: Optional[LinearMethodBase] = None,
    ) -> None:
        super().__init__()
        self.hidden_size = hidden_size
        tp_size = get_tensor_model_parallel_world_size()
        self.total_num_heads = num_heads
        assert self.total_num_heads % tp_size == 0
        self.num_heads = self.total_num_heads // tp_size
        self.total_num_kv_heads = num_kv_heads
        if self.total_num_kv_heads >= tp_size:
            # Number of KV heads is greater than TP size, so we partition
            # the KV heads across multiple tensor parallel GPUs.
            assert self.total_num_kv_heads % tp_size == 0
        else:
            # Number of KV heads is less than TP size, so we replicate
            # the KV heads across multiple tensor parallel GPUs.
            assert tp_size % self.total_num_kv_heads == 0
        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
        self.head_dim = hidden_size // self.total_num_heads
        self.q_size = self.num_heads * self.head_dim
        self.kv_size = self.num_kv_heads * self.head_dim
        self.scaling = self.head_dim**-0.5
        self.rope_theta = rope_theta
        self.max_position_embeddings = max_position_embeddings

        self.qkv_proj = QKVParallelLinear(
            hidden_size,
            self.head_dim,
            self.total_num_heads,
            self.total_num_kv_heads,
            bias=False,
            linear_method=linear_method,
        )
        self.o_proj = RowParallelLinear(
            self.total_num_heads * self.head_dim,
            hidden_size,
            bias=False,
            linear_method=linear_method,
        )

        self.rotary_emb = get_rope(
            self.head_dim,
            rotary_dim=self.head_dim,
            max_position=max_position_embeddings,
            base=rope_theta,
            rope_scaling=rope_scaling,
        )
        self.attn = PagedAttention(
            self.num_heads, self.head_dim, self.scaling, num_kv_heads=self.num_kv_heads
        )

    def forward(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        kv_cache: KVCache,
        input_metadata: InputMetadata,
    ) -> torch.Tensor:
        qkv, _ = self.qkv_proj(hidden_states)
        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
        q, k = self.rotary_emb(positions, q, k)
        k_cache, v_cache = kv_cache
        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)
        output, _ = self.o_proj(attn_output)
        return output


class LlamaDecoderLayer(nn.Module):

    def __init__(
        self,
        config: LlamaConfig,
        linear_method: Optional[LinearMethodBase] = None,
    ) -> None:
        super().__init__()
        self.hidden_size = config.hidden_size
        rope_theta = getattr(config, "rope_theta", 10000)
        rope_scaling = getattr(config, "rope_scaling", None)
        max_position_embeddings = getattr(config, "max_position_embeddings", 8192)
        self.self_attn = LlamaAttention(
            hidden_size=self.hidden_size,
            num_heads=config.num_attention_heads,
            num_kv_heads=config.num_key_value_heads,
            rope_theta=rope_theta,
            rope_scaling=rope_scaling,
            max_position_embeddings=max_position_embeddings,
            linear_method=linear_method,
        )
        self.mlp = LlamaMLP(
            hidden_size=self.hidden_size,
            intermediate_size=config.intermediate_size,
            hidden_act=config.hidden_act,
            linear_method=linear_method,
        )
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps
        )

    def forward(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        kv_cache: KVCache,
        input_metadata: InputMetadata,
        residual: Optional[torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        # Self Attention
        if residual is None:
            residual = hidden_states
            hidden_states = self.input_layernorm(hidden_states)
        else:
            hidden_states, residual = self.input_layernorm(hidden_states, residual)
        hidden_states = self.self_attn(
            positions=positions,
            hidden_states=hidden_states,
            kv_cache=kv_cache,
            input_metadata=input_metadata,
        )

        # Fully Connected
        hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)
        hidden_states = self.mlp(hidden_states)
        return hidden_states, residual


class LlamaModel(nn.Module):

    def __init__(
        self,
        config: LlamaConfig,
        linear_method: Optional[LinearMethodBase] = None,
    ) -> None:
        super().__init__()
        self.config = config
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size
        self.embed_tokens = VocabParallelEmbedding(
            config.vocab_size,
            config.hidden_size,
        )
        self.layers = nn.ModuleList(
            [
                LlamaDecoderLayer(config, linear_method)
                for _ in range(config.num_hidden_layers)
            ]
        )
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        input_emb: torch.Tensor,
        positions: torch.Tensor,
        kv_caches: List[KVCache],
        input_metadata: InputMetadata,
    ) -> torch.Tensor:
        hidden_states = input_emb
        residual = None
        for i in range(len(self.layers)):
            layer = self.layers[i]
            hidden_states, residual = layer(
                positions,
                hidden_states,
                kv_caches[i],
                input_metadata,
                residual,
            )
        hidden_states, _ = self.norm(hidden_states, residual)
        return hidden_states

    def load_weights(
        self,
        model_name_or_path: str,
        cache_dir: Optional[str] = None,
        load_format: str = "auto",
        revision: Optional[str] = None,
    ):
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            ("qkv_proj", "q_proj", "q"),
            ("qkv_proj", "k_proj", "k"),
            ("qkv_proj", "v_proj", "v"),
            ("gate_up_proj", "gate_proj", 0),
            ("gate_up_proj", "up_proj", 1),
        ]
        params_dict = dict(self.named_parameters())
        for name, loaded_weight in hf_model_weights_iterator(
            model_name_or_path, cache_dir, load_format, revision
        ):
            if "rotary_emb.inv_freq" in name:
                continue
            if "rotary_emb.cos_cached" in name or "rotary_emb.sin_cached" in name:
                # Models trained using ColossalAI may include these tensors in
                # the checkpoint. Skip them.
                continue
            for param_name, weight_name, shard_id in stacked_params_mapping:
                if weight_name not in name:
                    continue
                name = name.replace(weight_name, param_name)
                # Skip loading extra bias for GPTQ models.
                if name.endswith(".bias") and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                # Skip loading extra bias for GPTQ models.
                if name.endswith(".bias") and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = getattr(param, "weight_loader", default_weight_loader)
                weight_loader(param, loaded_weight)


class LlamaForCausalLM(nn.Module):

    def __init__(
        self,
        config: LlamaConfig,
        linear_method: Optional[LinearMethodBase] = None,
    ) -> None:
        super().__init__()
        self.config = config
        self.linear_method = linear_method
        self.model = LlamaModel(config, linear_method)
        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)
        self.sampler = Sampler(config.vocab_size)

    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        kv_caches: List[KVCache],
        input_metadata: InputMetadata,
    ) -> torch.Tensor:
        hidden_states = self.model(input_ids, positions, kv_caches, input_metadata)
        return hidden_states

    def sample(
        self,
        hidden_states: torch.Tensor,
        sampling_metadata: SamplingMetadata,
    ) -> Optional[SamplerOutput]:
        next_tokens = self.sampler(
            self.lm_head.weight, hidden_states, sampling_metadata
        )
        return next_tokens

    def load_weights(
        self,
        model_name_or_path: str,
        cache_dir: Optional[str] = None,
        load_format: str = "auto",
        revision: Optional[str] = None,
    ):
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            ("qkv_proj", "q_proj", "q"),
            ("qkv_proj", "k_proj", "k"),
            ("qkv_proj", "v_proj", "v"),
            ("gate_up_proj", "gate_proj", 0),
            ("gate_up_proj", "up_proj", 1),
        ]
        params_dict = dict(self.named_parameters())
        for name, loaded_weight in hf_model_weights_iterator(
            model_name_or_path, cache_dir, load_format, revision
        ):
            if "rotary_emb.inv_freq" in name:
                continue
            if "rotary_emb.cos_cached" in name or "rotary_emb.sin_cached" in name:
                # Models trained using ColossalAI may include these tensors in
                # the checkpoint. Skip them.
                continue
            for param_name, weight_name, shard_id in stacked_params_mapping:
                if weight_name not in name:
                    continue
                name = name.replace(weight_name, param_name)
                # Skip loading extra bias for GPTQ models.
                if name.endswith(".bias") and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                # Skip loading extra bias for GPTQ models.
                if name.endswith(".bias") and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = getattr(param, "weight_loader", default_weight_loader)
                weight_loader(param, loaded_weight)



================================================
FILE: ChatTTS/model/velocity/llm.py
================================================
from typing import List, Optional, Union

from tqdm import tqdm
from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast
from vllm.utils import Counter

from .configs import EngineArgs
from .llm_engine import LLMEngine
from .output import RequestOutput
from .sampling_params import SamplingParams


class LLM:
    """An LLM for generating texts from given prompts and sampling parameters.

    This class includes a tokenizer, a language model (possibly distributed
    across multiple GPUs), and GPU memory space allocated for intermediate
    states (aka KV cache). Given a batch of prompts and sampling parameters,
    this class generates texts from the model, using an intelligent batching
    mechanism and efficient memory management.

    NOTE: This class is intended to be used for offline inference. For online
    serving, use the `AsyncLLMEngine` class instead.
    NOTE: For the comprehensive list of arguments, see `EngineArgs`.

    Args:
        model: The name or path of a HuggingFace Transformers model.
        tokenizer: The name or path of a HuggingFace Transformers tokenizer.
        tokenizer_mode: The tokenizer mode. "auto" will use the fast tokenizer
            if available, and "slow" will always use the slow tokenizer.
        trust_remote_code: Trust remote code (e.g., from HuggingFace) when
            downloading the model and tokenizer.
        tensor_parallel_size: The number of GPUs to use for distributed
            execution with tensor parallelism.
        dtype: The data type for the model weights and activations. Currently,
            we support `float32`, `float16`, and `bfloat16`. If `auto`, we use
            the `torch_dtype` attribute specified in the model config file.
            However, if the `torch_dtype` in the config is `float32`, we will
            use `float16` instead.
        quantization: The method used to quantize the model weights. Currently,
            we support "awq", "gptq" and "squeezellm". If None, we first check
            the `quantization_config` attribute in the model config file. If
            that is None, we assume the model weights are not quantized and use
            `dtype` to determine the data type of the weights.
        revision: The specific model version to use. It can be a branch name,
            a tag name, or a commit id.
        tokenizer_revision: The specific tokenizer version to use. It can be a
            branch name, a tag name, or a commit id.
        seed: The seed to initialize the random number generator for sampling.
        gpu_memory_utilization: The ratio (between 0 and 1) of GPU memory to
            reserve for the model weights, activations, and KV cache. Higher
            values will increase the KV cache size and thus improve the model's
            throughput. However, if the value is too high, it may cause out-of-
            memory (OOM) errors.
        swap_space: The size (GiB) of CPU memory per GPU to use as swap space.
            This can be used for temporarily storing the states of the requests
            when their `best_of` sampling parameters are larger than 1. If all
            requests will have `best_of=1`, you can safely set this to 0.
            Otherwise, too small values may cause out-of-memory (OOM) errors.
        enforce_eager: Whether to enforce eager execution. If True, we will
            disable CUDA graph and always execute the model in eager mode.
            If False, we will use CUDA graph and eager execution in hybrid.
        max_context_len_to_capture: Maximum context len covered by CUDA graphs.
            When a sequence has context length larger than this, we fall back
            to eager mode.
    """

    def __init__(
        self,
        model: str,
        tokenizer: Optional[str] = None,
        tokenizer_mode: str = "auto",
        trust_remote_code: bool = False,
        tensor_parallel_size: int = 1,
        dtype: str = "auto",
        quantization: Optional[str] = None,
        revision: Optional[str] = None,
        tokenizer_revision: Optional[str] = None,
        seed: int = 0,
        gpu_memory_utilization: float = 0.9,
        swap_space: int = 4,
        enforce_eager: bool = False,
        max_context_len_to_capture: int = 8192,
        post_model_path: str = None,
        num_audio_tokens: int = 0,
        num_text_tokens: int = 0,
        **kwargs,
    ) -> None:
        if "disable_log_stats" not in kwargs:
            kwargs["disable_log_stats"] = True
        engine_args = EngineArgs(
            model=model,
            tokenizer=tokenizer,
            tokenizer_mode=tokenizer_mode,
            trust_remote_code=trust_remote_code,
            tensor_parallel_size=tensor_parallel_size,
            dtype=dtype,
            quantization=quantization,
            revision=revision,
            tokenizer_revision=tokenizer_revision,
            seed=seed,
            gpu_memory_utilization=gpu_memory_utilization,
            swap_space=swap_space,
            enforce_eager=enforce_eager,
            max_context_len_to_capture=max_context_len_to_capture,
            num_audio_tokens=num_audio_tokens,
            num_text_tokens=num_text_tokens,
            **kwargs,
        )
        self.llm_engine = LLMEngine.from_engine_args(engine_args, post_model_path)
        self.request_counter = Counter()

    def get_tokenizer(self) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]:
        return self.llm_engine.tokenizer

    def set_tokenizer(
        self,
        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
    ) -> None:
        self.llm_engine.tokenizer = tokenizer

    def generate(
        self,
        prompts: Optional[Union[str, List[str]]] = None,
        sampling_params: Optional[SamplingParams] = None,
        prompt_token_ids: Optional[List[List[int]]] = None,
        use_tqdm: bool = True,
    ) -> List[RequestOutput]:
        """Generates the completions for the input prompts.

        NOTE: This class automatically batches the given prompts, considering
        the memory constraint. For the best performance, put all of your prompts
        into a single list and pass it to this method.

        Args:
            prompts: A list of prompts to generate completions for.
            sampling_params: The sampling parameters for text generation. If
                None, we use the default sampling parameters.
            prompt_token_ids: A list of token IDs for the prompts. If None, we
                use the tokenizer to convert the prompts to token IDs.
            use_tqdm: Whether to use tqdm to display the progress bar.

        Returns:
            A list of `RequestOutput` objects containing the generated
            completions in the same order as the input prompts.
        """
        if prompts is None and prompt_token_ids is None:
            raise ValueError("Either prompts or prompt_token_ids must be " "provided.")
        if isinstance(prompts, str):
            # Convert a single prompt to a list.
            prompts = [prompts]
        if (
            prompts is not None
            and prompt_token_ids is not None
            and len(prompts) != len(prompt_token_ids)
        ):
            raise ValueError(
                "The lengths of prompts and prompt_token_ids " "must be the same."
            )
        if sampling_params is None:
            # Use default sampling params.
            sampling_params = SamplingParams()

        # Add requests to the engine.
        num_requests = len(prompts) if prompts is not None else len(prompt_token_ids)
        for i in range(num_requests):
            prompt = prompts[i] if prompts is not None else None
            token_ids = None if prompt_token_ids is None else prompt_token_ids[i]
            self._add_request(prompt, sampling_params, token_ids)

        rtns = self._run_engine(use_tqdm)
        for i, rtn in enumerate(rtns):
            token_ids = rtn.outputs[0].token_ids
            for j, token_id in enumerate(token_ids):
                if len(token_id) == 1:
                    token_ids[j] = token_id[0]
                else:
                    token_ids[j] = list(token_id)

        return rtns

    def _add_request(
        self,
        prompt: Optional[str],
        sampling_params: SamplingParams,
        prompt_token_ids: Optional[List[int]],
    ) -> None:
        request_id = str(next(self.request_counter))
        self.llm_engine.add_request(
            request_id, prompt, sampling_params, prompt_token_ids
        )

    def _run_engine(self, use_tqdm: bool) -> List[RequestOutput]:
        # Initialize tqdm.
        if use_tqdm:
            num_requests = self.llm_engine.get_num_unfinished_requests()
            pbar = tqdm(total=num_requests, desc="Processed prompts")
        # Run the engine.
        outputs: List[RequestOutput] = []
        while self.llm_engine.has_unfinished_requests():
            step_outputs = self.llm_engine.step()
            for output in step_outputs:
                if output.finished:
                    outputs.append(output)
                    if use_tqdm:
                        pbar.update(1)
        if use_tqdm:
            pbar.close()
        # Sort the outputs by request ID.
        # This is necessary because some requests may be finished earlier than
        # its previous requests.
        outputs = sorted(outputs, key=lambda x: int(x.request_id))
        return outputs



================================================
FILE: ChatTTS/model/velocity/llm_engine.py
================================================
import copy
from collections import defaultdict
import os
import time
from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Tuple, Union

from vllm.config import CacheConfig, ModelConfig, ParallelConfig, SchedulerConfig
from .scheduler import Scheduler, SchedulerOutputs
from .configs import EngineArgs
from vllm.engine.metrics import record_metrics
from vllm.engine.ray_utils import RayWorkerVllm, initialize_cluster, ray
from vllm.logger import init_logger
from .output import RequestOutput
from .sampling_params import SamplingParams
from .sequence import (
    SamplerOutput,
    Sequence,
    SequenceGroup,
    SequenceGroupOutput,
    SequenceOutput,
    SequenceStatus,
)
from vllm.transformers_utils.tokenizer import detokenize_incrementally, get_tokenizer
from vllm.utils import Counter, set_cuda_visible_devices, get_ip, get_open_port
import numpy as np

if ray:
    from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy

if TYPE_CHECKING:
    from ray.util.placement_group import PlacementGroup

logger = init_logger(__name__)

_LOGGING_INTERVAL_SEC = 5


class LLMEngine:
    """An LLM engine that receives requests and generates texts.

    This is the main class for the vLLM engine. It receives requests
    from clients and generates texts from the LLM. It includes a tokenizer, a
    language model (possibly distributed across multiple GPUs), and GPU memory
    space allocated for intermediate states (aka KV cache). This class utilizes
    iteration-level scheduling and efficient memory management to maximize the
    serving throughput.

    The `LLM` class wraps this class for offline batched inference and the
    `AsyncLLMEngine` class wraps this class for online serving.

    NOTE: The config arguments are derived from the `EngineArgs` class. For the
    comprehensive list of arguments, see `EngineArgs`.

    Args:
        model_config: The configuration related to the LLM model.
        cache_config: The configuration related to the KV cache memory
            management.
        parallel_config: The configuration related to distributed execution.
        scheduler_config: The configuration related to the request scheduler.
        placement_group: Ray placement group for distributed execution.
            Required for distributed execution.
        log_stats: Whether to log statistics.
    """

    def __init__(
        self,
        model_config: ModelConfig,
        cache_config: CacheConfig,
        parallel_config: ParallelConfig,
        scheduler_config: SchedulerConfig,
        placement_group: Optional["PlacementGroup"],
        post_model_path: str,
        log_stats: bool,
    ) -> None:
        logger.info(
            "Initializing an LLM engine with config: "
            f"model={model_config.model!r}, "
            f"tokenizer={model_config.tokenizer!r}, "
            f"tokenizer_mode={model_config.tokenizer_mode}, "
            f"revision={model_config.revision}, "
            f"tokenizer_revision={model_config.tokenizer_revision}, "
            f"trust_remote_code={model_config.trust_remote_code}, "
            f"dtype={model_config.dtype}, "
            f"max_seq_len={model_config.max_model_len}, "
            f"download_dir={model_config.download_dir!r}, "
            f"load_format={model_config.load_format}, "
            f"tensor_parallel_size={parallel_config.tensor_parallel_size}, "
            f"quantization={model_config.quantization}, "
            f"enforce_eager={model_config.enforce_eager}, "
            f"seed={model_config.seed}), "
            f"post_model_path={post_model_path!r}"
        )
        # TODO(woosuk): Print more configs in debug mode.

        self.model_config = model_config
        self.cache_config = cache_config
        self.parallel_config = parallel_config
        self.scheduler_config = scheduler_config
        self.log_stats = log_stats
        self._verify_args()
        self.post_model_path = post_model_path
        self.seq_counter = Counter()

        # Create the parallel GPU workers.
        if self.parallel_config.worker_use_ray:
            # Disable Ray usage stats collection.
            ray_usage = os.environ.get("RAY_USAGE_STATS_ENABLED", "0")
            if ray_usage != "1":
                os.environ["RAY_USAGE_STATS_ENABLED"] = "0"
            self._init_workers_ray(placement_group)
        else:
            self._init_workers()

        # Profile the memory usage and initialize the cache.
        self._init_cache()

        # Create the scheduler.
        self.scheduler = Scheduler(scheduler_config, cache_config)

        # Logging.
        self.last_logging_time = 0.0
        # List of (timestamp, num_tokens)
        self.num_prompt_tokens: List[Tuple[float, int]] = []
        # List of (timestamp, num_tokens)
        self.num_generation_tokens: List[Tuple[float, int]] = []

    def _init_workers(self):
        # Lazy import the Worker to avoid importing torch.cuda/xformers
        # before CUDA_VISIBLE_DEVICES is set in the Worker
        from .worker import Worker

        assert (
            self.parallel_config.world_size == 1
        ), "Ray is required if parallel_config.world_size > 1."

        self.workers: List[Worker] = []
        distributed_init_method = f"tcp://{get_ip()}:{get_open_port()}"
        self.driver_worker = Worker(
            self.model_config,
            self.parallel_config,
            self.scheduler_config,
            local_rank=0,
            rank=0,
            distributed_init_method=distributed_init_method,
            is_driver_worker=True,
            post_model_path=self.post_model_path,
        )
        self._run_workers("init_model")
        self._run_workers("load_model")

    def _init_workers_ray(self, placement_group: "PlacementGroup", **ray_remote_kwargs):
        if self.parallel_config.tensor_parallel_size == 1:
            num_gpus = self.cache_config.gpu_memory_utilization
        else:
            num_gpus = 1

        self.driver_dummy_worker: RayWorkerVllm = None
        self.workers: List[RayWorkerVllm] = []

        driver_ip = get_ip()
        for bundle_id, bundle in enumerate(placement_group.bundle_specs):
            if not bundle.get("GPU", 0):
                continue
            scheduling_strategy = PlacementGroupSchedulingStrategy(
                placement_group=placement_group,
                placement_group_capture_child_tasks=True,
                placement_group_bundle_index=bundle_id,
            )
            worker = ray.remote(
                num_cpus=0,
                num_gpus=num_gpus,
                scheduling_strategy=scheduling_strategy,
                **ray_remote_kwargs,
            )(RayWorkerVllm).remote(self.model_config.trust_remote_code)

            worker_ip = ray.get(worker.get_node_ip.remote())
            if worker_ip == driver_ip and self.driver_dummy_worker is None:
                # If the worker is on the same node as the driver, we use it
                # as the resource holder for the driver process.
                self.driver_dummy_worker = worker
            else:
                self.workers.append(worker)

        if self.driver_dummy_worker is None:
            raise ValueError(
                "Ray does not allocate any GPUs on the driver node. Consider "
                "adjusting the Ray placement group or running the driver on a "
                "GPU node."
            )

        driver_node_id, driver_gpu_ids = ray.get(
            self.driver_dummy_worker.get_node_and_gpu_ids.remote()
        )
        worker_node_and_gpu_ids = ray.get(
            [worker.get_node_and_gpu_ids.remote() for worker in self.workers]
        )

        node_workers = defaultdict(list)
        node_gpus = defaultdict(list)

        node_workers[driver_node_id].append(0)
        node_gpus[driver_node_id].extend(driver_gpu_ids)
        for i, (node_id, gpu_ids) in enumerate(worker_node_and_gpu_ids, start=1):
            node_workers[node_id].append(i)
            node_gpus[node_id].extend(gpu_ids)
        for node_id, gpu_ids in node_gpus.items():
            node_gpus[node_id] = sorted(gpu_ids)

        # Set CUDA_VISIBLE_DEVICES for the driver.
        set_cuda_visible_devices(node_gpus[driver_node_id])
        for worker, (node_id, _) in zip(self.workers, worker_node_and_gpu_ids):
            worker.set_cuda_visible_devices.remote(node_gpus[node_id])

        distributed_init_method = f"tcp://{driver_ip}:{get_open_port()}"

        # Lazy import the Worker to avoid importing torch.cuda/xformers
        # before CUDA_VISIBLE_DEVICES is set in the Worker
        from vllm.worker.worker import Worker

        # Initialize torch distributed process group for the workers.
        model_config = copy.deepcopy(self.model_config)
        parallel_config = copy.deepcopy(self.parallel_config)
        scheduler_config = copy.deepcopy(self.scheduler_config)

        for rank, (worker, (node_id, _)) in enumerate(
            zip(self.workers, worker_node_and_gpu_ids), start=1
        ):
            local_rank = node_workers[node_id].index(rank)
            worker.init_worker.remote(
                lambda rank=rank, local_rank=local_rank: Worker(
                    model_config,
                    parallel_config,
                    scheduler_config,
                    local_rank,
                    rank,
                    distributed_init_method,
                )
            )

        driver_rank = 0
        driver_local_rank = node_workers[driver_node_id].index(driver_rank)
        self.driver_worker = Worker(
            model_config,
            parallel_config,
            scheduler_config,
            driver_local_rank,
            driver_rank,
            distributed_init_method,
            is_driver_worker=True,
        )

        self._run_workers("init_model")
        self._run_workers(
            "load_model",
            max_concurrent_workers=self.parallel_config.max_parallel_loading_workers,
        )

    def _verify_args(self) -> None:
        self.model_config.verify_with_parallel_config(self.parallel_config)
        self.cache_config.verify_with_parallel_config(self.parallel_config)

    def _init_cache(self) -> None:
        """Profiles the memory usage and initializes the KV cache."""
        # Get the maximum number of blocks that can be allocated on GPU and CPU.
        num_blocks = self._run_workers(
            "profile_num_available_blocks",
            block_size=self.cache_config.block_size,
            gpu_memory_utilization=self.cache_config.gpu_memory_utilization,
            cpu_swap_space=self.cache_config.swap_space_bytes,
        )

        # Since we use a shared centralized controller, we take the minimum
        # number of blocks across all workers to make sure all the memory
        # operators can be applied to all workers.
        num_gpu_blocks = min(b[0] for b in num_blocks)
        num_cpu_blocks = min(b[1] for b in num_blocks)
        # FIXME(woosuk): Change to debug log.
        logger.info(
            f"# GPU blocks: {num_gpu_blocks}, " f"# CPU blocks: {num_cpu_blocks}"
        )

        if num_gpu_blocks <= 0:
            raise ValueError(
                "No available memory for the cache blocks. "
                "Try increasing `gpu_memory_utilization` when "
                "initializing the engine."
            )
        max_seq_len = self.cache_config.block_size * num_gpu_blocks
        if self.model_config.max_model_len > max_seq_len:
            raise ValueError(
                f"The model's max seq len ({self.model_config.max_model_len}) "
                "is larger than the maximum number of tokens that can be "
                f"stored in KV cache ({max_seq_len}). Try increasing "
                "`gpu_memory_utilization` or decreasing `max_model_len` when "
                "initializing the engine."
            )

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        # Initialize the cache.
        self._run_workers("init_cache_engine", cache_config=self.cache_config)
        # Warm up the model. This includes capturing the model into CUDA graph
        # if enforce_eager is False.
        self._run_workers("warm_up_model")

    @classmethod
    def from_engine_args(
        cls, engine_args: EngineArgs, post_model_path=None
    ) -> "LLMEngine":
        """Creates an LLM engine from the engine arguments."""
        # Create the engine configs.
        engine_configs = engine_args.create_engine_configs()
        parallel_config = engine_configs[2]
        # Initialize the cluster.
        placement_group = initialize_cluster(parallel_config)
        # Create the LLM engine.
        engine = cls(
            *engine_configs,
            placement_group,
            log_stats=not engine_args.disable_log_stats,
            post_model_path=post_model_path,
        )
        return engine

    def add_request(
        self,
        request_id: str,
        prompt: Optional[str],
        sampling_params: SamplingParams,
        prompt_token_ids: Optional[List[int]] = None,
        arrival_time: Optional[float] = None,
    ) -> None:
        """Add a request to the engine's request pool.

        The request is added to the request pool and will be processed by the
        scheduler as `engine.step()` is called. The exact scheduling policy is
        determined by the scheduler.

        Args:
            request_id: The unique ID of the request.
            prompt: The prompt string. Can be None if prompt_token_ids is
                provided.
            sampling_params: The sampling parameters for text generation.
            prompt_token_ids: The token IDs of the prompt. If None, we
                use the tokenizer to convert the prompts to token IDs.
            arrival_time: The arrival time of the request. If None, we use
                the current monotonic time.
        """
        if arrival_time is None:
            arrival_time = time.monotonic()

        assert prompt_token_ids is not None, "prompt_token_ids must be provided"
        # Create the sequences.
        block_size = self.cache_config.block_size
        seq_id = next(self.seq_counter)
        seq = Sequence(seq_id, prompt, prompt_token_ids, block_size)

        # Create the sequence group.
        seq_group = SequenceGroup(request_id, [seq], sampling_params, arrival_time)

        # Add the sequence group to the scheduler.
        self.scheduler.add_seq_group(seq_group)

    def abort_request(self, request_id: Union[str, Iterable[str]]) -> None:
        """Aborts a request(s) with the given ID.

        Args:
            request_id: The ID(s) of the request to abort.
        """
        self.scheduler.abort_seq_group(request_id)

    def get_model_config(self) -> ModelConfig:
        """Gets the model configuration."""
        return self.model_config

    def get_num_unfinished_requests(self) -> int:
        """Gets the number of unfinished requests."""
        return self.scheduler.get_num_unfinished_seq_groups()

    def has_unfinished_requests(self) -> bool:
        """Returns True if there are unfinished requests."""
        return self.scheduler.has_unfinished_seqs()

    def _check_beam_search_early_stopping(
        self,
        early_stopping: Union[bool, str],
        sampling_params: SamplingParams,
        best_running_seq: Sequence,
        current_worst_seq: Sequence,
    ) -> bool:
        assert sampling_params.use_beam_search
        length_penalty = sampling_params.length_penalty
        if early_stopping is True:
            return True

        current_worst_score = current_worst_seq.get_beam_search_score(
            length_penalty=length_penalty, eos_token_id=self.tokenizer.eos_token_id
        )
        if early_stopping is False:
            highest_attainable_score = best_running_seq.get_beam_search_score(
                length_penalty=length_penalty, eos_token_id=self.tokenizer.eos_token_id
            )
        else:
            assert early_stopping == "never"
            if length_penalty > 0.0:
                # If length_penalty > 0.0, beam search will prefer longer
                # sequences. The highest attainable score calculation is
                # based on the longest possible sequence length in this case.
                max_possible_length = max(
                    best_running_seq.get_prompt_len() + sampling_params.max_tokens,
                    self.scheduler_config.max_model_len,
                )
                highest_attainable_score = best_running_seq.get_beam_search_score(
                    length_penalty=length_penalty,
                    eos_token_id=self.tokenizer.eos_token_id,
                    seq_len=max_possible_length,
                )
            else:
                # Otherwise, beam search will prefer shorter sequences. The
                # highest attainable score calculation is based on the current
                # sequence length.
                highest_attainable_score = best_running_seq.get_beam_search_score(
                    length_penalty=length_penalty,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
        return current_worst_score >= highest_attainable_score

    def _process_sequence_group_outputs(
        self, seq_group: SequenceGroup, outputs: SequenceGroupOutput
    ) -> None:
        # Process prompt logprobs
        prompt_logprobs = outputs.prompt_logprobs
        if prompt_logprobs is not None:
            seq_group.prompt_logprobs = prompt_logprobs

        # Process samples
        samples = outputs.samples
        parent_seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)
        existing_finished_seqs = seq_group.get_finished_seqs()
        parent_child_dict = {parent_seq.seq_id: [] for parent_seq in parent_seqs}
        for sample in samples:
            parent_child_dict[sample.parent_seq_id].append(sample)
        # List of (child, parent)
        child_seqs: List[Tuple[Sequence, Sequence]] = []

        # Process the child samples for each parent sequence
        for parent in parent_seqs:
            child_samples: List[SequenceOutput] = parent_child_dict[parent.seq_id]
            if len(child_samples) == 0:
                # This parent sequence has no children samples. Remove
                # the parent sequence from the sequence group since it will
                # not be used in the future iterations.
                parent.status = SequenceStatus.FINISHED_ABORTED
                seq_group.remove(parent.seq_id)
                self.scheduler.free_seq(parent)
                continue
            # Fork the parent sequence if there are multiple child samples.
            for child_sample in child_samples[:-1]:
                new_child_seq_id = next(self.seq_counter)
                child = parent.fork(new_child_seq_id)
                child.append_token_id(
                    child_sample.output_token,
                    child_sample.logprobs,
                    child_sample.hidden_states,
                    child_sample.finished,
                )
                child_seqs.append((child, parent))
            # Continue the parent sequence for the last child sample.
            # We reuse the parent sequence here to reduce redundant memory
            # copies, especially when using non-beam search sampling methods.
            last_child_sample = child_samples[-1]
            parent.append_token_id(
                last_child_sample.output_token,
                last_child_sample.logprobs,
                last_child_sample.hidden_states,
                last_child_sample.finished,
            )
            child_seqs.append((parent, parent))

        for seq, _ in child_seqs:
            # self._decode_sequence(seq, seq_group.sampling_params)
            self._check_stop(seq, seq_group.sampling_params)

        # Non-beam search case
        if not seq_group.sampling_params.use_beam_search:
            # For newly created child sequences, add them to the sequence group
            # and fork them in block manager if they are not finished.
            for seq, parent in child_seqs:
                if seq is not parent:
                    seq_group.add(seq)
                    if not seq.is_finished():
                        self.scheduler.fork_seq(parent, seq)

            # Free the finished and selected parent sequences' memory in block
            # manager. Keep them in the sequence group as candidate output.
            # NOTE: we need to fork the new sequences before freeing the
            # old sequences.
            for seq, parent in child_seqs:
                if seq is parent and seq.is_finished():
                    self.scheduler.free_seq(seq)
            return

        # Beam search case
        # Select the child sequences to keep in the sequence group.
        selected_child_seqs = []
        unselected_child_seqs = []
        beam_width = seq_group.sampling_params.best_of
        length_penalty = seq_group.sampling_params.length_penalty

        # Select the newly finished sequences with the highest scores
        # to replace existing finished sequences.
        # Tuple of (seq, parent, is_new)
        existing_finished_seqs = [(seq, None, False) for seq in existing_finished_seqs]
        new_finished_seqs = [
            (seq, parent, True) for seq, parent in child_seqs if seq.is_finished()
        ]
        all_finished_seqs = existing_finished_seqs + new_finished_seqs
        # Sort the finished sequences by their scores.
        all_finished_seqs.sort(
            key=lambda x: x[0].get_beam_search_score(
                length_penalty=length_penalty, eos_token_id=self.tokenizer.eos_token_id
            ),
            reverse=True,
        )
        for seq, parent, is_new in all_finished_seqs[:beam_width]:
            if is_new:
                # A newly generated child sequence finishes and has a high
                # score, so we will add it into the sequence group.
                selected_child_seqs.append((seq, parent))
        for seq, parent, is_new in all_finished_seqs[beam_width:]:
            if is_new:
                # A newly generated child sequence finishes but has a low
                # score, so we will not add it into the sequence group.
                # Additionally, if this sequence is a continuation of a
                # parent sequence, we will need remove the parent sequence
                # from the sequence group.
                unselected_child_seqs.append((seq, parent))
            else:
                # An existing finished sequence has a low score, so we will
                # remove it from the sequence group.
                seq_group.remove(seq.seq_id)

        # select the top beam_width sequences from the running
        # sequences for the next iteration to continue the beam
        # search.
        running_child_seqs = [
            (seq, parent) for seq, parent in child_seqs if not seq.is_finished()
        ]
        # Sort the running sequences by their scores.
        running_child_seqs.sort(
            key=lambda x: x[0].get_beam_search_score(
                length_penalty=length_penalty, eos_token_id=self.tokenizer.eos_token_id
            ),
            reverse=True,
        )

        # Check if we can stop the beam search.
        if len(running_child_seqs) == 0:
            # No running sequences, stop the beam search.
            stop_beam_search = True
        elif len(all_finished_seqs) < beam_width:
            # Not enough finished sequences, continue the beam search.
            stop_beam_search = False
        else:
            # Check the early stopping criteria
            best_running_seq = running_child_seqs[0][0]
            current_worst_seq = all_finished_seqs[beam_width - 1][0]
            stop_beam_search = self._check_beam_search_early_stopping(
                seq_group.sampling_params.early_stopping,
                seq_group.sampling_params,
                best_running_seq,
                current_worst_seq,
            )

        if stop_beam_search:
            # Stop the beam search and remove all the running sequences from
            # the sequence group.
            unselected_child_seqs.extend(running_child_seqs)
        else:
            # Continue the beam search and select the top beam_width sequences
            # to continue the beam search.
            selected_child_seqs.extend(running_child_seqs[:beam_width])
            # The remaining running sequences will not be used in the next
            # iteration. Again, if these sequences are continuations of
            # parent sequences, we will need to remove the parent sequences
            # from the sequence group.
            unselected_child_seqs.extend(running_child_seqs[beam_width:])

        # For newly created child sequences, add them to the sequence group
        # and fork them in block manager if they are not finished.
        for seq, parent in selected_child_seqs:
            if seq is not parent:
                seq_group.add(seq)
                if not seq.is_finished():
                    self.scheduler.fork_seq(parent, seq)

        # Free the finished and selected parent sequences' memory in block
        # manager. Keep them in the sequence group as candidate output.
        for seq, parent in selected_child_seqs:
            if seq is parent and seq.is_finished():
                self.scheduler.free_seq(seq)

        # Remove the unselected parent sequences from the sequence group and
        # free their memory in block manager.
        for seq, parent in unselected_child_seqs:
            if seq is parent:
                # Remove the parent sequence if it is not selected for next
                # iteration
                seq_group.remove(seq.seq_id)
                self.scheduler.free_seq(seq)

    def _process_model_outputs(
        self, output: SamplerOutput, scheduler_outputs: SchedulerOutputs
    ) -> List[RequestOutput]:
        # Update the scheduled sequence groups with the model outputs.
        scheduled_seq_groups = scheduler_outputs.scheduled_seq_groups
        for seq_group, outputs in zip(scheduled_seq_groups, output):
            self._process_sequence_group_outputs(seq_group, outputs)

        # Free the finished sequence groups.
        self.scheduler.free_finished_seq_groups()

        # Create the outputs.
        request_outputs: List[RequestOutput] = []
        for seq_group in scheduled_seq_groups + scheduler_outputs.ignored_seq_groups:
            request_output = RequestOutput.from_seq_group(seq_group)
            request_outputs.append(request_output)

        if self.log_stats:
            # Log the system stats.
            self._log_system_stats(
                scheduler_outputs.prompt_run, scheduler_outputs.num_batched_tokens
            )
        return request_outputs

    def step(self) -> List[RequestOutput]:
        """Performs one decoding iteration and returns newly generated results.

        This function performs one decoding iteration of the engine. It first
        schedules the sequences to be executed in the next iteration and the
        token blocks to be swapped in/out/copy. Then, it executes the model
        and updates the scheduler with the model outputs. Finally, it decodes
        the sequences and returns the newly generated results.
        """
        seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()

        if not scheduler_outputs.is_empty():
            # Execute the model.
            all_outputs = self._run_workers(
                "execute_model",
                driver_kwargs={
                    "seq_group_metadata_list": seq_group_metadata_list,
                    "blocks_to_swap_in": scheduler_outputs.blocks_to_swap_in,
                    "blocks_to_swap_out": scheduler_outputs.blocks_to_swap_out,
                    "blocks_to_copy": scheduler_outputs.blocks_to_copy,
                },
            )

            # Only the driver worker returns the sampling results.
            output = all_outputs[0]
        else:
            output = []

        return self._process_model_outputs(output, scheduler_outputs)

    def _log_system_stats(
        self,
        prompt_run: bool,
        num_batched_tokens: int,
    ) -> None:
        now = time.monotonic()
        # Log the number of batched input tokens.
        if prompt_run:
            self.num_prompt_tokens.append((now, num_batched_tokens))
        else:
            self.num_generation_tokens.append((now, num_batched_tokens))

        should_log = now - self.last_logging_time >= _LOGGING_INTERVAL_SEC
        if not should_log:
            return

        # Discard the old stats.
        self.num_prompt_tokens = [
            (t, n) for t, n in self.num_prompt_tokens if now - t < _LOGGING_INTERVAL_SEC
        ]
        self.num_generation_tokens = [
            (t, n)
            for t, n in self.num_generation_tokens
            if now - t < _LOGGING_INTERVAL_SEC
        ]

        if len(self.num_prompt_tokens) > 1:
            total_num_tokens = sum(n for _, n in self.num_prompt_tokens[:-1])
            window = now - self.num_prompt_tokens[0][0]
            avg_prompt_throughput = total_num_tokens / window
        else:
            avg_prompt_throughput = 0.0
        if len(self.num_generation_tokens) > 1:
            total_num_tokens = sum(n for _, n in self.num_generation_tokens[:-1])
            window = now - self.num_generation_tokens[0][0]
            avg_generation_throughput = total_num_tokens / window
        else:
            avg_generation_throughput = 0.0

        total_num_gpu_blocks = self.cache_config.num_gpu_blocks
        num_free_gpu_blocks = self.scheduler.block_manager.get_num_free_gpu_blocks()
        num_used_gpu_blocks = total_num_gpu_blocks - num_free_gpu_blocks
        gpu_cache_usage = num_used_gpu_blocks / total_num_gpu_blocks

        total_num_cpu_blocks = self.cache_config.num_cpu_blocks
        if total_num_cpu_blocks > 0:
            num_free_cpu_blocks = self.scheduler.block_manager.get_num_free_cpu_blocks()
            num_used_cpu_blocks = total_num_cpu_blocks - num_free_cpu_blocks
            cpu_cache_usage = num_used_cpu_blocks / total_num_cpu_blocks
        else:
            cpu_cache_usage = 0.0

        record_metrics(
            avg_prompt_throughput=avg_prompt_throughput,
            avg_generation_throughput=avg_generation_throughput,
            scheduler_running=len(self.scheduler.running),
            scheduler_swapped=len(self.scheduler.swapped),
            scheduler_waiting=len(self.scheduler.waiting),
            gpu_cache_usage=gpu_cache_usage,
            cpu_cache_usage=cpu_cache_usage,
        )

        logger.info(
            "Avg prompt throughput: "
            f"{avg_prompt_throughput:.1f} tokens/s, "
            "Avg generation throughput: "
            f"{avg_generation_throughput:.1f} tokens/s, "
            f"Running: {len(self.scheduler.running)} reqs, "
            f"Swapped: {len(self.scheduler.swapped)} reqs, "
            f"Pending: {len(self.scheduler.waiting)} reqs, "
            f"GPU KV cache usage: {gpu_cache_usage * 100:.1f}%, "
            f"CPU KV cache usage: {cpu_cache_usage * 100:.1f}%"
        )
        self.last_logging_time = now

    def _decode_sequence(self, seq: Sequence, prms: SamplingParams) -> None:
        """Decodes the new token for a sequence."""
        (new_tokens, new_output_text, prefix_offset, read_offset) = (
            detokenize_incrementally(
                self.tokenizer,
                all_input_ids=seq.get_token_ids(),
                prev_tokens=seq.tokens,
                prefix_offset=seq.prefix_offset,
                read_offset=seq.read_offset,
                skip_special_tokens=prms.skip_special_tokens,
                spaces_between_special_tokens=prms.spaces_between_special_tokens,
            )
        )
        if seq.tokens is None:
            seq.tokens = new_tokens
        else:
            seq.tokens.extend(new_tokens)
        seq.prefix_offset = prefix_offset
        seq.read_offset = read_offset
        seq.output_text += new_output_text

    def _check_stop(self, seq: Sequence, sampling_params: SamplingParams) -> None:
        """Stop the finished sequences."""
        for stop_str in sampling_params.stop:
            if seq.output_text.endswith(stop_str):
                if not sampling_params.include_stop_str_in_output:
                    # Truncate the output text so that the stop string is
                    # not included in the output.
                    seq.output_text = seq.output_text[: -len(stop_str)]
                seq.status = SequenceStatus.FINISHED_STOPPED
                return
        if seq.data.finished:
            seq.status = SequenceStatus.FINISHED_STOPPED
            return

        for token_id in seq.get_last_token_id():
            if token_id == sampling_params.eos_token:
                seq.status = SequenceStatus.FINISHED_STOPPED
                return

        # Check if the sequence has reached max_model_len.
        if seq.get_len() > self.scheduler_config.max_model_len:
            seq.status = SequenceStatus.FINISHED_LENGTH_CAPPED
            return

        # Check if the sequence has reached max_tokens.
        if seq.get_output_len() == sampling_params.max_tokens:
            seq.status = SequenceStatus.FINISHED_LENGTH_CAPPED
            return

        # Check if the sequence has generated the EOS token.
        if (not sampling_params.ignore_eos) and seq.get_last_token_id()[
            0
        ] == sampling_params.eos_token:
            seq.status = SequenceStatus.FINISHED_STOPPED
            return

    def _run_workers(
        self,
        method: str,
        *args,
        driver_args: Optional[List[Any]] = None,
        driver_kwargs: Optional[Dict[str, Any]] = None,
        max_concurrent_workers: Optional[int] = None,
        **kwargs,
    ) -> Any:
        """Runs the given method on all workers."""

        if max_concurrent_workers:
            raise NotImplementedError("max_concurrent_workers is not supported yet.")

        # Start the ray workers first.
        ray_worker_outputs = [
            worker.execute_method.remote(method, *args, **kwargs)
            for worker in self.workers
        ]

        if driver_args is None:
            driver_args = args
        if driver_kwargs is None:
            driver_kwargs = kwargs

        # Start the driver worker after all the ray workers.
        driver_worker_output = getattr(self.driver_worker, method)(
            *driver_args, **driver_kwargs
        )

        # Get the results of the ray workers.
        if self.workers:
            ray_worker_outputs = ray.get(ray_worker_outputs)

        return [driver_worker_output] + ray_worker_outputs



================================================
FILE: ChatTTS/model/velocity/model_loader.py
================================================
"""Utilities for selecting and loading models."""

import contextlib

import torch
import torch.nn as nn

from vllm.config import ModelConfig
from vllm.model_executor.models import ModelRegistry
from vllm.model_executor.weight_utils import get_quant_config, initialize_dummy_weights

from .llama import LlamaModel


@contextlib.contextmanager
def _set_default_torch_dtype(dtype: torch.dtype):
    """Sets the default torch dtype to the given dtype."""
    old_dtype = torch.get_default_dtype()
    torch.set_default_dtype(dtype)
    yield
    torch.set_default_dtype(old_dtype)


def get_model(model_config: ModelConfig) -> nn.Module:
    # Get the (maybe quantized) linear method.
    linear_method = None
    if model_config.quantization is not None:
        quant_config = get_quant_config(
            model_config.quantization,
            model_config.model,
            model_config.hf_config,
            model_config.download_dir,
        )
        capability = torch.cuda.get_device_capability()
        capability = capability[0] * 10 + capability[1]
        if capability < quant_config.get_min_capability():
            raise ValueError(
                f"The quantization method {model_config.quantization} is not "
                "supported for the current GPU. "
                f"Minimum capability: {quant_config.get_min_capability()}. "
                f"Current capability: {capability}."
            )
        supported_dtypes = quant_config.get_supported_act_dtypes()
        if model_config.dtype not in supported_dtypes:
            raise ValueError(
                f"{model_config.dtype} is not supported for quantization "
                f"method {model_config.quantization}. Supported dtypes: "
                f"{supported_dtypes}"
            )
        linear_method = quant_config.get_linear_method()

    with _set_default_torch_dtype(model_config.dtype):
        # Create a model instance.
        # The weights will be initialized as empty tensors.
        with torch.device("cuda"):
            model = LlamaModel(model_config.hf_config, linear_method)
        if model_config.load_format == "dummy":
            # NOTE(woosuk): For accurate performance evaluation, we assign
            # random values to the weights.
            initialize_dummy_weights(model)
        else:
            # Load the weights from the cached or downloaded files.
            model.load_weights(
                model_config.model,
                model_config.download_dir,
                model_config.load_format,
                model_config.revision,
            )
    return model.eval()



================================================
FILE: ChatTTS/model/velocity/model_runner.py
================================================
import time
from typing import Dict, List, Optional, Tuple, Union

import numpy as np
import torch
import torch.nn as nn

from .configs import ModelConfig, ParallelConfig, SchedulerConfig
from vllm.logger import init_logger
from .model_loader import get_model
from vllm.model_executor import InputMetadata, SamplingMetadata
from vllm.model_executor.parallel_utils.communication_op import (
    broadcast,
    broadcast_object_list,
)
from .sampling_params import SamplingParams, SamplingType
from .sequence import (
    SamplerOutput,
    SequenceData,
    SequenceGroupMetadata,
    SequenceGroupOutput,
    SequenceOutput,
)
from vllm.utils import in_wsl
from ..embed import Embed
from .sampler import Sampler
from safetensors.torch import safe_open

logger = init_logger(__name__)

KVCache = Tuple[torch.Tensor, torch.Tensor]
_PAD_SLOT_ID = -1
# Capture graphs for batch size 1, 2, 4, 8, 16, 24, 32, 40, ..., 256.
# NOTE: _get_graph_batch_size needs to be updated if this list is changed.
_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [8 * i for i in range(1, 33)]


class ModelRunner:

    def __init__(
        self,
        model_config: ModelConfig,
        parallel_config: ParallelConfig,
        scheduler_config: SchedulerConfig,
        is_driver_worker: bool = False,
        post_model_path: str = None,
    ):
        self.model_config = model_config
        self.parallel_config = parallel_config
        self.scheduler_config = scheduler_config
        self.is_driver_worker = is_driver_worker
        self.post_model_path = post_model_path

        # model_config can be None in tests/samplers/test_sampler.py.
        # FIXME(woosuk): This is a hack to make the tests work. Refactor this.
        self.sliding_window = (
            model_config.get_sliding_window() if model_config is not None else None
        )
        self.model = None
        self.block_size = None  # Set after initial profiling.

        self.graph_runners: Dict[int, CUDAGraphRunner] = {}
        self.graph_memory_pool = None  # Set during graph capture.

        self.max_context_len_to_capture = (
            self.model_config.max_context_len_to_capture
            if self.model_config is not None
            else 0
        )
        # When using CUDA graph, the input block tables must be padded to
        # max_context_len_to_capture. However, creating the block table in
        # Python can be expensive. To optimize this, we cache the block table
        # in numpy and only copy the actual input content at every iteration.
        # The shape of the cached block table will be
        # (max batch size to capture, max context len to capture / block size).
        self.graph_block_tables = None  # Set after initial profiling.
        # cache in_wsl result
        self.in_wsl = in_wsl()

    def load_model(self) -> None:
        self.model = get_model(self.model_config)
        self.post_model = Embed(
            self.model_config.get_hidden_size(),
            self.model_config.num_audio_tokens,
            self.model_config.num_text_tokens,
        )
        state_dict_tensors = {}
        with safe_open(self.post_model_path, framework="pt", device=0) as f:
            for k in f.keys():
                state_dict_tensors[k] = f.get_tensor(k)
        self.post_model.load_state_dict(state_dict_tensors)
        self.post_model.to(next(self.model.parameters())).eval()
        self.sampler = Sampler(self.post_model, self.model_config.num_audio_tokens, 4)

    def set_block_size(self, block_size: int) -> None:
        self.block_size = block_size

        max_num_blocks = (
            self.max_context_len_to_capture + block_size - 1
        ) // block_size
        self.graph_block_tables = np.zeros(
            (max(_BATCH_SIZES_TO_CAPTURE), max_num_blocks), dtype=np.int32
        )

    def _prepare_prompt(
        self,
        seq_group_metadata_list: List[SequenceGroupMetadata],
    ) -> Tuple[torch.Tensor, torch.Tensor, InputMetadata, List[int]]:
        assert len(seq_group_metadata_list) > 0
        input_tokens: List[List[int]] = []
        input_positions: List[List[int]] = []
        slot_mapping: List[List[int]] = []

        prompt_lens: List[int] = []
        for seq_group_metadata in seq_group_metadata_list:
            assert seq_group_metadata.is_prompt
            seq_ids = list(seq_group_metadata.seq_data.keys())
            assert len(seq_ids) == 1
            seq_id = seq_ids[0]

            seq_data = seq_group_metadata.seq_data[seq_id]
            prompt_tokens = seq_data.get_token_ids()
            prompt_len = len(prompt_tokens)
            prompt_lens.append(prompt_len)

            input_tokens.append(prompt_tokens)
            # NOTE(woosuk): Here we assume that the first token in the prompt
            # is always the first token in the sequence.
            input_positions.append(list(range(prompt_len)))

            if seq_group_metadata.block_tables is None:
                # During memory profiling, the block tables are not initialized
                # yet. In this case, we just use a dummy slot mapping.
                slot_mapping.append([_PAD_SLOT_ID] * prompt_len)
                continue

            # Compute the slot mapping.
            slot_mapping.append([])
            block_table = seq_group_metadata.block_tables[seq_id]
            # Mask the [0, start_idx) tokens of the prompt with _PAD_SLOT_ID,
            # where start_idx is max(0, prompt_len - sliding_window).
            # For example, if the prompt len is 10, sliding window is 8, and
            # block size is 4, the first two tokens are masked and the slot
            # mapping will be [-1, -1, 2, 3, 4, 5, 6, 7, 0, 1].
            start_idx = 0
            if self.sliding_window is not None:
                start_idx = max(0, prompt_len - self.sliding_window)
            for i in range(prompt_len):
                if i < start_idx:
                    slot_mapping[-1].append(_PAD_SLOT_ID)
                    continue

                block_number = block_table[i // self.block_size]
                block_offset = i % self.block_size
                slot = block_number * self.block_size + block_offset
                slot_mapping[-1].append(slot)

        max_prompt_len = max(prompt_lens)
        input_tokens = _make_tensor_with_pad(
            input_tokens, max_prompt_len, pad=0, dtype=torch.long
        )
        input_positions = _make_tensor_with_pad(
            input_positions, max_prompt_len, pad=0, dtype=torch.long
        )
        slot_mapping = _make_tensor_with_pad(
            slot_mapping, max_prompt_len, pad=_PAD_SLOT_ID, dtype=torch.long
        )

        input_metadata = InputMetadata(
            is_prompt=True,
            slot_mapping=slot_mapping,
            max_context_len=None,
            context_lens=None,
            block_tables=None,
            use_cuda_graph=False,
        )
        return input_tokens, input_positions, input_metadata, prompt_lens

    def _prepare_decode(
        self,
        seq_group_metadata_list: List[SequenceGroupMetadata],
    ) -> Tuple[torch.Tensor, torch.Tensor, InputMetadata]:
        assert len(seq_group_metadata_list) > 0
        input_tokens: List[List[int]] = []
        input_positions: List[List[int]] = []
        slot_mapping: List[List[int]] = []
        context_lens: List[int] = []
        block_tables: List[List[int]] = []

        for seq_group_metadata in seq_group_metadata_list:
            assert not seq_group_metadata.is_prompt

            seq_ids = list(seq_group_metadata.seq_data.keys())
            for seq_id in seq_ids:
                seq_data = seq_group_metadata.seq_data[seq_id]
                generation_token = seq_data.get_last_token_id()
                input_tokens.append([generation_token])

                seq_len = seq_data.get_len()
                position = seq_len - 1
                input_positions.append([position])

                context_len = (
                    seq_len
                    if self.sliding_window is None
                    else min(seq_len, self.sliding_window)
                )
                context_lens.append(context_len)

                block_table = seq_group_metadata.block_tables[seq_id]
                block_number = block_table[position // self.block_size]
                block_offset = position % self.block_size
                slot = block_number * self.block_size + block_offset
                slot_mapping.append([slot])

                if self.sliding_window is not None:
                    sliding_window_blocks = self.sliding_window // self.block_size
                    block_table = block_table[-sliding_window_blocks:]
                block_tables.append(block_table)

        batch_size = len(input_tokens)
        max_context_len = max(context_lens)
        use_captured_graph = (
            not self.model_config.enforce_eager
            and batch_size <= _BATCH_SIZES_TO_CAPTURE[-1]
            and max_context_len <= self.max_context_len_to_capture
        )
        if use_captured_graph:
            # Pad the input tokens, positions, and slot mapping to match the
            # batch size of the captured graph.
            graph_batch_size = _get_graph_batch_size(batch_size)
            assert graph_batch_size >= batch_size
            for _ in range(graph_batch_size - batch_size):
                input_tokens.append([])
                input_positions.append([])
                slot_mapping.append([])
                context_lens.append(1)
                block_tables.append([])
            batch_size = graph_batch_size

        input_tokens = _make_tensor_with_pad(
            input_tokens, max_len=1, pad=0, dtype=torch.long, device="cuda"
        )
        input_positions = _make_tensor_with_pad(
            input_positions, max_len=1, pad=0, dtype=torch.long, device="cuda"
        )
        slot_mapping = _make_tensor_with_pad(
            slot_mapping, max_len=1, pad=_PAD_SLOT_ID, dtype=torch.long, device="cuda"
        )
        context_lens = torch.tensor(context_lens, dtype=torch.int, device="cuda")

        if use_captured_graph:
            # The shape of graph_block_tables is
            # [max batch size, max context len // block size].
            input_block_tables = self.graph_block_tables[:batch_size]
            for i, block_table in enumerate(block_tables):
                if block_table:
                    input_block_tables[i, : len(block_table)] = block_table
            block_tables = torch.tensor(input_block_tables, device="cuda")
        else:
            block_tables = _make_tensor_with_pad(
                block_tables,
                max_len=max_context_len,
                pad=0,
                dtype=torch.int,
                device="cuda",
            )

        input_metadata = InputMetadata(
            is_prompt=False,
            slot_mapping=slot_mapping,
            max_context_len=max_context_len,
            context_lens=context_lens,
            block_tables=block_tables,
            use_cuda_graph=use_captured_graph,
        )
        return input_tokens, input_positions, input_metadata

    def _prepare_sample(
        self,
        seq_group_metadata_list: List[SequenceGroupMetadata],
        prompt_lens: List[int],
    ) -> SamplingMetadata:
        seq_groups: List[Tuple[List[int], SamplingParams]] = []
        selected_token_indices: List[int] = []
        selected_token_start_idx = 0
        categorized_sample_indices = {t: [] for t in SamplingType}
        categorized_sample_indices_start_idx = 0

        max_prompt_len = max(prompt_lens) if prompt_lens else 1
        for i, seq_group_metadata in enumerate(seq_group_metadata_list):
            seq_ids = list(seq_group_metadata.seq_data.keys())
            sampling_params = seq_group_metadata.sampling_params
            seq_groups.append((seq_ids, sampling_params))

            if seq_group_metadata.is_prompt:
                assert len(seq_ids) == 1
                prompt_len = prompt_lens[i]
                if sampling_params.prompt_logprobs is not None:
                    # NOTE: prompt token positions do not need sample, skip
                    categorized_sample_indices_start_idx += prompt_len - 1

                categorized_sample_indices[sampling_params.sampling_type].append(
                    categorized_sample_indices_start_idx
                )
                categorized_sample_indices_start_idx += 1

                if sampling_params.prompt_logprobs is not None:
                    selected_token_indices.extend(
                        range(
                            selected_token_start_idx,
                            selected_token_start_idx + prompt_len - 1,
                        )
                    )
                selected_token_indices.append(selected_token_start_idx + prompt_len - 1)
                selected_token_start_idx += max_prompt_len
            else:
                num_seqs = len(seq_ids)
                selected_token_indices.extend(
                    range(selected_token_start_idx, selected_token_start_idx + num_seqs)
                )
                selected_token_start_idx += num_seqs

                categorized_sample_indices[sampling_params.sampling_type].extend(
                    range(
                        categorized_sample_indices_start_idx,
                        categorized_sample_indices_start_idx + num_seqs,
                    )
                )
                categorized_sample_indices_start_idx += num_seqs

        selected_token_indices = _async_h2d(
            selected_token_indices, dtype=torch.long, pin_memory=not self.in_wsl
        )
        categorized_sample_indices = {
            t: _async_h2d(seq_ids, dtype=torch.int, pin_memory=not self.in_wsl)
            for t, seq_ids in categorized_sample_indices.items()
        }

        seq_data: Dict[int, SequenceData] = {}
        for seq_group_metadata in seq_group_metadata_list:
            seq_data.update(seq_group_metadata.seq_data)

        sampling_metadata = SamplingMetadata(
            seq_groups=seq_groups,
            seq_data=seq_data,
            prompt_lens=prompt_lens,
            selected_token_indices=selected_token_indices,
            categorized_sample_indices=categorized_sample_indices,
        )
        return sampling_metadata

    def prepare_input_tensors(
        self,
        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],
    ) -> Tuple[torch.Tensor, torch.Tensor, InputMetadata, SamplingMetadata]:
        if self.is_driver_worker:
            # NOTE: We assume that all sequences in the group are all prompts or
            # all decodes.
            is_prompt = seq_group_metadata_list[0].is_prompt
            # Prepare input tensors.
            if is_prompt:
                (input_tokens, input_positions, input_metadata, prompt_lens) = (
                    self._prepare_prompt(seq_group_metadata_list)
                )
            else:
                (input_tokens, input_positions, input_metadata) = self._prepare_decode(
                    seq_group_metadata_list
                )
                prompt_lens = []
            sampling_metadata = self._prepare_sample(
                seq_group_metadata_list, prompt_lens
            )

            def get_size_or_none(x: Optional[torch.Tensor]):
                return x.size() if x is not None else None

            # Broadcast the input data. For input tensors, we first broadcast
            # its shape and then broadcast the tensor to avoid high
            # serialization cost.
            py_data = {
                "input_tokens_size": input_tokens.size(),
                "input_positions_size": input_positions.size(),
                "is_prompt": input_metadata.is_prompt,
                "slot_mapping_size": get_size_or_none(input_metadata.slot_mapping),
                "max_context_len": input_metadata.max_context_len,
                "context_lens_size": get_size_or_none(input_metadata.context_lens),
                "block_tables_size": get_size_or_none(input_metadata.block_tables),
                "use_cuda_graph": input_metadata.use_cuda_graph,
                "selected_token_indices_size": sampling_metadata.selected_token_indices.size(),
            }
            broadcast_object_list([py_data], src=0)
            # TODO(zhuohan): Combine the broadcasts or set async_op=True.
            broadcast(input_tokens, src=0)
            broadcast(input_positions, src=0)
            if input_metadata.slot_mapping is not None:
                broadcast(input_metadata.slot_mapping, src=0)
            if input_metadata.context_lens is not None:
                broadcast(input_metadata.context_lens, src=0)
            if input_metadata.block_tables is not None:
                broadcast(input_metadata.block_tables, src=0)
            broadcast(sampling_metadata.selected_token_indices, src=0)
        else:
            receving_list = [None]
            broadcast_object_list(receving_list, src=0)
            py_data = receving_list[0]
            input_tokens = torch.empty(
                *py_data["input_tokens_size"], dtype=torch.long, device="cuda"
            )
            broadcast(input_tokens, src=0)
            input_positions = torch.empty(
                *py_data["input_positions_size"], dtype=torch.long, device="cuda"
            )
            broadcast(input_positions, src=0)
            if py_data["slot_mapping_size"] is not None:
                slot_mapping = torch.empty(
                    *py_data["slot_mapping_size"], dtype=torch.long, device="cuda"
                )
                broadcast(slot_mapping, src=0)
            else:
                slot_mapping = None
            if py_data["context_lens_size"] is not None:
                context_lens = torch.empty(
                    *py_data["context_lens_size"], dtype=torch.int, device="cuda"
                )
                broadcast(context_lens, src=0)
            else:
                context_lens = None
            if py_data["block_tables_size"] is not None:
                block_tables = torch.empty(
                    *py_data["block_tables_size"], dtype=torch.int, device="cuda"
                )
                broadcast(block_tables, src=0)
            else:
                block_tables = None
            selected_token_indices = torch.empty(
                *py_data["selected_token_indices_size"], dtype=torch.long, device="cuda"
            )
            broadcast(selected_token_indices, src=0)
            input_metadata = InputMetadata(
                is_prompt=py_data["is_prompt"],
                slot_mapping=slot_mapping,
                max_context_len=py_data["max_context_len"],
                context_lens=context_lens,
                block_tables=block_tables,
                use_cuda_graph=py_data["use_cuda_graph"],
            )
            sampling_metadata = SamplingMetadata(
                seq_groups=None,
                seq_data=None,
                prompt_lens=None,
                selected_token_indices=selected_token_indices,
                categorized_sample_indices=None,
                perform_sampling=False,
            )

        return input_tokens, input_positions, input_metadata, sampling_metadata

    @torch.inference_mode()
    def execute_model(
        self,
        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],
        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],
    ) -> Optional[SamplerOutput]:
        input_tokens, input_positions, input_metadata, sampling_metadata = (
            self.prepare_input_tensors(seq_group_metadata_list)
        )
        # print(sampling_metadata.seq_data)
        seq_groups = []
        input_tokens_history = []
        for i, rtn in enumerate(sampling_metadata.seq_groups):
            seq_groups.append(rtn[0][0])
            tokens_history = sampling_metadata.seq_data[rtn[0][0]].output_token_ids
            if len(tokens_history) >= 1:
                if len(tokens_history[0]) == 1:
                    tokens_history = [token[0] for token in tokens_history]
                else:
                    tokens_history = [list(token) for token in tokens_history]
            input_tokens_history.append(tokens_history)
        input_tokens_history = torch.tensor(input_tokens_history).to(
            input_tokens.device
        )
        # token_ids = rtn.outputs[0].token_ids
        # for j, token_id in enumerate(token_ids):
        #     if len(token_id) == 1:
        #         token_ids[j] = token_id[0]
        #     else:
        #         token_ids[j] = list(token_id)

        # Execute the model.
        # print("it1",input_tokens)
        if len(input_tokens.shape) == 2:
            input_tokens = input_tokens.unsqueeze(2).repeat(1, 1, 4)
        if len(input_tokens_history.shape) == 2:
            input_tokens_history = input_tokens_history.unsqueeze(2).repeat(1, 1, 4)
        # print(input_tokens_history.shape)
        # print("it2",input_tokens.shape)
        text_mask = input_tokens != 0
        text_mask = text_mask[:, :, 0]

        if input_metadata.use_cuda_graph:
            graph_batch_size = input_tokens.shape[0]
            model_executable = self.graph_runners[graph_batch_size]
        else:
            model_executable = self.model

        infer_text = sampling_metadata.seq_groups[0][1].infer_text
        temperture = sampling_metadata.seq_groups[0][1].temperature
        if not infer_text:
            temperture = torch.tensor(temperture).to(input_tokens.device)
        logits_processors, logits_warpers = sampling_metadata.seq_groups[0][
            1
        ].logits_processors
        # print(logits_processors, logits_warpers)
        min_new_token = sampling_metadata.seq_groups[0][1].min_new_token
        eos_token = sampling_metadata.seq_groups[0][1].eos_token
        start_idx = sampling_metadata.seq_groups[0][1].start_idx
        if input_tokens.shape[-2] == 1:
            if infer_text:
                input_emb: torch.Tensor = self.post_model.emb_text(
                    input_tokens[:, :, 0]
                )
            else:
                code_emb = [
                    self.post_model.emb_code[i](input_tokens[:, :, i])
                    for i in range(self.post_model.num_vq)
                ]
                input_emb = torch.stack(code_emb, 3).sum(3)
                start_idx = (
                    input_tokens_history.shape[-2] - 1
                    if input_tokens_history.shape[-2] > 0
                    else 0
                )
        else:
            input_emb = self.post_model(input_tokens, text_mask)
        # print(input_emb.shape)
        hidden_states = model_executable(
            input_emb=input_emb,
            positions=input_positions,
            kv_caches=kv_caches,
            input_metadata=input_metadata,
        )
        # print(hidden_states.shape)
        # print(input_tokens)
        B_NO_PAD = input_tokens_history.shape[0]
        input_tokens = input_tokens[:B_NO_PAD, :, :]
        hidden_states = hidden_states[:B_NO_PAD, :, :]
        idx_next, logprob, finish = self.sampler.sample(
            inputs_ids=(
                input_tokens
                if input_tokens_history.shape[-2] == 0
                else input_tokens_history
            ),
            hidden_states=hidden_states,
            infer_text=infer_text,
            temperature=temperture,
            logits_processors=logits_processors,
            logits_warpers=logits_warpers,
            min_new_token=min_new_token,
            now_length=1,
            eos_token=eos_token,
            start_idx=start_idx,
        )
        # print(logprob.shape, idx_next.shape)
        if len(logprob.shape) == 2:
            logprob = logprob[:, None, :]
        logprob = torch.gather(logprob, -1, idx_next.transpose(-1, -2))[:, :, 0]
        # print("æµ‹è¯•",idx_next.shape, logprob.shape)
        # Sample the next token.
        # output = self.model.sample(
        #     hidden_states=hidden_states,
        #     sampling_metadata=sampling_metadata,
        # )
        results = []
        for i in range(idx_next.shape[0]):
            idx_next_i = idx_next[i, 0, :].tolist()
            logprob_i = logprob[i].tolist()
            tmp_hidden_states = hidden_states[i]
            if input_tokens[i].shape[-2] != 1:
                tmp_hidden_states = tmp_hidden_states[-1:, :]
            result = SequenceGroupOutput(
                samples=[
                    SequenceOutput(
                        parent_seq_id=seq_groups[i],
                        logprobs={tuple(idx_next_i): logprob_i},
                        output_token=tuple(idx_next_i),
                        hidden_states=tmp_hidden_states,
                        finished=finish[i].item(),
                    ),
                ],
                prompt_logprobs=None,
            )
            results.append(result)
        # print(results)
        # print(idx_next, idx_next.shape, logprob.shape)
        return results

    @torch.inference_mode()
    def profile_run(self) -> None:
        # Enable top-k sampling to reflect the accurate memory usage.
        vocab_size = self.model_config.get_vocab_size()
        sampling_params = SamplingParams(
            top_p=0.99, top_k=vocab_size - 1, infer_text=True
        )
        max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens
        max_num_seqs = self.scheduler_config.max_num_seqs

        # Profile memory usage with max_num_sequences sequences and the total
        # number of tokens equal to max_num_batched_tokens.
        seqs: List[SequenceGroupMetadata] = []
        for group_id in range(max_num_seqs):
            seq_len = max_num_batched_tokens // max_num_seqs + (
                group_id < max_num_batched_tokens % max_num_seqs
            )
            seq_data = SequenceData([0] * seq_len)
            seq = SequenceGroupMetadata(
                request_id=str(group_id),
                is_prompt=True,
                seq_data={group_id: seq_data},
                sampling_params=sampling_params,
                block_tables=None,
            )
            seqs.append(seq)

        # Run the model with the dummy inputs.
        num_layers = self.model_config.get_num_layers(self.parallel_config)
        kv_caches = [(None, None)] * num_layers
        self.execute_model(seqs, kv_caches)
        torch.cuda.synchronize()
        return

    @torch.inference_mode()
    def capture_model(self, kv_caches: List[KVCache]) -> None:
        assert not self.model_config.enforce_eager
        logger.info(
            "Capturing the model for CUDA graphs. This may lead to "
            "unexpected consequences if the model is not static. To "
            "run the model in eager mode, set 'enforce_eager=True' or "
            "use '--enforce-eager' in the CLI."
        )
        logger.info(
            "CUDA graphs can take additional 1~3 GiB memory per GPU. "
            "If you are running out of memory, consider decreasing "
            "`gpu_memory_utilization` or enforcing eager mode."
        )
        start_time = time.perf_counter()

        # Prepare dummy inputs. These will be reused for all batch sizes.
        max_batch_size = max(_BATCH_SIZES_TO_CAPTURE)
        input_emb = torch.zeros(
            max_batch_size,
            1,
            self.model_config.get_hidden_size(),
            dtype=next(self.model.parameters()).dtype,
        ).cuda()
        input_positions = torch.zeros(max_batch_size, 1, dtype=torch.long).cuda()
        slot_mapping = torch.empty(max_batch_size, 1, dtype=torch.long).cuda()
        slot_mapping.fill_(_PAD_SLOT_ID)
        context_lens = torch.ones(max_batch_size, dtype=torch.int32).cuda()
        block_tables = torch.from_numpy(self.graph_block_tables).cuda()

        # NOTE: Capturing the largest batch size first may help reduce the
        # memory usage of CUDA graph.
        for batch_size in reversed(_BATCH_SIZES_TO_CAPTURE):
            # Create dummy input_metadata.
            input_metadata = InputMetadata(
                is_prompt=False,
                slot_mapping=slot_mapping[:batch_size],
                max_context_len=self.max_context_len_to_capture,
                context_lens=context_lens[:batch_size],
                block_tables=block_tables[:batch_size],
                use_cuda_graph=True,
            )

            graph_runner = CUDAGraphRunner(self.model)
            graph_runner.capture(
                input_emb[:batch_size],
                input_positions[:batch_size],
                kv_caches,
                input_metadata,
                memory_pool=self.graph_memory_pool,
            )
            self.graph_memory_pool = graph_runner.graph.pool()
            self.graph_runners[batch_size] = graph_runner

        end_time = time.perf_counter()
        elapsed_time = end_time - start_time
        # This usually takes < 10 seconds.
        logger.info(f"Graph capturing finished in {elapsed_time:.0f} secs.")


class CUDAGraphRunner:

    def __init__(self, model: nn.Module):
        self.model = model
        self.graph = None
        self.input_buffers: Dict[str, torch.Tensor] = {}
        self.output_buffers: Dict[str, torch.Tensor] = {}

    def capture(
        self,
        input_emb: torch.Tensor,
        positions: torch.Tensor,
        kv_caches: List[KVCache],
        input_metadata: InputMetadata,
        memory_pool,
    ) -> None:
        assert self.graph is None
        # Run the model once without capturing the graph.
        # This is to make sure that the captured graph does not include the
        # kernel launches for initial benchmarking (e.g., Triton autotune).
        self.model(
            input_emb,
            positions,
            kv_caches,
            input_metadata,
        )
        torch.cuda.synchronize()

        # Capture the graph.
        self.graph = torch.cuda.CUDAGraph()
        with torch.cuda.graph(self.graph, pool=memory_pool):
            hidden_states = self.model(
                input_emb,
                positions,
                kv_caches,
                input_metadata,
            )
        torch.cuda.synchronize()

        # Save the input and output buffers.
        self.input_buffers = {
            "input_emb": input_emb,
            "positions": positions,
            "kv_caches": kv_caches,
            "slot_mapping": input_metadata.slot_mapping,
            "context_lens": input_metadata.context_lens,
            "block_tables": input_metadata.block_tables,
        }
        self.output_buffers = {"hidden_states": hidden_states}
        return

    def forward(
        self,
        input_emb: torch.Tensor,
        positions: torch.Tensor,
        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],
        input_metadata: InputMetadata,
    ) -> torch.Tensor:
        # KV caches are fixed tensors, so we don't need to copy them.
        del kv_caches

        # Copy the input tensors to the input buffers.
        self.input_buffers["input_emb"].copy_(input_emb, non_blocking=True)
        self.input_buffers["positions"].copy_(positions, non_blocking=True)
        self.input_buffers["slot_mapping"].copy_(
            input_metadata.slot_mapping, non_blocking=True
        )
        self.input_buffers["context_lens"].copy_(
            input_metadata.context_lens, non_blocking=True
        )
        self.input_buffers["block_tables"].copy_(
            input_metadata.block_tables, non_blocking=True
        )

        # Run the graph.
        self.graph.replay()

        # Return the output tensor.
        return self.output_buffers["hidden_states"]

    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)


def _pad_to_max(x: List[int], max_len: int, pad: int) -> List[int]:
    assert len(x) <= max_len
    if len(x) == max_len:
        return list(x)
    return list(x) + [pad] * (max_len - len(x))


def _make_tensor_with_pad(
    x: List[List[int]],
    max_len: int,
    pad: int,
    dtype: torch.dtype,
    device: Union[str, torch.device] = "cuda",
    pin_memory: bool = False,
) -> torch.Tensor:
    padded_x = []
    for x_i in x:
        pad_i = pad
        if isinstance(x[0][0], tuple):
            pad_i = (0,) * len(x[0][0])
        padded_x.append(_pad_to_max(x_i, max_len, pad_i))

    return torch.tensor(
        padded_x,
        dtype=dtype,
        device=device,
        pin_memory=pin_memory and str(device) == "cpu",
    )


def _get_graph_batch_size(batch_size: int) -> int:
    if batch_size <= 2:
        return batch_size
    elif batch_size <= 4:
        return 4
    else:
        return (batch_size + 7) // 8 * 8


def _async_h2d(data: list, dtype, pin_memory):
    t = torch.tensor(data, dtype=dtype, pin_memory=pin_memory)
    return t.to(device="cuda", non_blocking=True)



================================================
FILE: ChatTTS/model/velocity/output.py
================================================
from typing import List, Optional
import torch

from .sequence import (
    PromptLogprobs,
    SampleLogprobs,
    SequenceGroup,
    SequenceStatus,
)


class CompletionOutput:
    """The output data of one completion output of a request.

    Args:
        index: The index of the output in the request.
        text: The generated output text.
        token_ids: The token IDs of the generated output text.
        cumulative_logprob: The cumulative log probability of the generated
            output text.
        logprobs: The log probabilities of the top probability words at each
            position if the logprobs are requested.
        finish_reason: The reason why the sequence is finished.
    """

    def __init__(
        self,
        index: int,
        text: str,
        token_ids: List[int],
        cumulative_logprob: float,
        logprobs: Optional[SampleLogprobs],
        finish_reason: Optional[str] = None,
        hidden_states: Optional[torch.Tensor] = None,
    ) -> None:
        self.index = index
        self.text = text
        self.token_ids = token_ids
        self.cumulative_logprob = cumulative_logprob
        self.logprobs = logprobs
        self.finish_reason = finish_reason
        self.hidden_states = hidden_states

    def finished(self) -> bool:
        return self.finish_reason is not None

    def __repr__(self) -> str:
        return (
            f"CompletionOutput(index={self.index}, "
            f"text={self.text!r}, "
            f"token_ids={self.token_ids}, "
            f"cumulative_logprob={self.cumulative_logprob}, "
            f"logprobs={self.logprobs}, "
            f"finish_reason={self.finish_reason}, "
            f"hidden_states={self.hidden_states.shape if self.hidden_states is not None else None})"
        )


class RequestOutput:
    """The output data of a request to the LLM.

    Args:
        request_id: The unique ID of the request.
        prompt: The prompt string of the request.
        prompt_token_ids: The token IDs of the prompt.
        prompt_logprobs: The log probabilities to return per prompt token.
        outputs: The output sequences of the request.
        finished: Whether the whole request is finished.
    """

    def __init__(
        self,
        request_id: str,
        prompt: str,
        prompt_token_ids: List[int],
        prompt_logprobs: Optional[PromptLogprobs],
        outputs: List[CompletionOutput],
        finished: bool,
    ) -> None:
        self.request_id = request_id
        self.prompt = prompt
        self.prompt_token_ids = prompt_token_ids
        self.prompt_logprobs = prompt_logprobs
        self.outputs = outputs
        self.finished = finished

    @classmethod
    def from_seq_group(cls, seq_group: SequenceGroup) -> "RequestOutput":
        # Get the top-n sequences.
        n = seq_group.sampling_params.n
        seqs = seq_group.get_seqs()
        if seq_group.sampling_params.use_beam_search:
            sorting_key = lambda seq: seq.get_beam_search_score(
                seq_group.sampling_params.length_penalty
            )
        else:
            sorting_key = lambda seq: seq.get_cumulative_logprob()
        sorted_seqs = sorted(seqs, key=sorting_key, reverse=True)
        top_n_seqs = sorted_seqs[:n]

        # Create the outputs.
        outputs: List[CompletionOutput] = []
        for seq in top_n_seqs:
            logprobs = seq.output_logprobs
            if seq_group.sampling_params.logprobs is None:
                # NOTE: We need to take care of this case because the sequence
                # always has the logprobs of the sampled tokens even if the
                # logprobs are not requested.
                logprobs = None
            finshed_reason = SequenceStatus.get_finished_reason(seq.status)
            output = CompletionOutput(
                seqs.index(seq),
                seq.output_text,
                seq.get_output_token_ids(),
                seq.get_cumulative_logprob(),
                logprobs,
                finshed_reason,
                seq.data.hidden_states,
            )
            outputs.append(output)

        # Every sequence in the sequence group should have the same prompt.
        prompt = seq_group.prompt
        prompt_token_ids = seq_group.prompt_token_ids
        prompt_logprobs = seq_group.prompt_logprobs
        finished = seq_group.is_finished()
        return cls(
            seq_group.request_id,
            prompt,
            prompt_token_ids,
            prompt_logprobs,
            outputs,
            finished,
        )

    def __repr__(self) -> str:
        return (
            f"RequestOutput(request_id={self.request_id}, "
            f"prompt={self.prompt!r}, "
            f"prompt_token_ids={self.prompt_token_ids}, "
            f"prompt_logprobs={self.prompt_logprobs}, "
            f"outputs={self.outputs}, "
            f"finished={self.finished})"
        )



================================================
FILE: ChatTTS/model/velocity/sampler.py
================================================
import torch
from torch.functional import F
from typing import List, Callable

from ..embed import Embed


class Sampler:
    def __init__(self, post_model: Embed, num_audio_tokens: int, num_vq: int):
        self.post_model = post_model
        self.device = next(self.post_model.parameters()).device
        self.num_audio_tokens = num_audio_tokens
        self.num_vq = num_vq

    def sample(
        self,
        inputs_ids: torch.Tensor,
        hidden_states: torch.Tensor,
        infer_text: bool = False,
        temperature: torch.Tensor = 1.0,
        logits_processors: List[Callable] = [
            lambda logits_token, logits: logits,
        ],
        logits_warpers: List[Callable] = [
            lambda logits_token, logits: logits,
        ],
        min_new_token: int = 0,
        now_length: int = 0,
        eos_token: int = 0,
        start_idx: int = 0,
    ):
        # print(inputs_ids.shape)
        B = hidden_states.shape[0]

        end_idx = torch.zeros(
            inputs_ids.shape[0], device=inputs_ids.device, dtype=torch.long
        )
        finish = torch.zeros(inputs_ids.shape[0], device=inputs_ids.device).bool()
        if not infer_text:
            temperature = (
                temperature.unsqueeze(0)
                .expand(inputs_ids.shape[0], -1)
                .contiguous()
                .view(-1, 1)
            )

        if infer_text:
            logits: torch.Tensor = self.post_model.head_text(hidden_states)
        else:
            # logits = torch.stack([self.head_code[i](hidden_states) for i in range(self.num_vq)], 3)
            logits = torch.empty(
                hidden_states.size(0),
                hidden_states.size(1),
                self.num_audio_tokens,
                self.num_vq,
                dtype=torch.float,
                device=self.device,
            )
            for num_vq_iter in range(self.num_vq):
                x: torch.Tensor = self.post_model.head_code[num_vq_iter](hidden_states)
                logits[..., num_vq_iter] = x
                del x

        del hidden_states

        # logits = logits[:, -1].float()
        logits = logits.narrow(1, -1, 1).squeeze_(1).float()

        if not infer_text:
            # logits = rearrange(logits, "b c n -> (b n) c")
            logits = logits.permute(0, 2, 1)
            logits = logits.reshape(-1, logits.size(2))
            # logits_token = rearrange(inputs_ids[:, start_idx:], "b c n -> (b n) c")
            inputs_ids_sliced = inputs_ids[:, start_idx:].permute(0, 2, 1)
            logits_token = inputs_ids_sliced.reshape(
                inputs_ids_sliced.size(0) * inputs_ids_sliced.size(1),
                -1,
            ).to(self.device)
        else:
            logits_token = inputs_ids[:, start_idx:, 0].to(self.device)

        logits /= temperature

        for logitsProcessors in logits_processors:
            logits = logitsProcessors(logits_token, logits)

        for logitsWarpers in logits_warpers:
            logits = logitsWarpers(logits_token, logits)

        del logits_token

        if now_length < min_new_token:
            logits[:, eos_token] = -torch.inf

        scores = F.softmax(logits, dim=-1)
        idx_next = torch.multinomial(scores, num_samples=1).to(finish.device)
        if not infer_text:
            scores = scores.reshape(B, -1, scores.shape[-1])
        if not infer_text:
            # idx_next = rearrange(idx_next, "(b n) 1 -> b n", n=self.num_vq)
            idx_next = idx_next.view(-1, self.num_vq)
            finish_or = idx_next.eq(eos_token).any(1)
            finish.logical_or_(finish_or)
            del finish_or
        else:
            finish_or = idx_next.eq(eos_token).any(1)
            finish.logical_or_(finish_or)
            del finish_or

        del inputs_ids

        not_finished = finish.logical_not().to(end_idx.device)

        end_idx.add_(not_finished.int())
        idx_next = idx_next[:, None, :]
        return (
            idx_next,
            torch.log(scores),
            finish,
        )



================================================
FILE: ChatTTS/model/velocity/sampling_params.py
================================================
"""Sampling parameters for text generation."""

from enum import IntEnum
from functools import cached_property
from typing import Callable, List, Optional, Union

import torch

_SAMPLING_EPS = 1e-5


class SamplingType(IntEnum):
    GREEDY = 0
    RANDOM = 1
    BEAM = 2


LogitsProcessor = Callable[[List[int], torch.Tensor], torch.Tensor]
"""LogitsProcessor is a function that takes a list of previously generated
tokens and a tensor of the logits for the next token, and returns a modified
tensor of logits to sample from."""


class SamplingParams:
    """Sampling parameters for text generation.

    Overall, we follow the sampling parameters from the OpenAI text completion
    API (https://platform.openai.com/docs/api-reference/completions/create).
    In addition, we support beam search, which is not supported by OpenAI.

    Args:
        n: Number of output sequences to return for the given prompt.
        best_of: Number of output sequences that are generated from the prompt.
            From these `best_of` sequences, the top `n` sequences are returned.
            `best_of` must be greater than or equal to `n`. This is treated as
            the beam width when `use_beam_search` is True. By default, `best_of`
            is set to `n`.
        presence_penalty: Float that penalizes new tokens based on whether they
            appear in the generated text so far. Values > 0 encourage the model
            to use new tokens, while values < 0 encourage the model to repeat
            tokens.
        frequency_penalty: Float that penalizes new tokens based on their
            frequency in the generated text so far. Values > 0 encourage the
            model to use new tokens, while values < 0 encourage the model to
            repeat tokens.
        repetition_penalty: Float that penalizes new tokens based on whether
            they appear in the prompt and the generated text so far. Values > 1
            encourage the model to use new tokens, while values < 1 encourage
            the model to repeat tokens.
        temperature: Float that controls the randomness of the sampling. Lower
            values make the model more deterministic, while higher values make
            the model more random. Zero means greedy sampling.
        top_p: Float that controls the cumulative probability of the top tokens
            to consider. Must be in (0, 1]. Set to 1 to consider all tokens.
        top_k: Integer that controls the number of top tokens to consider. Set
            to -1 to consider all tokens.
        min_p: Float that represents the minimum probability for a token to be
            considered, relative to the probability of the most likely token.
            Must be in [0, 1]. Set to 0 to disable this.
        use_beam_search: Whether to use beam search instead of sampling.
        length_penalty: Float that penalizes sequences based on their length.
            Used in beam search.
        early_stopping: Controls the stopping condition for beam search. It
            accepts the following values: `True`, where the generation stops as
            soon as there are `best_of` complete candidates; `False`, where an
            heuristic is applied and the generation stops when is it very
            unlikely to find better candidates; `"never"`, where the beam search
            procedure only stops when there cannot be better candidates
            (canonical beam search algorithm).
        stop: List of strings that stop the generation when they are generated.
            The returned output will not contain the stop strings.
        stop_token_ids: List of tokens that stop the generation when they are
            generated. The returned output will contain the stop tokens unless
            the stop tokens are special tokens.
        include_stop_str_in_output: Whether to include the stop strings in output
            text. Defaults to False.
        ignore_eos: Whether to ignore the EOS token and continue generating
            tokens after the EOS token is generated.
        max_tokens: Maximum number of tokens to generate per output sequence.
        logprobs: Number of log probabilities to return per output token.
            Note that the implementation follows the OpenAI API: The return
            result includes the log probabilities on the `logprobs` most likely
            tokens, as well the chosen tokens. The API will always return the
            log probability of the sampled token, so there  may be up to
            `logprobs+1` elements in the response.
        prompt_logprobs: Number of log probabilities to return per prompt token.
        skip_special_tokens: Whether to skip special tokens in the output.
        spaces_between_special_tokens: Whether to add spaces between special
            tokens in the output.  Defaults to True.
        logits_processors: List of functions that modify logits based on
            previously generated tokens.
    """

    def __init__(
        self,
        n: int = 1,
        best_of: Optional[int] = None,
        presence_penalty: float = 0.0,
        frequency_penalty: float = 0.0,
        repetition_penalty: float = 1.0,
        temperature: float = 1.0,
        top_p: float = 1.0,
        top_k: int = -1,
        min_p: float = 0.0,
        use_beam_search: bool = False,
        length_penalty: float = 1.0,
        early_stopping: Union[bool, str] = False,
        stop: Optional[Union[str, List[str]]] = None,
        stop_token_ids: Optional[List[int]] = None,
        include_stop_str_in_output: bool = False,
        ignore_eos: bool = False,
        max_tokens: int = 16,
        logprobs: Optional[int] = None,
        prompt_logprobs: Optional[int] = None,
        skip_special_tokens: bool = True,
        spaces_between_special_tokens: bool = True,
        logits_processors: Optional[List[LogitsProcessor]] = (
            [
                lambda logits_token, logits: logits,
            ],
            [
                lambda logits_token, logits: logits,
            ],
        ),
        min_new_token: int = 0,
        max_new_token: int = 8192,
        infer_text: bool = False,
        eos_token: int = 0,
        spk_emb: str = None,
        start_idx: int = 0,
    ) -> None:
        self.n = n
        self.best_of = best_of if best_of is not None else n
        self.presence_penalty = presence_penalty
        self.frequency_penalty = frequency_penalty
        self.repetition_penalty = repetition_penalty
        self.temperature = temperature
        self.top_p = top_p
        self.top_k = top_k
        self.min_p = min_p
        self.use_beam_search = use_beam_search
        self.length_penalty = length_penalty
        self.early_stopping = early_stopping
        self.min_new_token = min_new_token
        self.max_new_token = max_new_token
        self.infer_text = infer_text
        self.eos_token = eos_token
        self.spk_emb = spk_emb
        self.start_idx = start_idx
        if stop is None:
            self.stop = []
        elif isinstance(stop, str):
            self.stop = [stop]
        else:
            self.stop = list(stop)
        if stop_token_ids is None:
            self.stop_token_ids = []
        else:
            self.stop_token_ids = list(stop_token_ids)
        self.ignore_eos = ignore_eos
        self.max_tokens = max_tokens
        self.logprobs = logprobs
        self.prompt_logprobs = prompt_logprobs
        self.skip_special_tokens = skip_special_tokens
        self.spaces_between_special_tokens = spaces_between_special_tokens
        self.logits_processors = logits_processors
        self.include_stop_str_in_output = include_stop_str_in_output
        self._verify_args()
        if self.use_beam_search:
            self._verify_beam_search()
        else:
            self._verify_non_beam_search()
            # if self.temperature < _SAMPLING_EPS:
            #     # Zero temperature means greedy sampling.
            #     self.top_p = 1.0
            #     self.top_k = -1
            #     self.min_p = 0.0
            #     self._verify_greedy_sampling()

    def _verify_args(self) -> None:
        if self.n < 1:
            raise ValueError(f"n must be at least 1, got {self.n}.")
        if self.best_of < self.n:
            raise ValueError(
                f"best_of must be greater than or equal to n, "
                f"got n={self.n} and best_of={self.best_of}."
            )
        if not -2.0 <= self.presence_penalty <= 2.0:
            raise ValueError(
                "presence_penalty must be in [-2, 2], got " f"{self.presence_penalty}."
            )
        if not -2.0 <= self.frequency_penalty <= 2.0:
            raise ValueError(
                "frequency_penalty must be in [-2, 2], got "
                f"{self.frequency_penalty}."
            )
        if not 0.0 < self.repetition_penalty <= 2.0:
            raise ValueError(
                "repetition_penalty must be in (0, 2], got "
                f"{self.repetition_penalty}."
            )
        # if self.temperature < 0.0:
        #     raise ValueError(
        #         f"temperature must be non-negative, got {self.temperature}.")
        if not 0.0 < self.top_p <= 1.0:
            raise ValueError(f"top_p must be in (0, 1], got {self.top_p}.")
        if self.top_k < -1 or self.top_k == 0:
            raise ValueError(
                f"top_k must be -1 (disable), or at least 1, " f"got {self.top_k}."
            )
        if not 0.0 <= self.min_p <= 1.0:
            raise ValueError("min_p must be in [0, 1], got " f"{self.min_p}.")
        if self.max_tokens < 1:
            raise ValueError(f"max_tokens must be at least 1, got {self.max_tokens}.")
        if self.logprobs is not None and self.logprobs < 0:
            raise ValueError(f"logprobs must be non-negative, got {self.logprobs}.")
        if self.prompt_logprobs is not None and self.prompt_logprobs < 0:
            raise ValueError(
                f"prompt_logprobs must be non-negative, got " f"{self.prompt_logprobs}."
            )

    def _verify_beam_search(self) -> None:
        if self.best_of == 1:
            raise ValueError(
                "best_of must be greater than 1 when using beam "
                f"search. Got {self.best_of}."
            )
        if self.temperature > _SAMPLING_EPS:
            raise ValueError("temperature must be 0 when using beam search.")
        if self.top_p < 1.0 - _SAMPLING_EPS:
            raise ValueError("top_p must be 1 when using beam search.")
        if self.top_k != -1:
            raise ValueError("top_k must be -1 when using beam search.")
        if self.early_stopping not in [True, False, "never"]:
            raise ValueError(
                f"early_stopping must be True, False, or 'never', "
                f"got {self.early_stopping}."
            )

    def _verify_non_beam_search(self) -> None:
        if self.early_stopping is not False:
            raise ValueError(
                "early_stopping is not effective and must be "
                "False when not using beam search."
            )
        if (
            self.length_penalty < 1.0 - _SAMPLING_EPS
            or self.length_penalty > 1.0 + _SAMPLING_EPS
        ):
            raise ValueError(
                "length_penalty is not effective and must be the "
                "default value of 1.0 when not using beam search."
            )

    def _verify_greedy_sampling(self) -> None:
        if self.best_of > 1:
            raise ValueError(
                "best_of must be 1 when using greedy sampling." f"Got {self.best_of}."
            )

    @cached_property
    def sampling_type(self) -> SamplingType:
        if self.use_beam_search:
            return SamplingType.BEAM
        # if self.temperature < _SAMPLING_EPS:
        #     return SamplingType.GREEDY
        return SamplingType.RANDOM

    def __repr__(self) -> str:
        return (
            f"SamplingParams(n={self.n}, "
            f"best_of={self.best_of}, "
            f"presence_penalty={self.presence_penalty}, "
            f"frequency_penalty={self.frequency_penalty}, "
            f"repetition_penalty={self.repetition_penalty}, "
            f"temperature={self.temperature}, "
            f"top_p={self.top_p}, "
            f"top_k={self.top_k}, "
            f"min_p={self.min_p}, "
            f"use_beam_search={self.use_beam_search}, "
            f"length_penalty={self.length_penalty}, "
            f"early_stopping={self.early_stopping}, "
            f"stop={self.stop}, "
            f"stop_token_ids={self.stop_token_ids}, "
            f"include_stop_str_in_output={self.include_stop_str_in_output}, "
            f"ignore_eos={self.ignore_eos}, "
            f"max_tokens={self.max_tokens}, "
            f"logprobs={self.logprobs}, "
            f"prompt_logprobs={self.prompt_logprobs}, "
            f"skip_special_tokens={self.skip_special_tokens}, "
            "spaces_between_special_tokens="
            f"{self.spaces_between_special_tokens}), "
            f"max_new_token={self.max_new_token}), "
            f"min_new_token={self.min_new_token}), "
            f"infer_text={self.infer_text})"
        )



================================================
FILE: ChatTTS/model/velocity/scheduler.py
================================================
import enum
import time
from typing import Dict, Iterable, List, Optional, Tuple, Union

from vllm.config import CacheConfig, SchedulerConfig
from .block_manager import AllocStatus, BlockSpaceManager
from vllm.core.policy import PolicyFactory
from vllm.logger import init_logger
from .sequence import (
    Sequence,
    SequenceData,
    SequenceGroup,
    SequenceGroupMetadata,
    SequenceStatus,
)

logger = init_logger(__name__)


class PreemptionMode(enum.Enum):
    """Preemption modes.

    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory
    and swap them back in when the sequences are resumed.
    2. Recomputation: Discard the blocks of the preempted sequences and
    recompute them when the sequences are resumed, treating the sequences as
    new prompts.
    """

    SWAP = enum.auto()
    RECOMPUTE = enum.auto()


class SchedulerOutputs:

    def __init__(
        self,
        scheduled_seq_groups: List[SequenceGroup],
        prompt_run: bool,
        num_batched_tokens: int,
        blocks_to_swap_in: Dict[int, int],
        blocks_to_swap_out: Dict[int, int],
        blocks_to_copy: Dict[int, List[int]],
        ignored_seq_groups: List[SequenceGroup],
    ) -> None:
        self.scheduled_seq_groups = scheduled_seq_groups
        self.prompt_run = prompt_run
        self.num_batched_tokens = num_batched_tokens
        self.blocks_to_swap_in = blocks_to_swap_in
        self.blocks_to_swap_out = blocks_to_swap_out
        self.blocks_to_copy = blocks_to_copy
        # Swap in and swap out should never happen at the same time.
        assert not (blocks_to_swap_in and blocks_to_swap_out)
        self.ignored_seq_groups = ignored_seq_groups

    def is_empty(self) -> bool:
        # NOTE: We do not consider the ignored sequence groups.
        return (
            not self.scheduled_seq_groups
            and not self.blocks_to_swap_in
            and not self.blocks_to_swap_out
            and not self.blocks_to_copy
        )


class Scheduler:

    def __init__(
        self,
        scheduler_config: SchedulerConfig,
        cache_config: CacheConfig,
    ) -> None:
        self.scheduler_config = scheduler_config
        self.cache_config = cache_config

        self.prompt_limit = min(
            self.scheduler_config.max_model_len,
            self.scheduler_config.max_num_batched_tokens,
        )

        # Instantiate the scheduling policy.
        self.policy = PolicyFactory.get_policy(policy_name="fcfs")
        # Create the block space manager.
        self.block_manager = BlockSpaceManager(
            block_size=self.cache_config.block_size,
            num_gpu_blocks=self.cache_config.num_gpu_blocks,
            num_cpu_blocks=self.cache_config.num_cpu_blocks,
            sliding_window=self.cache_config.sliding_window,
        )

        # TODO(zhuohan): Use deque instead of list for better performance.
        # Sequence groups in the WAITING state.
        self.waiting: List[SequenceGroup] = []
        # Sequence groups in the RUNNING state.
        self.running: List[SequenceGroup] = []
        # Sequence groups in the SWAPPED state.
        self.swapped: List[SequenceGroup] = []

    def add_seq_group(self, seq_group: SequenceGroup) -> None:
        # Add sequence groups to the waiting queue.
        self.waiting.append(seq_group)

    def abort_seq_group(self, request_id: Union[str, Iterable[str]]) -> None:
        if isinstance(request_id, str):
            request_id = (request_id,)
        request_ids = set(request_id)
        for state_queue in [self.waiting, self.running, self.swapped]:
            # We need to reverse the list as we are removing elements
            # from it as we iterate over it. If we don't do it,
            # indices will get messed up and we will skip over elements.
            for seq_group in reversed(state_queue):
                if seq_group.request_id in request_ids:
                    # Remove the sequence group from the state queue.
                    state_queue.remove(seq_group)
                    for seq in seq_group.get_seqs():
                        if seq.is_finished():
                            continue
                        seq.status = SequenceStatus.FINISHED_ABORTED
                        self.free_seq(seq)
                    request_ids.remove(seq_group.request_id)
                    if not request_ids:
                        return

    def has_unfinished_seqs(self) -> bool:
        return self.waiting or self.running or self.swapped

    def get_num_unfinished_seq_groups(self) -> int:
        return len(self.waiting) + len(self.running) + len(self.swapped)

    def _schedule(self) -> SchedulerOutputs:
        # Blocks that need to be swaped or copied before model execution.
        blocks_to_swap_in: Dict[int, int] = {}
        blocks_to_swap_out: Dict[int, int] = {}
        blocks_to_copy: Dict[int, List[int]] = {}

        # Fix the current time.
        now = time.monotonic()

        # Join waiting sequences if possible.
        if not self.swapped:
            ignored_seq_groups: List[SequenceGroup] = []
            scheduled: List[SequenceGroup] = []
            # The total number of sequences on the fly, including the
            # requests in the generation phase.
            num_curr_seqs = sum(
                seq_group.get_max_num_running_seqs() for seq_group in self.running
            )
            seq_lens: List[int] = []

            # Optimization: We do not sort the waiting queue since the preempted
            # sequence groups are added to the front and the new sequence groups
            # are added to the back.
            while self.waiting:
                seq_group = self.waiting[0]

                waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)
                assert len(waiting_seqs) == 1, (
                    "Waiting sequence group should have only one prompt " "sequence."
                )
                num_prompt_tokens = waiting_seqs[0].get_len()
                if num_prompt_tokens > self.prompt_limit:
                    logger.warning(
                        f"Input prompt ({num_prompt_tokens} tokens) is too long"
                        f" and exceeds limit of {self.prompt_limit}"
                    )
                    for seq in waiting_seqs:
                        seq.status = SequenceStatus.FINISHED_IGNORED
                    ignored_seq_groups.append(seq_group)
                    self.waiting.pop(0)
                    continue

                # If the sequence group cannot be allocated, stop.
                can_allocate = self.block_manager.can_allocate(seq_group)
                if can_allocate == AllocStatus.LATER:
                    break
                elif can_allocate == AllocStatus.NEVER:
                    logger.warning(
                        f"Input prompt ({num_prompt_tokens} tokens) is too long"
                        f" and exceeds the capacity of block_manager"
                    )
                    for seq in waiting_seqs:
                        seq.status = SequenceStatus.FINISHED_IGNORED
                    ignored_seq_groups.append(seq_group)
                    self.waiting.pop(0)
                    continue

                # If the number of batched tokens exceeds the limit, stop.
                new_seq_lens = seq_lens + [num_prompt_tokens]
                num_batched_tokens = len(new_seq_lens) * max(new_seq_lens)
                if num_batched_tokens > self.scheduler_config.max_num_batched_tokens:
                    break

                # The total number of sequences in the RUNNING state should not
                # exceed the maximum number of sequences.
                num_new_seqs = seq_group.get_max_num_running_seqs()
                if num_curr_seqs + num_new_seqs > self.scheduler_config.max_num_seqs:
                    break

                num_paddings = num_batched_tokens - sum(new_seq_lens)
                if num_paddings > self.scheduler_config.max_paddings:
                    break
                seq_lens = new_seq_lens

                seq_group = self.waiting.pop(0)
                self._allocate(seq_group)
                self.running.append(seq_group)
                num_curr_seqs += num_new_seqs
                scheduled.append(seq_group)

            if scheduled or ignored_seq_groups:
                scheduler_outputs = SchedulerOutputs(
                    scheduled_seq_groups=scheduled,
                    prompt_run=True,
                    num_batched_tokens=len(seq_lens) * max(seq_lens) if seq_lens else 0,
                    blocks_to_swap_in=blocks_to_swap_in,
                    blocks_to_swap_out=blocks_to_swap_out,
                    blocks_to_copy=blocks_to_copy,
                    ignored_seq_groups=ignored_seq_groups,
                )
                return scheduler_outputs

        # NOTE(woosuk): Preemption happens only when there is no available slot
        # to keep all the sequence groups in the RUNNING state.
        # In this case, the policy is responsible for deciding which sequence
        # groups to preempt.
        self.running = self.policy.sort_by_priority(now, self.running)

        # Reserve new token slots for the running sequence groups.
        running: List[SequenceGroup] = []
        preempted: List[SequenceGroup] = []
        while self.running:
            seq_group = self.running.pop(0)
            while not self.block_manager.can_append_slot(seq_group):
                if self.running:
                    # Preempt the lowest-priority sequence groups.
                    victim_seq_group = self.running.pop(-1)
                    self._preempt(victim_seq_group, blocks_to_swap_out)
                    preempted.append(victim_seq_group)
                else:
                    # No other sequence groups can be preempted.
                    # Preempt the current sequence group.
                    self._preempt(seq_group, blocks_to_swap_out)
                    preempted.append(seq_group)
                    break
            else:
                # Append new slots to the sequence group.
                self._append_slot(seq_group, blocks_to_copy)
                running.append(seq_group)
        self.running = running

        # Swap in the sequence groups in the SWAPPED state if possible.
        self.swapped = self.policy.sort_by_priority(now, self.swapped)
        if not preempted:
            num_curr_seqs = sum(
                seq_group.get_max_num_running_seqs() for seq_group in self.running
            )

            while self.swapped:
                seq_group = self.swapped[0]
                # If the sequence group cannot be swapped in, stop.
                if not self.block_manager.can_swap_in(seq_group):
                    break

                # The total number of sequences in the RUNNING state should not
                # exceed the maximum number of sequences.
                num_new_seqs = seq_group.get_max_num_running_seqs()
                if num_curr_seqs + num_new_seqs > self.scheduler_config.max_num_seqs:
                    break

                seq_group = self.swapped.pop(0)
                self._swap_in(seq_group, blocks_to_swap_in)
                self._append_slot(seq_group, blocks_to_copy)
                num_curr_seqs += num_new_seqs
                self.running.append(seq_group)

        # Each sequence in the generation phase only takes one token slot.
        # Therefore, the number of batched tokens is equal to the number of
        # sequences in the RUNNING state.
        num_batched_tokens = sum(
            seq_group.num_seqs(status=SequenceStatus.RUNNING)
            for seq_group in self.running
        )

        scheduler_outputs = SchedulerOutputs(
            scheduled_seq_groups=self.running,
            prompt_run=False,
            num_batched_tokens=num_batched_tokens,
            blocks_to_swap_in=blocks_to_swap_in,
            blocks_to_swap_out=blocks_to_swap_out,
            blocks_to_copy=blocks_to_copy,
            ignored_seq_groups=[],
        )
        return scheduler_outputs

    def schedule(self) -> Tuple[List[SequenceGroupMetadata], SchedulerOutputs]:
        # Schedule sequence groups.
        # This function call changes the internal states of the scheduler
        # such as self.running, self.swapped, and self.waiting.
        scheduler_outputs = self._schedule()

        # Create input data structures.
        seq_group_metadata_list: List[SequenceGroupMetadata] = []
        for seq_group in scheduler_outputs.scheduled_seq_groups:
            seq_data: Dict[int, SequenceData] = {}
            block_tables: Dict[int, List[int]] = {}
            for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
                seq_id = seq.seq_id
                seq_data[seq_id] = seq.data
                block_tables[seq_id] = self.block_manager.get_block_table(seq)

            seq_group_metadata = SequenceGroupMetadata(
                request_id=seq_group.request_id,
                is_prompt=scheduler_outputs.prompt_run,
                seq_data=seq_data,
                sampling_params=seq_group.sampling_params,
                block_tables=block_tables,
            )
            seq_group_metadata_list.append(seq_group_metadata)
        return seq_group_metadata_list, scheduler_outputs

    def fork_seq(self, parent_seq: Sequence, child_seq: Sequence) -> None:
        self.block_manager.fork(parent_seq, child_seq)

    def free_seq(self, seq: Sequence) -> None:
        self.block_manager.free(seq)

    def free_finished_seq_groups(self) -> None:
        self.running = [
            seq_group for seq_group in self.running if not seq_group.is_finished()
        ]

    def _allocate(self, seq_group: SequenceGroup) -> None:
        self.block_manager.allocate(seq_group)
        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):
            seq.status = SequenceStatus.RUNNING

    def _append_slot(
        self,
        seq_group: SequenceGroup,
        blocks_to_copy: Dict[int, List[int]],
    ) -> None:
        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
            ret = self.block_manager.append_slot(seq)
            if ret is not None:
                src_block, dst_block = ret
                if src_block in blocks_to_copy:
                    blocks_to_copy[src_block].append(dst_block)
                else:
                    blocks_to_copy[src_block] = [dst_block]

    def _preempt(
        self,
        seq_group: SequenceGroup,
        blocks_to_swap_out: Dict[int, int],
        preemption_mode: Optional[PreemptionMode] = None,
    ) -> None:
        # If preemption mode is not specified, we determine the mode as follows:
        # We use recomputation by default since it incurs lower overhead than
        # swapping. However, when the sequence group has multiple sequences
        # (e.g., beam search), recomputation is not currently supported. In
        # such a case, we use swapping instead.
        # FIXME(woosuk): This makes our scheduling policy a bit bizarre.
        # As swapped sequences are prioritized over waiting sequences,
        # sequence groups with multiple sequences are implicitly prioritized
        # over sequence groups with a single sequence.
        # TODO(woosuk): Support recomputation for sequence groups with multiple
        # sequences. This may require a more sophisticated CUDA kernel.
        if preemption_mode is None:
            if seq_group.get_max_num_running_seqs() == 1:
                preemption_mode = PreemptionMode.RECOMPUTE
            else:
                preemption_mode = PreemptionMode.SWAP
        if preemption_mode == PreemptionMode.RECOMPUTE:
            self._preempt_by_recompute(seq_group)
        elif preemption_mode == PreemptionMode.SWAP:
            self._preempt_by_swap(seq_group, blocks_to_swap_out)
        else:
            raise AssertionError("Invalid preemption mode.")

    def _preempt_by_recompute(
        self,
        seq_group: SequenceGroup,
    ) -> None:
        seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)
        assert len(seqs) == 1
        for seq in seqs:
            seq.status = SequenceStatus.WAITING
            self.block_manager.free(seq)
        # NOTE: For FCFS, we insert the preempted sequence group to the front
        # of the waiting queue.
        self.waiting.insert(0, seq_group)

    def _preempt_by_swap(
        self,
        seq_group: SequenceGroup,
        blocks_to_swap_out: Dict[int, int],
    ) -> None:
        self._swap_out(seq_group, blocks_to_swap_out)
        self.swapped.append(seq_group)

    def _swap_in(
        self,
        seq_group: SequenceGroup,
        blocks_to_swap_in: Dict[int, int],
    ) -> None:
        mapping = self.block_manager.swap_in(seq_group)
        blocks_to_swap_in.update(mapping)
        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):
            seq.status = SequenceStatus.RUNNING

    def _swap_out(
        self,
        seq_group: SequenceGroup,
        blocks_to_swap_out: Dict[int, int],
    ) -> None:
        if not self.block_manager.can_swap_out(seq_group):
            # FIXME(woosuk): Abort the sequence group instead of aborting the
            # entire engine.
            raise RuntimeError(
                "Aborted due to the lack of CPU swap space. Please increase "
                "the swap space to avoid this error."
            )
        mapping = self.block_manager.swap_out(seq_group)
        blocks_to_swap_out.update(mapping)
        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
            seq.status = SequenceStatus.SWAPPED



================================================
FILE: ChatTTS/model/velocity/sequence.py
================================================
"""Sequence and its related classes."""

import copy
import enum
from typing import Dict, List, Optional, Union
import torch
from vllm.block import LogicalTokenBlock
from .sampling_params import SamplingParams

PromptLogprobs = List[Optional[Dict[int, float]]]
SampleLogprobs = List[Dict[int, float]]


class SequenceStatus(enum.Enum):
    """Status of a sequence."""

    WAITING = enum.auto()
    RUNNING = enum.auto()
    SWAPPED = enum.auto()
    FINISHED_STOPPED = enum.auto()
    FINISHED_LENGTH_CAPPED = enum.auto()
    FINISHED_ABORTED = enum.auto()
    FINISHED_IGNORED = enum.auto()

    @staticmethod
    def is_finished(status: "SequenceStatus") -> bool:
        return status in [
            SequenceStatus.FINISHED_STOPPED,
            SequenceStatus.FINISHED_LENGTH_CAPPED,
            SequenceStatus.FINISHED_ABORTED,
            SequenceStatus.FINISHED_IGNORED,
        ]

    @staticmethod
    def get_finished_reason(status: "SequenceStatus") -> Union[str, None]:
        if status == SequenceStatus.FINISHED_STOPPED:
            finish_reason = "stop"
        elif status == SequenceStatus.FINISHED_LENGTH_CAPPED:
            finish_reason = "length"
        elif status == SequenceStatus.FINISHED_ABORTED:
            finish_reason = "abort"
        elif status == SequenceStatus.FINISHED_IGNORED:
            # The ignored sequences are the sequences whose prompt lengths
            # are longer than the model's length cap. Therefore, the stop
            # reason should also be "length" as in OpenAI API.
            finish_reason = "length"
        else:
            finish_reason = None
        return finish_reason


class SequenceData:
    """Data associated with a sequence.


    Args:
        prompt_token_ids: The token IDs of the prompt.

    Attributes:
        prompt_token_ids: The token IDs of the prompt.
        output_token_ids: The token IDs of the output.
        cumulative_logprob: The cumulative log probability of the output.
    """

    def __init__(
        self,
        prompt_token_ids: List[int],
    ) -> None:
        self.prompt_token_ids = prompt_token_ids
        self.output_token_ids: List[int] = []
        self.cumulative_logprob = 0.0
        self.hidden_states: Optional[torch.Tensor] = None
        self.finished = False

    def append_token_id(self, token_id: int, logprob: float) -> None:
        if isinstance(self.cumulative_logprob, float):
            self.cumulative_logprob = [
                0.0,
            ] * len(logprob)
        self.output_token_ids.append(token_id)
        for i in range(len(self.cumulative_logprob)):
            self.cumulative_logprob[i] += logprob[i]

    def append_hidden_states(self, hidden_states: torch.Tensor) -> None:
        if self.hidden_states is None:
            self.hidden_states = hidden_states
        else:
            self.hidden_states = torch.cat([self.hidden_states, hidden_states], dim=0)

    def get_len(self) -> int:
        return len(self.output_token_ids) + len(self.prompt_token_ids)

    def get_prompt_len(self) -> int:
        return len(self.prompt_token_ids)

    def get_output_len(self) -> int:
        return len(self.output_token_ids)

    def get_token_ids(self) -> List[int]:
        return self.prompt_token_ids + self.output_token_ids

    def get_last_token_id(self) -> int:
        if not self.output_token_ids:
            return self.prompt_token_ids[-1]
        return self.output_token_ids[-1]

    def __repr__(self) -> str:
        return (
            f"SequenceData("
            f"prompt_token_ids={self.prompt_token_ids}, "
            f"output_token_ids={self.output_token_ids}, "
            f"cumulative_logprob={self.cumulative_logprob}), "
            f"hidden_states={self.hidden_states.shape if self.hidden_states is not None else None}, "
            f"finished={self.finished})"
        )


class Sequence:
    """Stores the data, status, and block information of a sequence.

    Args:
        seq_id: The ID of the sequence.
        prompt: The prompt of the sequence.
        prompt_token_ids: The token IDs of the prompt.
        block_size: The block size of the sequence. Should be the same as the
            block size used by the block manager and cache engine.
    """

    def __init__(
        self,
        seq_id: int,
        prompt: str,
        prompt_token_ids: List[int],
        block_size: int,
    ) -> None:
        self.seq_id = seq_id
        self.prompt = prompt
        self.block_size = block_size

        self.data = SequenceData(prompt_token_ids)
        self.output_logprobs: SampleLogprobs = []
        self.output_text = ""

        self.logical_token_blocks: List[LogicalTokenBlock] = []
        # Initialize the logical token blocks with the prompt token ids.
        self._append_tokens_to_blocks(prompt_token_ids)
        self.status = SequenceStatus.WAITING

        # Used for incremental detokenization
        self.prefix_offset = 0
        self.read_offset = 0
        # Input + output tokens
        self.tokens: Optional[List[str]] = None

    def _append_logical_block(self) -> None:
        block = LogicalTokenBlock(
            block_number=len(self.logical_token_blocks),
            block_size=self.block_size,
        )
        self.logical_token_blocks.append(block)

    def _append_tokens_to_blocks(self, token_ids: List[int]) -> None:
        cursor = 0
        while cursor < len(token_ids):
            if not self.logical_token_blocks:
                self._append_logical_block()

            last_block = self.logical_token_blocks[-1]
            if last_block.is_full():
                self._append_logical_block()
                last_block = self.logical_token_blocks[-1]

            num_empty_slots = last_block.get_num_empty_slots()
            last_block.append_tokens(token_ids[cursor : cursor + num_empty_slots])
            cursor += num_empty_slots

    def append_token_id(
        self,
        token_id: int,
        logprobs: Dict[int, float],
        hidden_states: Optional[torch.Tensor] = None,
        finished: bool = False,
    ) -> None:
        assert token_id in logprobs
        self._append_tokens_to_blocks([token_id])
        self.output_logprobs.append(logprobs)
        self.data.append_token_id(token_id, logprobs[token_id])
        self.data.append_hidden_states(hidden_states)
        self.data.finished = finished

    def get_len(self) -> int:
        return self.data.get_len()

    def get_prompt_len(self) -> int:
        return self.data.get_prompt_len()

    def get_output_len(self) -> int:
        return self.data.get_output_len()

    def get_token_ids(self) -> List[int]:
        return self.data.get_token_ids()

    def get_last_token_id(self) -> int:
        return self.data.get_last_token_id()

    def get_output_token_ids(self) -> List[int]:
        return self.data.output_token_ids

    def get_cumulative_logprob(self) -> float:
        return self.data.cumulative_logprob

    def get_beam_search_score(
        self,
        length_penalty: float = 0.0,
        seq_len: Optional[int] = None,
        eos_token_id: Optional[int] = None,
    ) -> float:
        """Calculate the beam search score with length penalty.

        Adapted from

        https://github.com/huggingface/transformers/blob/ccb92be23def445f2afdea94c31286f84b89eb5b/src/transformers/generation/beam_search.py#L938
        """
        if seq_len is None:
            seq_len = self.get_len()
            # NOTE: HF implementation does not count the EOS token
            # towards the length, we align with that here for testing.
            if eos_token_id is not None and self.get_last_token_id() == eos_token_id:
                seq_len -= 1
        return self.get_cumulative_logprob() / (seq_len**length_penalty)

    def is_finished(self) -> bool:
        return SequenceStatus.is_finished(self.status)

    def fork(self, new_seq_id: int) -> "Sequence":
        new_seq = copy.deepcopy(self)
        new_seq.seq_id = new_seq_id
        return new_seq

    def __repr__(self) -> str:
        return (
            f"Sequence(seq_id={self.seq_id}, "
            f"status={self.status.name}, "
            f"num_blocks={len(self.logical_token_blocks)})"
        )


class SequenceGroup:
    """A group of sequences that are generated from the same prompt.

    Args:
        request_id: The ID of the request.
        seqs: The list of sequences.
        sampling_params: The sampling parameters used to generate the outputs.
        arrival_time: The arrival time of the request.
    """

    def __init__(
        self,
        request_id: str,
        seqs: List[Sequence],
        sampling_params: SamplingParams,
        arrival_time: float,
    ) -> None:
        self.request_id = request_id
        self.seqs_dict = {seq.seq_id: seq for seq in seqs}
        self.sampling_params = sampling_params
        self.arrival_time = arrival_time
        self.prompt_logprobs: Optional[PromptLogprobs] = None

    @property
    def prompt(self) -> str:
        # All sequences in the group should have the same prompt.
        # We use the prompt of an arbitrary sequence.
        return next(iter(self.seqs_dict.values())).prompt

    @property
    def prompt_token_ids(self) -> List[int]:
        # All sequences in the group should have the same prompt.
        # We use the prompt of an arbitrary sequence.
        return next(iter(self.seqs_dict.values())).data.prompt_token_ids

    def get_max_num_running_seqs(self) -> int:
        """The maximum number of sequences running in parallel in the remaining
        lifetime of the request."""
        if self.sampling_params.use_beam_search:
            # For beam search, maximally there will always be `best_of` beam
            # candidates running in the future.
            return self.sampling_params.best_of
        else:
            if self.sampling_params.best_of > self.num_seqs():
                # At prompt stage, the sequence group is not yet filled up
                # and only have one sequence running. However, in the
                # generation stage, we will have `best_of` sequences running.
                return self.sampling_params.best_of
            # At sampling stages, return the number of actual sequences
            # that are not finished yet.
            return self.num_unfinished_seqs()

    def get_seqs(
        self,
        status: Optional[SequenceStatus] = None,
    ) -> List[Sequence]:
        if status is None:
            return list(self.seqs_dict.values())
        else:
            return [seq for seq in self.seqs_dict.values() if seq.status == status]

    def get_unfinished_seqs(self) -> List[Sequence]:
        return [seq for seq in self.seqs_dict.values() if not seq.is_finished()]

    def get_finished_seqs(self) -> List[Sequence]:
        return [seq for seq in self.seqs_dict.values() if seq.is_finished()]

    def num_seqs(self, status: Optional[SequenceStatus] = None) -> int:
        return len(self.get_seqs(status))

    def num_unfinished_seqs(self) -> int:
        return len(self.get_unfinished_seqs())

    def num_finished_seqs(self) -> int:
        return len(self.get_finished_seqs())

    def find(self, seq_id: int) -> Sequence:
        if seq_id not in self.seqs_dict:
            raise ValueError(f"Sequence {seq_id} not found.")
        return self.seqs_dict[seq_id]

    def add(self, seq: Sequence) -> None:
        if seq.seq_id in self.seqs_dict:
            raise ValueError(f"Sequence {seq.seq_id} already exists.")
        self.seqs_dict[seq.seq_id] = seq

    def remove(self, seq_id: int) -> None:
        if seq_id not in self.seqs_dict:
            raise ValueError(f"Sequence {seq_id} not found.")
        del self.seqs_dict[seq_id]

    def is_finished(self) -> bool:
        return all(seq.is_finished() for seq in self.get_seqs())

    def __repr__(self) -> str:
        return (
            f"SequenceGroup(request_id={self.request_id}, "
            f"sampling_params={self.sampling_params}, "
            f"num_seqs={len(self.seqs_dict)})"
        )


class SequenceGroupMetadata:
    """Metadata for a sequence group. Used to create `InputMetadata`.


    Args:
        request_id: The ID of the request.
        is_prompt: Whether the request is at prompt stage.
        seq_data: The sequence data. (Seq id -> sequence data)
        sampling_params: The sampling parameters used to generate the outputs.
        block_tables: The block tables. (Seq id -> list of physical block
            numbers)
    """

    def __init__(
        self,
        request_id: str,
        is_prompt: bool,
        seq_data: Dict[int, SequenceData],
        sampling_params: SamplingParams,
        block_tables: Dict[int, List[int]],
    ) -> None:
        self.request_id = request_id
        self.is_prompt = is_prompt
        self.seq_data = seq_data
        self.sampling_params = sampling_params
        self.block_tables = block_tables


class SequenceOutput:
    """The model output associated with a sequence.

    Args:
        parent_seq_id: The ID of the parent sequence (for forking in beam
            search).
        output_token: The output token ID.
        logprobs: The logprobs of the output token.
            (Token id -> logP(x_i+1 | x_0, ..., x_i))
    """

    def __init__(
        self,
        parent_seq_id: int,
        output_token: int,
        logprobs: Dict[int, float],
        hidden_states: Optional[torch.Tensor] = None,
        finished: bool = False,
    ) -> None:
        self.parent_seq_id = parent_seq_id
        self.output_token = output_token
        self.logprobs = logprobs
        self.finished = finished
        self.hidden_states = hidden_states

    def __repr__(self) -> str:
        return (
            f"SequenceOutput(parent_seq_id={self.parent_seq_id}, "
            f"output_token={self.output_token}, "
            f"logprobs={self.logprobs}),"
            f"finished={self.finished}),"
            f"hidden_states={self.hidden_states.shape if self.hidden_states is not None else None}"
        )

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, SequenceOutput):
            raise NotImplementedError()
        return (
            self.parent_seq_id == other.parent_seq_id
            and self.output_token == other.output_token
            and self.logprobs == other.logprobs
        )


class SequenceGroupOutput:
    """The model output associated with a sequence group."""

    def __init__(
        self,
        samples: List[SequenceOutput],
        prompt_logprobs: Optional[PromptLogprobs],
    ) -> None:
        self.samples = samples
        self.prompt_logprobs = prompt_logprobs

    def __repr__(self) -> str:
        return (
            f"SequenceGroupOutput(samples={self.samples}, "
            f"prompt_logprobs={self.prompt_logprobs})"
        )

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, SequenceGroupOutput):
            raise NotImplementedError()
        return (
            self.samples == other.samples
            and self.prompt_logprobs == other.prompt_logprobs
        )


# For each sequence group, we generate a list of SequenceOutput object,
# each of which contains one possible candidate for the next token.
SamplerOutput = List[SequenceGroupOutput]



================================================
FILE: ChatTTS/model/velocity/worker.py
================================================
"""A GPU worker class."""

import os
from typing import Dict, List, Optional, Tuple

import torch
import torch.distributed

from vllm.config import CacheConfig, ModelConfig, ParallelConfig, SchedulerConfig
from vllm.model_executor import set_random_seed
from vllm.model_executor.parallel_utils.communication_op import broadcast_object_list
from vllm.model_executor.parallel_utils.parallel_state import initialize_model_parallel
from vllm.sequence import SamplerOutput, SequenceGroupMetadata
from vllm.worker.cache_engine import CacheEngine

from .model_runner import ModelRunner


class Worker:
    """A worker class that executes (a partition of) the model on a GPU.

    Each worker is associated with a single GPU. The worker is responsible for
    maintaining the KV cache and executing the model on the GPU. In case of
    distributed inference, each worker is assigned a partition of the model.
    """

    def __init__(
        self,
        model_config: ModelConfig,
        parallel_config: ParallelConfig,
        scheduler_config: SchedulerConfig,
        local_rank: int,
        rank: int,
        distributed_init_method: str,
        post_model_path: str,
        is_driver_worker: bool = False,
    ) -> None:
        self.model_config = model_config
        self.parallel_config = parallel_config
        self.scheduler_config = scheduler_config
        self.local_rank = local_rank
        self.rank = rank
        self.distributed_init_method = distributed_init_method
        self.is_driver_worker = is_driver_worker
        self.post_model_path = post_model_path

        if self.is_driver_worker:
            assert self.rank == 0, "The driver worker must have rank 0."

        self.model_runner = ModelRunner(
            model_config,
            parallel_config,
            scheduler_config,
            is_driver_worker,
            post_model_path,
        )
        # Uninitialized cache engine. Will be initialized by
        # self.init_cache_engine().
        self.cache_config = None
        self.cache_engine = None
        self.cache_events = None
        self.gpu_cache = None

    def init_model(self) -> None:
        # torch.distributed.all_reduce does not free the input tensor until
        # the synchronization point. This causes the memory usage to grow
        # as the number of all_reduce calls increases. This env var disables
        # this behavior.
        # Related issue:
        # https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573
        os.environ["TORCH_NCCL_AVOID_RECORD_STREAMS"] = "1"

        # This env var set by Ray causes exceptions with graph building.
        os.environ.pop("NCCL_ASYNC_ERROR_HANDLING", None)
        self.device = torch.device(f"cuda:{self.local_rank}")
        torch.cuda.set_device(self.device)

        _check_if_gpu_supports_dtype(self.model_config.dtype)

        # Initialize the distributed environment.
        _init_distributed_environment(
            self.parallel_config, self.rank, self.distributed_init_method
        )

        # Initialize the model.
        set_random_seed(self.model_config.seed)

    def load_model(self):
        self.model_runner.load_model()

    @torch.inference_mode()
    def profile_num_available_blocks(
        self,
        block_size: int,
        gpu_memory_utilization: float,
        cpu_swap_space: int,
    ) -> Tuple[int, int]:
        # Profile the memory usage of the model and get the maximum number of
        # cache blocks that can be allocated with the remaining free memory.
        torch.cuda.empty_cache()

        # Execute a forward pass with dummy inputs to profile the memory usage
        # of the model.
        self.model_runner.profile_run()

        # Calculate the number of blocks that can be allocated with the
        # profiled peak memory.
        torch.cuda.synchronize()
        free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()
        peak_memory = total_gpu_memory - free_gpu_memory

        cache_block_size = CacheEngine.get_cache_block_size(
            block_size, self.model_config, self.parallel_config
        )
        num_gpu_blocks = int(
            (total_gpu_memory * gpu_memory_utilization - peak_memory)
            // cache_block_size
        )
        num_cpu_blocks = int(cpu_swap_space // cache_block_size)
        num_gpu_blocks = max(num_gpu_blocks, 0)
        num_cpu_blocks = max(num_cpu_blocks, 0)
        torch.cuda.empty_cache()
        return num_gpu_blocks, num_cpu_blocks

    def init_cache_engine(self, cache_config: CacheConfig) -> None:
        self.cache_config = cache_config
        self.cache_engine = CacheEngine(
            self.cache_config, self.model_config, self.parallel_config
        )
        self.cache_events = self.cache_engine.events
        self.gpu_cache = self.cache_engine.gpu_cache
        self.model_runner.set_block_size(self.cache_engine.block_size)

    def warm_up_model(self) -> None:
        if not self.model_config.enforce_eager:
            self.model_runner.capture_model(self.gpu_cache)
        # Reset the seed to ensure that the random state is not affected by
        # the model initialization and profiling.
        set_random_seed(self.model_config.seed)

    def cache_swap(
        self,
        blocks_to_swap_in: Dict[int, int],
        blocks_to_swap_out: Dict[int, int],
        blocks_to_copy: Dict[int, List[int]],
    ) -> None:
        # Issue cache operations.
        issued_cache_op = False
        if blocks_to_swap_in:
            self.cache_engine.swap_in(blocks_to_swap_in)
            issued_cache_op = True
        if blocks_to_swap_out:
            self.cache_engine.swap_out(blocks_to_swap_out)
            issued_cache_op = True
        if blocks_to_copy:
            self.cache_engine.copy(blocks_to_copy)
            issued_cache_op = True

        cache_events = self.cache_events if issued_cache_op else None

        # Wait for cache operations to finish.
        # TODO(woosuk): Profile swapping overhead and optimize if needed.
        if cache_events is not None:
            for event in cache_events:
                event.wait()

    @torch.inference_mode()
    def execute_model(
        self,
        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None,
        blocks_to_swap_in: Optional[Dict[int, int]] = None,
        blocks_to_swap_out: Optional[Dict[int, int]] = None,
        blocks_to_copy: Optional[Dict[int, List[int]]] = None,
    ) -> Optional[SamplerOutput]:
        if self.is_driver_worker:
            assert seq_group_metadata_list is not None
            num_seq_groups = len(seq_group_metadata_list)
            assert blocks_to_swap_in is not None
            assert blocks_to_swap_out is not None
            assert blocks_to_copy is not None
            block_swapping_info = [
                blocks_to_swap_in,
                blocks_to_swap_out,
                blocks_to_copy,
            ]
            broadcast_object_list([num_seq_groups] + block_swapping_info, src=0)
        else:
            # num_seq_groups, blocks_to_swap_in, blocks_to_swap_out,
            # blocks_to_copy (4 elements)
            recv_data = [None] * 4
            broadcast_object_list(recv_data, src=0)
            num_seq_groups = recv_data[0]
            block_swapping_info = recv_data[1:]

        self.cache_swap(*block_swapping_info)

        # If there is no input, we don't need to execute the model.
        if num_seq_groups == 0:
            return {}

        output = self.model_runner.execute_model(
            seq_group_metadata_list, self.gpu_cache
        )
        return output


def _init_distributed_environment(
    parallel_config: ParallelConfig,
    rank: int,
    distributed_init_method: Optional[str] = None,
) -> None:
    """Initialize the distributed environment."""
    if torch.distributed.is_initialized():
        torch_world_size = torch.distributed.get_world_size()
        if torch_world_size != parallel_config.world_size:
            raise RuntimeError(
                "torch.distributed is already initialized but the torch world "
                "size does not match parallel_config.world_size "
                f"({torch_world_size} vs. {parallel_config.world_size})."
            )
    elif not distributed_init_method:
        raise ValueError(
            "distributed_init_method must be set if torch.distributed "
            "is not already initialized"
        )
    else:
        torch.distributed.init_process_group(
            backend="nccl",
            world_size=parallel_config.world_size,
            rank=rank,
            init_method=distributed_init_method,
        )

    # A small all_reduce for warmup.
    torch.distributed.all_reduce(torch.zeros(1).cuda())
    initialize_model_parallel(
        parallel_config.tensor_parallel_size, parallel_config.pipeline_parallel_size
    )


def _check_if_gpu_supports_dtype(torch_dtype: torch.dtype):
    # Check if the GPU supports the dtype.
    if torch_dtype == torch.bfloat16:
        compute_capability = torch.cuda.get_device_capability()
        if compute_capability[0] < 8:
            gpu_name = torch.cuda.get_device_name()
            raise ValueError(
                "Bfloat16 is only supported on GPUs with compute capability "
                f"of at least 8.0. Your {gpu_name} GPU has compute capability "
                f"{compute_capability[0]}.{compute_capability[1]}."
            )



================================================
FILE: ChatTTS/res/__init__.py
================================================



================================================
FILE: ChatTTS/res/homophones_map.json
================================================
{
    "ç²¡": "åŒ",
    "ç‚º": "ä½",
    "ç€¹": "æœˆ",
    "æ»†": "æ ¼",
    "æ‘²": "é¢¤",
    "æ¸¹": "è½°",
    "æ–¼": "é±¼",
    "æº€": "æ»¡",
    "é‘": "çˆ¶",
    "èˆ‡": "é›¨",
    "é˜ƒ": "æ†",
    "æ©¹": "é²",
    "éª": "å‰",
    "å²€": "å‡º",
    "é““": "å¿™",
    "æ©": "éª‚",
    "é": "å‘",
    "æ“¼": "é²",
    "å¾Œ": "å",
    "ä¾†": "æ¥",
    "å–”": "å“¦",
    "æœƒ": "ä¼š",
    "çˆ°": "åŸ",
    "ç…¡": "è¿›",
    "é—": "å‡",
    "å€‹": "ä¸ª",
    "æ¾¶": "ç¼ ",
    "æ™‚": "çŸ³",
    "å™¢": "å“¦",
    "é“š": "è‡³",
    "æ §": "æ„",
    "æµ˜": "ä¼Ÿ",
    "é€™": "è¿™",
    "ç©": "æ—…",
    "å¦³": "ä½ ",
    "ä½·": "å¾ˆ",
    "å°": "å¯¹",
    "ä¸¦": "ç—…",
    "åœ‹": "å›½",
    "æˆ ": "çŸ¥",
    "ç¬Ÿ": "å§‘",
    "å””": "æ— ",
    "å‹«": "ç¿»",
    "å¾ƒ": "ç½‘",
    "é‚„": "å­©",
    "å°‡": "å°†",
    "é–®": "åœ",
    "å±½": "æ±‰",
    "é": "è¿‡",
    "ç¶²": "ç½‘",
    "ç´…": "çº¢",
    "èªª": "è¯´",
    "é–‹": "å¼€",
    "å€": "åŒº",
    "è©²": "è¯¥",
    "å¾": "ä»",
    "é‘¹": "çªœ",
    "ç¡—": "æ•²",
    "å…§": "å†…",
    "æ¶˜": "å››",
    "å±„": "é€¼",
    "ç¨®": "ç§",
    "çˆ¾": "è€³",
    "è®“": "è®©",
    "æ§": "å±±",
    "ç”¯": "å‡",
    "æ™¢": "å“²",
    "æ„›": "çˆ±",
    "å–": "å‘€",
    "æ²’": "æ¢…",
    "é•·": "é•¿",
    "ç·š": "ç°",
    "ç†±": "çƒ­",
    "è‚": "æ“",
    "é¶": "æ­Œ",
    "å‚š": "ç¬‘",
    "ç„¡": "æ— ",
    "è™Ÿ": "å·",
    "å…©": "ä¸¤",
    "æ¦«": "æŸ",
    "å¼·": "å¼º",
    "ç“’": "èµ",
    "ç™¼": "å‘",
    "æ©Ÿ": "æœº",
    "é”": "è¾¾",
    "åœ–": "å›¾",
    "ç¨±": "ç§°",
    "å‰‡": "åˆ™",
    "é–“": "é—´",
    "é¦¬": "é©¬",
    "é»": "ç‚¹",
    "ç´š": "æ",
    "è¼ª": "ä¼¦",
    "å£¹": "ä¸€",
    "æ±": "ä¸œ",
    "éš": "ç¾Š",
    "çµ¦": "ç»™",
    "å­¸": "å­¦",
    "æ•¸": "æ ‘",
    "è»": "å›",
    "ç•¶": "å½“",
    "å—": "å—",
    "çˆ²": "ä½",
    "é‚": "æ¬§",
    "å ´": "åœº",
    "å¯®": "èŠ",
    "éˆ": "ç»ƒ",
    "ç": "è®²",
    "ç´„": "çº¦",
    "ç¾": "ç°",
    "è£": "æ",
    "ç¸£": "ç°",
    "é˜Œ": "æ–‡",
    "å©¬": "é“¶",
    "å€‘": "ä»¬",
    "è¬": "ä¸‡",
    "äº": "äºš",
    "ç¶“": "ç²¾",
    "è‰¹": "è‰",
    "å¼µ": "å¼ ",
    "éšŠ": "å¯¹",
    "é€²": "è¿›",
    "å¸¶": "å¸¦",
    "é•©": "çªœ",
    "æ›½": "å±‚",
    "éº½": "é­”",
    "é–²": "æœˆ",
    "è³½": "èµ›",
    "é®": "è¿",
    "è¦‹": "è§",
    "æ›¸": "ä¹¦",
    "ç¥‚": "ä»–",
    "é­": "ç‰",
    "é¾": "é¾™",
    "çŠ³": "ç€",
    "å¢": "æ•²",
    "é–€": "é—¨",
    "é€£": "è¿",
    "å˜…": "å‡¯",
    "è™•": "å¤„",
    "è¨­": "è®¾",
    "å±¬": "é¼ ",
    "æˆ¹": "æ¶",
    "å“¡": "åŸ",
    "é«”": "ä½“",
    "è»Š": "è½¦",
    "è£¡": "æ",
    "ç¾…": "ç½—",
    "æ†º": "è›‹",
    "é ­": "å¤´",
    "é¡": "æ³ª",
    "æˆ°": "æˆ˜",
    "è©±": "è¯",
    "å‹•": "åŠ¨",
    "éº¼": "ä¹ˆ",
    "çµ„": "ç»„",
    "åˆ¥": "åˆ«",
    "åš•": "é²",
    "è«‹": "è¯·",
    "è¨±": "è®¸",
    "ç¸½": "æ€»",
    "å³¶": "å¯¼",
    "è¯": "å",
    "å»": "ç¡®",
    "ç·¨": "ç¼–",
    "æ¥­": "å¤œ",
    "éœ": "å †",
    "å¸«": "è¯—",
    "é›™": "åŒ",
    "æº–": "å‡†",
    "é»ƒ": "é»„",
    "æ¶": "è‚¾",
    "å ±": "æŠ¥",
    "å€«": "ä¼¦",
    "è¦–": "æ˜¯",
    "é¸": "é€‰",
    "æ„": "é±¼",
    "é™": "äº’",
    "ç´”": "çº¯",
    "æ¨»": "æºƒ",
    "å¹¾": "å‡ ",
    "æ¿ˆ": "æ",
    "è¾¦": "åŠ",
    "æ‡‰": "åº”",
    "æ¥µ": "æ",
    "æŸŠ": "ä¸­",
    "ç´": "é‚£",
    "é“°": "è§’",
    "è¹‡": "å‡",
    "ä¿‚": "ç³»",
    "æ‘": "å…¶",
    "é£¾": "æ˜¯",
    "é—±": "ç»´",
    "åœ˜": "å›¢",
    "æ­³": "å²",
    "è®Š": "å˜",
    "é™³": "é™ˆ",
    "å¨": "æˆ",
    "ç¾©": "æ„",
    "å…’": "è€Œ",
    "ç¢¼": "é©¬",
    "æ¢": "æ¡",
    "è½‰": "è½¬",
    "é›£": "å—",
    "æ›œ": "è¦",
    "é‚Š": "ç¼–",
    "é …": "å‘",
    "ç©": "æœº",
    "æ»¿": "æ»¡",
    "å¹¹": "å¹²",
    "å¥§": "å¥¥",
    "è£": "è£…",
    "ç”º": "å¬",
    "å“œ": "è®°",
    "ç¬«": "å­",
    "çµ‘": "æœ±",
    "é¢¨": "é£",
    "é»¨": "æŒ¡",
    "æ™Ÿ": "æˆ",
    "é—ƒ": "å»",
    "è˜­": "è“",
    "æ¨£": "æ ·",
    "é­š": "é±¼",
    "è¼ƒ": "æ•™",
    "è²": "ç”Ÿ",
    "æˆ": "è´¼",
    "å–®": "å•",
    "æ°£": "æ°”",
    "ç²": "æˆ–",
    "èª": "é›¨",
    "æ¨‚": "ä¹",
    "å¯¦": "çŸ³",
    "è¦ª": "äº²",
    "æ²–": "å†²",
    "é®": "é•‡",
    "é˜’": "å»",
    "è¤š": "æ¥š",
    "ç–": "ä¹…",
    "æ¨“": "æ¥¼",
    "æ¥½": "ä¹",
    "é—¾": "é©´",
    "å¤ ": "å¤Ÿ",
    "å¬ª": "è´«",
    "å®®": "å·¥",
    "æ®‡": "ä¼¤",
    "åŠ‰": "åˆ˜",
    "èˆ‰": "ä¸¾",
    "å°»": "çƒ¤",
    "åœ’": "åŸ",
    "å‚³": "ä¼ ",
    "è¹™": "ä¿ƒ",
    "è¯": "è¿",
    "ç¶­": "ç»´",
    "éµ": "é“",
    "å®¸": "é™ˆ",
    "ç ²": "ç‚®",
    "è©¦": "æ˜¯",
    "å°": "å¯¼",
    "è£½": "è‡³",
    "å‹": "èƒœ",
    "èŠ": "æ¥",
    "çµ•": "è§‰",
    "åƒ…": "ç´§",
    "è–": "èƒœ",
    "ç‡": "åœ°",
    "éš¨": "éš",
    "æµœ": "å¸®",
    "æ…¾": "ç‰",
    "å„„": "æ„",
    "å±†": "å€Ÿ",
    "ç¡’": "è¥¿",
    "é›»": "ç”µ",
    "ç¥": "æœˆ",
    "æ¬Š": "å…¨",
    "æ®º": "æ²™",
    "åŸš": "é”…",
    "è²·": "ä¹°",
    "å‰µ": "åˆ›",
    "é›²": "äº‘",
    "è¨˜": "è®°",
    "è‡º": "å°",
    "èª°": "è°",
    "å¤¢": "æ¢¦",
    "é—œ": "å…³",
    "èœ": "å¿…",
    "å": "ä¸‰",
    "å±¤": "å±‚",
    "å¯¶": "å®",
    "å¯«": "å†™",
    "å•": "é—®",
    "å•²": "ä½",
    "å‚™": "è¢«",
    "é‘": "é’",
    "å‹™": "ç‰©",
    "ç": "å·",
    "é•‘": "æ£’",
    "ä½¢": "å–",
    "è«¾": "è¯º",
    "é§": "è½°",
    "è‘‰": "å¤œ",
    "ç¯€": "èŠ‚",
    "æ¨¹": "æ ‘",
    "èˆŠ": "å°±",
    "ç…œ": "ç‰",
    "é£›": "é£",
    "æ­²": "å²",
    "æ“Š": "æœº",
    "ç£": "å¼¯",
    "è˜‡": "è‹",
    "èª¿": "æ‰",
    "ç”¢": "é“²",
    "è¬": "è°¢",
    "é«®": "å‘",
    "å†‡": "å¸½",
    "åŠ‡": "å·¨",
    "å„ª": "ä¼˜",
    "è²»": "è´¹",
    "é“¬": "ä¸ª",
    "å§¦": "é—´",
    "å»£": "å¹¿",
    "è«–": "è®º",
    "é”°": "çŒ›",
    "çœ¾": "é‡",
    "é ": "è¿œ",
    "çµ²": "æ€",
    "ç¬": "æ ‡",
    "éº—": "åˆ©",
    "è¦ƒ": "è°ˆ",
    "çƒ¬": "è¿›",
    "é­¯": "é²",
    "æ©‹": "æ¡¥",
    "é’´": "å¤",
    "é¤¨": "ç®¡",
    "è¾Š": "æ»š",
    "è®€": "æ¯’",
    "çµ±": "ç»Ÿ",
    "ç­†": "æ¯”",
    "è•©": "è¡",
    "æ©«": "æ¨ª",
    "ç›¡": "ç´§",
    "éŒ¢": "é’±",
    "æŸŸ": "å—",
    "è³": "èµ",
    "è´°": "äºŒ",
    "é¡Œ": "æ",
    "æˆ¶": "äº’",
    "å¥·": "å‰",
    "æ³“": "çº¢",
    "å˜­": "ç °",
    "æ¥Š": "ç¾Š",
    "è¨ˆ": "è®°",
    "è–©": "è¨",
    "é™£": "é•‡",
    "èŒ—": "æ˜",
    "å°ˆ": "ä¸“",
    "é¬†": "æ¾",
    "åº«": "è£¤",
    "ç„±": "ç‡•",
    "éŠ‰": "ç‰",
    "æ»…": "ç­",
    "é—„": "å’¬",
    "æ¾¤": "åˆ™",
    "æ¢“": "å­",
    "é„‰": "ç›¸",
    "å´": "å†Œ",
    "è³ª": "è‡³",
    "ä½”": "æˆ˜",
    "å¾©": "çˆ¶",
    "å‰›": "åˆš",
    "å³": "æ— ",
    "æ´©": "è°¢",
    "å·³": "å››",
    "é¦": "è¯—",
    "æ¼¢": "æ±‰",
    "ç•°": "æ„",
    "çµ‚": "ä¸­",
    "è½": "å¬",
    "è·": "ç›´",
    "æ¨™": "æ ‡",
    "è³›": "èµ",
    "å’": "å¹²",
    "é›¢": "ç¦»",
    "æ­¡": "æ¬¢",
    "åŠ": "è§",
    "éˆ": "é›¶",
    "é‹": "è¿",
    "æ¦›": "çœŸ",
    "å§": "å¯¡",
    "æ­": "æ¬§",
    "é ˜": "é¢†",
    "æ½": "èŒ¶",
    "æ›": "æ¢",
    "ç»«": "é›¶",
    "é¤˜": "é±¼",
    "æ¤¿": "æ˜¥",
    "ç€¬": "èµ–",
    "è«": "å‘",
    "æº†": "ç»­",
    "è—¥": "è¦",
    "é ‚": "é¡¶",
    "å¾µ": "ç",
    "å‹": "å¯¸",
    "ç’°": "ç¯",
    "ç®": "ä¼Ÿ",
    "çµ": "èŠ‚",
    "é™½": "ç¾Š",
    "æ“š": "å·¨",
    "èˆˆ": "æ€§",
    "è©": "ç“·",
    "è²": "è¢«",
    "æ•—": "è´¥",
    "é™¸": "è·¯",
    "å„˜": "ç´§",
    "äº‚": "ä¹±",
    "ç´€": "è®°",
    "è²´": "è´µ",
    "è­°": "æ„",
    "è¡“": "æ ‘",
    "è©©": "è¯—",
    "éŠ€": "é“¶",
    "é”": "å‡",
    "å»": "ä¹",
    "æ»": "å¹´",
    "å¬´": "è¥",
    "ç¢º": "ç¡®",
    "ç ¼": "åŒ",
    "è¼‰": "åœ¨",
    "ä¹œ": "ç­",
    "ç›…": "ä¸­",
    "é “": "é¡¿",
    "é²²": "æ˜†",
    "å» ": "åœº",
    "é’‘": "è¨",
    "å§§": "é—´",
    "ç›¤": "ç›˜",
    "è¡›": "ä½",
    "è®¬": "æ‹–",
    "ç¸": "ç˜¦",
    "é§": "åŠ©",
    "ç™º": "å‘",
    "ç›ƒ": "æ¯",
    "å–": "è¯º",
    "é•¢": "è§‰",
    "é»’": "é»‘",
    "åƒ•": "è‘¡",
    "åˆ": "è®°",
    "é€±": "å‘¨",
    "ç»¾": "ç¢—",
    "è“‹": "æ¦‚",
    "é”º": "ä¸­",
    "æº«": "æ¸©",
    "é½Š": "å…¶",
    "å¹«": "å¸®",
    "æ–·": "æ–­",
    "å§": "ä¹¦",
    "é£š": "æ ‡",
    "å ": "é”…",
    "ç’Ÿ": "äº•",
    "ç¥¢": "è¿·",
    "ä¹‡": "æ‹–",
    "è—": "è“",
    "æ³": "çŸ¿",
    "æ˜±": "ç‰",
    "ç¶«": "ç°",
    "å‚·": "ä¼¤",
    "é˜": "ä¸­",
    "è‡§": "è„",
    "æ ¾": "é¸¾",
    "é„": "å›¢",
    "å›—": "ç»´",
    "æ¶§": "è§",
    "è­·": "äº’",
    "ç½¡": "åˆš",
    "æµ—": "æ±‚",
    "è¼•": "é’",
    "é¦€": "é±¼",
    "é¡": "é¢",
    "éš»": "çŸ¥",
    "æ¹ª": "å›¢",
    "è‡¨": "æ—",
    "å¥š": "è¥¿",
    "é”›": "å¥”",
    "å©§": "é™",
    "éŒ¯": "é”™",
    "é †": "é¡º",
    "æµ ": "è¥¿",
    "èŠ®": "ç‘",
    "ç¶š": "ç»­",
    "å¥ª": "å¤º",
    "é ": "ç‰",
    "é ˆ": "éœ€",
    "å­«": "å­™",
    "æ·©": "é›¶",
    "è³£": "å–",
    "ç´°": "ç³»",
    "é¡¯": "æ˜¾",
    "é ": "å¤œ",
    "ç": "çˆ¸",
    "é‚¬": "ä¹Œ",
    "æ˜‡": "ç”Ÿ",
    "èŒ²": "å§¿",
    "å½ˆ": "è›‹",
    "å®Ÿ": "çŸ³",
    "è´": "è¥",
    "ç‹€": "å£®",
    "ç‡Ÿ": "è¥",
    "è": "ç²¾",
    "å‹¢": "æ˜¯",
    "ç•«": "è¯",
    "è®š": "èµ",
    "åƒ¹": "é©¾",
    "é—†": "æ¿",
    "å£“": "å‘€",
    "è­": "åƒ",
    "ç¨": "æ¯’",
    "éº¥": "å–",
    "è³‡": "å§¿",
    "ç‰ ": "ä»–",
    "é‡": "çœŸ",
    "ç‘ª": "é©¬",
    "éŠ": "ç”±",
    "è¬›": "è®²",
    "æ±º": "è§‰",
    "èª¬": "è¯´",
    "å–º": "ä¹ ",
    "è±”": "ç‡•",
    "ç¹¼": "è®°",
    "å§‰": "å­",
    "å‚‘": "èŠ‚",
    "æ–®": "é”™",
    "æ­¸": "å½’",
    "æ­·": "åˆ©",
    "è­¯": "æ„",
    "é„­": "æŒ£",
    "æ²¢": "åˆ™",
    "éŒ„": "è·¯",
    "æ§": "æª",
    "å»³": "å¬",
    "è¶™": "ç…§",
    "éœ": "é£",
    "å’»": "ä¿®",
    "é": "ç»´",
    "è²“": "çŒ«",
    "æ®‘": "æƒ…",
    "è§€": "å…³",
    "æ…±": "å›¢",
    "ç›§": "èŠ¦",
    "è®œ": "æŒ¡",
    "ç¥‡": "å…¶",
    "æ‰ ": "æ’",
    "æ¬¸": "å“€",
    "åœ": "ç»´",
    "ç‚œ": "ä¼Ÿ",
    "ç¼ƒ": "ç›¸",
    "çµ¶": "è§‰",
    "éŠˆ": "æœº",
    "ç¶±": "åˆš",
    "æ…‹": "å¤ª",
    "é¨": "å…¶",
    "èŠ": "è£…",
    "éºµ": "é¢",
    "é¡”": "é¢œ",
    "æ« ": "è´¹",
    "çƒ": "ä¹Œ",
    "è…³": "è§’",
    "è«‡": "è°ˆ",
    "é’¼": "æœ¨",
    "å¯¬": "å®½",
    "æ¦®": "å®¹",
    "é•‰": "æ ¼",
    "èª": "é¢",
    "å•": "ç¿ ",
    "æ¿ ": "è±ª",
    "çºŒ": "ç»­",
    "è": "æ–‡",
    "é’¨": "ä¹Œ",
    "è•™": "ä¼š",
    "èª²": "å®¢",
    "åš’": "ä¹ˆ",
    "æ¯“": "ç‰",
    "æ¼¸": "è§",
    "é»¶": "çœ¼",
    "å¯§": "å‡",
    "ç°¡": "å‡",
    "é¤Š": "å…»",
    "æ¹¯": "æ±¤",
    "å£": "åŸ",
    "è­‰": "æŒ£",
    "æ—¡": "è®°",
    "éš›": "è®°",
    "éˆ¥": "ç«",
    "æ¸›": "å‡",
    "è³": "çŸ³",
    "éŸ“": "å«",
    "é”­": "å®š",
    "å‡±": "å‡¯",
    "éŸ¿": "æƒ³",
    "ç": "è§‰",
    "ä¼±": "ä½ ",
    "ç ·": "æ·±",
    "ç¦®": "æ",
    "è¦º": "è§‰",
    "åŠµ": "å€¦",
    "æ³ ": "é›¶",
    "æ¿®": "è‘¡",
    "è«¸": "æœ±",
    "è‰¦": "è§",
    "ç¥—": "çŸ¥",
    "é†«": "ä¸€",
    "è±": "é£",
    "å": "ä¸‡",
    "è¤‡": "çˆ¶",
    "æ…¶": "åº†",
    "è¨‚": "å®š",
    "è³´": "èµ–",
    "è°™": "å®‰",
    "æ—³": "åœ°",
    "ç¹”": "çŸ¥",
    "ç¡¼": "æœ‹",
    "é³¥": "å°¿",
    "æ™": "æœ",
    "è‘­": "å®¶",
    "é„°": "æ—",
    "æ¶Š": "å¹´",
    "èª ": "æˆ",
    "çˆ­": "ç",
    "è£œ": "æ•",
    "å¦º": "å¢¨",
    "å†Š": "å†Œ",
    "ç¬º": "é—´",
    "æ¨½": "å°Š",
    "è‡‰": "è„¸",
    "åœ©": "ç»´",
    "å¡Š": "å¿«",
    "ç‘›": "åº”",
    "å²±": "å¸¦",
    "ç´": "ä»»",
    "ç·’": "ç»­",
    "è­˜": "çŸ³",
    "å††": "åŸ",
    "å·»": "å€¦",
    "è² ": "çˆ¶",
    "æ°": "å¹³",
    "ç·Š": "ç´§",
    "æ­†": "å¿ƒ",
    "æ•µ": "æ•Œ",
    "é": "èŠ",
    "æ˜ƒ": "è´£",
    "è—": "æ„",
    "ç€›": "è¥",
    "åŸ": "æœ",
    "éš¼": "æŸ",
    "ç¿": "é›¶",
    "æ": "å«©",
    "å°‚": "ä¸“",
    "é¡˜": "é™¢",
    "æ°—": "æ°”",
    "åŸ ": "ä¸",
    "è”º": "å",
    "åƒ": "å‚",
    "è½§": "äºš",
    "ç¨": "é€†",
    "é“€": "ç”±",
    "æµ": "ä¼š",
    "æ¡": "é‡‡",
    "ä»Ÿ": "å‰",
    "è¬‚": "ä½",
    "éŸ‹": "ç»´",
    "ä½˜": "è›‡",
    "é°­": "å…¶",
    "å²¡": "åˆš",
    "è¾¼": "è¿‚",
    "æˆ²": "ç³»",
    "æ²": "å·",
    "ç…¨": "å¾®",
    "æŒ": "å…«",
    "é£¯": "é¥­",
    "é»": "é¼ ",
    "é´": "ä¹ ",
    "æ“¡": "å°",
    "ç´™": "æŒ‡",
    "éµ": "å®¶",
    "é–«": "æ†",
    "å’²": "ç¬‘",
    "éŠ†": "å¢¨",
    "ç­±": "å°",
    "è»Ÿ": "è½¯",
    "æ•¤": "å¯",
    "å•œ": "è¸¹",
    "é¡§": "å›º",
    "å„€": "å®œ",
    "è¿©": "è€³",
    "é«™": "é«˜",
    "è³±": "å…",
    "åº¾": "é›¨",
    "è©•": "å¹³",
    "æ˜€": "äº‘",
    "è²¬": "åˆ™",
    "ä½µ": "ç—…",
    "å–™": "ä¼š",
    "è¸«": "ç¢°",
    "å©•": "èŠ‚",
    "é€¹": "è¾¾",
    "æº´": "ç§€",
    "ç…¸": "ç¼–",
    "é“£": "æ´—",
    "é§": "é—´",
    "å€‰": "ä»“",
    "ç¨¥": "ç›¸",
    "è¼": "ç°",
    "é©š": "ç²¾",
    "ç¿Š": "æ„",
    "å¶": "æ¯",
    "æ§˜": "æ ·",
    "é·": "å‰",
    "è¾²": "å†œ",
    "ç¶ ": "ç»¿",
    "é¸¢": "å†¤",
    "é‘½": "é’»",
    "å¸›": "åš",
    "å¿“": "å¹²",
    "è”£": "è®²",
    "ç·£": "åŸ",
    "åª“": "é»„",
    "ç‚": "æ°”",
    "é•Œ": "æ",
    "é’–": "ç¾Š",
    "è¼›": "äº®",
    "å£Š": "å",
    "é’’": "çƒ¦",
    "å": "é”™",
    "é‹¼": "åˆš",
    "é¦—": "å¥",
    "é‡†": "å˜",
    "è¿": "ç‰",
    "å¨†": "æ‰°",
    "è¤ƒ": "è‚¯",
    "å™´": "å–·",
    "åŒ¯": "ä¼š",
    "åš´": "é¢œ",
    "é›": "æœº",
    "è˜": "æ·±",
    "è›¹": "æ°¸",
    "å¸": "é™¢",
    "æ™”": "å¤œ",
    "è°¥": "æ˜¯",
    "èˆ": "æ˜¯",
    "å­‘": "èŠ‚",
    "å³¯": "é£",
    "é•ª": "æª",
    "åœ“": "åŸ",
    "æ¸‡": "å¯",
    "å‰Œ": "å•¦",
    "æ¶œ": "æ¯’",
    "æ›": "æŒ‚",
    "æ¸¬": "å†Œ",
    "è²¼": "è´´",
    "ç¬": "é‡‡",
    "é©": "æ˜¯",
    "çˆ": "å®¶",
    "å¾³": "å¾·",
    "è½„": "ä¾ ",
    "å…‡": "èƒ¸",
    "èª": "ä»»",
    "å¶…": "æ•–",
    "æƒ¡": "æ¶",
    "å¬«": "å®¹",
    "é›": "æ–­",
    "ç‹ƒ": "çº½",
    "å”": "é‹",
    "æŸ»": "æ‰",
    "åŠƒ": "è¯",
    "å·½": "è®­",
    "ç·´": "ç»ƒ",
    "è²¨": "æˆ–",
    "å—³": "å“€",
    "ç¯„": "é¥­",
    "å¯¤": "ç‰©",
    "ç°½": "å‰",
    "ç¥¯": "çœŸ",
    "å…": "è¨",
    "éƒœ": "å‘Š",
    "è¶•": "æ„Ÿ",
    "æ¹«": "è§’",
    "æ™‰": "è¿›",
    "æš": "ç¾Š",
    "å»Ÿ": "åº™",
    "è„«": "æ‹–",
    "æ¿ƒ": "å†œ",
    "ç«´": "æ‘",
    "å•–": "è›‹",
    "é†": "èƒ¡",
    "æ¤": "æœ",
    "å¦¤": "é±¼",
    "è¡†": "é‡",
    "æ˜¶": "åœº",
    "è®·": "å‘¢",
    "é®‘": "æŠ¥",
    "é’œ": "å·¨",
    "é¢": "çƒ¦",
    "ç¥": "å³",
    "å‹®": "å·¨",
    "æˆ€": "ç»ƒ",
    "é¢¦": "è´«",
    "åš¯": "æˆ–",
    "ç’": "èª",
    "å£": "å",
    "ç°": "çœ¼",
    "å‹": "çŒ›",
    "éœ": "é™",
    "é¨": "è´¹",
    "è¯‡": "èƒ¸",
    "å‘": "å°º",
    "é¡†": "ç§‘",
    "æˆ¦": "æˆ˜",
    "é—€": "çº¢",
    "ç‘„": "å®£",
    "è³¢": "é—²",
    "è³€": "è´º",
    "è¨ª": "è®¿",
    "æ°²": "æ™•",
    "é”‰": "é”™",
    "éš…": "é±¼",
    "é™°": "å› ",
    "ç•": "å‚¬",
    "å‘‚": "æ—…",
    "å": "è®¾",
    "å——": "æŒ–",
    "ç›£": "é—´",
    "åª½": "å¦ˆ",
    "æ›‰": "å°",
    "å¬¢": "å¨˜",
    "æ¶¼": "è‰¯",
    "ç‰": "è˜",
    "å¨‘": "ç¼©",
    "ç˜ ": "æ",
    "å‰": "å¯¡",
    "æ‡·": "æ€€",
    "è°›": "åœ°",
    "å°‹": "å¯»",
    "é–£": "æ ¼",
    "ä¿¾": "æ¯”",
    "é•­": "é›·",
    "å©¦": "çˆ¶",
    "åœƒ": "æ™®",
    "ä¹›": "å‘€",
    "å‹": "åŠ³",
    "è…š": "å®š",
    "è²«": "çŒ",
    "éŒ¦": "ç´§",
    "ç™£": "é€‰",
    "é ’": "ç­",
    "ä¸…": "ä¸‹",
    "è³·": "æœº",
    "è“Ÿ": "è®°",
    "å¬©": "é±¼",
    "è“®": "è¿",
    "é’": "æ±‚",
    "å¿»": "å¿ƒ",
    "é’": "æŸ”",
    "å ‡": "ç´§",
    "è¼¸": "ä¹¦",
    "è¿": "æˆ",
    "èˆ": "æ‹‰",
    "è±¬": "æœ±",
    "è°Œ": "é™ˆ",
    "ç®": "é…",
    "è´®": "åŠ©",
    "é–­": "é©´",
    "å‡Š": "åº†",
    "è´Š": "èµ",
    "å¾¹": "æ’¤",
    "æœ­": "ç‚¸",
    "ç”£": "é“²",
    "ç„—": "å±€",
    "ä¿ª": "åˆ©",
    "å‘·": "å˜",
    "å–¬": "æ¡¥",
    "æª”": "è¡",
    "é›–": "è™½",
    "æ­": "æ¢",
    "é—‡": "æŒ‰",
    "è¥²": "ä¹ ",
    "å¶¨": "å­¦",
    "å‡ˆ": "é™",
    "æ••": "èµ¤",
    "åŸ·": "ç›´",
    "å™¸": "è¹²",
    "æ“": "æŠ–",
    "é£²": "å¼•",
    "æ¯—": "çš®",
    "çŒ¢": "èƒ¡",
    "è†˜": "æ ‡",
    "å»¢": "è´¹",
    "éº": "å®œ",
    "ç­ ": "äº‘",
    "å£‡": "è°ˆ",
    "æ•": "å¿…",
    "æ§": "å‡",
    "çŠŸ": "é™",
    "å¥®": "æ„¤",
    "é’¡": "è¢«",
    "è²¡": "æ‰",
    "å¡¬": "åŸ",
    "å¿š": "è¥¿",
    "å­¬": "è„‘",
    "æ°©": "äºš",
    "æ§¿": "ç´§",
    "é’¯": "é¶",
    "é«’": "è„",
    "å¤”": "å¥",
    "æ›†": "åˆ©",
    "è…¦": "è„‘",
    "æ³¤": "å››",
    "æµš": "ä¿Š",
    "å¶‚": "å¸",
    "è¼»": "ç¦",
    "æ·": "çˆ¬",
    "è½¶": "æ„",
    "å˜¬": "è¸¹",
    "ç‡ˆ": "ç¯",
    "é«»": "è®°",
    "ä¼§": "ä»“",
    "æ©": "å¼€",
    "å¨‰": "å¹³",
    "éšª": "æ˜¾",
    "ç": "è½",
    "å‰œ": "å¼¯",
    "æ§‹": "å¤Ÿ",
    "æ¿Ÿ": "è®°",
    "è„ˆ": "å–",
    "ç´—": "æ²™",
    "ä¾‘": "å³",
    "é »": "è´«",
    "èªŒ": "è‡³",
    "é½’": "å°º",
    "é": "å®—",
    "å¹”": "æ…¢",
    "å§«": "æœº",
    "æ“¬": "ä½ ",
    "ä¼": "äº‘",
    "ç³¸": "å¯†",
    "æŠœ": "æ‹”",
    "å®“": "å¯†",
    "é¨·": "éªš",
    "è¡": "å†²",
    "å§£": "æ•™",
    "è¦": "å½’",
    "è““": "è¢«",
    "è¬€": "è°‹",
    "æ¯€": "æ¯",
    "è­œ": "æ™®",
    "å¯µ": "å® ",
    "é‘": "é»„",
    "ç•€": "å¿…",
    "éŠ®": "é¸¾",
    "å¸‹": "æŒ‡",
    "ç®": "åº†",
    "æ¶": "æŠ¢",
    "é”‘": "è¸¢",
    "é®®": "å…ˆ",
    "ç¼": "å§¿",
    "é˜š": "ç½•",
    "æ¸": "èƒ†",
    "æ¡œ": "åº”",
    "èª¤": "ç‰©",
    "ç“¤": "åš·",
    "è•­": "æ¶ˆ",
    "å™Œ": "å±‚",
    "æ§Œ": "å‚",
    "è¼”": "è¾…",
    "é¡": "é™",
    "æ³·": "é¾™",
    "é³ƒ": "å¡",
    "æ¤´": "æ–­",
    "å°™": "ä¸Š",
    "é›‰": "è‡³",
    "è¤Œ": "æ˜†",
    "æ¥”": "äº›",
    "é‚¨": "æ‘",
    "ç©": "è¡Œ",
    "æºŸ": "æ˜",
    "éš±": "å¼•",
    "æ°": "ä½",
    "ç´‹": "æ–‡",
    "å†¼": "æ˜¾",
    "ç»¥": "éš",
    "æ·¬": "ç¿ ",
    "æŠ»": "é™ˆ",
    "é¦®": "é€¢",
    "è´²": "å¥”",
    "é§›": "ä½¿",
    "é‚": "çŸ¿",
    "ç¥¼": "çŒ",
    "æŸ˜": "è¿™",
    "æ–›": "èƒ¡",
    "æ¬": "å±…",
    "è•ª": "æ— ",
    "ç•¢": "å¿…",
    "é¬¥": "è±†",
    "æ†‘": "å¹³",
    "ç¿’": "ä¹ ",
    "æª¢": "å‡",
    "è¼¯": "æ",
    "å³": "å¥¥",
    "è³¼": "å¤Ÿ",
    "å•Ÿ": "èµ·",
    "ç¯‰": "åŠ©",
    "éœ": "è®°",
    "é‡‹": "æ˜¯",
    "æ§¼": "å½’",
    "å¼¼": "å¿…",
    "é‚°": "å°",
    "è¹¶": "è§‰",
    "ç¸¾": "æœº",
    "ç…¦": "ç»­",
    "åšŸ": "ç¦»",
    "å¯¾": "å¯¹",
    "ç·¯": "ä¼Ÿ",
    "çŸ¾": "çƒ¦",
    "è³ˆ": "å‡",
    "æ£„": "æ°”",
    "ç“¯": "æ¬§",
    "æ¨¾": "æœˆ",
    "å¾‘": "é™",
    "ç…™": "çƒŸ",
    "æ´Ÿ": "æ›¿",
    "æ®˜": "æ®‹",
    "æµ”": "å¯»",
    "ç£¬": "åº†",
    "å´": "æ±‚",
    "é—ˆ": "ç»´",
    "æ”": "è®¾",
    "ç¨": "ç§‘",
    "ç»‰": "æ˜¼",
    "å²«": "ç§€",
    "è¾­": "ç“·",
    "å¢—": "å¢",
    "å¡š": "ç§",
    "å£": "äº²",
    "è¤‹": "å ",
    "é·¹": "åº”",
    "é³³": "å¥‰",
    "ç‡’": "çƒ§",
    "ç“´": "é›¶",
    "é¡": "é“",
    "è§¸": "å¤„",
    "æ°«": "é’",
    "æ±›": "è®­",
    "è·¡": "æœº",
    "ç¹": "ç»•",
    "é•™": "ç½—",
    "æ—»": "æ°‘",
    "å’‚": "åŒ",
    "ç¦": "è§‰",
    "å™«": "ä¸€",
    "æ¡¿": "æ„Ÿ",
    "çˆº": "çˆ·",
    "åŸ": "æ•",
    "è¨“": "è®­",
    "æ¹´": "åŠ",
    "æ²…": "åŸ",
    "å§¹": "å·®",
    "åµ‡": "æœº",
    "æ»¢": "è¥",
    "èŸ¥": "é»„",
    "ç¼™": "è¿›",
    "æµ¼": "ç¾",
    "é¡": "é¢œ",
    "åš": "æ‘‡",
    "ç„": "ç‰",
    "å …": "é—´",
    "è½Ÿ": "è½°",
    "æ«ˆ": "å‡³",
    "æˆ": "æ ‘",
    "æ–¬": "å±•",
    "ç“š": "èµ",
    "æ–": "ä¸€",
    "å¸¥": "å¸…",
    "æ‰ˆ": "äº’",
    "è¯ª": "å‘¨",
    "å‘¤": "å¦",
    "æ±†": "çªœ",
    "ç–£": "ç”±",
    "äºœ": "äºš",
    "è»¸": "è½´",
    "æ˜´": "å¸½",
    "ç®¸": "åŠ©",
    "é‘³": "è§",
    "æ¿•": "è¯—",
    "è³“": "å½¬",
    "è†£": "è‡³",
    "ç€¨": "èµ–",
    "å‰": "ä¼Ÿ",
    "çº": "ä¿Š",
    "äº“": "å…¶",
    "å•“": "èµ·",
    "æ²£": "é£",
    "æ¾¹": "è›‹",
    "æ›³": "å¤œ",
    "è’¯": "å¿«",
    "ç—‚": "å®¶",
    "çˆ›": "çƒ‚",
    "éš": "æ¥",
    "å½§": "ç‰",
    "èœƒ": "è‚¾",
    "å™™": "ç´",
    "è¤‰": "è°¢",
    "é„§": "å‡³",
    "å£½": "ç˜¦",
    "é·„": "æœº",
    "éƒ—": "è¥¿",
    "å": "å¾®",
    "éˆ´": "é›¶",
    "è¡¿": "é‡‘",
    "ä¿Ÿ": "å…¶",
    "åŠ‘": "è®°",
    "è³²": "å®",
    "èŸ²": "è™«",
    "åºƒ": "å¹¿",
    "å›": "æ´—",
    "æ´µ": "å¯»",
    "ç«¶": "é™",
    "æ§ƒ": "ç›˜",
    "é¶´": "è´º",
    "é”—": "è€…",
    "æ½¤": "æ¶¦",
    "æ®’": "å…",
    "è‹“": "é›¶",
    "ç¸®": "ç¼©",
    "åœ": "å˜´",
    "ç¦Œ": "å§¿",
    "é“³": "å†²",
    "å¶º": "é¢†",
    "é†š": "è¿·",
    "è€": "ç¢—",
    "é¹½": "é¢œ",
    "ç™¡": "åƒ",
    "å®¥": "å³",
    "è¨»": "åŠ©",
    "æ¿‚": "è¿",
    "è–¨": "è½°",
    "é²›": "æ•™",
    "èˆ‚": "å†²",
    "æ£Ÿ": "åŠ¨",
    "é¢‰": "èŠ‚",
    "é–‘": "é—²",
    "æ·º": "æµ…",
    "å½œ": "å®œ",
    "æ±": "åŸƒ",
    "é½¡": "é›¶",
    "éŠ·": "æ¶ˆ",
    "æ…˜": "æƒ¨",
    "æ»›": "é“¶",
    "èƒ": "å…¨",
    "è¾‡": "å¹´",
    "éŒ¶": "è¡¨",
    "å¯©": "å®¡",
    "è¨Š": "è®­",
    "ç¹ª": "ä¼š",
    "è»Œ": "é¬¼",
    "å¶": "é‚£",
    "å‹": "è¿›",
    "è²": "çœŸ",
    "è®˜": "è‚",
    "é›’": "è½",
    "é”†": "å‘Š",
    "å¦£": "æ¯”",
    "æ½": "è·¯",
    "å ": "è´¹",
    "é­ˆ": "æ¶ˆ",
    "é„¢": "çƒŸ",
    "æ «": "è§",
    "å€†": "ä¿©",
    "åš": "é•¿",
    "ä½ˆ": "ä¸",
    "é™‚": "æ¯",
    "è™›": "éœ€",
    "è¯²": "ä¼š",
    "éŠ…": "åŒ",
    "è°’": "å¤œ",
    "åŸ´": "ç›´",
    "ç˜€": "è¿‚",
    "å™˜": "ç»",
    "é•": "ç»´",
    "é„Œ": "å”",
    "è†·": "ç›¸",
    "éªŠ": "ç¦»",
    "å›¯": "å›½",
    "æš«": "èµ",
    "é‰„": "ç›´",
    "å‡¦": "æ¥š",
    "æ‹š": "åˆ¤",
    "éŒ«": "è¥¿",
    "éƒ¦": "åˆ©",
    "å˜¢": "ä¹Ÿ",
    "é–ƒ": "é—ª",
    "ä»": "åŒ",
    "è®›": "æ„",
    "æ½”": "èŠ‚",
    "æ›": "åˆ©",
    "æ¿±": "å½¬",
    "è€¢": "çƒ™",
    "é‚": "å–",
    "é¾": "ä¸­",
    "è®«": "æ°”",
    "ç´¹": "ç»",
    "è¶º": "å¤«",
    "ç¥": "ä¸€",
    "ç·»": "è‡³",
    "å”¿": "å‘¼",
    "çŠ²": "æŸ´",
    "ç…³": "èƒ¡",
    "è³µ": "å¥‰",
    "è† ": "æ•™",
    "å™‰": "è›‹",
    "æ„¬": "é€Ÿ",
    "æ†": "æ¨ª",
    "å½¥": "ç‡•",
    "å›£": "å›¢",
    "å¤¥": "ç«",
    "é”Ÿ": "æ˜†",
    "ç": "è„¸",
    "ç¶‹": "çº¢",
    "ç»": "ç°",
    "æˆ¸": "äº’",
    "é›œ": "æ‚",
    "ç…": "è¯—",
    "éœ§": "ç‰©",
    "æ†²": "ç°",
    "æ’ƒ": "æœº",
    "è´ˆ": "èµ ",
    "é£¨": "æƒ³",
    "æ«»": "åº”",
    "å²¬": "å‡",
    "å": "ä¸‡",
    "é€„": "æ—",
    "æ°³": "æ™•",
    "è’½": "æ©",
    "ç¥¿": "è·¯",
    "é": "å°Š",
    "éœ": "è®­",
    "å¤‰": "å˜",
    "æ¿¯": "ç€",
    "æ“º": "æ‘†",
    "å ": "é“¶",
    "æ·¨": "é™",
    "ç…Š": "å®£",
    "è»³": "è¢",
    "æ—Œ": "ç²¾",
    "æ®": "å¢¨",
    "å¶‹": "å¯¼",
    "é—": "æ",
    "çš‹": "é«˜",
    "éª‹": "æˆ",
    "è°": "éœ€",
    "è­½": "ç‰",
    "èŸ»": "ä»¥",
    "æ´": "å‰",
    "è¨´": "é€Ÿ",
    "é¯‰": "æ",
    "èŸ¿": "æ°”",
    "ç¾": "æ¬¢",
    "è¡": "æ±‰",
    "ä¸Ÿ": "ä¸¢",
    "åŒ": "å¼±",
    "å—„": "å•Š",
    "é’º": "æœˆ",
    "å´": "æ°‘",
    "æ¿‰": "è™½",
    "å¿¿": "æ„¤",
    "ç¡Œ": "ä¸ª",
    "ç’": "åº”",
    "é“„": "ç¡•",
    "ç’": "æ•–",
    "é‹’": "é£",
    "è¸": "å·¨",
    "ç‚€": "ç¾Š",
    "å£¯": "å£®",
    "æ±™": "ä¹Œ",
    "é–¢": "å…³",
    "æ¨": "èˆ",
    "ç²µ": "æœˆ",
    "ç¨": "æ˜†",
    "é¢": "å½±",
    "è³œ": "æ¬¡",
    "å¤²": "æ",
    "éƒ¢": "å½±",
    "æ·š": "æ³ª",
    "èµŠ": "èˆ",
    "ç¤": "æ¥š",
    "è±Š": "æ",
    "å¤¾": "å®¶",
    "èŠˆ": "ç±³",
    "ç©©": "ç¨³",
    "è¯°": "å‘Š",
    "ç¿Œ": "æ„",
    "é–‰": "å¿…",
    "é³´": "æ˜",
    "é‹": "é”…",
    "å…": "å„",
    "é–": "é”",
    "æ½ƒ": "ä¿®",
    "ç¼¦": "æ…¢",
    "çˆ": "èŠ¦",
    "å—Ÿ": "æ¥",
    "ç¸±": "å®—",
    "æ‘º": "å“²",
    "é‹ª": "ç€‘",
    "æ˜°": "æ˜¯",
    "ç®“": "è·¯",
    "æ¾§": "æ",
    "é¦¥": "çˆ¶",
    "ä¾—": "åŠ¨",
    "æ­©": "ä¸",
    "æ¯": "ç¾",
    "ç‘¨": "è¿›",
    "ç§†": "æ„Ÿ",
    "å§’": "å››",
    "æ¤½": "ä¼ ",
    "è¹¤": "å®—",
    "é¹¹": "é—²",
    "ç•¦": "å…¶",
    "æŒ¿": "æ’",
    "æ£": "æ’¤",
    "ç¶¾": "é›¶",
    "é‰": "ç˜¦",
    "è»’": "å®£",
    "èŠ ": "æ–‡",
    "é½‹": "æ‘˜",
    "å²„": "æœˆ",
    "æ˜": "èµ",
    "æ½›": "é’±",
    "ä¼¢": "ç‰™",
    "éª": "é²",
    "è•ƒ": "ç¿»",
    "æ¥": "å°º",
    "è¨": "è®¨",
    "å ": "å†Œ",
    "ç®©": "ç½—",
    "å½¡": "å±±",
    "å¼": "å˜",
    "è²¢": "å…±",
    "é´»": "çº¢",
    "æ°¬": "äºš",
    "å¢»": "å¼º",
    "èŠ¾": "è´¹",
    "äº³": "åš",
    "é¸©": "é•‡",
    "ç‰†": "å¼º",
    "è­š": "è°ˆ",
    "é²§": "æ»š",
    "é“–": "æˆ",
    "æ°¡": "ä¸œ",
    "è•": "æœ€",
    "æ±š": "ä¹Œ",
    "ç¼¨": "åº”",
    "åŸ™": "ç†",
    "é¥·": "æƒ³",
    "é—Š": "æ‰©",
    "é“±": "ä¸€",
    "é’¤": "é’±",
    "æ¼²": "é•¿",
    "æ“¢": "ç€",
    "æ®¼": "å’³",
    "é¥§": "å”",
    "æ³": "æŒ‡",
    "åŸ‚": "æ¢—",
    "è³Š": "è´¼",
    "åƒ": "åŠ¨",
    "å“‚": "å®¡",
    "é¬²": "æ ¼",
    "å¼‹": "æ„",
    "æ–«": "ç€",
    "ç‡": "çœ¯",
    "æ§Š": "ç¡•",
    "è¨—": "æ‹–",
    "å—¬": "å–",
    "é¾œ": "å½’",
    "é”¶": "æ€",
    "æ½µ": "æ´’",
    "å”¸": "å¿µ",
    "æˆ": "æ’",
    "ç®•": "æœº",
    "å´¥": "çš®",
    "æ¿†": "åŸ",
    "ç¹«": "ç³»",
    "é¨°": "è…¾",
    "éŠ˜": "æ˜",
    "èš©": "åƒ",
    "æ¤¤": "ç½—",
    "éš—": "å¥",
    "ç¬…": "è§’",
    "ç¥•": "å¯†",
    "æ»¾": "æ»š",
    "æ€›": "è¾¾",
    "æª«": "èŒ¶",
    "ç™‚": "èŠ",
    "ç¨…": "ç¡",
    "éŸ»": "è¿",
    "çš": "è§’",
    "éŠ": "ç»ƒ",
    "ç·": "æ€»",
    "æ¸‘": "å…",
    "é“µ": "ä¿º",
    "æ¤¹": "è‚¾",
    "èˆ«": "è®¿",
    "ç›œ": "åˆ°",
    "é²": "æŒ",
    "é¾": "æ—",
    "é¨™": "ç‰‡",
    "ç…©": "çƒ¦",
    "æº": "å‹¾",
    "ä¿…": "æ±‚",
    "ç¼": "ç°",
    "é£„": "é£˜",
    "é¢š": "æ¶",
    "é›": "å±…",
    "æ³—": "å››",
    "è³³": "ç¾",
    "è¤°": "å‰",
    "é–’": "é—²",
    "é©—": "ç‡•",
    "é…†": "é£",
    "ç‚": "å‘›",
    "çŠ‡": "å¥”",
    "çŒ·": "ç”±",
    "ç±Œ": "æ„",
    "èƒ«": "é™",
    "å¡µ": "é™ˆ",
    "è»": "æ•Œ",
    "æš": "æ˜",
    "è¨¾": "å§¿",
    "ç‰’": "å ",
    "èŸ½": "è¾¾",
    "å±": "è¯—",
    "é¤Œ": "è€³",
    "èˆ–": "ç€‘",
    "é“Œ": "å°¼",
    "åŠ­": "ç»",
    "ç†˜": "æºœ",
    "ç›‚": "é±¼",
    "ç‡®": "è°¢",
    "å‘”": "å‘†",
    "æ‚…": "æœˆ",
    "å°•": "å°¬",
    "å½Œ": "è¿·",
    "çµ": "è£‚",
    "æ™": "æ ·",
    "çƒƒ": "å¬",
    "æ¶": "é—²",
    "å²™": "å¥¥",
    "è²‰": "è±ª",
    "é“‰": "ç‚«",
    "å´—": "å²—",
    "é‹": "æ—…",
    "è±•": "ä½¿",
    "è »": "ç’",
    "ä¿ ": "ä¾ ",
    "æ¡¢": "çœŸ",
    "è®—": "é‹",
    "é•—": "æ±¤",
    "åŸ¸": "æ„",
    "è¼": "æ¶",
    "é…”": "æœ€",
    "é¤µ": "ä½",
    "å¬Œ": "æ•™",
    "æ“‹": "æŒ¡",
    "æ": "æŸ",
    "é’½": "å¦",
    "é«¯": "ç„¶",
    "å¹£": "å¿…",
    "ç´’": "è®°",
    "å›¡": "å–ƒ",
    "æ·–": "é—¹",
    "è¦½": "æ‡’",
    "æ°–": "å¥¶",
    "è½²": "ç§‘",
    "è™¬": "æ±‚",
    "é¥": "å¦¾",
    "è¡¾": "äº²",
    "ç’œ": "é»„",
    "åµ˜": "å®¹",
    "éš£": "æ—",
    "æ¿": "äº²",
    "æ‹‹": "æŠ›",
    "å³": "å¸½",
    "è…": "åˆ©",
    "å‹¬": "æ",
    "æ²±": "é©®",
    "å—®": "èµ›",
    "ç»º": "æŸ³",
    "ç”‘": "èµ ",
    "éµ": "è§",
    "ç¯‘": "æºƒ",
    "è¿´": "å›",
    "æ†©": "æ°”",
    "å›³": "å›¾",
    "è¨¼": "æŒ£",
    "é“Ÿ": "å› ",
    "æ„®": "æ‘‡",
    "ç²¿": "æœ",
    "è¡¹": "æŒ‡",
    "ç‰½": "å‰",
    "å·¿": "ç¦",
    "æ£²": "ä¸ƒ",
    "æ©¾": "ä¹¦",
    "æ™©": "ç¢—",
    "éŒ²": "è·¯",
    "è±º": "æŸ´",
    "ç’¨": "ç¿",
    "è¯": "å¿…",
    "æ´º": "æ˜",
    "é¢¢": "å·",
    "é¬§": "é—¹",
    "è‰–": "æ’",
    "è³º": "èµš",
    "å©€": "é˜¿",
    "æ·¦": "å¹²",
    "ç»¡": "æ¶ˆ",
    "è¿³": "é™",
    "æ‚¶": "é—·",
    "è‰·": "ç‡•",
    "å‹³": "ç†",
    "èœ‡": "é®",
    "è’¼": "ä»“",
    "ç¬£": "åŒ…",
    "æ¬¤": "é±¼",
    "å¬²": "å°¿",
    "æ—¸": "ç¾Š",
    "å£²": "å–",
    "è“¦": "å¢¨",
    "å–ª": "ä¸§",
    "è¤”": "çˆ¶",
    "é¢”": "æ±‰",
    "è‹’": "æŸ“",
    "éš ": "å¼•",
    "è§": "ä¼¤",
    "å…—": "çœ¼",
    "å¼¾": "è›‹",
    "å¬¬": "å¦‚",
    "æ³‘": "ä¼˜",
    "å§˜": "æ‹¼",
    "ç®": "ä»",
    "å»‹": "æœ",
    "é˜": "æ˜",
    "æ¤‹": "è‰¯",
    "å—•": "å…¥",
    "å›©": "äº‘",
    "çº¾": "ä¹¦",
    "çµµ": "ä¼š",
    "é": "è§‰",
    "ç¥™": "å¦¹",
    "å €": "å“­",
    "é“‹": "å¿…",
    "æ¡‰": "å®‰",
    "ç¤¦": "çŸ¿",
    "é«": "æ",
    "ç¥š": "åš",
    "å¹‡": "å¸®",
    "æ»¦": "é¸¾",
    "ä»": "ä»»",
    "é»": "é”",
    "é˜•": "ç¡®",
    "å½²": "åƒ",
    "å‚¾": "é’",
    "ç®¬": "å¼±",
    "ç†¶": "çªœ",
    "çš´": "æ‘",
    "é¦Š": "æœ",
    "æ¹„": "æ¢…",
    "éª": "å…¶",
    "é•“": "å®¶",
    "é›": "ç”µ",
    "æ‚ª": "æ¶",
    "å¾•": "æ¥",
    "é¯°": "å¹´",
    "é§…": "æ„",
    "é¾‹": "æ›²",
    "ç­µ": "é¢œ",
    "æ´™": "æœ±",
    "çŠ¼": "å¼",
    "å»¿": "å¿µ",
    "éœ­": "çŸ®",
    "é²": "å",
    "èŠ¶": "ç‹—",
    "å‘¯": "å¹³",
    "å¡—": "å›¾",
    "å¦": "è£…",
    "é¹„": "å¤",
    "ä½š": "æ„",
    "å˜": "ä¸ƒ",
    "æ¨‘": "è‰¯",
    "è–¦": "è§",
    "å—š": "ä¹Œ",
    "å‹¸": "åŠ",
    "æ¥®": "æ¥š",
    "ç £": "é©®",
    "è‘³": "å¾®",
    "æ£°": "å‚",
    "èˆ·": "é—²",
    "è…": "é—´",
    "èª˜": "å³",
    "å¤Š": "è™½",
    "é —": "å¡",
    "æ½¼": "åŒ",
    "ç•ˆ": "é¥­",
    "ç¬": "ç¢—",
    "ä½¸": "æ´»",
    "é‚º": "å¤œ",
    "å§µ": "é…",
    "éƒµ": "ç”±",
    "å„²": "æ¥š",
    "é‘‘": "è§",
    "æ—ƒ": "å ",
    "èº": "æœˆ",
    "ç‰": "æ°‘",
    "çš¬": "å’Œ",
    "è‹¡": "ä»¥",
    "è´»": "å®œ",
    "é¹œ": "ç‰©",
    "é§•": "é©¾",
    "é¦”": "èµš",
    "æ¿›": "ç›Ÿ",
    "ä»º": "ä»“",
    "é”": "æ¶",
    "é": "åœ°",
    "éˆº": "ç‰",
    "ç½": "ç¾",
    "å¸": "å·",
    "é‚•": "åº¸",
    "è¯¤": "æŒ£",
    "èœ±": "çš®",
    "ç ¾": "åˆ©",
    "ç¦›": "çœŸ",
    "ç¿³": "æ„",
    "æ½‹": "ç»ƒ",
    "é’š": "ä¸",
    "é†´": "æ",
    "ç´": "çº½",
    "æ®³": "ä¹¦",
    "åƒ": "çœ¼",
    "ç¢´": "èŒ¶",
    "çŒ¶": "ç”±",
    "å½": "ä¼Ÿ",
    "åšŒ": "è®°",
    "è°‚": "å®¡",
    "èœ": "å…¶",
    "åˆª": "å±±",
    "å©³": "è¯",
    "é•’": "æ„",
    "æ…®": "ç»¿",
    "è": "å’Œ",
    "ç™¥": "ç",
    "è¤": "ç«¯",
    "æ¨º": "è¯",
    "å”µ": "ä¿º",
    "æ²¤": "æ¬§",
    "æ£": "å­—",
    "æ¬½": "äº²",
    "æ ": "åˆ©",
    "å œ": "ç»ƒ",
    "æ½†": "è¥",
    "æƒ": "æ‰«",
    "é": "åˆ˜",
    "æ±¨": "å¯†",
    "å‚": "æ¢",
    "ç¹š": "èŠ",
    "å”": "åˆ©",
    "ç€µ": "æ„¤",
    "è³®": "è¿›",
    "å¢œ": "ç¼€",
    "å³’": "åŠ¨",
    "æ›Œ": "ç…§",
    "è˜": "è€",
    "å‰‹": "å®¢",
    "é¼": "è€",
    "ç‘": "å‡",
    "æ¥¹": "è¥",
    "ä¹—": "æˆ",
    "è®": "ç‡•",
    "è¬": "è¿·",
    "éƒ": "æ—",
    "é­‘": "åƒ",
    "å®": "çŸ¥",
    "è¼©": "è¢«",
    "è³­": "å µ",
    "è¸": "çƒŸ",
    "é†¬": "é™",
    "æ·µ": "å†¤",
    "é“¯": "è‰²",
    "ç…‰": "ç»ƒ",
    "è”µ": "è„",
    "æ“": "æ ‡",
    "ç‡Š": "æ·±",
    "æ¸Œ": "è·¯",
    "å“‹": "å ",
    "çš": "å±…",
    "æ¦€": "å“",
    "è¥¹": "è¯—",
    "è™¢": "å›½",
    "ç‘—": "é™¢",
    "ç¢": "è™½",
    "ç¹³": "è§’",
    "è¯˜": "æ",
    "ç½µ": "éª‚",
    "é“": "æ‰¹",
    "åŸ„": "å´©",
    "è†‘": "å®¾",
    "æŒ¶": "å±…",
    "é±¸": "èŠ¦",
    "é™": "æ‘‡",
    "ç²ˆ": "æŸ”",
    "æ“¾": "æ‰°",
    "æ“”": "å•",
    "ç½·": "çˆ¸",
    "æ½°": "æºƒ",
    "ç‚…": "çª˜",
    "è¯“": "æ¡†",
    "æˆ•": "æª",
    "æ”": "è§’",
    "å£˜": "å’",
    "é½": "å·¨",
    "é´¨": "å‘€",
    "é‚´": "é¥¼",
    "ç¡ª": "å§",
    "çº­": "äº‘",
    "è™±": "è¯—",
    "æ®": "ç°",
    "ç‹": "è¢",
    "å ‘": "æ¬ ",
    "å»ˆ": "ç…",
    "å›¬": "å›",
    "æ¾": "æ ‘",
    "è¯³": "ç‹‚",
    "é£": "ç¾Š",
    "ç£š": "ä¸“",
    "è„³": "è„‘",
    "æ‹": "æ–¹",
    "æµ„": "é™",
    "å›¿": "å³",
    "è®™": "æ¬¢",
    "çœ": "çœŸ",
    "æ¾": "å’‹",
    "é’­": "å·",
    "æ…³": "å‰",
    "è°¯": "æ¡¥",
    "å€": "ä¹¦",
    "å‡ª": "æŒ‡",
    "å”›": "éª‚",
    "ç¼¶": "å¦",
    "ç—ˆ": "åº¸",
    "è„£": "çº¯",
    "ç¸›": "çˆ¶",
    "æ†‚": "ä¼˜",
    "ç³§": "è‰¯",
    "è¯": "æ•–",
    "ç•¼": "å”±",
    "ç¿¦": "å‡",
    "æº¼": "è¯—",
    "é£": "ç§°",
    "å¨Œ": "æ",
    "é‡": "ç¦»",
    "å¦«": "å½’",
    "èŸº": "å–„",
    "å¢¦": "çƒ¦",
    "é’•": "å¥³",
    "ç‘": "æœº",
    "é’": "ä¸²",
    "å©": "èŠ‚",
    "å ¥": "æ¯›",
    "æ°°": "æƒ…",
    "è°—": "ç¼ ",
    "éŠ³": "ç‘",
    "ç’©": "å–",
    "å°": "èŠ‚",
    "æ¨": "ä¹¦",
    "å¯³": "å®",
    "å·–": "é¢œ",
    "è²¸": "å¸¦",
    "ç»¦": "æ",
    "éŠ‡": "æ³ª",
    "ç–": "å–„",
    "é“‘": "è€",
    "è†½": "èƒ†",
    "è© ": "æ°¸",
    "æ›¬": "ç­›",
    "ç²²": "ç¿",
    "æ €": "çŸ¥",
    "é’": "å‰",
    "ç–ƒ": "å›¢",
    "æ„½": "åš",
    "çª®": "ç©·",
    "çª©": "çª",
    "é“¡": "ç‚¸",
    "æš¢": "å”±",
    "å‰": "æ²™",
    "éµ¬": "æœ‹",
    "æµ": "é—®",
    "æ¹§": "æ°¸",
    "ç¬†": "å…«",
    "ç·©": "ç¼“",
    "å»": "æ¥¼",
    "é‚ˆ": "ç§’",
    "è¡": "ä½",
    "éƒƒ": "å’Œ",
    "é’·": "å¡",
    "èƒ„": "æ˜¼",
    "æ™": "è¥¿",
    "æ³®": "åˆ¤",
    "æ»": "é™¤",
    "éœ°": "ç°",
    "æ¡": "æ¨ª",
    "æ¼": "çŒ",
    "é—–": "é—¯",
    "æ’¥": "æ³¢",
    "å‘–": "åˆ©",
    "åš‡": "ä¸‹",
    "æ£¹": "ç…§",
    "æ€": "æ ·",
    "åŸ­": "å¸¦",
    "çµ¡": "è½",
    "é•": "æ—",
    "ç™’": "ç‰",
    "æ´Œ": "è£‚",
    "è¹š": "æ±¤",
    "å": "æ”¶",
    "æ¶": "æ¥",
    "äº»": "äºº",
    "ç¶œ": "å®—",
    "æ‡¸": "æ—‹",
    "ç‡š": "æ„",
    "è“¼": "äº†",
    "æ´‡": "å› ",
    "ç—¢": "åˆ©",
    "åº¢": "è‡³",
    "æ¿‹": "æ¥š",
    "å¸¯": "å¸¦",
    "å’": "æ€",
    "ç½°": "ç½š",
    "å½": "å¼¯",
    "è®Ÿ": "æ¯’",
    "é˜¡": "å‰",
    "éœ": "ä¾ ",
    "è†»": "å±±",
    "é¢¡": "æ¡‘",
    "èµ“": "è€•",
    "è©³": "ç¿”",
    "æ§": "èŒ¶",
    "æ¡·": "è§‰",
    "æ‡‹": "å¸½",
    "å†«": "å†°",
    "ç§¸": "æ¥",
    "é†®": "æ•™",
    "æ°½": "å",
    "è®´": "æ¬§",
    "å…‚": "èµ",
    "åµ": "è“",
    "é¸·": "è‡³",
    "éƒ": "ç‹¼",
    "å›‰": "ç½—",
    "é­ƒ": "æ‹”",
    "é¼‹": "åŸ",
    "ç± ": "é¾™",
    "é“§": "å",
    "æ’²": "é“º",
    "æ“": "åº¸",
    "è¤": "é€Ÿ",
    "ç•²": "èˆ",
    "æ™Œ": "èµ",
    "å§®": "æ¨ª",
    "å„‡": "å®£",
    "æ¾ª": "é›¶",
    "å£…": "åº¸",
    "é©…": "åŒº",
    "ç¦ª": "ç¼ ",
    "é¥": "å¤œ",
    "è©”": "ç…§",
    "å—°": "è‘›",
    "é¬ƒ": "å®—",
    "å°“": "è€³",
    "äºŸ": "æ",
    "ç³": "ä¸‰",
    "æ“¤": "é†’",
    "ç«œ": "é¾™",
    "ç¾¸": "é›·",
    "éŒ¾": "èµ",
    "ç‘ ": "åˆ˜",
    "æ¹Š": "å‡‘",
    "æ¥£": "æ¢…",
    "å·œ": "å¿«",
    "è‹«": "å±±",
    "èŠƒ": "æœ‹",
    "ç½’": "ç½‘",
    "åƒ®": "åŒ",
    "å“": "æŸ",
    "å®€": "çœ ",
    "å˜": "å•",
    "é¯": "ç‹¼",
    "åª": "æ˜¯",
    "è¬™": "å‰",
    "æ„": "æ•",
    "è™º": "ç°",
    "é‘°": "è¦",
    "æ¨˜": "å”",
    "æ²µ": "ç±³",
    "éª…": "å",
    "è“": "æœ",
    "é”": "å¾·",
    "é†¯": "è¥¿",
    "å‡": "åŠ¨",
    "é’¹": "åš",
    "é¼": "èŠ",
    "æ‚Œ": "æ›¿",
    "é¤…": "é¥¼",
    "ç‹": "é™ˆ",
    "çŸ½": "ç³»",
    "è¹Ÿ": "æœº",
    "å¥‚": "æ¢",
    "æ¥“": "é£",
    "æ’¿": "å‡",
    "å³¤": "æ•™",
    "ä½ƒ": "ç”µ",
    "å¢’": "ä¼¤",
    "å€–": "æ€§",
    "è ": "äº›",
    "çœ€": "æ˜",
    "æ¼¿": "å°†",
    "æª¯": "å°",
    "ç¾£": "ç¾¤",
    "ç–Š": "å ",
    "ä½º": "å…¨",
    "æ‰¦": "å‰",
    "è•ˆ": "è®­",
    "å½": "è½°",
    "å˜†": "æ¢",
    "æ¶ª": "ç¦",
    "è³¤": "è§",
    "ç¥": "è€³",
    "å­›": "è¢«",
    "ç°‹": "é¬¼",
    "è‹•": "å‹º",
    "é°": "æ„",
    "è—": "ç§’",
    "æ°±": "å…»",
    "æ¶¸": "å’Œ",
    "æ’«": "è¾…",
    "é–½": "æ˜",
    "ç•": "æ¯",
    "èŠ©": "ç´",
    "å˜—": "é•¿",
    "çŒ¹": "èŒ¶",
    "å½˜": "è‡³",
    "è°": "èª",
    "æ·¸": "é’",
    "æ³«": "ç‚«",
    "è¦³": "å…³",
    "é†œ": "ä¸‘",
    "æ¡…": "ç»´",
    "ç‘·": "çˆ±",
    "èœ": "ç”±",
    "åº£": "æŒ‘",
    "ç™¿": "èŒ„",
    "æ°š": "ç©¿",
    "çœ¦": "å­—",
    "å ƒ": "æ˜†",
    "ç¯™": "é«˜",
    "ç˜‹": "é£",
    "æ·™": "ä»",
    "è€’": "å’",
    "ç……": "æ–­",
    "ç©‚": "å²",
    "é‰†": "æ€",
    "å‡œ": "å",
    "é¸": "å¤º",
    "ç©": "ä¼š",
    "ä¹„": "äº”",
    "é˜—": "ç”°",
    "ç½‚": "åº”",
    "è˜…": "æ¨ª",
    "å•±": "é¢œ",
    "ç‘­": "å”",
    "èŠª": "å…¶",
    "å±¼": "ç‰©",
    "é¬š": "éœ€",
    "æ´§": "ä¼Ÿ",
    "å‚¢": "å®¶",
    "é˜‡": "ç£",
    "é–˜": "ç‚¸",
    "æ–": "æ‘‡",
    "æ°˜": "åˆ€",
    "é€‘": "æ±‚",
    "é•¬": "æˆ–",
    "å¨›": "é±¼",
    "å«’": "çˆ±",
    "ç¸«": "å¥‰",
    "å‚˜": "ä¸‰",
    "åµ‹": "æ¢…",
    "ç„“": "å«",
    "è €": "åˆº",
    "è ¡": "ç¦»",
    "æ¹Ÿ": "é»„",
    "é€¯": "è·¯",
    "è¢‚": "å¦¹",
    "è€†": "å…¶",
    "é‰…": "å·¨",
    "è¤Ÿ": "ä»–",
    "å¦—": "è¿›",
    "ç¶¿": "çœ ",
    "è³¦": "çˆ¶",
    "ç©”": "é»„",
    "è°": "å",
    "é‚™": "å¿™",
    "ç‚”": "è´µ",
    "ç‡": "ç§€",
    "é’…": "é‡‘",
    "ç¯¤": "å µ",
    "åµ´": "å‡ ",
    "ç©€": "å¤",
    "ä¸¡": "ä¸¤",
    "å½™": "ä¼š",
    "é“¨": "å…¨",
    "ç•‘": "ç”°",
    "ç¥œ": "äº’",
    "æ¶™": "æ³ª",
    "æ’·": "é‹",
    "ç‰": "æ¯’",
    "åˆ‚": "åˆ€",
    "å¤": "èƒ¸",
    "å”": "ä¹Ÿ",
    "è²˜": "å¢¨",
    "å•»": "èµ¤",
    "é·": "é©¬",
    "ç· ": "åœ°",
    "é": "é”",
    "èˆ„": "ç³»",
    "ç¨”": "å¿",
    "å…": "è€Œ",
    "æ‡¶": "æ‡’",
    "å¿¤": "äº”",
    "è²ª": "è´ª",
    "è‰™": "ä»“",
    "é“Š": "ä»–",
    "é±—": "æ—",
    "éš·": "åˆ©",
    "è ¹": "åº¦",
    "éªœ": "å¥¥",
    "ç ¦": "å¯¨",
    "åŒ": "åŠ¨",
    "æ¶": "é¢",
    "æ¹": "å›¢",
    "åŸ•": "æˆ",
    "é•§": "è“",
    "ç": "å¼ ",
    "ç¶": "ç»‘",
    "é“¼": "æ¥",
    "è„©": "ä¿®",
    "å²·": "æ°‘",
    "åŠ¹": "ç¬‘",
    "é§¿": "ä¿Š",
    "å»š": "é™¤",
    "è¤˜": "ç°",
    "å«±": "å¼º",
    "åˆˆ": "æ„",
    "å­º": "å¦‚",
    "æš": "ä¼š",
    "ç˜": "è´ª",
    "è²§": "è´«",
    "ç¶¦": "å…¶",
    "çˆ¨": "çªœ",
    "é—‚": "çº¢",
    "é¡¼": "éœ€",
    "æ‘€": "äº”",
    "æ»ƒ": "ç“®",
    "é–¿": "æ–‡",
    "æº§": "åˆ©",
    "ç£": "ç˜¦",
    "ç´›": "åˆ†",
    "ç«¦": "è€¸",
    "è½": "ä¹¦",
    "è’": "ä¸¾",
    "å©": "æˆ‘",
    "ç•¿": "æœº",
    "èš¬": "æ˜¾",
    "å³‹": "å¯»",
    "æ ¢": "æ‘†",
    "éš¸": "åˆ©",
    "ç¼—": "æ°‘",
    "å­“": "è§‰",
    "é˜†": "ç‹¼",
    "æ³º": "è½",
    "éœˆ": "é…",
    "å¿": "èˆ”",
    "å…–": "çœ¼",
    "é¥¯": "è§",
    "æˆ¯": "å‘¼",
    "å‘°": "å­",
    "ç”™": "å¸¦",
    "å«š": "æ…¢",
    "ç¶´": "ç¼€",
    "é•„": "è´¹",
    "ç™": "èµ–",
    "è¿¨": "å¸¦",
    "ç»”": "è£¤",
    "è…†": "èˆ”",
    "è‚…": "é€Ÿ",
    "è‡¥": "å§",
    "åœ»": "å…¶",
    "ç˜˜": "æ¼",
    "å±¢": "æ—…",
    "é…¢": "ä¿ƒ",
    "è¾»": "çŸ³",
    "æ“´": "æ‰©",
    "æ’³": "äº²",
    "å¾“": "ä»",
    "æ†¶": "æ„",
    "é¯¨": "ç²¾",
    "è²¿": "å¸½",
    "ç£": "å¯»",
    "åµ¬": "ç»´",
    "ç¢©": "ç¡•",
    "è‰¶": "ç‡•",
    "å£©": "çˆ¸",
    "ç©": "å½±",
    "æ›·": "å’Œ",
    "éŠœ": "é—²",
    "ç¡": "åŠ¨",
    "éŸ«": "è¿",
    "å¯›": "å®½",
    "é¢€": "å…¶",
    "è°Ÿ": "é­”",
    "æ»Ÿ": "ç‡•",
    "ç¬„": "æœº",
    "ç¢": "å…¶",
    "é–¥": "ç½š",
    "ç¼‘": "å‹¾",
    "é": "é±¼",
    "é’Œ": "äº†",
    "è‹Œ": "é•¿",
    "è±¢": "æ¢",
    "é–©": "æ•",
    "éª¶": "åº•",
    "æ£‚": "é›¶",
    "é¹": "è¦",
    "é‘’": "è§",
    "ç“Š": "ç©·",
    "å„": "å‚»",
    "æ„ ": "è¿",
    "å±™": "é˜¿",
    "å­–": "å¦ˆ",
    "é’¿": "ç”µ",
    "å…²": "å¤©",
    "é”·": "æ¶",
    "è½­": "æ¶",
    "çª": "å½’",
    "ç¤¡": "åš",
    "å•": "ç€",
    "ä¸Œ": "æœº",
    "å£º": "èƒ¡",
    "è›­": "è‡³",
    "å¢": "é”…",
    "å¸": "ç»´",
    "å¾‡": "è®­",
    "è©¢": "å¯»",
    "ç¬ª": "è¾¾",
    "å¥": "ä¸",
    "æ®›": "æ",
    "æ²": "æ",
    "æ»¬": "äº’",
    "è¨º": "æ•",
    "éš¹": "è¿½",
    "ç£™": "æ»š",
    "å £": "é±¼",
    "ä¹º": "é”",
    "è‹‹": "ç°",
    "é›¹": "è–„",
    "å•¶": "å®š",
    "ç¹‡": "æ‘‡",
    "å‡«": "ç¦",
    "æŠŸ": "å›¢",
    "æ²„": "äº‘",
    "ç¿•": "è¥¿",
    "ç": "ä¸",
    "é": "ç½—",
    "åƒ‘": "æ¡¥",
    "è™§": "äº",
    "ç¯": "é»„",
    "ç‘€": "é›¨",
    "åµŠ": "èƒœ",
    "æŸ°": "è€",
    "å©º": "ç‰©",
    "æ´¹": "ç¯",
    "å–°": "å‚",
    "é’‡": "ä»¥",
    "é•Š": "è‚",
    "è¤²": "è£¤",
    "è…“": "è‚¥",
    "è•²": "å…¶",
    "æª„": "ä¹ ",
    "ç¬‰": "å¯",
    "é™Ÿ": "è‡³",
    "ç²": "ç°",
    "çŸ‡": "ç›Ÿ",
    "è®»": "èƒ¸",
    "ä½—": "é©®",
    "è½¸": "æ•",
    "å´": "è„‘",
    "å´§": "æ¾",
    "æ¾": "é—®",
    "é“ˆ": "æ˜¯",
    "æ¸¾": "é­‚",
    "ä¸¼": "äº•",
    "å°›": "é­”",
    "é Œ": "å®‹",
    "å‡¼": "è¡",
    "æŒ›": "é¸¾",
    "é–": "å°˜",
    "å¡": "æ¯”",
    "æŒˆ": "å¦¾",
    "ä½¾": "æ„",
    "é»±": "å¸¦",
    "ä¿": "ç»„",
    "èµ­": "è€…",
    "è¶¨": "åŒº",
    "å¬­": "å¥¶",
    "æ’": "ç§°",
    "ç£¯": "æœº",
    "è‡Ÿ": "è—",
    "æ»„": "ä»“",
    "è–œ": "å¿…",
    "è§š": "å§‘",
    "æ—–": "ä»¥",
    "è‰": "æ‰‹",
    "è‹»": "ç¦",
    "çµŒ": "ç²¾",
    "ç­": "å¡”",
    "çº": "ç¼ ",
    "çº”": "æ‰",
    "é‘¾": "é¸¾",
    "èµŸ": "æ™•",
    "äº": "å¤„",
    "èŠ«": "é¢œ",
    "å‘“": "æ„",
    "è“": "çœŸ",
    "é¦³": "æŒ",
    "ç¦": "æˆ–",
    "æ‰": "æ„Ÿ",
    "å¸³": "å¸",
    "æ°™": "å…ˆ",
    "ç¼ˆ": "ç§’",
    "æ“˜": "æ°",
    "åº–": "è¢",
    "éª§": "ç›¸",
    "è¨£": "è§‰",
    "å¢Š": "ç”µ",
    "ç€§": "é¾™",
    "å«°": "å«©",
    "åˆ–": "æœˆ",
    "ç¦¦": "ç‰",
    "å§©": "å¿µ",
    "å°…": "å®¢",
    "å‰£": "è§",
    "é†…": "åŸ¹",
    "é”•": "å•Š",
    "åœª": "æ­Œ",
    "é››": "é™¤",
    "é‘²": "ç›¸",
    "ç¹©": "ç”Ÿ",
    "ç ": "å€Ÿ",
    "æ¾": "é™¢",
    "å€Œ": "å…³",
    "ç¬": "äº’",
    "è«": "æ˜¯",
    "èŒ†": "æ¯›",
    "ç‘": "å¸½",
    "ç–½": "å±…",
    "çœŒ": "ç°",
    "æ“‡": "åˆ™",
    "é ƒ": "è¯·",
    "ç»¶": "ç˜¦",
    "èˆ›": "å–˜",
    "å”Œ": "é—²",
    "åµ": "çœŸ",
    "æ¬„": "è“",
    "è°ª": "å“²",
    "è»¢": "è½¬",
    "æ½": "è¿",
    "è°¶": "è¡¬",
    "è¤œ": "è¢",
    "æ¸": "ç—›",
    "è": "å…¶",
    "å¨ˆ": "é¸¾",
    "é£½": "å®",
    "é¥´": "å®œ",
    "ä¸·": "å…«",
    "å­ƒ": "å¨˜",
    "ç´®": "åŒ",
    "ç¶‰": "é€",
    "ç¾Ÿ": "æŠ¢",
    "é¬": "å·",
    "éŸ": "ç”Ÿ",
    "å›˜": "å›",
    "è³¬": "å¸",
    "é„": "é“¶",
    "é’": "åœŸ",
    "é—¢": "å±",
    "éŒ˜": "å‚",
    "éƒ": "å¤¹",
    "è„˜": "ç¢—",
    "é³…": "ç§‹",
    "éš": "é™",
    "æ¥¦": "ç‚«",
    "è†©": "é€†",
    "è¾¯": "å˜",
    "å²¢": "å¯",
    "éˆˆ": "æ‰¹",
    "ç‡§": "å²",
    "å‚µ": "å¯¨",
    "æ¥¸": "ç§‹",
    "æ› ": "çŸ¿",
    "å¨š": "å—",
    "é¾‡": "å§¿",
    "ç»€": "å¹²",
    "å‰¡": "å–„",
    "æ…£": "çŒ",
    "è…¸": "é•¿",
    "å‚©": "æŒª",
    "æ›ˆ": "åŒ",
    "è¾": "å®¹",
    "æ­´": "åˆ©",
    "å¸°": "å½’",
    "ç£”": "å“²",
    "éƒ§": "äº‘",
    "ä¾¶": "æ—…",
    "æ¼": "é±¼",
    "ä½¶": "æ",
    "å†¨": "çˆ¶",
    "è‡Œ": "å¤",
    "é†ª": "åŠ³",
    "æ¨¯": "å¼º",
    "é­": "ç½‘",
    "æ§¸": "æ„",
    "ä½¹": "é¬¼",
    "è¦": "è™¾",
    "é¯›": "é›•",
    "é€…": "å",
    "é¸«": "ä¸œ",
    "é¹³": "çŒ",
    "ç¡·": "å‡",
    "åš©": "é­”",
    "ç¯ ": "å°",
    "é”²": "å¦¾",
    "æ²­": "æ ‘",
    "åš¥": "ç‡•",
    "ä¹‚": "æ„",
    "çŸ¬": "æ“",
    "é”˜": "è¯º",
    "è¯®": "å·§",
    "ç» ": "æ¢—",
    "åº ": "ç¿”",
    "é¤“": "æ¶",
    "ç¨œ": "æ£±",
    "ç¥‰": "æŒ‡",
    "æ´": "ä¹–",
    "å™": "é¥¿",
    "è–¬": "è¦",
    "å©": "å°±",
    "ç±¤": "å‰",
    "å›±": "èª",
    "è¸": "é±¼",
    "èª­": "æ¯’",
    "å›¹": "é›¶",
    "æ“ ": "å‡ ",
    "é¢": "ç§‘",
    "ç¹†": "è°‹",
    "æ „": "å®¹",
    "ç¶º": "èµ·",
    "ç©°": "åš·",
    "ç–”": "ä¸",
    "è¼": "çº¯",
    "å‡ƒ": "å›¾",
    "å¢": "ä»",
    "é•": "ä½",
    "é‘": "è¾¾",
    "è¹¼": "æ™®",
    "æ§¬": "è¯",
    "å„Ÿ": "é•¿",
    "ç°Ÿ": "ç”µ",
    "è…ˆ": "ç²¾",
    "åº¹": "å¦¥",
    "å—µ": "é€š",
    "éº": "ç»„",
    "ç²": "å“²",
    "è˜‹": "å¹³",
    "çªˆ": "å’¬",
    "éƒ„": "å¦¾",
    "é’²": "ç",
    "å±¾": "æ·±",
    "æ—": "ä½ ",
    "æ¡¡": "æ‰°",
    "é¹¬": "ç‰",
    "æšˆ": "æ™•",
    "è˜¿": "ç½—",
    "è‰‚": "é€¢",
    "èµ‰": "èµ–",
    "è¶": "è‚",
    "å¶ˆ": "æª",
    "ç‘": "æ˜",
    "ç„": "ç‰©",
    "é…": "ä¿®",
    "è¸µ": "ç§",
    "æ»“": "å­",
    "æ¥—": "è§",
    "å·”": "é¢ ",
    "ç¢“": "å¯¹",
    "çŠ": "ç‰™",
    "ç¢›": "æ°”",
    "å«µ": "äº”",
    "åº¡": "ä»¥",
    "é‡£": "æ‰",
    "é’¬": "ç«",
    "å³": "é¢œ",
    "å†¦": "æ‰£",
    "è¥¦": "å¦‚",
    "å": "çŸ¿",
    "é›«": "å“ª",
    "è«¡": "æ˜¯",
    "è‹†": "åˆ‡",
    "è¨·": "æ·±",
    "å’­": "æœº",
    "ç’˜": "æ—",
    "å–š": "æ¢",
    "é”¨": "å…ˆ",
    "æƒ‡": "è¹²",
    "ç·‘": "ç»¿",
    "æ°­": "ä¸œ",
    "çš": "å”±",
    "æŸ": "å’‹",
    "é”´": "å‡¯",
    "æ¯–": "å¿…",
    "é¦": "ä¿®",
    "æ¹": "èµ¤",
    "é„£": "å¼ ",
    "å„†": "äº•",
    "èŠ˜": "çš®",
    "ä¿š": "æ",
    "åŸ": "å¿µ",
    "å–‘": "å› ",
    "è§¥": "å·¥",
    "å›Ÿ": "ä¿¡",
    "è‚“": "è’",
    "è ›": "ç­",
    "é»œ": "å¤„",
    "æ­™": "è®¾",
    "ç‹¹": "ä¾ ",
    "é¬»": "ç‰",
    "äº€": "å½’",
    "æŸ¢": "åº•",
    "èª•": "è›‹",
    "å•®": "è‚",
    "è°°": "è“",
    "äºŠ": "æ˜¯",
    "ä½½": "æ¬¡",
    "äº…": "è§‰",
    "å³‡": "å…«",
    "é—©": "æ‹´",
    "æ£¢": "ç½‘",
    "åœ§": "å‘€",
    "æ¨": "è£…",
    "è§ª": "æ˜Ÿ",
    "å¤‹": "ç¾¤",
    "é‹†": "äº‘",
    "ç€°": "è¿·",
    "å‘¶": "è„‘",
    "æ°º": "æ°´",
    "è‘¯": "è¦",
    "åœ": "åœˆ",
    "èŸ": "æ˜",
    "ä¹€": "ç¦",
    "æ”œ": "é‹",
    "ä»µ": "äº”",
    "ç²³": "ç²¾",
    "éƒ…": "è‡³",
    "çˆ¿": "ç›˜",
    "é†£": "å”",
    "æ¿¤": "æ",
    "è‘": "æ",
    "æ²¨": "é£",
    "è‡¬": "è‚",
    "å±": "è¡",
    "æ¦·": "ç¡®",
    "å•­": "èµš",
    "é©": "å²",
    "é’ƒ": "ç«¹",
    "è†š": "å¤«",
    "æ½º": "ç¼ ",
    "è·–": "ç›´",
    "è–™": "æ›¿",
    "é—³": "çº¢",
    "ç³": "å¸¦",
    "é“·": "å¦‚",
    "è…´": "é±¼",
    "å¹µ": "é—´",
    "å‘’": "äº”",
    "é¹—": "æ¶",
    "èŸ¬": "ç¼ ",
    "æˆ¢": "æ",
    "è‡": "æ€§",
    "æ¬‘": "æ”’",
    "èª…": "æœ±",
    "é˜": "ç‚¸",
    "é› ": "æ„",
    "è¶": "è¡€",
    "é™": "ç”Ÿ",
    "æ»˜": "æ•™",
    "æ”¤": "è´ª",
    "å€¤": "ç›´",
    "æ§‘": "æ¢…",
    "ç‰¸": "å­—",
    "å¡©": "é¢œ",
    "ç»—": "è¡Œ",
    "è³¯": "èƒ¸",
    "ç¬•": "å‡",
    "ç±ƒ": "è“",
    "å¯¢": "å¯",
    "åŸ": "ç¾Š",
    "é±·": "æ¶",
    "ç¯¦": "å¿…",
    "é‘»": "åˆ¤",
    "ä¿¦": "æ„",
    "çœš": "çœ",
    "å‘‹": "å¤«",
    "é§’": "å±…",
    "èƒ”": "å­—",
    "é‘„": "åŠ©",
    "é©·": "å››",
    "æ¥«": "æ",
    "ç»Œ": "å¤„",
    "æ¼‰": "è·¯",
    "å´": "ç",
    "æ’š": "å¹´",
    "å”³": "åˆ©",
    "ç‘©": "è¥",
    "æ»": "é¾™",
    "çŒŠ": "å°¼",
    "å½€": "å¤Ÿ",
    "ç„¼": "çƒ§",
    "ç¬‚": "å®Œ",
    "é—˜": "è±†",
    "çŒ—": "ä¸€",
    "å—": "å®³",
    "å’": "å¼±",
    "å‡‡": "æ¾",
    "ç¢²": "åœ°",
    "è°": "å§‘",
    "å º": "å€Ÿ",
    "è™»": "ç›Ÿ",
    "èª¼": "æ„",
    "ç¯": "è€",
    "è£¾": "å±…",
    "æ†": "è¿",
    "è¢¢": "åˆ¤",
    "å§€": "å’Œ",
    "æ»™": "ä¼š",
    "æ¤œ": "å‡",
    "æº": "å”",
    "é«ª": "å‘",
    "ç„˜": "åˆ°",
    "æ§”": "é«˜",
    "é¾…": "åŒ…",
    "é­‰": "ä¸¤",
    "æ«ƒ": "è´µ",
    "æ‹": "æ‹¿",
    "è„²": "å°¿",
    "ç¼Ÿ": "æ",
    "åªª": "è¢„",
    "å²": "åˆ©",
    "å ¯": "æ‘‡",
    "è‡˜": "èœ¡",
    "å…•": "å››",
    "è¸…": "å­¦",
    "ç®…": "å¿…",
    "åœ‰": "é›¨",
    "å© ": "å¼¯",
    "æ‚­": "å‰",
    "æ•˜": "ç»­",
    "å¤‚": "æŒ‡",
    "è¤¨": "é”",
    "ç¬ˆ": "æ",
    "äº¾": "ç‹",
    "é²¢": "è¿",
    "è³°": "æ˜¥",
    "é¨’": "éªš",
    "æ€¿": "æ„",
    "ç¨²": "åˆ°",
    "æ˜": "åˆ†",
    "æ¨¸": "æ™®",
    "ç¬³": "å®¶",
    "ç–¥": "å€Ÿ",
    "é¨“": "ç‡•",
    "ç®‡": "ä¸ª",
    "é€‹": "ä¸",
    "æ¬¹": "ä¸€",
    "ç—¾": "é˜¿",
    "åŸµ": "æœµ",
    "éµ": "é¢",
    "å—¥": "è±ª",
    "ç Ÿ": "çœ¨",
    "èªŠ": "è…¾",
    "èŒ”": "è¥",
    "è´‡": "æ™•",
    "å±®": "æ’¤",
    "ç¬": "åƒ",
    "å‰": "æ³¢",
    "é—¼": "è¸",
    "é‰›": "å‰",
    "é’£": "æ¿",
    "ç¶": "é™ˆ",
    "æª": "ç§’",
    "ç¿€": "å†²",
    "è…­": "æ¶",
    "éƒ¤": "ç³»",
    "åƒ¦": "å°±",
    "é–±": "æœˆ",
    "é’ª": "æŠ—",
    "é‘¿": "å‡¿",
    "é½£": "å‡º",
    "èˆ¸": "è‘›",
    "æ±œ": "å››",
    "ç†·": "å¢",
    "å´‘": "æ˜†",
    "åƒ–": "è¥¿",
    "ä¸„": "ä¸Š",
    "å­": "ç‡•",
    "è«­": "ç‰",
    "æ": "èª",
    "çƒ": "ç",
    "å¬°": "åº”",
    "å’¥": "ç³»",
    "èš“": "å¼•",
    "è¯œ": "æ·±",
    "æº±": "ç´",
    "çŠ»": "åš",
    "ç¼„": "é—´",
    "é¸®": "æ¶ˆ",
    "ç¯¾": "ç­",
    "è˜": "æ¾",
    "é¥’": "æ‰°",
    "å…˜": "ä½¿",
    "å”»": "èµ–",
    "é°»": "ç’",
    "ç’º": "é—®",
    "å¡": "ç‘",
    "åŠ¢": "å–",
    "æ¹œ": "çŸ³",
    "è¾¶": "ç»°",
    "ç½…": "ä¸‹",
    "çª¨": "ç†",
    "æª©": "å",
    "ç½´": "çš®",
    "é‚¾": "æœ±",
    "é»™": "å¢¨",
    "é„’": "é‚¹",
    "æ¡–": "è¡€",
    "å©“": "é£",
    "ç¼±": "æµ…",
    "è": "å¿",
    "æ´„": "å›",
    "éœ‚": "æœ¨",
    "çº": "å–‰",
    "é ¸": "äº•",
    "èŠ°": "è®°",
    "æ½": "ç¢°",
    "è›": "ç§°",
    "ç¯": "ç®¡",
    "è”¸": "å…œ",
    "å€¬": "æ‰",
    "è¦š": "è§‰",
    "çƒœ": "é€‰",
    "è–®": "æœ",
    "æ°": "æˆ‘",
    "çœ‡": "ç§’",
    "é˜": "çˆ¶",
    "å¶¼": "é›¨",
    "å³½": "ä¾ ",
    "çº›": "åˆ°",
    "éƒ·": "ç›¸",
    "ç»±": "ä¸Š",
    "ç¨Ÿ": "é¥¼",
    "é£¼": "å››",
    "å¶„": "å±•",
    "å½µ": "å¦¥",
    "å‹µ": "åˆ©",
    "é•²": "å·®",
    "è±": "æ ‘",
    "é„¯": "å–„",
    "è„…": "é‹",
    "å´®": "å›º",
    "å—Œ": "çˆ±",
    "é„±": "å©†",
    "å¢€": "æŒ",
    "é¢³": "ç“œ",
    "æ¼¯": "è½",
    "è“‘": "ç¼©",
    "è³ ": "åŸ¹",
    "ç––": "æ¥",
    "è±ˆ": "èµ·",
    "ç»‹": "ç¦",
    "éšˆ": "å¾®",
    "æ´³": "å…¥",
    "å“™": "å¿«",
    "é¹‚": "ç¦»",
    "ç‘¤": "æ‘‡",
    "æ±Š": "å·®",
    "éº´": "åŒº",
    "èœš": "é£",
    "çµ¨": "å®¹",
    "è°‰": "å®¡",
    "é‡˜": "ä¸",
    "å‹»": "äº‘",
    "è§´": "ä¼¤",
    "å²˜": "ç°",
    "éªˆ": "å",
    "å£†": "å­¦",
    "æ«£": "è¿",
    "å¦ª": "ç‰",
    "è–¹": "å°",
    "é²ˆ": "èŠ¦",
    "è·¶": "è¾¾",
    "å­": "å°¼",
    "å§ª": "ç›´",
    "æ´®": "æ¡ƒ",
    "ç®§": "å¦¾",
    "æŒ¹": "æ„",
    "å›": "å‡",
    "ä¹©": "æœº",
    "èŠ‘": "èµ·",
    "ä¹š": "å¼•",
    "ç": "å›°",
    "è…«": "ç§",
    "å²": "çŒœ",
    "è¥": "è¡Œ",
    "æ“²": "è‡³",
    "è›”": "å›",
    "è®£": "çˆ¶",
    "è†¦": "å",
    "é¤®": "è´´",
    "å¶½": "æœˆ",
    "å€¨": "å·¨",
    "ç ­": "ç¼–",
    "ç‰€": "åºŠ",
    "æ¿": "å¿…",
    "è¬¹": "ç´§",
    "è…": "è‚¾",
    "åœŒ": "ä¼ ",
    "è‹œ": "æœ¨",
    "è‚±": "å·¥",
    "èƒ´": "åŠ¨",
    "çºŸ": "æ€",
    "çŠ½": "äºš",
    "è‚¼": "äº•",
    "é…Š": "ä¸",
    "ç¶": "è™½",
    "ç±": "è…¾",
    "æ¸‰": "è®¾",
    "ç¼¯": "å¢",
    "æ‚†": "ç‰",
    "é””": "å±…",
    "æ¬": "ç½—",
    "è¬‡": "å‡",
    "æ©": "é©®",
    "éœ": "ç…",
    "æ®„": "èˆ”",
    "ç¦‡": "è€…",
    "é‰¤": "å‹¾",
    "å™ ": "ç­”",
    "éƒ‡": "ç¯",
    "è•–": "å–",
    "æˆ†": "æ ",
    "å‹­": "åŒ",
    "å‹–": "ç»­",
    "å©­": "äºš",
    "å’›": "å‡",
    "ç‡¦": "ç¿",
    "è‚Ÿ": "å§",
    "æŠº": "å¦¹",
    "å¼­": "ç±³",
    "è¶¸": "è¹²",
    "è•": "çŸ³",
    "æµ": "ç€",
    "è‡": "ç¿ ",
    "åµ": "ä¿º",
    "ç¬±": "ç‹—",
    "é‚›": "ç©·",
    "å¿œ": "åº”",
    "å¢®": "å •",
    "å¢³": "åŸ",
    "æ“": "æœ",
    "èšº": "ç„¶",
    "ç»": "å¸¦",
    "é«": "ç§‘",
    "è¹": "å",
    "ç°¾": "è¿",
    "å¶": "æ€",
    "é ¼": "èµ–",
    "è‹„": "å˜",
    "å™¤": "è¿›",
    "éˆ": "å›",
    "è“–": "å¿…",
    "å»»": "å›",
    "è‡µ": "æ ¼",
    "æ…„": "åˆ©",
    "ç´˜": "çº¢",
    "æŸ©": "å°±",
    "å©": "æ¶",
    "è …": "è¥",
    "å†": "çª˜",
    "æ§­": "ä¸ƒ",
    "å¹·": "ç—…",
    "ç·": "æœº",
    "çº¡": "è¿‚",
    "é·²": "å°±",
    "è¿¤": "å®œ",
    "èŠ": "åº¦",
    "æ˜‰": "è®¿",
    "æ²”": "å…",
    "åµ": "å¡",
    "éƒ¯": "è°ˆ",
    "æ¡ ": "å‘€",
    "ç¿¹": "å·§",
    "å¿„": "å¿ƒ",
    "é©½": "åŠª",
    "åŸ§": "å·¨",
    "ä½¥": "å‰",
    "è•¤": "ç‘",
    "ç¬¥": "å››",
    "æ¥ƒ": "å§",
    "é¢": "è‚",
    "éª“": "è¿½",
    "å—ª": "ç´",
    "æ—’": "åˆ˜",
    "å²œ": "å…«",
    "å£": "çƒŸ",
    "ç¦³": "åš·",
    "æ«": "åŠ¨",
    "æ¨©": "å…¨",
    "åŒš": "æ–¹",
    "æ„†": "å‰",
    "ç‰–": "æœ‰",
    "é’": "å±±",
    "é²Ÿ": "å¯»",
    "æ ": "çœ‹",
    "è¯Œ": "å‘¨",
    "ä»‰": "é•¿",
    "å¼": "æ’¤",
    "é¯Š": "æ²™",
    "è²©": "é¥­",
    "ç¡": "è¡Œ",
    "èµ": "æœº",
    "æ´": "è®°",
    "æŒ": "ç‰©",
    "éˆ”": "è¶…",
    "é´": "æ—",
    "åœœ": "ç¯",
    "é½¢": "é›¶",
    "è²³": "äºŒ",
    "ç˜¿": "å½±",
    "é••": "å®¹",
    "å‘­": "æ„",
    "æ†¿": "è§’",
    "èŠ": "ç¼",
    "çµ": "è§’",
    "é œ": "å’Œ",
    "è‚«": "å‡†",
    "é¢›": "ä¸“",
    "è¨³": "æ„",
    "é¬±": "ç‰",
    "é†Œ": "æ˜†",
    "å“": "å†œ",
    "ç´¡": "è®¿",
    "å°¢": "ç”±",
    "å¿–": "å­˜",
    "ç‚˜": "å¿ƒ",
    "é“’": "è€³",
    "ç£´": "å‡³",
    "è«§": "é‹",
    "ç¼³": "ç¯",
    "çº¥": "æ­Œ",
    "åš§": "èŠ¦",
    "è²¶": "æ‰",
    "çª¯": "æ‘‡",
    "å¬—": "å–„",
    "éƒ¸": "å•",
    "è¥ ": "å½“",
    "å´™": "ä¼¦",
    "å½Š": "é™",
    "æ°¿": "é¬¼",
    "å¹›": "å¸",
    "ç»»": "çŠ¬",
    "éª¢": "èª",
    "é•›": "åº¸",
    "å­€": "åŒ",
    "ç²„": "æ¿",
    "è£¨": "å¿…",
    "èª": "å­™",
    "ç¤™": "çˆ±",
    "èƒª": "èŠ¦",
    "æˆ": "å¤¹",
    "è„¿": "æ ‡",
    "é€–": "æ›¿",
    "è…": "è¥¿",
    "é€": "é²",
    "è˜†": "èŠ¦",
    "è†": "å¿…",
    "åš": "ä¸€",
    "éº¸": "å¤«",
    "çŸ": "è§‰",
    "ç‡”": "çƒ¦",
    "æµ¥": "æ„",
    "ç¢£": "èŠ‚",
    "æ¤Ÿ": "æ¯’",
    "é¯": "é—´",
    "è ": "è®°",
    "é•«": "å‡³",
    "å ": "å…”",
    "æ‹°": "æ‚¨",
    "èŠ£": "ç¦",
    "å•«": "è€…",
    "æ¸": "æ‰",
    "ç‹": "ä¾ ",
    "å’¤": "å’‹",
    "éƒ“": "è¿",
    "å‚›": "æ°¸",
    "æ‚©": "è„‘",
    "å‘‘": "å",
    "æ”": "ç½—",
    "èŸ": "å›¾",
    "æ´°": "å·¨",
    "å´†": "ç©º",
    "å±": "æœº",
    "ç«½": "é±¼",
    "å—¾": "æœ",
    "é³©": "çº ",
    "ä¿¿": "è™",
    "è‰¸": "è‰",
    "è±¸": "è‡³",
    "æ””": "è“",
    "ç»‚": "ç¦",
    "å„·": "åˆ©",
    "è¡²": "é‚£",
    "è‹˜": "è¯·",
    "ç œ": "é£",
    "æ´": "è¡Œ",
    "éª–": "å‚",
    "ç": "ç¦»",
    "ç˜¥": "æ‹†",
    "ä½": "å®",
    "å½³": "èµ¤",
    "é¾˜": "è¾¾",
    "ç €": "è¡",
    "åº": "åŒ",
    "ç›": "å’Œ",
    "çƒ€": "å‘¼",
    "å³„": "æ„",
    "ç©‘": "è‰²",
    "é˜Š": "æ˜Œ",
    "è‡‹": "å",
    "ç¡–": "ä¾ ",
    "å—«": "è‚",
    "è‹´": "å±…",
    "é¾”": "å·¥",
    "èª‡": "å¤¸",
    "æ§“": "æ ",
    "æ„ª": "äº‘",
    "å‘™": "é”…",
    "è›": "åœ",
    "é†": "çŒ",
    "å£": "æœ‰",
    "èƒ": "å•",
    "é–»": "é¢œ",
    "æ»€": "å¤„",
    "æ€™": "äº’",
    "è¯’": "å®œ",
    "å–«": "åƒ",
    "ç¼‚": "å®¢",
    "æ±": "å°",
    "é•±": "æ„",
    "è–«": "ç†",
    "è®µ": "å·¨",
    "æ©›": "è§‰",
    "èš§": "å€Ÿ",
    "è˜Š": "è¿",
    "åº‘": "äº”",
    "é£§": "å­™",
    "ç‘£": "é”",
    "æƒ˜": "ç½‘",
    "æ·ª": "ä¼¦",
    "æ±…": "å…",
    "å„‹": "å•",
    "é†­": "å“º",
    "èŸ®": "å–„",
    "æ’˜": "ç­”",
    "ç™": "ç­",
    "çŸ¸": "å¹²",
    "è‹¼": "ç”Ÿ",
    "å˜Œ": "ç¥¨",
    "æ¼": "åŠ©",
    "å©": "æ¥¼",
    "é“™": "è„‘",
    "è­": "çº¢",
    "æ": "å·",
    "æ­": "æ‘†",
    "ç¾¨": "ç°",
    "ç—©": "ç˜¦",
    "ç¼¡": "ç¦»",
    "å§": "æ",
    "é–”": "æ•",
    "æ£—": "æ—©",
    "è°¡": "é€Ÿ",
    "ç³": "æ„¤",
    "é¢¯": "è¨",
    "ç·¬": "å…",
    "èˆ": "é±¼",
    "æ•“": "å¤º",
    "è°²": "è§‰",
    "é¾Š": "ç»°",
    "è‰±": "é—´",
    "ç‚†": "æ–‡",
    "åº‹": "é¬¼",
    "éˆ": "å‹¾",
    "é€•": "é™",
    "ç®œ": "ç©º",
    "èŠ": "ç²¾",
    "è ƒ": "è£¸",
    "ç¨—": "è´¥",
    "æ¹‰": "ç”°",
    "å©¼": "ç»°",
    "èŸ¼": "äº•",
    "èš¶": "æ†¨",
    "å²½": "ä¸œ",
    "çœ­": "è™½",
    "å¡†": "å¼¯",
    "å‘®": "æ°”",
    "å´": "çœ‹",
    "æ–¿": "ç”±",
    "æ…Š": "æ¬ ",
    "çª‰": "é¥¼",
    "å™‚": "å°Š",
    "ç¢‡": "å®š",
    "æ¥": "ç»ƒ",
    "é±‚": "å°†",
    "éˆ£": "æ¦‚",
    "æ†£": "ç¿»",
    "é…½": "ç‡•",
    "æ¯¬": "æ±‚",
    "æ±": "é¥­",
    "å¾¼": "è§’",
    "ç‘¯": "ç‹¼",
    "æ†": "å¥",
    "æº": "æ‘‡",
    "é“˜": "çˆ·",
    "èŠ—": "ç›¸",
    "è§": "è¿›",
    "è›©": "ç©·",
    "è³": "è±†",
    "é‹¸": "å·¨",
    "é•¹": "ä¹…",
    "éœŠ": "é›¶",
    "é•‹": "èºº",
    "æ¿": "ç€",
    "é–¤": "æ ¼",
    "ç™": "æ‹±",
    "è¢ª": "åŒº",
    "èš¨": "ç¦",
    "æ¦§": "åŒª",
    "è»½": "è‡³",
    "å†‚": "çª˜",
    "ç•¹": "ç¢—",
    "å»": "æŒ",
    "æ€": "åš",
    "æ»·": "é²",
    "é”’": "ç‹¼",
    "å›·": "ç¾¤",
    "èƒ±": "å…‰",
    "è€„": "å¸½",
    "å¨ ": "æ·±",
    "ç†¸": "é—´",
    "å­’": "è§‰",
    "å†´": "äº’",
    "ç »": "é¾™",
    "æ²©": "ç»´",
    "æ”µ": "é“º",
    "æ¾€": "è‰²",
    "æ€†": "åˆ›",
    "ç…¥": "æ¢",
    "æ°·": "å†°",
    "ä¾ª": "æŸ´",
    "å¼”": "æ‰",
    "å¦˜": "äº‘",
    "å¥": "è¿",
    "é°“": "å¡",
    "æ ‰": "è‡³",
    "å•‰": "æ—",
    "è´ ": "åŸ",
    "æ¿¬": "ä¿Š",
    "è˜": "è£…",
    "é¼": "é©®",
    "ä¿·": "è´¹",
    "å ›": "å¿…",
    "è©": "ç§‹",
    "è”­": "å› ",
    "é«¡": "æ˜†",
    "é‹ˆ": "ç‰©",
    "éˆª": "æ¶",
    "å¿•": "æ˜¯",
    "é“•": "æœ‰",
    "ç˜": "ç¿ ",
    "å«‘": "è–„",
    "çµ†": "åŠ",
    "å¸™": "è‡³",
    "è—": "æ",
    "èŠ´": "ç‰©",
    "æ¸ˆ": "è®°",
    "è§³": "èƒ¡",
    "é»‰": "çº¢",
    "æ¸ƒ": "å¼±",
    "çº–": "å…ˆ",
    "è´…": "ç¼€",
    "åŸ—": "ä¸",
    "èŒ­": "æ•™",
    "é‹…": "å¿ƒ",
    "æ¿": "æ ¼",
    "è™œ": "é²",
    "é¹•": "èƒ¡",
    "æˆœ": "å ",
    "ç´³": "æ·±",
    "ç“™": "åˆ°",
    "è°–": "å®£",
    "é”": "å¼€",
    "é§": "åš",
    "è’Œ": "æ¥¼",
    "éš": "é»„",
    "è°€": "é±¼",
    "è‘¶": "åœ",
    "é‹": "æ»¡",
    "ç‘": "æ´’",
    "è©ˆ": "åˆ©",
    "å ¿": "å‡",
    "å¤¼": "æ—·",
    "é‘·": "è‚",
    "å¼¢": "æ",
    "å¡": "æˆ",
    "æ—‚": "å…¶",
    "ç¾§": "ç¼©",
    "å¸±": "æ„",
    "å¢¾": "è‚¯",
    "é¸ª": "å§‘",
    "æ½´": "æœ±",
    "éŠ": "å¥¥",
    "çŒ": "æ»¡",
    "å‚¥": "èºº",
    "èƒ": "ç“œ",
    "è‹": "ä¸ƒ",
    "èµ€": "å§¿",
    "ä¿£": "é›¨",
    "é‚³": "æ‰¹",
    "è®": "å ",
    "æš±": "é€†",
    "é²µ": "å°¼",
    "äº½": "æ",
    "åš­": "åŒ¹",
    "èµœ": "åˆ™",
    "æš¾": "å",
    "å™‘": "è±ª",
    "ç½Ÿ": "å¤",
    "ç‡™": "çƒ«",
    "å¥¬": "è®²",
    "é†¢": "æµ·",
    "æœ®": "æ ‘",
    "éª›": "ç‰©",
    "è¾º": "ç¼–",
    "å‹ª": "è§‰",
    "æº²": "æœ",
    "æ†¤": "æ„¤",
    "è¦§": "æ‡’",
    "æš‰": "ç°",
    "å„": "ç¿",
    "é—•": "ç¡®",
    "è’": "æœ",
    "æ–ƒ": "å¿…",
    "æ¨¨": "è¥¿",
    "é³•": "é›ª",
    "æª ": "æƒ…",
    "åªº": "ç¾",
    "æ‡¼": "å·¨",
    "è–": "ç²¾",
    "è‘‘": "é£",
    "è£": "æˆ",
    "ä¿¶": "å¤„",
    "è‚œ": "å®¹",
    "éŒ": "å¢¨",
    "é¾¢": "å’Œ",
    "é—…": "æ–‡",
    "è¯¨": "æ··",
    "é¾‘": "çœ¼",
    "é•¨": "æ™®",
    "å‡–": "å‡†",
    "è‰‰": "ä¼Ÿ",
    "æ¿«": "çƒ‚",
    "ä»©": "é•¿",
    "ä»®": "å",
    "ç¢œ": "å°˜",
    "ç‰¯": "å¤",
    "é£†": "æ ‡",
    "çœ„": "å…",
    "é«‚": "æ°",
    "è¾”": "é…",
    "æ»¯": "è‡³",
    "æ»—": "å¿…",
    "æµˆ": "çœŸ",
    "å•—": "è›‹",
    "å†®": "åˆš",
    "ç’ ": "çƒ¦",
    "ç": "ç’",
    "çµ¢": "ç‚«",
    "è·¹": "å…ˆ",
    "æˆ—": "æª",
    "è‹£": "å·¨",
    "å™¹": "å½“",
    "æ³¬": "è§‰",
    "è›¸": "çƒ§",
    "ç‹»": "é…¸",
    "å”": "ç‡•",
    "è€‹": "å ",
    "çœ": "ç",
    "è¨«": "ä¿¡",
    "å‰": "æœº",
    "å•": "å’Œ",
    "å¤Œ": "é›¶",
    "é ‘": "å®Œ",
    "ç­": "è·¯",
    "æ››": "ç†",
    "è§¯": "è‡³",
    "å»ª": "å",
    "å¼’": "æ˜¯",
    "æ®“": "ç»ƒ",
    "ç¹°": "æ—©",
    "å…Œ": "å¯¹",
    "é³": "æ‘‡",
    "ç¥§": "æŒ‘",
    "æ“„": "é²",
    "èŒ‘": "å°¿",
    "æªš": "æ¥š",
    "æµƒ": "å®¶",
    "é©•": "æ•™",
    "å–¶": "è¥",
    "è˜–": "è‚",
    "è„‡": "é‹",
    "ä¾“": "è·¯",
    "åˆ‹": "æ¬ ",
    "é½‘": "æœº",
    "ç¨¹": "æ•",
    "é¬£": "è£‚",
    "è‰¿": "å¥¶",
    "é’†": "å˜",
    "é—": "æª",
    "é¡›": "é¢ ",
    "é›©": "é±¼",
    "éº‚": "å‡ ",
    "ç³º": "çº ",
    "é": "åº¦",
    "åœ¬": "ä¹Œ",
    "ç–‹": "åŒ¹",
    "é™‰": "è¡Œ",
    "å©‚": "çœ ",
    "æˆ£": "å¥",
    "æ¯˜": "çš®",
    "ç¢": "å½¬",
    "æ‘­": "ç›´",
    "ç˜¢": "ç­",
    "è‹": "èª",
    "æˆ»": "æ›¿",
    "å¶‡": "åŒº",
    "æƒ¢": "é”",
    "è›": "æ‰°",
    "è««": "è§",
    "ç¬": "è°¢",
    "è¾‚": "è·¯",
    "æ¥€": "é›¨",
    "éŒ ": "å®š",
    "å–¾": "è£¤",
    "ç›ª": "è¡",
    "ç½": "é›·",
    "é‡€": "å¨˜",
    "è°¿": "è¥¿",
    "æ¿¾": "ç»¿",
    "æ¦‰": "ä¸¾",
    "è¨¥": "å‘¢",
    "ç’½": "æ´—",
    "è¢›": "ä½",
    "å†§": "æ—",
    "æ¤": "å±…",
    "è©°": "èŠ‚",
    "ç¹": "æ—‹",
    "å½‚": "å‘",
    "ç¬®": "åˆ™",
    "æš": "æ¤°",
    "ç†³": "æ…¢",
    "ä½¤": "ç“¦",
    "é–": "æ¶¦",
    "é¥”": "åº¸",
    "æ—†": "é…",
    "è²…": "ä¿®",
    "é™¬": "é‚¹",
    "çŒ±": "è„‘",
    "è¿“": "äºš",
    "å—": "å§¿",
    "é ”": "æ•Œ",
    "é…": "æ˜¼",
    "ç·‹": "é£",
    "é“ª": "å“ˆ",
    "æ‡²": "æˆ",
    "èˆ": "è®¾",
    "è¡": "èª",
    "é«­": "å§¿",
    "ç¤»": "æ˜¯",
    "è˜§": "å–",
    "æ”´": "é“º",
    "ç®¨": "æ‹“",
    "ç½¨": "çœ¼",
    "å“": "è¯¥",
    "å‹°": "é‹",
    "æ¼¬": "å­—",
    "è Ÿ": "èœ¡",
    "é„„": "å€¦",
    "ç¼·": "è°¢",
    "å“•": "ä¼š",
    "åœ®": "åŒ¹",
    "è¯": "åŒº",
    "åŸ½": "æ‡†",
    "ç¾™": "é«˜",
    "å»‚": "ç›¸",
    "ä¾”": "è°‹",
    "é¡": "æ¶",
    "æ‘ˆ": "å®¾",
    "ç«„": "çªœ",
    "æ¿": "æ°¸",
    "ä¾©": "å¿«",
    "æ°¤": "å› ",
    "é–": "æœº",
    "è”¥": "èª",
    "é¹µ": "é²",
    "ç„œ": "æ˜†",
    "è°˜": "å§¿",
    "éª ": "æ ‡",
    "æ”ª": "è§’",
    "èŸ€": "å¸…",
    "éŒ": "è¿½",
    "æ‚›": "åœˆ",
    "è ‚": "è®¾",
    "é¸": "æ",
    "æŒ": "åŸ",
    "ç€•": "å½¬",
    "åŸ¤": "çš®",
    "é²": "æƒ³",
    "å¾´": "ç",
    "æ·†": "è‚–",
    "å ": "å’¬",
    "è«’": "äº®",
    "æ‡‡": "è‚¯",
    "å¿": "å‘Š",
    "æš": "å°",
    "èŒ": "é¾™",
    "çŒ­": "ç©¿",
    "æœŠ": "è½¯",
    "ä¿œ": "å¹³",
    "è¯¿": "ä¼Ÿ",
    "ä¿³": "æ’",
    "å€»": "æ¤°",
    "å‹¯": "å•",
    "æ§": "æ",
    "èšµ": "å’Œ",
    "ç“ ": "äº’",
    "æ¸«": "è°¢",
    "é²·": "é›•",
    "è‘º": "æ°”",
    "é•": "åˆ˜",
    "å¶": "å‡³",
    "é³": "å¡”",
    "ç¿®": "å’Œ",
    "ç¦¤": "å®£",
    "é’‹": "å¡",
    "æ ": "ç¡•",
    "æ”¬": "æ‡’",
    "è¸": "è§",
    "è›‰": "é›¶",
    "å¸‘": "èºº",
    "åŸŒ": "æµª",
    "è€ª": "æ—",
    "ç…„": "ç§",
    "è¡½": "ä»»",
    "æ™§": "å·",
    "çŠ´": "æŒ‰",
    "é²‚": "æˆ¿",
    "è·¬": "é­",
    "è ": "æƒ³",
    "æ´¸": "å…‰",
    "å‚œ": "æ‘‡",
    "éš°": "ä¹ ",
    "å–Ÿ": "æºƒ",
    "éŒ¨": "æ¯›",
    "å‰°": "èƒœ",
    "è¥ª": "è¢œ",
    "é”‡": "é¢",
    "ä¼²": "é€†",
    "å¿”": "æ°”",
    "åŒ": "è¢",
    "ç¹¡": "ç§€",
    "å¼": "äºŒ",
    "è¡„": "å¥³",
    "é§": "å‡¯",
    "é™²": "å‚",
    "é©›": "æ„",
    "æ»‚": "æ—",
    "è°®": "æ€",
    "é«¹": "ä¿®",
    "ç’¿": "æ—‹",
    "é¾•": "çœ‹",
    "å—‰": "é€Ÿ",
    "é“¥": "ä¸¢",
    "è©£": "æ„",
    "è½": "æ˜¥",
    "å Ÿ": "èµš",
    "ç”¦": "è‹",
    "åŒ": "ç¦",
    "å£": "çœ¼",
    "å¶†": "æ›¹",
    "åª¸": "åƒ",
    "ç…–": "æš–",
    "æ®¤": "ä¼¤",
    "å€®": "è£¸",
    "ç°Œ": "é€Ÿ",
    "å¯”": "çŸ³",
    "æ©": "è£‚",
    "ç«‡": "è±†",
    "é®‹": "ç”±",
    "çµ": "æˆ",
    "å‘¬": "ç³»",
    "èœ": "è´´",
    "å‡": "å­",
    "æ¢±": "æ†",
    "ç¬¹": "è¸¢",
    "å¦": "ç¡•",
    "ç‘±": "é•‡",
    "æ—¯": "å•¦",
    "çªª": "æŒ–",
    "æ§„": "æ",
    "åœ¹": "çŸ¿",
    "å„¡": "å’",
    "ç¼Œ": "æ€",
    "é¹›": "æ¢…",
    "æ•": "æ•™",
    "æ’ˆ": "æ",
    "å¾‚": "ä¿ƒ",
    "å‘º": "æ¶ˆ",
    "å½–": "å›¢",
    "æ¨": "æˆ",
    "èˆ˜": "ç®¡",
    "å®¬": "æˆ",
    "é² ": "æ¢—",
    "éˆ¦": "å¤ª",
    "æ¤‚": "è·¯",
    "å™º": "å¿ƒ",
    "é": "é¥¼",
    "è “": "çŒ›",
    "æ‚»": "æ€§",
    "è“ ": "ç¦»",
    "æ³…": "æ±‚",
    "å¦§": "ä¸‡",
    "é•¡": "ç¼ ",
    "å—˜": "æœº",
    "ç½": "å¤",
    "é…ƒ": "é›¶",
    "è¡ª": "å®œ",
    "è¦“": "å¯†",
    "èª¦": "å®‹",
    "è ˆ": "è´¼",
    "ç²‹": "ç¿ ",
    "ç¼§": "é›·",
    "æŸ½": "ç§°",
    "å‰¤": "è®°",
    "åŒ¦": "é¬¼",
    "é£": "æŸ”",
    "æ¾†": "æ•™",
    "æ¢ƒ": "æŒº",
    "æ½…": "çŒ",
    "ç™¢": "å…»",
    "å†ª": "å¯†",
    "æº¦": "å¾®",
    "æ²¬": "å¦¹",
    "é®¨": "æ„",
    "æš": "ä¼Ÿ",
    "å›µ": "ä¼¦",
    "é›±": "æ—",
    "æµ¯": "æ— ",
    "ç€¾": "è“",
    "è›´": "å…¶",
    "èº‡": "é™¤",
    "é•†": "å¢¨",
    "ä¾€": "è¡Œ",
    "ç†º": "è¥¿",
    "çª¬": "é±¼",
    "ç–´": "ç§‘",
    "è–ˆ": "ä¼š",
    "ä»‚": "ä¹",
    "éœ‘": "å ",
    "é“½": "ç‰¹",
    "åª¾": "å¤Ÿ",
    "é€­": "æ¢",
    "é»¾": "æ•",
    "ç‡": "æ—",
    "é…": "å¤ª",
    "é’„": "è“",
    "å“Œ": "æ´¾",
    "ç²§": "è£…",
    "é„¹": "é‚¹",
    "ä»¼": "ç‹",
    "æŸ": "æ‹“",
    "æ¸¦": "çª",
    "ç¿¥": "åŠ©",
    "ç¼¬": "é‹",
    "çº®": "çº¢",
    "è˜¼": "è¿·",
    "é»¥": "æƒ…",
    "å ": "å ",
    "é²‡": "å¹´",
    "å¬–": "å¿…",
    "æŸ™": "ä¾ ",
    "ç®»": "ç»¿",
    "ç¡ƒ": "æœ±",
    "æ€¹": "è´ª",
    "è«±": "ä¼š",
    "åŸ˜": "çŸ³",
    "é©": "èˆ”",
    "è¹": "å±…",
    "æ„€": "å·§",
    "éš³": "ç°",
    "åœ": "æœµ",
    "çŠº": "æŠ—",
    "è¢": "è¥",
    "è§œ": "å§¿",
    "çŸš": "ä¸»",
    "ä¾–": "ä¼¦",
    "è§Š": "è®°",
    "è ²": "æ",
    "ç‡¿": "è¦",
    "åµ": "ç§‹",
    "å¬¿": "ç‡•",
    "å´±": "è´£",
    "æ€„": "æ¬§",
    "è·—": "å¤«",
    "è¯”": "å’",
    "é´‰": "å‘€",
    "é¥‘": "æœº",
    "ç": "èµ–",
    "æ‚³": "å¾·",
    "è¥¯": "è¡¬",
    "å": "è‡³",
    "æ»²": "è‚¾",
    "æŸ": "å •",
    "ç’": "çº¢",
    "å±›": "å¹³",
    "ç¯ª": "æŒ",
    "ç±¼": "å…ˆ",
    "å´˜": "ä¼¦",
    "ä½‰": "åŒº",
    "å›²": "é€š",
    "ç™®": "å¼•",
    "å€œ": "æ›¿",
    "èƒ": "çŸ¥",
    "å¾­": "æ‘‡",
    "è® ": "é¢œ",
    "è½": "è™½",
    "å†": "æ˜¥",
    "éµ°": "é›•",
    "å—™": "æ—",
    "å®„": "é¬¼",
    "é•˜": "æ…¢",
    "é¼‰": "é©®",
    "æ®²": "é—´",
    "ä¼‹": "æ",
    "è«œ": "å ",
    "è´": "çª",
    "å„‚": "å†œ",
    "ä½»": "æŒ‘",
    "éŠ": "ç©·",
    "ç–³": "å¹²",
    "çŠµ": "æ­Œ",
    "å««": "é­”",
    "å¨¡": "è‡³",
    "è¤•": "é±¼",
    "æ¼¤": "æ‡’",
    "å¸”": "é…",
    "æ¾˜": "å±±",
    "æ—„": "æ¯›",
    "é”": "æŸ³",
    "è±‡": "å°†",
    "æ Œ": "èŠ¦",
    "å§": "èµ",
    "é–¾": "ç‰",
    "æ™³": "è¥¿",
    "è³—": "ä¸²",
    "å¤": "ä¸€",
    "ç²œ": "è·³",
    "æ‹Š": "è¾…",
    "æš¸": "èŠ",
    "è¤›": "æ—…",
    "ç˜—": "æ„",
    "é“": "æ",
    "ç¾°": "æ±¤",
    "å„µ": "ä¹¦",
    "è³": "è„",
    "é©¢": "é©´",
    "å˜": "å¤",
    "èš£": "å·¥",
    "é¼": "ä¸‹",
    "éª": "è°‹",
    "å…": "åˆ†",
    "æ¢Ÿ": "æ¶ˆ",
    "æ‡”": "å",
    "ç¹–": "ä¸‰",
    "è‡¿": "æ’",
    "å³©": "é¢",
    "ç¯©": "ç­›",
    "çŒ“": "æœ",
    "æ³": "ä¹",
    "æ’„": "åº”",
    "éˆ": "é¡¿",
    "ç»": "è°¢",
    "é©": "æ¶ˆ",
    "è¤«": "å°º",
    "ç‘‹": "ä¼Ÿ",
    "é–¨": "å½’",
    "è¦—": "å››",
    "ç§£": "å¢¨",
    "æŒ™": "ä¸¾",
    "ç“¿": "ä¸",
    "ç¹ƒ": "å´©",
    "è±³": "å½¬",
    "å™“": "éœ€",
    "åŸ’": "è£‚",
    "æ¶ ": "ç»´",
    "æ¶‘": "é€Ÿ",
    "åƒ": "ä¼Ÿ",
    "å„‰": "å‡",
    "é»¼": "è¾…",
    "è´›": "å¹²",
    "çª•": "æŒ‘",
    "æ¹": "æ¯",
    "æ¶´": "å§",
    "å³¿": "é›¨",
    "ç›Œ": "ç¢—",
    "å›¨": "å",
    "ç—±": "è´¹",
    "æ·": "å·",
    "è“": "å…¥",
    "æ¹…": "ç»ƒ",
    "ç©¢": "ä¼š",
    "ç¶”": "äº’",
    "æ½‘": "å¡",
    "é…": "å¹²",
    "è ": "æœ‰",
    "æ‘³": "å£",
    "éˆ‰": "é‚£",
    "é ¤": "å®œ",
    "è° ": "æŒ¡",
    "è™": "ç²¾",
    "è©": "å’‹",
    "å‹›": "ç†",
    "æ¦–": "å¤",
    "ç«Š": "å¦¾",
    "å…": "å…”",
    "å°¥": "æ–™",
    "å›“": "è‚",
    "é€¡": "ç¾¤",
    "é¸±": "åƒ",
    "ç­®": "æ˜¯",
    "å˜¯": "ç¬‘",
    "å·›": "ç©¿",
    "åšœ": "ä¹ˆ",
    "æ‰•": "å",
    "çµƒ": "é—²",
    "é¡«": "é¢¤",
    "æŠ”": "å‰–",
    "å¶·": "å®œ",
    "é³œ": "è´µ",
    "é±ˆ": "é›ª",
    "æ”": "å„",
    "åƒ‰": "å‰",
    "çš¤": "å©†",
    "å»¬": "èŠ¦",
    "é™œ": "ä¾ ",
    "é”ª": "éœ",
    "åƒ³": "é€Ÿ",
    "é’": "æ¡¥",
    "è–¤": "è°¢",
    "éœª": "é“¶",
    "è©­": "é¬¼",
    "æˆ™": "åŠ¨",
    "æ‰ƒ": "çª˜",
    "ç½˜": "ç¦",
    "é‚": "ç¾",
    "èƒ—": "çœŸ",
    "çµ¹": "å€¦",
    "é¼–": "åŸ",
    "ç¥…": "é‚€",
    "èº": "å",
    "å¯ƒ": "å†¤",
    "å†¹": "ç¦",
    "å›ª": "èª",
    "æ©¼": "åŸ",
    "è±¨": "è¥¿",
    "é¸¬": "èŠ¦",
    "é–¬": "æµª",
    "é¤": "ç‡•",
    "ç‘§": "çœŸ",
    "çŠ­": "çŠ¬",
    "æ°¾": "é¥­",
    "é‚¶": "è¢«",
    "å¬±": "æ¬ ",
    "é…¹": "æ³ª",
    "æ“—": "åŒ¹",
    "å¡": "æ°¸",
    "ç¼¢": "æ„",
    "é¹¼": "å‡",
    "æ¶«": "çŒ",
    "æ ²": "è€ƒ",
    "è¬Š": "è°",
    "èƒ™": "åš",
    "é¦ƒ": "æœ",
    "å“": "ç‰™",
    "å‘¿": "å»",
    "ç¶™": "è®°",
    "é©Ÿ": "æ˜¼",
    "é—": "ç£",
    "è¥™": "æ“",
    "å©±": "é—²",
    "æ¹¡": "é±¼",
    "åŠ¼": "èŠ‚",
    "ç– ": "åˆ©",
    "çª“": "çª—",
    "åªµ": "ç¡¬",
    "æ§ ": "æœ±",
    "ç¸": "åŸ",
    "ä»": "ä½›",
    "é§­": "å®³",
    "è·": "é©®",
    "å®": "è‚‰",
    "è¢´": "è£¤",
    "ç¡¶": "å°˜",
    "é§±": "è½",
    "èº”": "ç¼ ",
    "æœ": "å–",
    "èŒ•": "ç©·",
    "é€¶": "å¾®",
    "äº¶": "èƒ†",
    "åª„": "ç¾",
    "æš¦": "åˆ©",
    "ç“˜": "çŒ",
    "æ´¨": "è‚–",
    "ç¶„": "ç¯",
    "å°ª": "æ±ª",
    "è„¹": "å¸",
    "è·«": "ç©·",
    "è‚Š": "æ„",
    "æ°¹": "è¡",
    "é³": "è‚",
    "å“š": "æœµ",
    "æˆ‹": "é—´",
    "å“¿": "è‘›",
    "éª˜": "è‡³",
    "é’€": "è‚",
    "è”": "ç¦",
    "å†­": "å¤ª",
    "æ¸¶": "åº”",
    "æº·": "æ··",
    "æ²š": "æŒ‡",
    "è›˜": "ç¾Š",
    "ç¢š": "è¢«",
    "ç­Œ": "å…¨",
    "éµº": "å¤œ",
    "æŒ²": "æ’’",
    "é’«": "æ–¹",
    "é“©": "æ²™",
    "æ’“": "è„‘",
    "è´½": "è‡³",
    "ç»¨": "æ",
    "è§‡": "æ€",
    "ç‚ª": "æ‰",
    "å´¾": "å’¬",
    "å ‹": "æœ‹",
    "ä»ƒ": "ä¸",
    "éªŸ": "å–„",
    "æ™·": "é¬¼",
    "å¡±": "æµª",
    "æµ": "é“²",
    "æ®‚": "ä¿ƒ",
    "ç¼’": "ç¼€",
    "å": "æ€ª",
    "ç°": "å½“",
    "ç¶¸": "ä¼¦",
    "å€£": "è®¿",
    "è–‘": "å°†",
    "æœ‚": "ç»­",
    "è””": "åš",
    "éŠƒ": "å†²",
    "æ‚•": "è¥¿",
    "æ‚": "å¯»",
    "é´¿": "æ­Œ",
    "ç³¾": "çº ",
    "çƒ": "ç¼©",
    "è€§": "æ¥¼",
    "èƒ›": "å‡",
    "å˜§": "å¯†",
    "å¸¼": "å›½",
    "é«€": "å¿…",
    "é•¥": "é²",
    "å˜¹": "èŠ",
    "èŒˆ": "ç“·",
    "ç‹": "å…",
    "é¢™": "åº¸",
    "æ¨—": "å‡º",
    "ç‹·": "å€¦",
    "åŠ¬": "å–",
    "è¢¤": "å¸½",
    "æ Š": "é¾™",
    "å„“": "å°",
    "éƒ«": "çš®",
    "é“—": "å¤¹",
    "è¨": "æµª",
    "èµ•": "èƒ†",
    "ç ‰": "æˆ–",
    "æ¥¯": "é¡¿",
    "å£±": "ä¸€",
    "è´": "å¬",
    "è¬ ": "æ‘‡",
    "èˆ": "åŸ",
    "ç‚²": "å°",
    "ç”¾": "ç¾",
    "æ«º": "é›¶",
    "è½": "ä¸­",
    "é˜„": "çº ",
    "ç¦": "çœŸ",
    "å†": "çˆ±",
    "ä½§": "å¡",
    "å„¸": "ç½—",
    "æ”¢": "èµ",
    "èš´": "å³",
    "è½": "æ•™",
    "å†": "èŠ¦",
    "é¥«": "ç‰",
    "ç´‘": "å¦",
    "å´¤": "è‚–",
    "è•": "é™¤",
    "ç¸„": "ç”Ÿ",
    "è¸”": "æˆ³",
    "å¢ ": "å–„",
    "å¢": "æ…¢",
    "ç¹­": "å‡",
    "å¾¨": "é»„",
    "æ¤": "å†°",
    "éŠ¹": "ç§€",
    "çŒ": "é›¶",
    "è¡¤": "ä¸€",
    "ä¿¢": "ä¿®",
    "é­¨": "å",
    "éª": "åˆ˜",
    "æ©¥": "æœ±",
    "æˆ¡": "çœ‹",
    "æ™¡": "ä¸",
    "è€œ": "å››",
    "å§¤": "å¤Ÿ",
    "åŒœ": "å®œ",
    "éˆ¾": "ç”±",
    "ç›": "å±•",
    "ç·¹": "æ",
    "éš•": "å…",
    "æ• ": "å¤š",
    "æ±": "å¤§",
    "ç½ ": "æ°‘",
    "åŸ": "å±±",
    "æ“": "è’™",
    "æ–": "å‡",
    "å™¾": "å› ",
    "é³”": "æ ‡",
    "è¯¹": "é‚¹",
    "æ‚Š": "å“²",
    "ä¾½": "å—",
    "æ¦˜": "ä¸¾",
    "é²”": "ä¼Ÿ",
    "ç“": "æ„¤",
    "è¼": "æ¥¼",
    "é©²": "æ—¥",
    "è“¥": "è¥",
    "æ³–": "å¸½",
    "æ¶’": "å",
    "å¦¯": "è½´",
    "æ™": "æ˜¼",
    "çš": "ç‰™",
    "ç¯¼": "å…œ",
    "ç°«": "æ¶ˆ",
    "å±¦": "å·¨",
    "ç†»": "è¥¿",
    "æ“±": "æ­Œ",
    "é„´": "å¤œ",
    "æ®»": "å·§",
    "å²£": "ç‹—",
    "åš®": "å‘",
    "å—": "ç¼©",
    "ç…º": "é€€",
    "è‹Š": "æ¶",
    "é·º": "è·¯",
    "èœ®": "ç‰",
    "è„½": "è°",
    "è‘¸": "æ´—",
    "é‚—": "å«",
    "é”": "å®¢",
    "å¤¤": "é“¶",
    "ç«ˆ": "é€ ",
    "é”œ": "å…¶",
    "å‚­": "åº¸",
    "é­": "é•‡",
    "æœ©": "ç­‰",
    "æ‚½": "ä¸ƒ",
    "ç°·": "é¢œ",
    "è¸£": "åš",
    "è‡¾": "é±¼",
    "æ®": "é’±",
    "æ¦Š": "ç¥",
    "æ”‰": "éœ",
    "è„”": "é¸¾",
    "æ­º": "æ¶",
    "ç½¾": "å¢",
    "æ´¶": "èƒ¸",
    "æ¯Œ": "çŒ",
    "è€±": "å¢¨",
    "æŒ»": "å±±",
    "é™": "é—ª",
    "çºœ": "æ‡’",
    "æ¥": "åˆ©",
    "ç• ": "ç”°",
    "æµ": "ä¼š",
    "å¬¨": "ç“·",
    "èŒ": "è®°",
    "æ·": "è‚¥",
    "æ¤¼": "çœ¼",
    "æ¿Š": "ä¼š",
    "ç€": "é™",
    "é³™": "åº¸",
    "ç«‘": "çº¢",
    "ç": "å·",
    "å‘ª": "æ˜¼",
    "è¸°": "é±¼",
    "è–": "æ˜Œ",
    "å†©": "å†™",
    "èœ†": "ç°",
    "æ»": "è¾…",
    "æ±©": "å¤",
    "å‹…": "èµ¤",
    "å¢¿": "æ„",
    "å•": "å",
    "é…©": "å‘½",
    "è§Œ": "æ•Œ",
    "ç¸·": "æ—…",
    "é¶¯": "åº”",
    "æ„°": "è’",
    "ç¶µ": "é‡‡",
    "å‘´": "è®¸",
    "è„›": "é™",
    "å": "è´º",
    "è“¿": "é¡»",
    "èƒ¨": "åŠ¨",
    "åœ": "åˆ©",
    "åˆ³": "å“­",
    "é¸º": "ä¿®",
    "å·£": "æœ",
    "ç´": "çŒ›",
    "å°’": "è€³",
    "çš¯": "æ„Ÿ",
    "èŒ³": "å°†",
    "è™¿": "æ‹†",
    "é¦­": "ç‰",
    "è ”": "è±ª",
    "æ„¦": "æºƒ",
    "æ³­": "ç¦",
    "é¢": "æ ‡",
    "ç§«": "ç†Ÿ",
    "é²³": "æ˜Œ",
    "ç­˜": "æ‰£",
    "ç˜•": "å‡",
    "å–": "æ­ª",
    "å²¿": "äº",
    "ç¥": "å±",
    "è»€": "åŒº",
    "è´": "å®‰",
    "æ›˜": "å¦‚",
    "ç…•": "è¥¿",
    "æœ": "æœ",
    "è‹¾": "å¿…",
    "æ¡¤": "ä¸ƒ",
    "ç±¹": "å¥³",
    "ç”": "ç”­",
    "ç¼£": "é—´",
    "é‡¦": "æ‰£",
    "å¨¿": "é˜¿",
    "çŠ¸": "éª‚",
    "å—": "çœ",
    "é‰¢": "æ³¢",
    "è¼": "å¬",
    "å´Ÿ": "é“¶",
    "å¡¢": "ç‰©",
    "ç¤": "ç§°",
    "é«Œ": "å®¾",
    "ç ¹": "çˆ±",
    "ç¬": "ç«¹",
    "é«Ÿ": "æ ‡",
    "å§Ÿ": "è¯¥",
    "å­¥": "åŠª",
    "è‰½": "æ•™",
    "è£": "ç‹",
    "è„¢": "æ¢…",
    "æ¾”": "å·",
    "ä¼•": "å¤«",
    "ç ˜": "é¡¿",
    "å„›": "äº”",
    "ç´º": "å¹²",
    "ç›„": "æ‹›",
    "æ‡†": "è‰",
    "è¼¥": "æ»š",
    "é¬˜": "ç’",
    "ç— ": "é…¸",
    "æ¨¼": "çœŸ",
    "å““": "æ¶ˆ",
    "è„¬": "æŠ›",
    "ä¾¡": "å››",
    "ä¼‰": "æŠ—",
    "è•“": "äº‘",
    "å…": "é‡",
    "æ‰±": "è¥¿",
    "ä¹¸": "å“ª",
    "ç›‰": "å’Œ",
    "æ¼­": "å¿™",
    "é¸": "é¸¾",
    "çªº": "äº",
    "æ—": "å¯¼",
    "æ„”": "å› ",
    "è“": "è¯—",
    "å¹„": "å§",
    "èµ§": "ç”·",
    "é¥©": "ç³»",
    "é†ƒ": "çƒŸ",
    "é‘¼": "ç½—",
    "é": "æ‹±",
    "å™¯": "å“€",
    "å§³": "å‘½",
    "å´ƒ": "æ¥",
    "ç—": "å®œ",
    "å„‘": "å²¸",
    "å–¹": "å¥",
    "å‰": "å",
    "é‡": "ç‚«",
    "é²…": "çˆ¸",
    "ç¬¾": "ç¼–",
    "é²‹": "çˆ¶",
    "æ‚«": "ç¡®",
    "é¹": "æ˜†",
    "ç¦š": "ç€",
    "ç­‡": "ç©·",
    "æª‘": "é›·",
    "æ–": "æ‘˜",
    "éš": "å‚",
    "é³¶": "å†¤",
    "é•”": "å½¬",
    "ç¶¬": "ç˜¦",
    "æ„": "å¿…",
    "è‹": "åŠ©",
    "éƒ": "å¿«",
    "æ¥": "å’Œ",
    "è¯–": "æŒ‚",
    "å´": "æ˜†",
    "è¸": "é¼»",
    "éŸª": "ä¼Ÿ",
    "çšº": "æ˜¼",
    "éª£": "é“²",
    "è°‡": "å²",
    "ç˜°": "è£¸",
    "å²ƒ": "ä»»",
    "éŠ“": "å…¨",
    "é‘º": "å–",
    "ç²¬": "åŒº",
    "é£‘": "æ ‡",
    "è¼¿": "é±¼",
    "è¤´": "è“",
    "è½": "å“²",
    "æ—€": "å¦¹",
    "ç¡": "é¢œ",
    "æ³©": "ç”Ÿ",
    "ç•›": "æ•",
    "é¾‰": "é›¨",
    "éª±": "å€Ÿ",
    "å†–": "å¯†",
    "è¤": "å…",
    "é®": "å¹´",
    "æ®š": "å•",
    "æš»": "äº•",
    "å«»": "é—²",
    "è¦‡": "çˆ¸",
    "å•": "å“‘",
    "å ™": "å› ",
    "é»Ÿ": "ä¸€",
    "è€¥": "æ±¤",
    "è¤": "ç§€",
    "æ‚": "äº",
    "èš‹": "ç‘",
    "ç±´": "æ•Œ",
    "é–": "æ•–",
    "æ±": "çˆ¬",
    "ä¿¬": "æ€",
    "æ’»": "è¸",
    "è°": "å¥",
    "é‰»": "è½",
    "ç²¢": "å§¿",
    "è„¨": "ä¿ƒ",
    "è‚„": "æ„",
    "é§†": "åŒº",
    "ç‚¤": "ç…§",
    "é…¡": "é©®",
    "é¬": "èƒ¡",
    "æ„·": "å‡¯",
    "é…": "æŒ",
    "éˆŠ": "å¿ƒ",
    "é„": "ä¼ ",
    "ç»²": "æ»š",
    "é¦“": "ä¸‰",
    "çƒ”": "åŒ",
    "é¼": "è¾¾",
    "ç•¾": "é›·",
    "è·£": "æ˜¾",
    "æ¹³": "ç”·",
    "æ¦‡": "è¡¬",
    "è Š": "è¿",
    "ç µ": "æ³¢",
    "ç¦¿": "çª",
    "ç§•": "æ¯”",
    "é‹°": "æ",
    "çµ·": "ç›´",
    "æ­¯": "å°º",
    "æ‡": "æ€€",
    "ç‡ ": "ç‰",
    "åŒ": "ç³»",
    "åŒ": "ç¬¨",
    "æµ¬": "æ",
    "å  ": "å",
    "å¼": "çˆ¸",
    "å¤¶": "æ¯”",
    "ç­²": "çƒ§",
    "ç‘¢": "å®¹",
    "é¾€": "è¡¬",
    "å¢˜": "é’±",
    "ç‚±": "å°",
    "å¤ˆ": "æ‘˜",
    "è®": "çˆ¶",
    "ç­…": "æ˜¾",
    "ç†¾": "èµ¤",
    "æ¬": "åˆ©",
    "è‰¡": "å½“",
    "æ¸Ÿ": "åœ",
    "éƒ¾": "çœ¼",
    "è·": "åˆ©",
    "é„º": "çŸ¿",
    "é—¡": "é“²",
    "æƒ±": "è„‘",
    "é³¯": "å¥‰",
    "æ›‡": "è°ˆ",
    "ç¡¯": "ç‡•",
    "è„¥": "æµ…",
    "åº’": "è£…",
    "å¯²": "å®œ",
    "è¼’": "å“²",
    "æ¬": "ç‰",
    "æ·’": "ä¸ƒ",
    "ç›¥": "çŒ",
    "å©ƒ": "ä»",
    "æ²†": "è¡Œ",
    "åº…": "é­”",
    "äº¼": "æ",
    "ç¾ˆ": "æœº",
    "è·„": "æª",
    "ç˜³": "æŠ½",
    "é›‘": "æ‚",
    "æ°…": "åœº",
    "é¥£": "çŸ³",
    "æ¬’": "é¸¾",
    "æŸµ": "å±±",
    "èŠ„": "å®Œ",
    "æ§»": "å½’",
    "å¨": "æ‰‹",
    "éªº": "å–‰",
    "å²ˆ": "ç‰™",
    "æ¤›": "èŠ±",
    "å‚•": "è§‰",
    "è£Ÿ": "æ²™",
    "ç©¦": "æ‹¼",
    "éˆ¹": "æ‰¹",
    "åŸ": "å·",
    "è’‰": "æºƒ",
    "ç”•": "ç“®",
    "å—»": "é®",
    "æ¶š": "ç¡",
    "é‘": "å¦¾",
    "ç™œ": "ç”µ",
    "æ¿„": "é”…",
    "æ‚": "å·",
    "è–·": "å¦‚",
    "è§": "å¤Ÿ",
    "æ–‚": "è„¸",
    "å»": "å†Œ",
    "å–ˆ": "æ¥",
    "å›¦": "å†¤",
    "æš˜": "ç¾Š",
    "æ§²": "èƒ¡",
    "çœ¬": "é¾™",
    "é²®": "é›¶",
    "é•¦": "å¯¹",
    "åº¥": "ä¿®",
    "è¢·": "å¤¹",
    "å™": "æ•™",
    "è£¥": "å‡",
    "è¶¼": "å‡",
    "å«": "ç”µ",
    "å²º": "é›¶",
    "æ„­": "å…¶",
    "ç¼": "å˜",
    "é”¸": "æ’",
    "å°¨": "å¿™",
    "ç•¨": "æ”€",
    "æ„©": "å·¥",
    "ç¸¢": "è…¾",
    "ç§Š": "å¹´",
    "æ¥": "å‚",
    "é§¡": "éª‚",
    "èŠ¨": "æœº",
    "ç˜": "èµ·",
    "å»¾": "æ‹±",
    "è°³": "ç‡•",
    "èª": "é¢",
    "èŒŒ": "æŒ",
    "è²¯": "åŠ©",
    "çœ": "å…»",
    "æ¼ˆ": "è®°",
    "è³ƒ": "å",
    "å¡": "ç½š",
    "é¹£": "é—´",
    "è¯‚": "å¤",
    "éŸ": "é“²",
    "é‰—": "é’±",
    "ç—": "è§‰",
    "æ´¿": "ä¹Œ",
    "æ£¨": "èµ·",
    "ç‡‰": "é¡¿",
    "é–ª": "è‰²",
    "æ§™": "é¢ ",
    "è –": "æˆ–",
    "é¾ ": "æœˆ",
    "é‘±": "ç¼ ",
    "ç¶": "ç¦",
    "å€¥": "ç©º",
    "æˆ¥": "ç­‰",
    "åœ„": "é›¨",
    "é¼—": "æ¡ƒ",
    "å“¾": "è¯´",
    "é½": "ç§€",
    "åƒ±": "å›º",
    "ç": "ç°",
    "é‰€": "å‡",
    "è£¼": "æ›¿",
    "é·—": "æ¬§",
    "æ™«": "ç€",
    "æ¶": "æŒº",
    "æ¿‡": "è‰²",
    "æ«’": "è¨",
    "é±»": "å…ˆ",
    "ç±º": "å’Œ",
    "å«˜": "é›·",
    "å‚§": "å½¬",
    "å¬": "æ˜‚",
    "æ¡": "ç”Ÿ",
    "è‚": "åœ°",
    "é¸¸": "è€Œ",
    "å±º": "èµ·",
    "ç°²": "æ’",
    "é–°": "å±€",
    "å³“": "å®œ",
    "æƒ": "è¿›",
    "è£¢": "è¿",
    "å›™": "å› ",
    "è„—": "ç¨³",
    "ç¥†": "å…ˆ",
    "å¥­": "æ˜¯",
    "èˆ¡": "ä¼ ",
    "ç—¼": "å›º",
    "éª€": "å¸¦",
    "è²Š": "å¢¨",
    "çšµ": "ç¡®",
    "è´¶": "çŸ¿",
    "ç¥“": "ç¦",
    "è¬¦": "è¯·",
    "éŠ­": "é’±",
    "è¤": "ç§€",
    "é®«": "æ•™",
    "ç§¾": "å†œ",
    "æ†·": "å¤„",
    "é­Ÿ": "çº¢",
    "äº°": "ç²¾",
    "è”Ÿ": "ä¿ƒ",
    "åŒ‚": "èƒ¸",
    "æ¹°": "é¾™",
    "ä»³": "åŒ¹",
    "æ­ƒ": "ç…",
    "å¬…": "è¯",
    "æ ¬": "æœ€",
    "è¾": "å‡‘",
    "é¬¯": "å”±",
    "å²": "å‰",
    "é€¦": "æ",
    "æª»": "ç ",
    "åº€": "åŒ¹",
    "è‰š": "æ›¹",
    "è¾": "å§¿",
    "è£‰": "è‚¯",
    "åª—": "å®£",
    "é²­": "é’",
    "ç»": "é‡‘",
    "è¤‘": "é™¢",
    "æ‰†": "ä»¥",
    "é§Œ": "å†¤",
    "æ’´": "è¹²",
    "ç®Œ": "åˆ°",
    "è”˜": "æ·±",
    "å ¢": "å®",
    "æ•«": "è§’",
    "å°¯": "æºƒ",
    "åµ›": "é±¼",
    "ç”—": "çœ¼",
    "æœ ": "åº”",
    "ç ‘": "äºš",
    "å‡µ": "æµ…",
    "é£": "æ‹›",
    "åº": "å¬",
    "ç¸»": "è¿·",
    "æƒ€": "ä¼¦",
    "å³ ": "æ°",
    "å½": "è®°",
    "é¸¹": "ç“œ",
    "ç…‹": "æ˜Ÿ",
    "å»›": "ç¼ ",
    "èŒš": "å°",
    "å€¶": "å·¨",
    "æ«›": "è‡³",
    "åƒ¾": "çˆ±",
    "èŠ²": "èŠ±",
    "ç•‹": "ç”°",
    "å©„": "å‰–",
    "è ¶": "æ®‹",
    "è½º": "æ‘‡",
    "è¿•": "ç‰©",
    "æ›Ÿ": "é™ˆ",
    "å»": "æ€",
    "æ": "ä¿º",
    "ç…’": "ä¼Ÿ",
    "ç´µ": "åŠ©",
    "è¤¡": "ç­”",
    "ç‡„": "ç‡•",
    "è¢“": "å·¨",
    "æœ¶": "æœµ",
    "æ‘»": "ç¿",
    "ç…‡": "ç°",
    "æ¾Œ": "æ€",
    "ç”ª": "è·¯",
    "é‰¯": "ä»¥",
    "è’¹": "é—´",
    "å˜": "ç ",
    "é°": "ç§‹",
    "å½«": "é›•",
    "æ¤€": "ç¢—",
    "ç‡­": "ç«¹",
    "ä¼ˆ": "ä¿¡",
    "æŠ": "å…",
    "ç“‘": "åˆ©",
    "è‘™": "ç›¸",
    "ä¼¥": "æ˜Œ",
    "ç": "ä¹",
    "é²†": "å¹³",
    "é´": "è¢«",
    "ç‹†": "é‡",
    "éˆ•": "çº½",
    "èŠŸ": "å±±",
    "ç­¯": "åŠ©",
    "ç™²": "é¢ ",
    "è«": "ç´§",
    "æ®": "åˆ©",
    "éˆ³": "ç§‘",
    "èŠ¤": "å£",
    "æ™¥": "ç¢—",
    "å©²": "èŠ±",
    "ç¬¸": "å¡",
    "éª‰": "æ ‡",
    "å„¶": "ä¼š",
    "æˆ“": "æ­Œ",
    "é¹±": "äº’",
    "å§Œ": "æŸ“",
    "å’´": "ç°",
    "ç†": "å§¿",
    "çµ€": "å¤„",
    "æ ¥": "å§¿",
    "ç…¶": "æ˜¯",
    "è†°": "çƒ¦",
    "é²Š": "çœ¨",
    "èˆ³": "ç«¹",
    "å¸»": "åˆ™",
    "å¡…": "æ–­",
    "å¡„": "æ£±",
    "æ•‰": "ç±³",
    "é» ": "ä¾ ",
    "æŒŠ": "å¼„",
    "å­³": "å§¿",
    "ç´¬": "æ„",
    "å¤": "é”™",
    "åš…": "å¦‚",
    "é»§": "ç¦»",
    "å†š": "ç ",
    "çŸ§": "å®¡",
    "æ·": "æ’",
    "å¿‰": "åˆ€",
    "èƒ‚": "è‚¾",
    "é¦´": "å¯»",
    "è¿": "åˆ°",
    "é³": "å…³",
    "æ¬": "å°±",
    "å„´": "åš·",
    "ç¢": "ç›Ÿ",
    "æº": "å¿ƒ",
    "èµ³": "çº ",
    "ç¥Œ": "é‡",
    "ç˜›": "èµ¤",
    "é©º": "é‚¹",
    "èŒ›": "æ ¹",
    "ç£²": "å–",
    "æ¦": "è¯º",
    "æœ¹": "é¬¼",
    "æ™": "ç",
    "è®": "æ„",
    "è¥": "å¿…",
    "å¿®": "è‡³",
    "ç¥«": "ä¾ ",
    "è†®": "æ¶ˆ",
    "ç€‰": "è°¢",
    "åª": "é€¼",
    "éƒ": "éœ",
    "å‘¡": "ç¨³",
    "èª£": "ä¹Œ",
    "è‹§": "å‡",
    "è¸": "ç”±",
    "æ»Œ": "æ•Œ",
    "ç½±": "æ‡’",
    "éµœ": "æ",
    "ç©«": "æˆ–",
    "çƒ": "é“¶",
    "ç°¦": "ç¯",
    "é’": "åŠ³",
    "çŠ±": "å‡ ",
    "çŒŸ": "è£‚",
    "é“«": "æ‰",
    "æª—": "è–„",
    "ç¹•": "å–„",
    "ä¼„": "æ‰",
    "é½": "é£",
    "ç¯Œ": "å–‰",
    "ç”": "ç›Ÿ",
    "éº£": "é¢œ",
    "é²½": "å ",
    "éºº": "é¢",
    "æ¯´": "é€¼",
    "çŸ¯": "è§’",
    "è„¶": "ç½—",
    "è»‹": "äºš",
    "ç²": "åˆ©",
    "å»´": "å¼•",
    "è´–": "ç†Ÿ",
    "å·": "å·¥",
    "æ½²": "ç»",
    "æµ¤": "çº¢",
    "å’˜": "ä¸",
    "æ ±": "æ‹±",
    "å’‘": "ç­”",
    "å²": "å‡¯",
    "å´¦": "çƒŸ",
    "æ¹€": "é¬¼",
    "è½¾": "è‡³",
    "æ¸€": "ç¬¨",
    "æ‚—": "ç’",
    "é…²": "æˆ",
    "åŠ“": "æ„",
    "ç°’": "çªœ",
    "åƒª": "å±€",
    "å»¨": "è°¢",
    "é¸²": "å–",
    "å¹": "é“²",
    "è·": "å®¶",
    "èºœ": "é’»",
    "è¦": "è½",
    "éš´": "å„",
    "æ†": "æ•™",
    "é—¶": "åº·",
    "é«": "å±…",
    "ä»«": "æœ¨",
    "è‰„": "çƒ§",
    "ç¼½": "æ³¢",
    "åœ": "é¸¾",
    "è„°": "è±†",
    "çª³": "é›¨",
    "è¥»": "åˆ¤",
    "èˆ»": "èŠ¦",
    "å›‘": "ä¸»",
    "æ§³": "è®²",
    "é¼™": "çš®",
    "ç•š": "æœ¬",
    "åˆ": "ç¨³",
    "çš²": "å›",
    "çš": "å·",
    "è©": "ç¦",
    "ä»¡": "æ­Œ",
    "å»ƒ": "è´¹",
    "æŒ¾": "é‹",
    "å§": "ç’",
    "ç±²": "ç‰",
    "è¿º": "å¥¶",
    "æ‹": "è´¥",
    "é³Š": "ç¼–",
    "åœ¯": "å®œ",
    "é˜": "å¤Ÿ",
    "ä¹£": "ä¹…",
    "ç‹¨": "å®¹",
    "éŠ‘": "æ˜¾",
    "é¹©": "èŠ",
    "è©": "è¿›",
    "æ¶¢": "äº‘",
    "æ’º": "çªœ",
    "ç±€": "æ˜¼",
    "ç±¬": "ç¦»",
    "å³£": "æ‘‡",
    "å˜©": "èŠ±",
    "éº¹": "åŒº",
    "ä¼…": "é¡¿",
    "è›„": "å§‘",
    "å“¢": "å¼„",
    "å»¯": "å…ˆ",
    "ç‹²": "å­™",
    "ç“¨": "ç¿”",
    "ç«·": "ç ",
    "ç„ƒ": "è´º",
    "çŒ¡": "ç½—",
    "è™“": "æ¶ˆ",
    "é‘¸": "å’",
    "æ´£": "ç±³",
    "æ¸¼": "ç¾",
    "æ±¸": "æ–¹",
    "å¢–": "å¡”",
    "æ¨¶": "å˜´",
    "å…¿": "æ„",
    "è¸Ÿ": "æŒ",
    "æ‹": "æ£",
    "å¯˜": "è‡³",
    "å²µ": "äº’",
    "åº¤": "è‡³",
    "è•¹": "ç“®",
    "æ‘¯": "è‡³",
    "çŸ…": "è¦",
    "é¹": "å®‹",
    "æ´": "è€Œ",
    "ç¨‚": "ç‹¼",
    "å‘‰": "æ— ",
    "è«¦": "åœ°",
    "é‰‘": "åš",
    "æ›": "é—´",
    "è‰‹": "çŒ›",
    "é³‡": "é»„",
    "ç¹˜": "ç‰",
    "æ¡´": "ç¦",
    "ç¼": "æœµ",
    "è ‰": "å®£",
    "å©…": "å±€",
    "è¯¼": "ç€",
    "æ¾—": "è§",
    "è¿£": "è‡³",
    "çˆ‡": "å¼±",
    "é²©": "æ¢",
    "ç°®": "èµ",
    "é¾ƒ": "ä¸¾",
    "ç°–": "æ–­",
    "ç›±": "éœ€",
    "ç ¬": "å•¦",
    "ç•³": "å ",
    "æ½—": "æ",
    "ç·‚": "ç”°",
    "é—¿": "å‡¯",
    "é»»": "ç¦",
    "è‹": "æ",
    "æŒ¢": "è§’",
    "é¹¾": "æ“",
    "æ‡‘": "é—·",
    "çƒ": "å‘†",
    "è³": "è€¸",
    "å¡": "å’",
    "è¬¨": "é­”",
    "æ· ": "å±",
    "ä½€": "å››",
    "ç ©": "ç¦",
    "çª†": "æ‰",
    "æ‹¡": "æ‰©",
    "é°ˆ": "å ",
    "åŸœ": "ä¹Ÿ",
    "ç®": "é’±",
    "å¢": "äº²",
    "è„¤": "è‚¾",
    "ç°¬": "è·¯",
    "æ°¶": "æ•´",
    "è›¯": "è€",
    "è¢®": "è¿·",
    "è«·": "å¥‰",
    "é©Š": "å",
    "æªœ": "è´µ",
    "æ±­": "ç‘",
    "å„±": "å„",
    "éŒš": "ç",
    "é›‹": "å€¦",
    "ç—º": "å¿…",
    "æ¹‡": "æ°”",
    "ç‚´": "å…»",
    "è‹º": "æ¢…",
    "æ³š": "æ­¤",
    "ç": "é™",
    "é®­": "å½’",
    "æ³”": "å¹²",
    "ç§­": "å­",
    "ç¿¾": "å®£",
    "ç®¦": "åˆ™",
    "è’¨": "æ¬ ",
    "èœ¢": "çŒ›",
    "é• ": "åˆ˜",
    "ç²¨": "æ‘†",
    "ç¬¤": "æ¡",
    "ç©¸": "è¥¿",
    "èŒ¼": "åŒ",
    "æ ": "ç“œ",
    "é™”": "è¯¥",
    "æ´…": "åœ¨",
    "è¨Ÿ": "å®‹",
    "é£¢": "æœº",
    "æ£¬": "åœˆ",
    "ç¬Š": "ç…§",
    "åƒ°": "åš",
    "ç¤ª": "åˆ©",
    "æ—°": "å¹²",
    "è¶µ": "æŠ¥",
    "ä½´": "äºŒ",
    "éƒ›": "ç¦",
    "æ»": "èŠ‚",
    "æ ¨": "æ¬¡",
    "ç•¬": "èˆ",
    "é³Ÿ": "å°Š",
    "å„”": "æ„",
    "æ¹": "å…",
    "å—›": "æµ…",
    "ç®£": "å†Œ",
    "ç¾®": "è€•",
    "ç“": "å ",
    "å¾‰": "ç¾Š",
    "è’”": "çŸ³",
    "æ¯³": "ç¿ ",
    "éº¿": "è¿·",
    "é¹§": "è¿™",
    "è€”": "å­",
    "è™¼": "ä¸ª",
    "ç¼›": "å…¥",
    "æŠ": "ç›",
    "è¶": "å…ˆ",
    "ç©Œ": "è‹",
    "ç³ˆ": "è®¸",
    "çŠ·": "å¹¿",
    "äº ": "å¤´",
    "ä¾‰": "å®",
    "èˆ¾": "è¥¿",
    "è‹³": "ä¸œ",
    "æŸƒ": "é›¶",
    "å²–": "åŒº",
    "å½¯": "é£˜",
    "èš±": "å’‹",
    "é¢": "çª˜",
    "è¸¬": "è‡³",
    "ä¼’": "è¿›",
    "ç¡­": "å¿™",
    "æš€": "ç½‘",
    "æ¾": "äº‘",
    "æª†": "å±±",
    "å« ": "ç¦»",
    "ç†€": "è°",
    "è±": "å…«",
    "æ¡•": "å°±",
    "åœŠ": "é’",
    "ç˜†": "è‚¾",
    "ç‰´": "åº•",
    "ç¼µ": "é’»",
    "å£¸": "æ†",
    "æ³¶": "å­¦",
    "å¤¬": "æ€ª",
    "è›": "è¥",
    "é…": "ä»¥",
    "è¶¿": "ä»–",
    "é¤‰": "æƒ³",
    "ç½": "æŒº",
    "å˜–": "åˆ™",
    "å¼": "ä¸‰",
    "ç¦˜": "åœ°",
    "é²¡": "ç¦»",
    "é¦¿": "é©´",
    "è®¦": "èŠ‚",
    "å¿¸": "çº½",
    "åŸ‡": "æ°¸",
    "æ ˜": "å®œ",
    "ç —": "è½¦",
    "æº‡": "æ¥¼",
    "ç–’": "å‘¢",
    "å„": "ä¿Š",
    "å¿": "äºº",
    "æ¹”": "é—´",
    "è‘¦": "ä¼Ÿ",
    "è„·": "åˆ©",
    "æ½“": "ä¼š",
    "ç¬Œ": "ç‰™",
    "ç®": "å‰",
    "æ‘°": "è‚",
    "é•…": "æ¢…",
    "é ": "é»„",
    "ç¥”": "çˆ¶",
    "å½´": "ç€",
    "å„¹": "èµ",
    "å¿‹": "æ”¹",
    "ç–„": "å",
    "è­»": "åº”",
    "é¾’": "é¾™",
    "è¿®": "åˆ™",
    "é¦¡": "é£",
    "è‘š": "ä»»",
    "å«€": "ç´",
    "ç´œ": "äº‘",
    "å¥¨": "è®²",
    "å«ƒ": "çœŸ",
    "å¬®": "çƒŸ",
    "è££": "è„¸",
    "å™": "æ°”",
    "æ»ˆ": "å·",
    "æ©¦": "åŒ",
    "å": "æ",
    "è˜©": "çƒ¦",
    "çŒ": "åˆ©",
    "æ¨«": "é—´",
    "äº‡": "å—",
    "è‹ ": "æ°‘",
    "å›¥": "æŠ—",
    "ç”“": "å±",
    "å¿ª": "æ¾",
    "åµ¯": "æ“",
    "æµ¿": "é…",
    "èª®": "èŠ±",
    "è¶„": "å±…",
    "èŸ›": "æœ‹",
    "è®§": "çº¢",
    "è»": "ä½",
    "æ…": "ç‰¹",
    "éƒ•": "æˆ",
    "è©¤": "è°",
    "è‘": "ç¯",
    "æ½¶": "é»‘",
    "æ“": "æ¢",
    "é±‰": "æ†‹",
    "ç®‹": "é—´",
    "ç‚»": "çŸ³",
    "è¤Š": "æ‰",
    "è½«": "ä»»",
    "æŠ": "åˆ™",
    "ç¡‡": "è„‘",
    "ç¸¦": "å®—",
    "ç¦Š": "ç³»",
    "æ¢¶": "ä¼Ÿ",
    "é‹­": "ç‘",
    "é•¤": "è‘¡",
    "çœˆ": "å•",
    "é¡—": "ä»¥",
    "å‡¥": "å±…",
    "è ±": "å¤",
    "å´‹": "è¯",
    "å¤": "å ",
    "æ…Ÿ": "ç—›",
    "è·š": "å±±",
    "ç¤¤": "æ“¦",
    "ç¶˜": "é€¢",
    "ç": "åŒ",
    "ç˜¡": "çª—",
    "é¢±": "å°",
    "è—´": "è¿",
    "æ ¦": "æ„",
    "ç¶½": "ç»°",
    "è—”": "èŠ",
    "è¼¶": "ç”±",
    "ç°": "æ¥¼",
    "é½¤": "å…¨",
    "ç“": "é¾™",
    "å½¿": "ç¦",
    "å¬¶": "é¼»",
    "é¾–": "è¾¾",
    "å«²": "å—",
    "è­²": "è®©",
    "é˜‹": "ç³»",
    "ç®ª": "å•",
    "éºˆ": "ä¸»",
    "ä¸†": "ç½•",
    "ç¢¹": "ç‚«",
    "å¤": "åˆ©",
    "è£’": "å‰–",
    "å»¸": "æ•Œ",
    "ç˜": "é›¨",
    "ç±“": "ç¿»",
    "ç­": "æŸ",
    "é’¶": "ç§‘",
    "ç‘º": "é•¿",
    "çœ¢": "å†¤",
    "é¾¤": "é‹",
    "ç¿š": "ç°",
    "ä»´": "å§",
    "æ’™": "å°Š",
    "ä¿": "è‰¯",
    "ç·±": "å‹¾",
    "æ‚“": "æ¬ ",
    "ç¹¹": "æ„",
    "ä¸—": "æ˜¯",
    "é‚ ": "å½¬",
    "ä¾·": "å±€",
    "ç‹…": "ç‹‚",
    "å‚": "ç¼©",
    "èœ©": "æ¡",
    "è¤“": "å®",
    "æ±”": "æ°”",
    "æŸ": "æ•´",
    "æ¯µ": "ä¸‰",
    "è‡": "è¿",
    "ç¹‹": "è®°",
    "æ›¡": "å ",
    "åŸ¼": "å…¶",
    "ç¼‹": "ä¼š",
    "å¶‰": "å‚¬",
    "è™®": "å‡ ",
    "ç¾": "ç…§",
    "é¸¶": "æ€",
    "ç‘†": "æ˜Ÿ",
    "æ­¿": "å¢¨",
    "æ…‰": "ç»­",
    "è¬": "å¤œ",
    "å‘¾": "è¾¾",
    "æ¿º": "è§",
    "æ¶³": "ç©º",
    "é¾": "å’Œ",
    "è‰§": "æˆ–",
    "è²": "ç¦»",
    "å¶¶": "å¾®",
    "è¥": "è¥¿",
    "ç¯ƒ": "å¦¹",
    "è‚·": "æµ…",
    "çµ³": "é™",
    "ç¢¶": "æ°”",
    "é¯–": "ç",
    "ç¿›": "æ¶ˆ",
    "é˜": "æ¶",
    "è½µ": "æŒ‡",
    "è£": "å®š",
    "é¶": "æœº",
    "è¨›": "é¢",
    "å„¯": "è…¾",
    "çœ›": "å¦¹",
    "æ‚±": "åŒª",
    "é‹¬": "åˆ¤",
    "çµœ": "èŠ‚",
    "ç¹": "æˆ",
    "ç€": "åˆ©",
    "ç›¦": "å®‰",
    "éŠŠ": "ç»­",
    "çœµ": "åƒ",
    "å©¹": "å’¬",
    "å¶ƒ": "å±•",
    "ç°": "è·¯",
    "è¥¤": "è“",
    "å’¾": "è€",
    "ç–": "ä¹¦",
    "æ ": "æ€ª",
    "è„§": "æ",
    "ç‰®": "è§",
    "æ¶—": "ç¡",
    "è…™": "å®—",
    "æ½": "åš",
    "ç¤…": "è¹²",
    "æ»º": "ä¼˜",
    "ç«…": "å·§",
    "æ¯": "åˆ€",
    "å´¡": "å«",
    "é¢¥": "å¦‚",
    "éœ™": "åº”",
    "èŒ“": "å­¦",
    "é§„": "é©®",
    "é °": "å¤¹",
    "è”´": "éº»",
    "çª­": "å·¨",
    "ç‡»": "ç†",
    "é´": "æ¶ˆ",
    "é”¾": "ç¯",
    "å„": "ä½ ",
    "èŸŠ": "æ¯›",
    "æ¡«": "ç¼©",
    "æ£¿": "å°¼",
    "çŠ®": "æ‹”",
    "ç®¢": "å†¤",
    "æ£§": "æˆ˜",
    "å´š": "æ£±",
    "éŸœ": "æ",
    "æ¶‡": "ç²¾",
    "è´„": "è‡³",
    "å„": "å‚",
    "è¦Œ": "å…³",
    "é‰¨": "æ´—",
    "è’‡": "é“²",
    "ç€": "å«",
    "å¹Ÿ": "è‡³",
    "æ¾¥": "è°¢",
    "é¯¤": "æ˜†",
    "ç¾¥": "æŠ¢",
    "å‰‰": "é”™",
    "é¤¸": "å®‹",
    "èš": "è€",
    "å¹": "ç¦",
    "å—†": "æª",
    "å‰€": "å‡¯",
    "ç°¨": "æŸ",
    "éŠ": "æ£’",
    "å¬¸": "å®¡",
    "è¡•": "ç—›",
    "å„": "è½¯",
    "èš¿": "é—²",
    "è•º": "æ",
    "æ›–": "çˆ±",
    "ç´“": "ä¹¦",
    "å„š": "ç›Ÿ",
    "æµ": "æ¶ˆ",
    "è­™": "å·§",
    "ä¼€": "ä¸­",
    "å›„": "ç¦»",
    "çº™": "è½",
    "èº‹": "æœº",
    "é¾": "æ¸©",
    "æ¸•": "å†¤",
    "å¹‹": "ç›˜",
    "æµ": "é’±",
    "èº…": "ç«¹",
    "è": "é™ˆ",
    "æº½": "å…¥",
    "ç¾": "ä½",
    "æ˜œ": "ç¾Š",
    "æ£¼": "åŸ",
    "é‚¡": "æ–¹",
    "éˆ»": "å››",
    "ç«¤": "çº¢",
    "ç¼«": "éªš",
    "ç–": "è›‹",
    "æ²": "æ¥",
    "é§™": "çˆ¶",
    "çƒ ": "å›",
    "åŸ–": "èŠ±",
    "å¦¡": "å¿ƒ",
    "æ †": "æ—©",
    "æ€«": "ç¦",
    "å¾": "æ„¤",
    "è˜š": "æ˜¾",
    "è¶±": "èµ",
    "å«œ": "å¼ ",
    "è§‹": "ä¹ ",
    "è‰£": "é²",
    "ç‰‹": "é—´",
    "å˜": "æ¥¼",
    "åˆ¿": "è´µ",
    "åª¿": "æºƒ",
    "è„ ": "å±±",
    "ç": "è¿›",
    "å©‡": "é‡‡",
    "è€©": "è®²",
    "é”–": "æª",
    "è¡³": "ä¸­",
    "ä»Œ": "å†°",
    "å": "ç§°",
    "é¹†": "ç‰",
    "ç¼²": "æ•²",
    "é¥¸": "å’Œ",
    "å‹£": "æœº",
    "æ—›": "ç¿»",
    "è¤€": "å…¶",
    "ç—²": "éº»",
    "æ¢": "å›º",
    "ç¦±": "å¯¼",
    "å†‘": "æ˜¼",
    "ç‰": "åº¸",
    "èˆ—": "ç€‘",
    "åµ—": "å²",
    "å‹¹": "åŒ…",
    "çŠ°": "æ±‚",
    "æ£»": "åˆ†",
    "ä¾Ÿ": "å­˜",
    "å¡’": "çŸ³",
    "è›‘": "è°‹",
    "å«‡": "æ˜",
    "çˆ€": "è´º",
    "å±»": "ä»»",
    "è„•": "ä¸‡",
    "é‚½": "å½’",
    "é•Ÿ": "ç‚«",
    "ç‚µ": "é€š",
    "èº¶": "è£¸",
    "äº–": "å››",
    "éŒ³": "çŒ›",
    "é¾": "è¢«",
    "é¤¡": "ç°",
    "ç¬€": "å¿™",
    "é¤": "é¡¶",
    "åŠ„": "æ‰",
    "å†‹": "çª˜",
    "å«‹": "å°¿",
    "å•": "æ‹›",
    "æ‰º": "æŒ‡",
    "ç™": "é±¼",
    "ç¦°": "è¿·",
    "æ¦": "å‡",
    "å¤¨": "è´£",
    "å£’": "çˆ±",
    "éš‚": "å› ",
    "æ²": "å ",
    "æ ­": "è€Œ",
    "è‹€": "è¡Œ",
    "æ›š": "ç›Ÿ",
    "ç«": "ç—…",
    "è ‹": "ç«¹",
    "è§": "é±¼",
    "æ€®": "ä¼˜",
    "ç±": "ä¸‹",
    "æ…¬": "ç´",
    "ç¤¿": "æœˆ",
    "æ¥¢": "ç”±",
    "æŒ": "å±•",
    "ä¸¬": "å¼º",
    "é§": "é©®",
    "å¾§": "å˜",
    "ç²": "è¥¿",
    "æº»": "ä»–",
    "ä¿†": "å¾",
    "æ¿œ": "è¿›",
    "åµ«": "å§¿",
    "é¢ƒ": "è¡Œ",
    "å‹©": "æ„",
    "åˆ­": "äº•",
    "é”¼": "æœ",
    "è„": "å“¼",
    "éŒ": "è¿",
    "æ…«": "è€¸",
    "åš": "å‡",
    "å²": "ä½›",
    "å¢": "ç³»",
    "æ°¼": "é€†",
    "é¹®": "ç¯",
    "å°": "è°¢",
    "å´": "é”…",
    "æª": "ç´",
    "ç˜œ": "è¥¿",
    "å»²": "ç¦»",
    "æ¯†": "æ¬§",
    "æ´«": "ç»­",
    "è": "è›‹",
    "æ£©": "å†¤",
    "å´„": "æ˜¾",
    "å‚ˆ": "åˆ©",
    "æ§¦": "åº¸",
    "é¦±": "é©®",
    "é™¥": "ç°",
    "åº¨": "æ¶ˆ",
    "å¡": "æ£’",
    "æ¨¿": "å–„",
    "ç¸": "ç€",
    "ç˜": "å…³",
    "é¾†": "æ¡",
    "è‰Ÿ": "å†²",
    "å—±": "æ‹¿",
    "é¡¸": "æ†¨",
    "èš": "çš®",
    "é¬ˆ": "å…¨",
    "æª›": "æŠ“",
    "ç •": "å²",
    "èˆ£": "ä»¥",
    "è¶”": "è£‚",
    "è°µ": "å ",
    "èš°": "ç”±",
    "æ²º": "ç”°",
    "æ–‰": "å…¶",
    "çƒº": "æµª",
    "å‹": "æƒ…",
    "å¶ ": "æ•™",
    "å™": "éœ",
    "é°²": "æ•–",
    "ç«’": "å…¶",
    "è ¼": "å–",
    "å†ƒ": "å¸½",
    "å¾": "äº†",
    "ç¯": "ç†",
    "å€³": "å­—",
    "è„™": "ä¿®",
    "è³": "å¿…",
    "è„¡": "æŒº",
    "æ½¯": "å¯»",
    "çŒ": "èˆ",
    "æ›": "è®°",
    "é»´": "æ¢…",
    "å‹²": "ç†",
    "å’¢": "æ¶",
    "é¡’": "åº¸",
    "æœ£": "åŒ",
    "è†‚": "æ—…",
    "è†¿": "å†œ",
    "ç”": "å¹³",
    "æ¹²": "åŸ",
    "æŒ’": "è£‚",
    "ç”¶": "ç¦",
    "çº": "è°¢",
    "è‰¼": "å¬",
    "ä¾˜": "å·®",
    "çš¢": "å°",
    "å¦½": "æ·±",
    "ç¬¢": "æ•",
    "æˆ”": "é—´",
    "é”ƒ": "èµ ",
    "ä¿µ": "æ ‡",
    "é‰¬": "æœ¨",
    "ç²": "è¢«",
    "è’´": "ç¡•",
    "å±²": "æŒ–",
    "æ–»": "è¡Œ",
    "æŠ": "å‰–",
    "æ‚ƒ": "æ†",
    "å„–": "è“",
    "é¹š": "ç“·",
    "å¥€": "æ©",
    "ä½²": "å‘½",
    "å°œ": "å˜",
    "èª": "è¡",
    "è›": "æ‰©",
    "è„’": "ç±³",
    "æ°": "å–",
    "æ„¯": "è€¸",
    "é³š": "ä½",
    "è°": "ç ",
    "é°¨": "å¡”",
    "æ„": "å‰",
    "è£": "ç‰¹",
    "è·½": "è®°",
    "å—­": "ç›´",
    "ä»›": "æ‹–",
    "æˆ½": "äº’",
    "è³„": "ä¼š",
    "é‡µ": "æ‹†",
    "å¯–": "è¿›",
    "æ©€": "è¥¿",
    "çƒ": "æŸ“",
    "é”³": "åº”",
    "è«®": "å§¿",
    "å½": "æ",
    "ç®": "ç",
    "è­": "æœº",
    "æ·°": "å¹´",
    "ç§": "æ‘‡",
    "æ¶¶": "æ‹–",
    "é­®": "çš®",
    "è„Ÿ": "è£‚",
    "æ¯": "çŸ®",
    "è“…": "åˆ˜",
    "ä¾Š": "å…‰",
    "ç•Š": "è€•",
    "å»’": "æ•–",
    "äº£": "å¤§",
    "è·¢": "å •",
    "é˜": "æª",
    "å™": "è®¾",
    "å¦‹": "å¤«",
    "é…¤": "å§‘",
    "åƒ¬": "æ•™",
    "å³«": "é‹",
    "çœ°": "å ",
    "ç¥„": "è°¢",
    "éˆ¡": "ä¸­",
    "éƒ¿": "æ¢…",
    "å¿": "æ°‘",
    "æ˜": "ç‘",
    "æ¼": "å¤œ",
    "å’": "èºº",
    "è€": "é™ˆ",
    "åŠ": "é›¨",
    "å±¸": "é¾™",
    "ç™”": "æ„",
    "ç¡‚": "å…¨",
    "éŒ¡": "å…¶",
    "è£°": "å¤š",
    "è¿—": "é¢",
    "é“¹": "åŠ³",
    "åŸ¯": "ä¿º",
    "å”´": "å‘›",
    "ç•¤": "è‡³",
    "çœ³": "æ˜",
    "æµ‰": "è¯—",
    "æ–": "æ„",
    "é±§": "æ",
    "é»¡": "çœ¼",
    "ç™±": "è´ª",
    "è½¹": "åˆ©",
    "è·¸": "å¿…",
    "å‚": "é’±",
    "è´“": "è„",
    "ä»ˆ": "å…«",
    "æš ": "æ",
    "å£·": "èƒ¡",
    "æ’¶": "å",
    "é”“": "å¯",
    "é¦‡": "æ’",
    "é˜¼": "åš",
    "è­": "å",
    "æ‘…": "ä¹¦",
    "èˆ": "å¢¨",
    "æƒ": "å‡",
    "è„ª": "ä¿¡",
    "è‹ˆ": "åˆ©",
    "èº°": "ä½“",
    "åŠ´": "åŠ³",
    "èŸ™": "ç›´",
    "é–¹": "çƒŸ",
    "çŠ": "é€Ÿ",
    "ç­§": "å‡",
    "æ€´": "ç»­",
    "ä¼–": "èºº",
    "æ—¼": "æ°‘",
    "å†„": "æŸ“",
    "å´€": "æµª",
    "æª¤": "åˆ°",
    "è„­": "æˆ",
    "ç’¾": "å§¿",
    "å¢¼": "æœº",
    "æƒ£": "æ€»",
    "èª¾": "é“¶",
    "ç†¼": "æ„",
    "é¦‘": "ç´§",
    "æ­": "å¤©",
    "ç¶®": "èµ·",
    "èª¥": "å‘Š",
    "äº±": "å¤œ",
    "èˆ²": "é›¶",
    "çª°": "æ‘‡",
    "å²": "ç»",
    "æ‹¶": "åŒ",
    "é‘š": "é’»",
    "ç¤½": "ä»",
    "æ¹“": "ç›†",
    "è¨–": "æ°”",
    "æœ¿": "æ¬¡",
    "å„¼": "çœ¼",
    "è„": "æ“",
    "æ¡„": "å…‰",
    "å¸€": "åŒ",
    "é¹": "æ¯",
    "æ–º": "é“²",
    "ç": "èƒœ",
    "é¹‡": "é—²",
    "ç˜®": "è‚¾",
    "èµ±": "èµ°",
    "è¶¯": "æ›¿",
    "æ›±": "çº¦",
    "ç¶‘": "æ†",
    "ç‡¹": "æ˜¾",
    "æ¸–": "å®¡",
    "å½": "æ­Œ",
    "ç©": "ç¨³",
    "æ£¤": "é”™",
    "é²€": "å",
    "ç£¡": "çœ‹",
    "é€º": "è¿œ",
    "å™": "å…‰",
    "æšŒ": "å¥",
    "èˆ¯": "ä¸­",
    "è‚": "åˆ«",
    "è¦€": "è¥¿",
    "å²•": "å€Ÿ",
    "æ¸‹": "è‰²",
    "å ": "æ„£",
    "æš": "ç¡¬",
    "ç‡": "å¤œ",
    "è´³": "æ˜¯",
    "é³¢": "æ",
    "è“³": "ç´§",
    "èš¡": "åŸ",
    "ç§‚": "äºº",
    "é¦˜": "å›½",
    "ç„Œ": "ä¿Š",
    "å¿": "è½°",
    "ä¹¬": "å·¨",
    "å„•": "æŸ´",
    "èˆ´": "åˆ™",
    "æ»": "ä¸‘",
    "é¨": "å¥",
    "é‘´": "è¥¿",
    "é’”": "é—¨",
    "é«‘": "æ¯’",
    "ä¼›": "é›¨",
    "ç’¶": "è¿›",
    "è©": "æºƒ",
    "æœ²": "äºº",
    "é”©": "å·",
    "ç¥˜": "ç®—",
    "æ ¤": "ç—…",
    "æµ•": "è¿›",
    "éŸ¡": "ä¼Ÿ",
    "ç‰¬": "è¢«",
    "æ¿…": "è¿›",
    "æ ´": "å ",
    "åŸ›": "çª˜",
    "çµº": "åƒ",
    "ç¡": "ç‰™",
    "è“†": "ä¹ ",
    "æ‡¾": "è®¾",
    "æ·¯": "ç‰",
    "æ‘½": "æ ‡",
    "æ‹ ": "å·¨",
    "éºª": "é¢",
    "å¼": "äºŒ",
    "å‰·": "é“²",
    "ç‰": "ç‚¸",
    "æ¦ƒ": "è°ˆ",
    "ç³¨": "é™",
    "èŠ": "äº’",
    "é¹ª": "æ•™",
    "ç": "è®¸",
    "çº“": "åº”",
    "è…˜": "å›½",
    "ç™¯": "å–",
    "ç´¶": "åŒº",
    "æ˜º": "é¥¼",
    "è¢": "æ»š",
    "è†": "ä¸“",
    "è“Š": "ç“®",
    "æ´€": "ç›˜",
    "æ’±": "ä¼Ÿ",
    "åª": "ç—…",
    "æ½½": "é“º",
    "ç·¢": "æ",
    "åš†": "è’¿",
    "é»": "åƒ",
    "æ°‡": "æ’¸",
    "å¬ˆ": "æ‰°",
    "ç™€": "é»„",
    "æ°ˆ": "å ",
    "å„³": "ç¼ ",
    "è‘": "ç¾",
    "å§": "æ‰",
    "ç‚°": "è¢",
    "ä½‡": "åŠ©",
    "ç«": "å®¡",
    "è›±": "å¤¹",
    "èµ™": "çˆ¶",
    "æ¶–": "åˆ©",
    "è¸¯": "ç›´",
    "é•š": "ç”­",
    "å»‘": "ç´§",
    "è¥": "æ¯›",
    "æ®ª": "æ„",
    "æ®¢": "æ›¿",
    "æ¦±": "å‚¬",
    "ç¹": "çˆ±",
    "çª": "è›‹",
    "æ¾‹": "çº¢",
    "å·¯": "æ±‚",
    "ç±£": "è“",
    "ç†´": "æ˜†",
    "åº¯": "ä¸",
    "ç€£": "è°¢",
    "ç§": "å·",
    "è£‘": "æ·±",
    "é•ˆ": "åš",
    "é”«": "åŸ¹",
    "è½±": "å§‘",
    "è¸½": "ä¸¾",
    "å›": "è‚",
    "èœ‰": "ç¦",
    "æ’¹": "è§’",
    "é—": "å‘",
    "å¢•": "ç‡•",
    "ç¢¥": "æ‰",
    "è„¦": "çš„",
    "è³§": "æ¢",
    "è“": "ç´",
    "è•»": "çº¢",
    "æ‚˜": "ä¸€",
    "å›‚": "æ¶ˆ",
    "è”°": "äº’",
    "æ—‡": "æ‰¹",
    "ç¾‘": "æœ‰",
    "ç¥¤": "é›¨",
    "è…§": "æ ‘",
    "è·˜": "ç›˜",
    "æ€±": "èª",
    "ç²›": "é€Ÿ",
    "å¢¬": "åœ°",
    "é¯¶": "æ¢",
    "ç¹": "ç§€",
    "éº‡": "å›",
    "é”¿": "å“€",
    "è‰¨": "ç›Ÿ",
    "ç½": "å±…",
    "ç½ˆ": "è°ˆ",
    "æ¾‰": "æ„Ÿ",
    "å‡•": "å‘½",
    "æƒ“": "å…¨",
    "èª¨": "ä¼š",
    "å²†": "å’¬",
    "è¸´": "æ°¸",
    "ç˜‰": "ç‰",
    "ç€·": "æ„",
    "æ†³": "å¦",
    "å•": "å",
    "å¿›": "ç¿»",
    "æ": "å®£",
    "è©®": "å…¨",
    "ç¸¯": "çœ¼",
    "ç³°": "å›¢",
    "é µ": "æ™•",
    "å¯": "å‡",
    "æº¿": "åˆ¤",
    "é¢": "ä¹Œ",
    "å¬": "æ€»",
    "èŒ": "æ‹†",
    "çš’": "é¢",
    "éªµ": "ä½“",
    "ç´¥": "åŒ",
    "åš„": "éœ",
    "é˜¬": "å‘",
    "è›²": "è„‘",
    "å¹“": "å±±",
    "æª¸": "å‡",
    "èŸ³": "å¯»",
    "æ¨‹": "é€š",
    "ç•": "çŠ¬",
    "å“–": "å¹´",
    "çˆ£": "èºº",
    "èœŠ": "ç¦»",
    "é ¹": "æ¨",
    "é™´": "çš®",
    "æ«“": "é²",
    "çœ™": "å®œ",
    "çŸ": "ç‰",
    "é¡±": "èŠ¦",
    "é¯": "é¡¿",
    "å‹§": "åŠ",
    "è¬š": "æ˜¯",
    "æ¤¾": "é—´",
    "ç”¡": "æ·±",
    "è„œ": "æœ‰",
    "åµ": "æ¥¼",
    "è¾‹": "ç½‘",
    "å½†": "è¹©",
    "ç’£": "æœº",
    "èŒ": "æŒ‰",
    "è… ": "å‡‘",
    "è”": "ç›Ÿ",
    "æ¨°": "é›ª",
    "è¥Œ": "å•",
    "ç½¥": "å€¦",
    "éµŸ": "ç‹‚",
    "ç·µ": "å®—",
    "ç’‰": "è„¸",
    "èŸœ": "è§’",
    "é‡ˆ": "æ˜¯",
    "ç·": "æ–­",
    "ç˜µ": "å¯¨",
    "çŒ‹": "æ ‡",
    "æ‘Ÿ": "æ¥¼",
    "äº": "å¹²",
    "å©": "å¹²",
    "èª¡": "å€Ÿ",
    "é£ˆ": "æ ‡",
    "å—¶": "å¿…",
    "æ“°": "å‡",
    "åŠ§": "æŒ‡",
    "é°•": "è™¾",
    "è’—": "æµª",
    "å‹¶": "æ’¤",
    "é—”": "å’Œ",
    "è‰": "è·¯",
    "é‘£": "æ ‡",
    "çœ•": "æ•",
    "å¾ ": "æ¥",
    "å¿¾": "å¼€",
    "å™µ": "åˆ°",
    "æ¾‚": "æˆ",
    "è’„": "å…³",
    "å¹™": "æœ¨",
    "ç‰œ": "ç‰›",
    "è›¦": "å®œ",
    "éˆ·": "å¤",
    "çŒ€": "æ²™",
    "å¢": "ä¸‘",
    "ç°°": "æ’",
    "ç˜§": "è™",
    "ç¹™": "ç¿»",
    "è–Š": "è®°",
    "å¡": "æˆ˜",
    "ç†¥": "è…¾",
    "é¹¨": "å…­",
    "æªº": "æ",
    "å½›": "å®œ",
    "çª€": "å‡†",
    "æ²²": "é©®",
    "å¹€": "æŒ£",
    "å’ƒ": "æ‹–",
    "é¸€": "æ¥š",
    "ç—": "é±¼",
    "ç¤Œ": "é›·",
    "è¨¶": "å–",
    "ç”Œ": "æ¬§",
    "é¥‹": "æºƒ",
    "å˜ ": "å˜",
    "é¼´": "çœ¼",
    "æ·‚": "å¾·",
    "è“Œ": "é”™",
    "æ³¦": "å±€",
    "ç¸ ": "èƒ¡",
    "ç§": "é—¨",
    "æ": "å¤¹",
    "ç­Š": "è‚–",
    "æ½Ÿ": "ç³»",
    "å«": "è„‘",
    "é¤¼": "ç³»",
    "é†‘": "è®¸",
    "è¨‡": "è½°",
    "å”…": "å«",
    "é„ ": "äº’",
    "æª³": "å½¬",
    "æ¸“": "è¥¿",
    "å¡­": "æ¸©",
    "åª±": "æ‘‡",
    "é–": "å˜",
    "é²±": "é£",
    "å†¿": "é—´",
    "è¤™": "è¢«",
    "ç¬µ": "é¥­",
    "é‰‰": "ç‚«",
    "æ•": "é›¶",
    "å¼‡": "çœ¼",
    "èµ‘": "å¿…",
    "æ”°": "è´µ",
    "ç‘": "ç¼ ",
    "éŒ•": "æ˜†",
    "è€‘": "ç«¯",
    "èŒº": "å†²",
    "æ¥¨": "çœŸ",
    "è¤": "æœ",
    "è»": "ä½ ",
    "è¨": "é¢œ",
    "ä½": "å‹¾",
    "ç·ˆ": "æ€§",
    "ç´±": "ç¦",
    "å¹ˆ": "å¹³",
    "å’µ": "å®",
    "è¦ˆ": "å’Œ",
    "æ¶": "çˆ·",
    "è¢": "æŠ›",
    "è•—": "è·¯",
    "ç™ƒ": "é¾™",
    "å¡²": "é•¿",
    "ç…": "åº™",
    "ç¶": "è´µ",
    "å¢š": "è‰¯",
    "è…": "ç­”",
    "æ‘": "æ",
    "éˆ§": "æŠ—",
    "é²¦": "æ¡",
    "ç¡š": "æ¡¥",
    "æ–¾": "é…",
    "æµ›": "å«",
    "æ¦": "è°¢",
    "è«³": "å®‰",
    "è¼‹": "èˆ",
    "çœ‚": "æ˜¯",
    "é¬": "é—´",
    "æ¼£": "è¿",
    "è¬": "ä¹°",
    "é«„": "é«“",
    "å‹": "æ˜¯",
    "æ±§": "å‰",
    "ç": "å°¬",
    "é…´": "å›¾",
    "æ¦¼": "ç§‘",
    "åˆ¬": "é“²",
    "çœ": "æ¥¼",
    "å¾": "æ±‰",
    "é”¬": "è°ˆ",
    "èƒ¼": "å",
    "é²": "å‹¾",
    "åµ": "çœ‹",
    "åŠ": "æ±¤",
    "ç’¢": "åˆ˜",
    "æ“": "ä¹¦",
    "è¢¼": "æ­Œ",
    "ç²º": "è´¥",
    "é©’": "é©®",
    "ç‡¬": "æ¯",
    "ç„ ": "ç¿ ",
    "èŸ„": "å“²",
    "åŠ…": "ç€",
    "ç–‡": "æ„",
    "å¤": "ç”±",
    "ç·¥": "å®",
    "çš›": "å°",
    "è„": "è¯¥",
    "å¨¸": "ä¸ƒ",
    "ç «": "åŠ©",
    "å±£": "æ´—",
    "çµ«": "å’",
    "ç©¨": "æ¨",
    "ç·ƒ": "å®—",
    "éŠ¨": "ä¿º",
    "å›": "å¬",
    "æ´‘": "ç¦",
    "ç®¹": "çº¦",
    "è¹“": "æºœ",
    "å€": "å¹²",
    "ç˜Š": "å–‰",
    "æ±Œ": "ä¸²",
    "æ‡Ÿ": "å¯¹",
    "ç‹³": "é±¼",
    "é¸š": "åº”",
    "æ³‡": "å®¶",
    "è¾®": "å˜",
    "ç¥µ": "æ†",
    "ä½‹": "æ‹›",
    "ç‹Œ": "ç”Ÿ",
    "æ…œ": "æ•",
    "æŒ±": "æ’’",
    "é‘µ": "çŒ",
    "ç€": "å¸½",
    "ç”´": "ç‚¸",
    "é¬": "çº ",
    "æ»": "æ„",
    "é½§": "è‚",
    "èª¸": "é—²",
    "é®°": "å›",
    "é”±": "å§¿",
    "æ‰™": "å¸",
    "å£Ÿ": "å„",
    "æ•¹": "èŠ",
    "å¤›": "å¤š",
    "è±‹": "ç¯",
    "æ¥„": "å",
    "å¸¹": "ç…",
    "é‰": "æœˆ",
    "è¯": "æœ¨",
    "æ´“": "è‰²",
    "æ•": "ç»­",
    "é¬¨": "çº¢",
    "è—ª": "æœ",
    "å¹†": "æ„",
    "åŸ¡": "å‘€",
    "ä¾": "å¦‚",
    "ç‹´": "å¿…",
    "èœ": "é™¤",
    "é¥": "å–„",
    "è®": "å“²",
    "è˜€": "æ‹“",
    "åš¶": "åº”",
    "é…¾": "ç­›",
    "ç«»": "ä¹",
    "é“": "å¯¹",
    "ç««": "é™",
    "è¦¬": "è®°",
    "è‚€": "ç‰",
    "è–": "è€",
    "æ€ƒ": "äº”",
    "æ¸§": "åœ°",
    "ç³¬": "é¼ ",
    "é": "ç—…",
    "å²": "å¼•",
    "ç€Ÿ": "æ¶ˆ",
    "é¥—": "æƒ³",
    "çº": "ç†",
    "å·’": "é¸¾",
    "è‚ˆ": "ç…§",
    "é¥¹": "äº†",
    "å•€": "åŸƒ",
    "æ¿°": "ç»´",
    "æ¬µ": "æ¬¾",
    "è‘œ": "æ",
    "æ¤ ": "æ¬ ",
    "æ¹": "çƒŸ",
    "äº¸": "æœµ",
    "è": "å†Œ",
    "é¼": "çœŸ",
    "ç›": "ç§€",
    "å€¢": "èŠ‚",
    "æ€": "å‡",
    "è·‚": "å…¶",
    "è¢‘": "ç»",
    "å€“": "è°ˆ",
    "é–™": "é—¹",
    "é±½": "åˆ€",
    "çµ": "è£¤",
    "æªŠ": "å¹²",
    "é¼©": "å–",
    "å€": "ç­",
    "é±¨": "é•¿",
    "é‰š": "æŸ³",
    "ç¦¸": "æŸ”",
    "æ": "æ··",
    "å’¹": "æ¶",
    "æ¾´": "ç¯",
    "æ‚’": "æ„",
    "è²": "å°¼",
    "åˆ": "å®¹",
    "è²½": "å®œ",
    "å¯": "å“’",
    "èŠ¼": "å¸½",
    "å·‰": "ç¼ ",
    "ç­¢": "çˆ¬",
    "é”": "ç’",
    "é‚‡": "è€³",
    "åš™": "è‚",
    "é„”": "ä¹Œ",
    "ç•": "å¹²",
    "å”¦": "æ²™",
    "é»¹": "æŒ‡",
    "å„œ": "å‡",
    "æº¾": "å“€",
    "ç¬“": "å¿…",
    "ç˜‘": "é”…",
    "ç° ": "è¾…",
    "ç±¶": "æ·±",
    "ç—„": "å’‹",
    "æ†š": "è›‹",
    "å›®": "é¢",
    "èº„": "å¿…",
    "è¼“": "ç¢—",
    "å§½": "é¬¼",
    "å‚—": "å¤„",
    "ç°©": "åŠ³",
    "æ™¸": "æ•´",
    "èƒ¬": "åŠª",
    "å§ˆ": "é›¶",
    "è¹": "å‡",
    "æ¬·": "è¥¿",
    "å¸‡": "è‚",
    "è‹–": "æ•Œ",
    "ç˜…": "å•",
    "è­º": "çˆ±",
    "åª–": "åº”",
    "é¡•": "æ˜¾",
    "ç¾–": "å¤",
    "ç¶…": "äº²",
    "è¹°": "é™¤",
    "çœŠ": "å¸½",
    "ç°³": "æ„Ÿ",
    "ç²¦": "æ—",
    "é†µ": "å·¨",
    "é•": "æ‹¿",
    "æ©ˆ": "æ‰°",
    "ç€": "æœ",
    "é ·": "æ±‰",
    "ç¤«": "åˆ©",
    "å•˜": "å¤œ",
    "è·": "åˆº",
    "è¡": "çœ‹",
    "æŸ¸": "åŸ¹",
    "æ¹š": "å°",
    "æ": "å–„",
    "è‚¸": "è¥¿",
    "è³‘": "é•‡",
    "æ¹£": "æ•",
    "çšŠ": "é›¶",
    "å£ˆ": "æ‡’",
    "ç¿™": "ä¼š",
    "åŒ‹": "æ¡ƒ",
    "æšŸ": "å‡¯",
    "è¾": "å…¨",
    "æ‘": "è´£",
    "å«„": "åŸ",
    "å®”": "ä¸»",
    "å²‹": "æ¶",
    "é€‡": "é¡¿",
    "å…Š": "å¯¹",
    "ç·Œ": "ç‘",
    "è€²": "æ€€",
    "é“": "æ‰",
    "è±": "æ•–",
    "é£": "å±•",
    "ä»¹": "é£",
    "è®±": "ä»»",
    "æ˜³": "å ",
    "è‡“": "è—",
    "è¬¾": "ç’",
    "å–": "åº¸",
    "å“": "å§",
    "æ¯Š": "æ¶ˆ",
    "ç”ƒ": "æ˜¼",
    "é£–": "æ‘‡",
    "é„š": "å¸½",
    "æ¾": "å®",
    "é’": "è§‰",
    "é£´": "å®œ",
    "è½³": "èŠ¦",
    "å„ ": "è£‚",
    "çŠ¢": "æ¯’",
    "éº¯": "åŒº",
    "çšŸ": "åˆ™",
    "ç“©": "å‰",
    "èˆ¨": "æ¿",
    "è ": "æ”€",
    "é”": "å®¹",
    "èŸª": "ä¼š",
    "è‹": "åš",
    "å§": "å±±",
    "é’‚": "èºº",
    "æ•”": "é›¨",
    "å¼™": "ä¹Œ",
    "è³¶": "ä»“",
    "æ ¿": "ç¦",
    "è©": "å¤",
    "é²¥": "çŸ³",
    "ç‡‚": "è°ˆ",
    "èƒ©": "å¡",
    "æª“": "æ¯",
    "å¾": "ç£",
    "è»": "å®‰",
    "å„¨": "è‡³",
    "éˆ…": "æœˆ",
    "ç¨ƒ": "å¤«",
    "ç°ƒ": "å®œ",
    "æ¥›": "äº’",
    "èˆ¢": "å±±",
    "é©¥": "è®°",
    "èŠ»": "é™¤",
    "é€": "å¾·",
    "ç«®": "å¹³",
    "å ”": "æ·±",
    "é‘›": "çŸ¿",
    "è»": "ç”·",
    "é™": "å‡³",
    "è–†": "çˆ±",
    "ç¥¦": "æ— ",
    "ç¤": "æ¶¦",
    "é„—": "å·",
    "ç±”": "æœ",
    "æ§": "å¥³",
    "çš•": "å¿…",
    "ç€˜": "èŠ¦",
    "é‡—": "æ‹›",
    "å‚’": "è¥¿",
    "åˆ•": "ç¦»",
    "è»š": "å¸¦",
    "èŸ“": "å‘",
    "æ¿©": "æˆ–",
    "æ’": "å‘€",
    "çœ": "è¥",
    "è¨": "è¯—",
    "è¹»": "ç»",
    "æ‹ƒ": "çœ¨",
    "æ¿“": "è¿",
    "å˜¡": "æ±¤",
    "è¹§": "ç³Ÿ",
    "å¥¼": "å·®",
    "æ­›": "æ†¨",
    "çº": "å¡”",
    "æ´œ": "è½",
    "ç¤“": "å°†",
    "å€": "é™",
    "æ½": "ç‰",
    "æ«§": "æœ±",
    "æ¢¾": "æ¥",
    "èš¸": "åˆ©",
    "å·˜": "çœ¼",
    "è¾µ": "ç»°",
    "è©‡": "æ ·",
    "å§¸": "é¢œ",
    "å”½": "è¥¿",
    "è¦©": "å µ",
    "éµ²": "ç¡®",
    "ä¾": "æ·±",
    "æ§—": "æ¡¥",
    "æ™™": "ä¿Š",
    "ç¶¯": "æ¡ƒ",
    "å£´": "åŠ©",
    "ä¼¾": "æ‰¹",
    "å„­": "è¡¬",
    "å£ ": "å„",
    "å¡´": "ç”­",
    "é¼¡": "é¼ ",
    "è‚­": "é‚£",
    "ç€": "ç¼ ",
    "æŒŒ": "æ ¼",
    "ç‰±": "æ­Œ",
    "åŠ¦": "é‹",
    "éŠ«": "è‰²",
    "æœ™": "æ˜",
    "è€–": "åµ",
    "å¦¸": "é˜¿",
    "è§­": "æœº",
    "éº¤": "ç²—",
    "ç¢": "é¢œ",
    "å½”": "è·¯",
    "ç’‚": "å…¶",
    "æ°¥": "è¥¿",
    "é‘¶": "è—",
    "ä¾«": "å®",
    "ç¦©": "å››",
    "ç»–": "å ",
    "åŠ‚": "è§‰",
    "è¢†": "ç°",
    "æ§š": "å‡",
    "äº™": "æ ¹",
    "è‹": "ä½",
    "çµª": "å› ",
    "è¬«": "å“²",
    "æ«Ÿ": "åˆ©",
    "è†ª": "è¸¹",
    "å˜«": "ç„¶",
    "ç©¤": "è¯º",
    "è°«": "å‡",
    "é¤¬": "èƒ¡",
    "æ™…": "é€‰",
    "è¨¢": "å¿ƒ",
    "é¢": "çˆ¶",
    "å”Š": "å¤¹",
    "æƒª": "å¾·",
    "ä»§": "é•¿",
    "æ§": "å¿™",
    "è³": "ç”·",
    "åŸ†": "ç¡®",
    "è›»": "é€€",
    "æ¥…": "é€¼",
    "é«ˆ": "ç»‘",
    "æ¥±": "æ",
    "æœ§": "é¾™",
    "é®": "ä½",
    "ç‹›": "åš",
    "éº": "å½“",
    "ç£": "ç °",
    "ç¨": "ä¾ ",
    "èŒ¤": "è®°",
    "å’°": "æ ‘",
    "æ¬™": "é›·",
    "å¸´": "æ•£",
    "åº³": "å¿…",
    "å¶": "é…",
    "è¥•": "è“",
    "è¬Œ": "æ­Œ",
    "ç€‹": "å®¡",
    "æƒ—": "è‚",
    "å±š": "æ¼",
    "ç’ˆ": "æ•–",
    "é‰": "é™¤",
    "ç™¬": "é€‰",
    "å¥Œ": "ç‚¹",
    "è®": "æ˜¼",
    "é¬ª": "è±†",
    "ç": "æ—‹",
    "é²»": "å§¿",
    "èŸ": "æ»¡",
    "å¡™": "ç¡®",
    "æ¬–": "æ‡’",
    "è¦‘": "å",
    "ç–°": "åŠ©",
    "ç‘‰": "æ°‘",
    "æœ…": "å¦¾",
    "æ‚": "å§",
    "è”¹": "è„¸",
    "é¨«": "å‰",
    "æª™": "æˆ",
    "è•": "å",
    "åº": "ç´§",
    "ç´": "é¢",
    "å¯œ": "å‡",
    "ç‰¤": "å¿™",
    "éˆ": "é’±",
    "æ¨": "è¸",
    "å": "è¢«",
    "é‡’": "é‡‘",
    "åŠ": "æ•²",
    "ç’¥": "äº•",
    "ç«¾": "æŒ",
    "å¨°": "å››",
    "è»¼": "æ„",
    "å¥“": "æ‰",
    "çº": "é›·",
    "è®": "å—",
    "é²£": "é—´",
    "é¹": "åš",
    "å‚º": "èµ¤",
    "è¢": "å¤¹",
    "èŸ": "ç»­",
    "ä»": "ä¸‰",
    "ç¾•": "æ ·",
    "æ½¾": "æ—",
    "è¬¡": "æ‘‡",
    "å…": "å®£",
    "å€·": "å¥¶",
    "è¿€": "å¹²",
    "å«º": "é—²",
    "å ¬": "é±¼",
    "æ”": "å‰",
    "è•¥": "å“‘",
    "è•": "æ¡¥",
    "çµ™": "ç¯",
    "æ»¹": "å‘¼",
    "æ¯¹": "ä¹¦",
    "æƒ›": "æ˜",
    "é€¬": "ç”­",
    "æ„¨": "ç¡®",
    "é¶‡": "ä¸œ",
    "æ» ": "è®¾",
    "ç½º": "æœ",
    "ç´¸": "åŠ©",
    "èƒ ": "åŒº",
    "æ‚‡": "å›¾",
    "è‘°": "è™½",
    "æ¬“": "æŒ¡",
    "é©µ": "è„",
    "å«ª": "çƒ™",
    "ç¿ƒ": "çº¢",
    "é¯½": "è´¼",
    "ä¼“": "æ‰¹",
    "å¯€": "é‡‡",
    "æ ": "äºš",
    "è„®": "å†…",
    "é½®": "ä»¥",
    "ç·¾": "ç¼ ",
    "å¿­": "å˜",
    "æ£¶": "æ¥",
    "é—": "åŒ…",
    "è¾": "é¾™",
    "è½·": "å‘¼",
    "ç¤€": "è§",
    "æ–±": "ç€",
    "é‚": "ç½—",
    "è³¸": "èƒœ",
    "çƒ´": "å¬",
    "è¬•": "æ",
    "æ£": "åŒª",
    "å»†": "å½’",
    "ç—³": "æ—",
    "æ†¡": "å†Œ",
    "ä¹": "å®œ",
    "è‡": "é•¿",
    "åµ": "é±¼",
    "ç¶ª": "æ¬ ",
    "è»›": "æ¶",
    "ç² ": "çº¢",
    "ç¾¼": "é¢¤",
    "ç—¦": "ç‰©",
    "èŸ´": "æ€",
    "æ·ƒ": "å€¦",
    "æª’": "é£",
    "è”€": "ä¸",
    "åš¸": "ç‚¹",
    "å‘": "é£",
    "é²™": "å¿«",
    "å‚‰": "æ€’",
    "çº": "ç»ƒ",
    "æ´š": "é™",
    "å ¦": "æ¥",
    "é©‹": "æ³¢",
    "ç“Œ": "å½’",
    "é«": "èŠ",
    "ç¸´": "æ¬ ",
    "ç´ˆ": "å®Œ",
    "æŒ—": "è§‰",
    "å”": "ç¢—",
    "è¥¬": "æ‘†",
    "é†¸": "å¨˜",
    "æ¼€": "è¯·",
    "ç¥Š": "å´©",
    "ç¤³": "å¢¨",
    "é¬¢": "å®¾",
    "ç°‘": "ç¼©",
    "ç‹": "æ³•",
    "æ‰‚": "ç”µ",
    "è¦·": "å»",
    "é®ƒ": "å¹³",
    "å„£": "æ—·",
    "å„»": "èºº",
    "æ»‰": "è’",
    "é“": "è¡Œ",
    "è’º": "æ",
    "å•‹": "é‡‡",
    "å¬¡": "çˆ±",
    "éŒµ": "èŠ±",
    "åš¨": "é¾™",
    "æ‰¡": "æ‹–",
    "è­Ÿ": "é€ ",
    "æ‚§": "åˆ©",
    "é¦«": "å¿ƒ",
    "å½¶": "æ",
    "é²¼": "æ„¤",
    "ç˜¼": "å¢¨",
    "è¬": "æ›¹",
    "çµ¾": "æˆ",
    "é¤š": "æ‘‡",
    "è½…": "åŸ",
    "å´¯": "é“¶",
    "å´•": "ç‰™",
    "ç¾‹": "ç±³",
    "æ¥": "ä¸‡",
    "å”¢": "é”",
    "èµ": "é£˜",
    "è¹€": "å ",
    "è¬": "é—®",
    "å©—": "å°¼",
    "æ‘¢": "äº’",
    "ç§–": "çŸ¥",
    "èƒ²": "æµ·",
    "è‘‡": "æŸ”",
    "ç¶·": "ç¿ ",
    "ç—": "å…ˆ",
    "ç¸‚": "æ€»",
    "éŒ’": "ç§‘",
    "è”Œ": "é€Ÿ",
    "è¼¦": "å¹´",
    "åªƒ": "æŸ”",
    "ç¹": "ç„¶",
    "é¥Œ": "èµš",
    "ç³¹": "æ€",
    "ç§ ": "æ‰¹",
    "èºª": "å",
    "è§§": "è§£",
    "æ½": "å½±",
    "é¼½": "æ±‚",
    "æŸ¾": "å°±",
    "é²": "å°",
    "é„«": "å¢",
    "å˜": "å¶",
    "é“": "ç“®",
    "ç£œ": "æ°”",
    "ç½½": "è®°",
    "ç³‡": "å–‰",
    "ä¼¹": "åŒº",
    "æ…‡": "å› ",
    "å©": "æ€§",
    "åœ•": "å›¾",
    "ç‹": "æŸ³",
    "ç°º": "èµ›",
    "æ¢¼": "æ¡ƒ",
    "é‹¤": "é™¤",
    "æŸ…": "ä½ ",
    "è¦¦": "é±¼",
    "ä¹§": "æŠ–",
    "å£¢": "åˆ©",
    "é©ƒ": "æ ‡",
    "å•¯": "é”…",
    "åµ¨": "ç‰©",
    "è¹ ": "ç›´",
    "ç˜¯": "ä¿ƒ",
    "ä»¸": "å’¬",
    "è‡‘": "é—¹",
    "æ„": "çœ¼",
    "é’¸": "ä¸",
    "å§": "è®¸",
    "ç© ": "å†œ",
    "è¢Š": "é¢†",
    "è¬˜": "æŒ",
    "æ•±": "åŸƒ",
    "è“§": "æ‰",
    "æ´–": "æ— ",
    "å¬¤": "å¦ˆ",
    "éˆ½": "ä¸",
    "ç": "æœ",
    "å¾¬": "æ—",
    "å„": "åœ¨",
    "å”£": "é€ ",
    "æ›¢": "äº†",
    "æ¯": "çš®",
    "é": "åº¸",
    "é¼”": "å¤",
    "è’¡": "æ£’",
    "æŸ": "è¿",
    "è³¨": "ä»",
    "ç°•": "ä¹",
    "ç¨ˆ": "æ„Ÿ",
    "æ¹‘": "éœ€",
    "æ¥™": "å¸½",
    "å”": "æŠ–",
    "ç´¦": "å…«",
    "è†¾": "å¿«",
    "é–Ÿ": "å¿…",
    "é‚‰": "ç¼–",
    "æ´¯": "å¦¾",
    "è£›": "æ„",
    "éŒ": "æ˜¾",
    "ç±¥": "æœˆ",
    "æ­“": "æ¬¢",
    "è¡Š": "ç­",
    "æ‘±": "æ…¢",
    "ç‹": "è¥¿",
    "æ—£": "è®°",
    "ç’": "é—ª",
    "ç¿«": "å®Œ",
    "è²°": "æ˜¯",
    "æ‡®": "æœ‰",
    "å²": "åš",
    "è¼Ÿ": "ç»°",
    "å—": "æ’",
    "èŒ‡": "æ‹”",
    "ä»š": "å…ˆ",
    "å§‡": "å¤«",
    "ç©": "æ˜Œ",
    "å¹": "ç»­",
    "å±": "å¯»",
    "ç­³": "åœ",
    "ç¡“": "è€",
    "å Œ": "å›º",
    "å¬": "çº¢",
    "éª’": "å®¢",
    "å¦­": "æ‹”",
    "å‡": "å¹²",
    "æ©": "çƒ¦",
    "ç²¶": "è·¯",
    "éš": "æ§",
    "æ½•": "äº”",
    "å‡": "å¥¥",
    "å°°": "ç§",
    "è”®": "å›½",
    "ç‡¾": "åˆ°",
    "çŒ„": "ç²¾",
    "å¹ª": "ç›Ÿ",
    "å‡”": "åˆ›",
    "ç•¯": "ä¿Š",
    "ç´­": "çº¢",
    "å¬¾": "æ‡’",
    "å»": "çˆ¸",
    "æ®¹": "æ„",
    "è¥›": "å†œ",
    "è“£": "ç‰",
    "ç‰¿": "å›º",
    "æ˜«": "ç»­",
    "å»Œ": "è‡³",
    "ç§ˆ": "å…ˆ",
    "å”š": "äº²",
    "éŠ–": "æœ±",
    "å‚¦": "å¤",
    "æ½©": "æ„",
    "ç©‰": "è‡³",
    "èº": "è£‚",
    "è…": "ä»»",
    "å¸": "äº’",
    "èµ‡": "æ±‚",
    "èˆƒ": "ç³»",
    "æ›•": "ç‡•",
    "çŠ": "å",
    "ç·–": "ç»­",
    "é": "å›¾",
    "çº»": "åŠ©",
    "è™’": "æ€",
    "ç˜Œ": "èœ¡",
    "è¨š": "é“¶",
    "æ¬": "åŸ",
    "è•°": "æ¸©",
    "æ¨‰": "åŒ",
    "é»": "æ—",
    "å¤†": "é€¢",
    "ç›¨": "è®¸",
    "æ†": "å¤Ÿ",
    "æ€©": "å°¼",
    "åŸ": "é»„",
    "å˜¸": "è¾…",
    "æš": "é¼ ",
    "åŠ»": "æ¡†",
    "åª¤": "æ€",
    "ç€": "åˆ˜",
    "å¹—": "å›½",
    "æ´": "æŒ‰",
    "ç‘’": "å”±",
    "å": "èŒ¶",
    "ç¦‹": "å› ",
    "çµ”": "æ‘†",
    "è€µ": "ä¸",
    "ç¸Š": "æ„",
    "åŸ¶": "æ„",
    "éˆ¸": "åš",
    "é–†": "é¢œ",
    "æ›º": "æ›¹",
    "å­¿": "é¸¾",
    "å¢ˆ": "çœ‹",
    "ç­¥": "ä¸¾",
    "ç£‰": "æ¡‘",
    "æšš": "æ‘‡",
    "å„": "å¿µ",
    "ä»¯": "åµ",
    "ç¾´": "å±±",
    "èº": "å¸½",
    "ç¸ˆ": "è¥",
    "ç‹": "æ˜¾",
    "æ†": "å¯¹",
    "æ´¢": "ä¸€",
    "ç¶»": "æˆ˜",
    "ç¯š": "åŒª",
    "çº©": "çŸ¿",
    "é…ˆ": "åˆ©",
    "ç§·": "è‡³",
    "ä¹¿": "è‡³",
    "è ª": "é¾™",
    "æ½š": "é€Ÿ",
    "é©©": "æ¬¢",
    "æ—ˆ": "åˆ˜",
    "æ·": "ç”°",
    "å†±": "äº’",
    "èª«": "é•‡",
    "åœ": "é¸¾",
    "ç‘ˆ": "æŸ”",
    "æ‹«": "å¾ˆ",
    "å": "é¬¼",
    "æ·‰": "æœ",
    "æ‚°": "ä»",
    "é¥": "ç¼ ",
    "æ": "å‡ ",
    "çª": "ç‰",
    "é²š": "è®°",
    "æ‹": "çƒ¦",
    "åƒŠ": "å…ˆ",
    "å³§": "æ•™",
    "åµ™": "ç§‘",
    "å–": "æ—",
    "å£": "å”",
    "æ‚œ": "æˆ",
    "è§": "åº”",
    "ç¥ƒ": "éª‚",
    "çª£": "è‹",
    "å©¥": "é—¹",
    "é—Œ": "è“",
    "ä¿«": "æ¥",
    "é»©": "æ¯’",
    "æ«¶": "æ˜¾",
    "ä¹†": "ä¹…",
    "éŸ˜": "è®¾",
    "çœ": "å£",
    "ç": "çœŸ",
    "å‹Š": "å®¢",
    "æ“­": "å§",
    "èˆ": "å–˜",
    "è•‘": "é—´",
    "å€€": "æ˜Œ",
    "é„€": "å¼±",
    "éƒª": "ä¸ƒ",
    "ç‚‘": "æœ¨",
    "æµ½": "è™½",
    "ä¸µ": "ç€",
    "åª": "ç«¯",
    "æ¼–": "æ•™",
    "å…": "é•¿",
    "é°º": "æ·±",
    "å±": "è´¹",
    "æ•®": "ä¾ ",
    "ç©£": "åš·",
    "å™": "åœ°",
    "ç’†": "æ±‚",
    "è‹": "è¢«",
    "ç¸": "æ",
    "çƒ": "é£",
    "éŸ®": "ä¹…",
    "è®ƒ": "èµ",
    "é¢©": "æ ‡",
    "é²´": "å›º",
    "ç§ª": "çŸ¥",
    "å´Œ": "å±…",
    "å¬": "æŒ‚",
    "éŠ¼": "é”™",
    "å¿¥": "ç³»",
    "è›": "é—¨",
    "ç¼Š": "æ™•",
    "ç­š": "å¿…",
    "è½†": "è·¯",
    "ç§": "ç”±",
    "ç¬˜": "å±±",
    "é‡¹": "å¥³",
    "å§´": "è£‚",
    "å§±": "å¤¸",
    "é¿": "æ‹¿",
    "ç‡œ": "é—·",
    "èŒ–": "æ ¼",
    "è¦²": "è¿›",
    "ç‘…": "æ",
    "é„œ": "å¤«",
    "ç¶ƒ": "æ¶ˆ",
    "åµ‚": "ç»¿",
    "æœ“": "æŒ‘",
    "ç˜½": "ç´",
    "æ”": "ä¿¡",
    "æ„¶": "é‹",
    "ç±": "å…¶",
    "ç„": "ç”Ÿ",
    "ç¶–": "é¢œ",
    "éŸŒ": "ä»»",
    "æ™": "é¥¿",
    "è‹­": "å’¬",
    "é¤ƒ": "è§’",
    "è¤¢": "æ€€",
    "æ¼¨": "é€¢",
    "ç‚Œ": "å¼€",
    "èµ¿": "æŒ",
    "åœ€": "å›½",
    "èº": "èŒ„",
    "è§–": "è§‰",
    "ç†’": "è¥",
    "å®‘": "äº•",
    "å˜¼": "å¤„",
    "ä¾š": "è®­",
    "é® ": "ç»´",
    "æ£Œ": "èœ",
    "æ”£": "é¸¾",
    "ç²…": "ç‰©",
    "é ": "æ¶",
    "éŒ€": "ä¼¦",
    "é‚š": "å¦‚",
    "ä½‚": "ç",
    "è‰Œ": "å¿µ",
    "å¦ ": "é‚£",
    "è²™": "å‡º",
    "é©ª": "ç¦»",
    "ç–¬": "åˆ©",
    "ç‡¼": "è¿›",
    "åŸ«": "å® ",
    "åœ´": "ç€",
    "ç‚©": "å¦",
    "ä»’": "å†°",
    "å«™": "æ—‹",
    "è§ƒ": "ç‡•",
    "é³®": "æœº",
    "ç‡‹": "æ•™",
    "æ†‡": "æ°”",
    "èŒ€": "ç¦",
    "å˜™": "å©†",
    "å“³": "æ‰",
    "å–¥": "å¤º",
    "é´‡": "å®",
    "é¢‹": "æŒº",
    "æ«Œ": "ä¼˜",
    "éµ ": "èƒ¡",
    "å§ƒ": "ç",
    "ç¾“": "å…«",
    "å„": "å½¬",
    "é‡¿": "é‡‘",
    "å š": "é­‚",
    "æƒ”": "è°ˆ",
    "æ…š": "æ®‹",
    "å¯š": "å®",
    "è‡¯": "é«˜",
    "é¾—": "é›¶",
    "è£€": "å› ",
    "æ©": "ä¼š",
    "è¨": "äºš",
    "çˆ": "ç¡•",
    "æƒ": "ä¿Š",
    "æ®½": "è‚–",
    "æ¾±": "ç”µ",
    "å ½": "åˆš",
    "å“ƒ": "åŒ",
    "æµ¡": "åš",
    "è­": "æ",
    "æ¿": "è‰²",
    "è’±": "è‘¡",
    "ç‚¶": "é—ª",
    "è¤": "æ±‚",
    "è‰…": "é±¼",
    "é®€": "é©®",
    "åªœ": "ç",
    "è‹©": "åš",
    "é‘": "æ•Œ",
    "çœ”": "å¤§",
    "ç¾¶": "å±±",
    "æ¸": "å†¤",
    "éŸ": "å°†",
    "è": "æ‹”",
    "æº¡": "çŸ³",
    "æ™›": "ç°",
    "æ ³": "è€",
    "æ•º": "åŒº",
    "æ¥‹": "èœ¡",
    "å¾¯": "è¥¿",
    "ç·Ÿ": "è™«",
    "æœ¥": "åŠ³",
    "é–¡": "çˆ±",
    "æ™£": "å“²",
    "è¸º": "è§",
    "æ¡­": "çœŸ",
    "é²º": "è¯—",
    "æº‹": "è¥",
    "å§™": "ä»»",
    "çŠ‚": "ç¦»",
    "è‘": "ç»¿",
    "ç±¾": "å°¼",
    "çŒ¸": "æ¢…",
    "é¢Ÿ": "æ…¢",
    "å¨–": "ç»°",
    "é­œ": "äºº",
    "å˜®": "åŠ³",
    "ä¿": "é›¨",
    "è«—": "å®¡",
    "æ®": "æ¼‚",
    "é¦": "èˆ”",
    "å ‰": "ç‰",
    "å«ˆ": "åº”",
    "æª": "æ",
    "å¯‘": "å¯",
    "çŒ": "å¿…",
    "ç¹": "ç´",
    "è£ˆ": "æ˜†",
    "æº˜": "å®¢",
    "é²": "ç€",
    "å‹": "ç—…",
    "å°": "æ–™",
    "é±º": "ç¦»",
    "æ¹": "åŒ…",
    "æ®­": "å°†",
    "è£µ": "åŸ¹",
    "è™": "å‘¼",
    "å¼¸": "æœ‹",
    "è«º": "ç‡•",
    "è®’": "ç¼ ",
    "ç ": "å¢¨",
    "é­´": "æˆ¿",
    "å³´": "ç°",
    "å„¬": "åº†",
    "ç¨": "è§‰",
    "ç‚Ÿ": "è¾¾",
    "è‡”": "ç°",
    "å¶§": "æ„",
    "é„": "çŸ®",
    "é‹": "çº¢",
    "å£™": "çŸ¿",
    "ç·": "è§",
    "å“¯": "ç°",
    "ç¨†": "æ—…",
    "è™¯": "æ±‚",
    "é·‡": "æ‰£",
    "éˆ¿": "ç”°",
    "æ–ˆ": "å­¦",
    "æ§¡": "ä¸§",
    "å”": "æ¶",
    "ç‘": "é»„",
    "å¾": "æ¶",
    "å§–": "å·¨",
    "è‹ª": "é¥¼",
    "å®": "å°",
    "éœ": "æ±¤",
    "ç²°": "ç¦",
    "çª": "æ„",
    "éˆ­": "å§¿",
    "ç¾†": "çš®",
    "å«­": "äº’",
    "è‘": "å°†",
    "ä¼©": "ä¿¡",
    "è‹®": "å…ˆ",
    "é„•": "ç›¸",
    "çŒ‡": "æ¶ˆ",
    "èŠº": "è¢„",
    "çš€": "æ",
    "é½¦": "è‚¯",
    "ç­´": "å†Œ",
    "æ‡½": "æ¬¢",
    "æ¢ ": "æ—…",
    "æ£ˆ": "æ¬ ",
    "æ‡º": "é¢¤",
    "å¦": "é—®",
    "å–¦": "è‚",
    "èˆ¦": "å¤ª",
    "çˆœ": "ä»",
    "éˆ‡": "å¤«",
    "å»©": "å",
    "è¸": "çª",
    "é˜©": "ç”Ÿ",
    "æ»³": "ä¼¤",
    "é©¡": "é¾™",
    "ç­": "åŒº",
    "çš": "é«˜",
    "æŒ": "æ•™",
    "ç¯º": "çš®",
    "çµ¬": "è°¢",
    "å§›": "åŠ¨",
    "è‘·": "æ˜",
    "ç¥¹": "æ¡ƒ",
    "æ”·": "è€ƒ",
    "è—º": "å",
    "å½¨": "åƒ",
    "ç­": "å¦‚",
    "å©": "é¥­",
    "æ¾": "è‰²",
    "æ™»": "æŒ‰",
    "æ…¡": "åŒ",
    "æº": "éªš",
    "æ‹‘": "é’±",
    "é‡": "ä¿¡",
    "æš”": "å—",
    "è†„": "ç˜¦",
    "æ›„": "å¤œ",
    "ç³¢": "é­”",
    "é£­": "èµ¤",
    "å¼Œ": "ä¸€",
    "çŠ‹": "å·¨",
    "è©ª": "å¾ˆ",
    "æœ¢": "å¿˜",
    "æ•­": "ç¾Š",
    "æ¦¦": "å¹²",
    "å¯•": "å‡",
    "éˆ¤": "æ—¥",
    "é±²": "è£‚",
    "çµ": "æ—",
    "æ†«": "æ•",
    "è›": "æ¯”",
    "ç˜": "ç©·",
    "çœ…": "æ”€",
    "è–—": "åŸ",
    "èŠš": "å",
    "é©«": "æ ‡",
    "ç•": "ç¦",
    "æœ": "åŒª",
    "é²¹": "æ·±",
    "æ »": "æ˜¯",
    "é¼¯": "æ— ",
    "ç–­": "å®—",
    "è³¥": "å²",
    "èŸ": "æ†‹",
    "å˜°": "æœº",
    "çŠ¾": "é“¶",
    "ç·†": "è¥¿",
    "å˜”": "å¶",
    "çª¸": "è¥¿",
    "é¹“": "å†¤",
    "é©¤": "ç›¸",
    "å§": "å…»",
    "è": "å‡",
    "æ«‚": "ç…§",
    "å«½": "èŠ",
    "é†¤": "é™",
    "å¶¸": "å®¹",
    "æ›µ": "å¤œ",
    "è¾š": "æ—",
    "æ½ª": "è¿™",
    "ç‰¾": "äº”",
    "å„—": "ä½ ",
    "è•¡": "åŸ",
    "ç… ": "ç‚¸",
    "åš¦": "åˆ©",
    "å“Š": "å³",
    "ç½™": "æ·±",
    "å‹‘": "èµ¤",
    "é˜¯": "æŒ‡",
    "æ²·": "å‘",
    "å‰»": "æ§",
    "è‘½": "é‚€",
    "å”ª": "å¥‰",
    "å„°": "ä¼Ÿ",
    "å€§": "å®—",
    "é‡©": "å",
    "é­•": "å‡ ",
    "ç­": "è´«",
    "çµ˜": "æ¬¡",
    "å¡¥": "æ ¼",
    "æ£«": "ç‰",
    "é‰¾": "è°‹",
    "é±”": "å–„",
    "æ£¸": "é‚¹",
    "éŒ¬": "ç»ƒ",
    "å·Œ": "é¢œ",
    "è“º": "æ„",
    "ç¾­": "é±¼",
    "æŠ‡": "èƒ¡",
    "æ’–": "æ±‰",
    "çº´": "ä»»",
    "æš…": "æ›´",
    "è¿š": "è¾¾",
    "é“”": "å‘€",
    "æ µ": "è£‚",
    "ç›º": "å¿ƒ",
    "ç¶¢": "æ„",
    "æ…¥": "é€ ",
    "å­­": "ç­",
    "é¸›": "çŒ",
    "æ‘‘": "ä¹–",
    "è¼œ": "å§¿",
    "å°": "åœº",
    "åƒ£": "é“",
    "ç„„": "ç†",
    "å¹–": "æ ‡",
    "èš¦": "ç„¶",
    "çš": "é€ ",
    "å¸“": "å¢¨",
    "é©": "æ—",
    "é´’": "é›¶",
    "å¥¡": "å¥¥",
    "æ€Š": "è¶…",
    "é¼·": "è¥¿",
    "é­˜": "çœ¼",
    "æ˜ª": "å˜",
    "æ¹¸": "äº®",
    "é¸»": "æ¨ª",
    "æ»": "éªš",
    "éœ½": "è®°",
    "æ•©": "ç¬‘",
    "ç˜¨": "é¢ ",
    "éªƒ": "å› ",
    "å€°": "æ„£",
    "ç·³": "é‹",
    "ç–Œ": "èŠ‚",
    "è²®": "äºŒ",
    "èŸ": "ä½",
    "æ¾›": "é²",
    "å¦´": "é™¢",
    "å±ƒ": "ç³»",
    "å": "æ—",
    "æŸ¤": "æ‰",
    "ç“": "åœ°",
    "è©¬": "å¤Ÿ",
    "é‚¤": "å¿ƒ",
    "ä¾‚": "æ‹–",
    "è˜‚": "ç‘",
    "ç–‚": "å ",
    "å‘§": "åº•",
    "é¶": "ç»•",
    "å€…": "ç¿ ",
    "æ¥ª": "å¤œ",
    "è‡š": "èŠ¦",
    "ç´ª": "ä¸ƒ",
    "èˆ­": "æ¯”",
    "å›¸": "æ—¥",
    "ç«¡": "æ‘†",
    "è“‚": "æ˜",
    "å·‡": "è¥¿",
    "ç­¦": "ç®¡",
    "éš„": "ä½",
    "å¯": "åˆ©",
    "æ¡Š": "å€¦",
    "ç´": "èƒ†",
    "æ‡": "ä¹Œ",
    "å¼œ": "é™",
    "æ®": "å…",
    "å“±": "æ³¢",
    "ç¸¥": "æ•",
    "å–“": "é‚€",
    "åº›": "æ¬¡",
    "ç¯¸": "æƒ¨",
    "èª§": "ä¸",
    "è‹¶": "æ",
    "è£‡": "éœ€",
    "ç—ƒ": "æ—‹",
    "ç³¥": "è¯º",
    "æ¯¦": "è€³",
    "æºš": "å¡”",
    "æ³²": "å‡ ",
    "çº": "å‘",
    "çŸŸ": "ç¡•",
    "å½‡": "æ¶ˆ",
    "ç·²": "ç§’",
    "æ¾¬": "å§¿",
    "æŸ¦": "è›‹",
    "é·¸": "ç‰",
    "è»†": "ä½“",
    "æ‘›": "åƒ",
    "ç‰": "çª—",
    "å": "ç‡•",
    "è’ˆ": "å‡¯",
    "èƒ•": "è¾…",
    "æ—ª": "é‹",
    "æ¬¬": "å¼€",
    "ç•»": "æˆ",
    "ç°Š": "æœº",
    "å¡½": "åŒ",
    "å‡¨": "é£",
    "ç‰˜": "æ¯’",
    "å”–": "å“‘",
    "ç¦µ": "æ",
    "å¢º": "å¥¥",
    "è—Œ": "å¯†",
    "é««": "æ¡",
    "å™": "è¥¿",
    "æ‰–": "å…¥",
    "é¹–": "å’Œ",
    "å¹": "å¯†",
    "æŸº": "æ‹",
    "å”ƒ": "å¤",
    "é§®": "åš",
    "æšª": "é—·",
    "å¥»": "æš–",
    "å¾¦": "å‡",
    "ç‹˜": "è¡€",
    "é– ": "æ¶¦",
    "è¤†": "æ",
    "ç˜–": "å› ",
    "å¦¬": "åº¦",
    "éœ«": "ä¹ ",
    "åŸ…": "æˆ¿",
    "å€": "ä¹¦",
    "é˜½": "ç”µ",
    "æ•¾": "å–„",
    "è¦ ": "å›",
    "æ”²": "ä¸ƒ",
    "å™š": "å¯»",
    "æ¹¥": "çª",
    "é˜§": "æŠ–",
    "ç­­": "ç®—",
    "é¹‹": "æ",
    "è€ ": "éœ",
    "æ¹¶": "å…¨",
    "å‘«": "è´´",
    "ç—‹": "è…¾",
    "ç‡´": "ä¼š",
    "è§¢": "æ˜¯",
    "è£Š": "å°¿",
    "ç’±": "è‰²",
    "ç ®": "åŠª",
    "æ´Š": "è§",
    "æ—": "äºš",
    "æ§‡": "é¢ ",
    "èµª": "ç§°",
    "é¦‚": "ä¿Š",
    "ç•µ": "è¯",
    "ç²”": "å·¨",
    "æ¸": "ä¸€",
    "ç™Ÿ": "åˆ«",
    "é’˜": "è¡Œ",
    "é·": "æ¶ˆ",
    "é¨": "æˆ",
    "è—": "å”",
    "æƒ²": "è¿",
    "çš": "è’",
    "äº´": "å³",
    "éŠ¬": "çƒ¤",
    "æ²": "ç¼€",
    "å½‰": "é”…",
    "è™": "å›",
    "è‰´": "ç¦",
    "ç±˜": "è…¾",
    "ç’": "é‡‘",
    "åƒ†": "ç»ƒ",
    "ç´©": "è‡³",
    "è©«": "å·®",
    "ç—™": "é™",
    "ç³€": "èŠ±",
    "æ˜„": "æ¿",
    "é®": "æ³¢",
    "éˆ": "çŸ³",
    "æ²‡": "çœ¼",
    "è¥–": "è¢„",
    "é„–": "äº‘",
    "é¤½": "æºƒ",
    "å´¢": "ç",
    "å§ ": "å‘",
    "è’¾": "è¿·",
    "é¹": "ç‰",
    "è¡¼": "çŸ¥",
    "é²’": "èŠ‚",
    "ç¨™": "çŸ¥",
    "çª¾": "æ¬¾",
    "è¦": "çƒ§",
    "æŸ‰": "çƒ¦",
    "å»": "ä¸",
    "å«¤": "ç´§",
    "ç“€": "è½¯",
    "å·Ÿ": "è’",
    "æ±£": "ä¹…",
    "ç †": "å¤«",
    "é„˜": "åº¸",
    "ç™": "å¿…",
    "èš®": "å¸¦",
    "ç—–": "å“‘",
    "é ¡": "é‹",
    "é¤´": "åˆ†",
    "ç‹‰": "æ‰¹",
    "ç´Ÿ": "é‡‘",
    "å„Š": "å¤„",
    "æš": "æ‘˜",
    "è–º": "è®°",
    "æŠ²": "å–",
    "è„ƒ": "ç¿ ",
    "ç™": "å¢¨",
    "è®–": "è¡¬",
    "æ™¹": "æ„",
    "å§¢": "æ",
    "é–": "çº¢",
    "æ«¼": "é—´",
    "ç": "å¢¨",
    "å¼½": "è®¾",
    "å”—": "å…œ",
    "å”“": "è½¦",
    "é«–": "å®½",
    "è¤": "å•",
    "è”„": "æ…¢",
    "é˜": "åˆ©",
    "æŠ°": "å…»",
    "æŠ·": "æ‰¹",
    "é³†": "çˆ¶",
    "é¤”": "ä¸",
    "è¬Ÿ": "æ",
    "æ«": "è°",
    "æ±–": "è˜",
    "ç¸‰": "è¿›",
    "æ­€": "æ¬¾",
    "è¸­": "ç",
    "ç™": "è£‚",
    "ç°µ": "è·¯",
    "é¯¡": "è´¹",
    "èº¾": "ç¾",
    "æ¸™": "æ¢",
    "å©»": "éš¾",
    "å¼»": "å¿…",
    "é¼ˆ": "æ†‹",
    "ç¶°": "ç¢—",
    "é‡­": "åˆš",
    "åˆ§": "èŠ‚",
    "æ½¡": "é¡¿",
    "æ““": "å¿«",
    "ç€¼": "åš·",
    "çƒš": "ä¾ ",
    "ç‚": "èƒ–",
    "å¶": "æ˜Ÿ",
    "äº¹": "ä¼Ÿ",
    "é´…": "æ¬¢",
    "è¥¶": "å¸¦",
    "çŠª": "å¥",
    "ç™©": "èµ–",
    "åˆ„": "ä»»",
    "ä½®": "æ ¼",
    "æˆ‰": "æœˆ",
    "è¤–": "å›¢",
    "è¹º": "æ•²",
    "ç¹»": "éœ€",
    "æš‹": "æ•",
    "ç‚": "å…¶",
    "é¯§": "æ˜Œ",
    "ç›µ": "æ°”",
    "è´”": "å¿…",
    "åš—": "åš",
    "ç°€": "åˆ™",
    "æ›ª": "è£¸",
    "ç‹‹": "å®œ",
    "å«¹": "æ",
    "å›": "æ’",
    "å«¨": "æ†¨",
    "è¡±": "èŠ‚",
    "çº€": "è‘¡",
    "ä¸": "å…",
    "éŠ": "å–„",
    "è”¦": "å°¿",
    "é…‚": "æ“",
    "æ¢œ": "å®¶",
    "ç": "è¥¿",
    "ç¤›": "é—´",
    "è³”": "å½¬",
    "é¯”": "å§¿",
    "æŒ·": "æœ‹",
    "è¾€": "å‘¨",
    "å’…": "å‰–",
    "ä¸ ": "ç§‹",
    "ä¼ƒ": "é±¼",
    "ç¿£": "ç…",
    "æ³ƒ": "å±…",
    "ç¹€": "å²",
    "æœ": "æ„",
    "é“»": "æ— ",
    "é¢º": "ç¾Š",
    "æ½™": "ç»´",
    "æ˜½": "é¾™",
    "è•…": "å¶",
    "é‘ª": "èŠ¦",
    "èº­": "å•",
    "éª¹": "æ•²",
    "çœ¹": "é•‡",
    "å¼": "è€³",
    "è˜«": "æ±‰",
    "é¡¥": "å·",
    "æ ": "æŸ³",
    "å±Ÿ": "è°¢",
    "çœ´": "ç‚«",
    "ç´‚": "æ˜¼",
    "é²ƒ": "å…«",
    "æ³¿": "é“¶",
    "ç¸": "é™ˆ",
    "è’": "æ´¾",
    "å‹ ": "è·¯",
    "æ½„": "æ ‘",
    "ç…¬": "ç¾Š",
    "å¥": "åœˆ",
    "æ²": "æŒ–",
    "è‡": "å…¶",
    "æ¿¶": "æ‰©",
    "ç—·": "å®‰",
    "è¾¿": "æ€",
    "æ¶¬": "æ€§",
    "æ¸·": "çœ¼",
    "æ£": "ç­",
    "æ®": "è´¹",
    "è¸‡": "æ¯",
    "ç¹Š": "å…ˆ",
    "é•": "å›",
    "æ·“": "æ–¹",
    "é¸°": "é›¶",
    "è’": "ä¼Ÿ",
    "ç—": "è‰²",
    "ç€½": "å‡",
    "æš£": "æ°”",
    "å«†": "å®¹",
    "éš": "å‘€",
    "é´›": "å†¤",
    "æ™": "æ„",
    "å·º": "è®­",
    "é²•": "è€Œ",
    "é™": "é©®",
    "çƒ¸": "æµ·",
    "è–": "ç‰",
    "æŠ": "å‰–",
    "è’": "åˆ©",
    "æ‹": "åº•",
    "ç€": "æ ·",
    "ç¢": "ç¡®",
    "æ¾‡": "çƒ™",
    "åš¿": "æˆ–",
    "ç™ˆ": "è´¹",
    "è¡´": "èƒ†",
    "é¹Ÿ": "ç“®",
    "é¨‘": "é£",
    "è”²": "æ‰£",
    "é†¨": "ç¦»",
    "æ½¬": "å–„",
    "é³": "é›·",
    "éº‘": "å°¼",
    "é¬½": "å¦¹",
    "å©": "èµ·",
    "å»¼": "å¥¶",
    "é¬­": "è±†",
    "å¡¹": "æ¬ ",
    "é¹€": "æ— ",
    "ç¸µ": "æ…¢",
    "èµ†": "è¿›",
    "å–¯": "å–·",
    "å’¡": "äºŒ",
    "å»„": "å°±",
    "è»”": "ä»»",
    "è¢ƒ": "æ‹†",
    "ç±Ÿ": "èµ–",
    "ç¿": "é­‚",
    "å€—": "æœ‹",
    "æ«„": "æ˜¥",
    "ç¡ˆ": "æ°",
    "æŒƒ": "è‡³",
    "å¼¶": "é™",
    "ç¦”": "çŸ¥",
    "è¼¾": "å±•",
    "è‚¶": "çš®",
    "ä¾Œ": "å› ",
    "ç®™": "ç¦",
    "éŒ›": "å¥”",
    "è‹½": "å§‘",
    "å€": "çŸ¥",
    "ç ": "èˆ”",
    "æ´ƒ": "ç°",
    "ç¡„": "æ¡†",
    "è©Ÿ": "å“²",
    "è“": "é±¼",
    "æº": "è¿›",
    "å Š": "æ¶",
    "è˜˜": "åš·",
    "ç§º": "åº¦",
    "æ§…": "æ ¼",
    "åœ": "æœ",
    "é£": "é¢",
    "èµ»": "æ˜¾",
    "è”¶": "åˆ™",
    "ç˜º": "æ¼",
    "æ™½": "æ—",
    "ç¶†": "æ¢—",
    "è¬¢": "æ’¸",
    "åƒ¢": "å–˜",
    "æ˜‹": "è´µ",
    "å©Œ": "ç†Ÿ",
    "è¬": "å¯†",
    "å¬‹": "ç¼ ",
    "å”²": "è€Œ",
    "é‘Š": "æˆ–",
    "ç²–": "å¢¨",
    "é¤¾": "å…­",
    "æ¥‰": "å¼±",
    "æ«¨": "èŠ¦",
    "æ‚µ": "å”±",
    "å€•": "å‚",
    "æ£½": "æ·±",
    "è": "åŸ",
    "é‡¬": "æ±‰",
    "ç›": "è¥",
    "ç¬¶": "ä½¿",
    "å»¡": "äº”",
    "é°©": "æ‘‡",
    "åš": "å“²",
    "æ¡»": "é£",
    "ç·—": "ç›¸",
    "æ–²": "ç€",
    "è¬‹": "æˆ–",
    "è­": "å®£",
    "æ‡": "æ¡†",
    "ç•": "æ¯",
    "ç€†": "æ¯’",
    "å·†": "å®¹",
    "æ¼§": "å¹²",
    "éµ‘": "æ",
    "çª›": "æ‰£",
    "ç«ª": "æ ‘",
    "ç¡™": "ç»´",
    "é£": "é£",
    "é‡§": "ä¸²",
    "å£µ": "å£®",
    "çº˜": "é’»",
    "ç°­": "æ˜¯",
    "æ†™": "è¥¿",
    "æ™²": "ä½ ",
    "è£": "ç”±",
    "æ†": "åˆš",
    "ç£˜": "æ‘‡",
    "é”": "ä¿¡",
    "é¨": "å…¶",
    "å· ": "ç²¾",
    "è— ": "æ•™",
    "è†‹": "èŠ",
    "æŠ…": "å±…",
    "é»¢": "åŒº",
    "æ®®": "ç»ƒ",
    "æ›€": "æ„",
    "å¢«": "å°Š",
    "æ®©": "çªœ",
    "å®†": "ç©·",
    "ç“": "å®Œ",
    "æ«¤": "é™",
    "è”¯": "é™ˆ",
    "è–€": "è¿",
    "ç†": "æ¢",
    "è½¡": "é…",
    "åŒ¼": "ç§‘",
    "æ¨·": "ä»",
    "ç…¿": "åš",
    "æ´˜": "è€ƒ",
    "ç¥": "çŸ³",
    "çš¦": "è§’",
    "é˜·": "ç§°",
    "é‹³": "åŠ©",
    "è©¡": "è®¸",
    "è´": "å–„",
    "é”½": "é»„",
    "é¾¥": "ç‰",
    "å”’": "æ±‚",
    "æ‚¾": "ç©º",
    "åµ¥": "èŠ‚",
    "è¿†": "å®œ",
    "ç§´": "å’Œ",
    "ç‘": "æ¢",
    "ç…": "åº”",
    "æƒ‚": "ç ",
    "è“€": "å­™",
    "å¨¤": "è£…",
    "è„": "è¨",
    "çš¥": "å·",
    "ç®°": "æŸ",
    "å„¢": "æ—…",
    "çˆ": "è§‰",
    "ç“Ÿ": "åš",
    "ä¿›": "è¾…",
    "è…¨": "æ¶®",
    "ç¡‘": "ç °",
    "çª…": "å’¬",
    "ç¯­": "é¾™",
    "è¤¦": "è€",
    "æ¥’": "æ€",
    "ç¿¯": "è´º",
    "é¥ƒ": "é­”",
    "æ¦ ": "æ˜",
    "é»“": "æ„",
    "ç¬–": "ä»¥",
    "èœ‘": "è›‹",
    "ç†²": "çª˜",
    "é®±": "è€",
    "æŠª": "ä¸",
    "ç³Œ": "èµ",
    "ç® ": "å‚",
    "ç‰—": "æœ‰",
    "ç¸¹": "æ¼‚",
    "å˜œ": "éª‚",
    "é¶¸": "å¼±",
    "è©“": "æ›²",
    "æ£‘": "æ’",
    "æ©¢": "å¦¥",
    "çœ—": "å±…",
    "ç‚£": "å¯",
    "è“°": "æ´—",
    "ç¹„": "ä¸€",
    "æ™µ": "èµ·",
    "åµ†": "æœº",
    "è»": "å",
    "è»«": "æ•",
    "é¤¹": "å”",
    "å¾": "ç½‘",
    "åˆ¼": "èŠ‚",
    "è±·": "æ„",
    "ç´‡": "å’Œ",
    "è’»": "å¼±",
    "è³¡": "è€•",
    "é³ª": "å“º",
    "æ¾£": "æ¢",
    "å¦¼": "å¿…",
    "æ¥¬": "èŠ‚",
    "è¢•": "å­¦",
    "è£¯": "æ„",
    "æ•¼": "ä»¥",
    "æŸ·": "å¤„",
    "é‘€": "çˆ±",
    "æ¹­": "æ±‚",
    "æ“©": "ä¹³",
    "é´ˆ": "ç‡•",
    "è‹°": "çº¢",
    "å–Œ": "å‘¨",
    "è»»": "ç§‘",
    "é¯“": "æ·±",
    "æ’”": "çº¢",
    "é—‰": "å› ",
    "æµµ": "åŒ",
    "æ‘‚": "è®¾",
    "ç·˜": "é—´",
    "æº": "è¥",
    "è»¦": "çŸ¿",
    "æ¹‹": "ç»´",
    "é¶š": "æ¶",
    "èœ£": "æª",
    "å³˜": "ç¯",
    "å˜¦": "æ•™",
    "çµ£": "å´©",
    "éŠ™": "å®",
    "ä¸‚": "è€ƒ",
    "æœ·": "åˆ€",
    "æ’¾": "çª",
    "å": "ç»¿",
    "éŠ»": "æ",
    "ç¥": "æ•™",
    "æƒ‰": "å ",
    "å»•": "å°",
    "çƒ±": "çª˜",
    "æ³´": "çŒ",
    "å™°": "åº¸",
    "æ“·": "é‹",
    "æŒ´": "ç¾",
    "å›»": "å›½",
    "å": "åº•",
    "ç‘³": "æ“",
    "ç›»": "ç³»",
    "å°": "å¡",
    "åŠš": "ç«¹",
    "ä¿™": "è¥¿",
    "å‹·": "åš·",
    "ç¤š": "ç§‘",
    "è˜—": "è–„",
    "æ·¢": "ç‰",
    "ç„®": "ä¿¡",
    "å¨®": "é¢œ",
    "éº": "æœµ",
    "åŒ„": "æ¦‚",
    "åŒ½": "çœ¼",
    "é‰": "å¿…",
    "è¦": "èª",
    "åŸ³": "ç ",
    "é «": "è¾…",
    "å‘…": "æ¢…",
    "å‘": "ç”Ÿ",
    "æº¸": "é€Ÿ",
    "å™": "æ€",
    "æƒ": "åœº",
    "æ¢": "è¿›",
    "éª•": "é€Ÿ",
    "çªŠ": "æŒ–",
    "éŠ¥": "ä¸€",
    "é˜¢": "ç‰©",
    "æ’—": "é€›",
    "é±": "åŸ",
    "æ§º": "åº·",
    "ç¿º": "æ•–",
    "é²Œ": "çˆ¸",
    "ç²¯": "ç°",
    "äº·": "è¿",
    "éª": "æ˜Ÿ",
    "å¥£": "ç“®",
    "é¨§": "ç“œ",
    "ç®˜": "ä¿Š",
    "è¨•": "å–„",
    "å”": "å…±",
    "è–³": "ä¼Ÿ",
    "å½": "å’",
    "å¢½": "æ•²",
    "ä¾¢": "å¸¦",
    "å¨’": "æ¢…",
    "ç™˜": "åˆ©",
    "é ´": "å½±",
    "è«´": "é—²",
    "ç€¯": "è¥",
    "è¨": "è¿",
    "é‰±": "çŸ¿",
    "è˜º": "ç¦»",
    "é®“": "çœ¨",
    "è’“": "çº¯",
    "ç‡": "ç‰",
    "ç“›": "ç¯",
    "é†": "æ",
    "çŸ·": "å­",
    "æ®£": "è¿›",
    "å‰…": "æ¥¼",
    "æ•³": "åŸƒ",
    "ç²™": "æ˜¼",
    "ç•±": "åˆ˜",
    "é‡·": "åœŸ",
    "éº…": "è¢",
    "è§": "åº•",
    "è¬—": "æ£’",
    "å‡²": "å¹²",
    "åš°": "é­”",
    "æƒˆ": "æœ",
    "æ’£": "èƒ†",
    "ç„¢": "è½°",
    "è—": "å•¦",
    "æ“«": "å¤œ",
    "è³š": "èµ–",
    "é¯™": "çº¯",
    "èº‰": "è¹²",
    "å§…": "åŠ",
    "çˆ¼": "ç»„",
    "æƒ¤": "é—´",
    "åœµ": "è¡",
    "è‘‚": "å…",
    "ç¾›": "æ„",
    "æŸˆ": "åŠ",
    "æ•§": "æœº",
    "ç¯œ": "ç",
    "é¯·": "æ",
    "è‡¸": "çŸ¥",
    "ç­©": "åŒ",
    "æ°‚": "æ¯›",
    "é¨®": "åˆ˜",
    "æ­": "æ¶",
    "å­…": "å‰",
    "çº’": "ç¼ ",
    "åŠ": "éœ",
    "å•£": "é—²",
    "å": "è°¢",
    "æ¬¥": "æ„",
    "æ˜»": "æ˜‚",
    "ç‚": "é¢œ",
    "é‚˜": "é±¼",
    "ç“…": "åˆ©",
    "è« ": "å®£",
    "è¿‹": "å¿˜",
    "æ¬±": "å–",
    "ä¾¹": "æŒº",
    "æ£ª": "çœ¼",
    "ä¹¢": "æ¦‚",
    "ç­¬": "æˆ",
    "å–…": "ç‰",
    "ç¤": "ç›Ÿ",
    "ç›¿": "æ°‘",
    "çŸ": "è¦",
    "é¡": "é€Ÿ",
    "å¼ª": "é™",
    "æ§ª": "æ¦‚",
    "å‚ª": "å‚",
    "é´": "å–",
    "åˆ²": "äº",
    "ç‹¥": "è®­",
    "æ‘¡": "æ¦‚",
    "æŒ“": "æ‰",
    "ç‘®": "åˆ©",
    "è": "å›½",
    "ç“²": "å“‡",
    "è©€": "å ",
    "æ»½": "åº¸",
    "é‹‡": "è¢«",
    "å„…": "è¡",
    "ç": "å¢¨",
    "ç¡¿": "ç©º",
    "å’": "å­©",
    "è¬¿": "æœ",
    "çŠ…": "åˆš",
    "è¹Œ": "æª",
    "é“‡": "æŠ¥",
    "æˆ¤": "æ¦‚",
    "å»°": "å¬",
    "è¬…": "å‘¨",
    "è±„": "æ¯’",
    "ç°¹": "å½“",
    "æ‘·": "è§’",
    "æ ƒ": "åˆ©",
    "èŸ«": "é“¶",
    "è¿–": "è¾¾",
    "ç¾ƒ": "å¯†",
    "ç±‡": "è±ª",
    "é¼‡": "æ•–",
    "åª·": "å…¥",
    "ç½¿": "å†²",
    "ç³¡": "é™",
    "æ‰": "ä¹",
    "é½¶": "æ¶",
    "é–¼": "æ¶",
    "é‚²": "å¿…",
    "è†¬": "ç¿ ",
    "æ ‚": "æ¢…",
    "è®": "æ„",
    "æ…": "åº†",
    "æ½": "ä¸‘",
    "ç ¢": "ç§‘",
    "æš’": "æƒ…",
    "æ…¼": "ä¸ƒ",
    "æ‘¶": "å›¢",
    "é” ": "æ˜Œ",
    "åƒ‡": "è·¯",
    "ç¼¾": "å¹³",
    "é“¦": "å…ˆ",
    "ç¦«": "è›‹",
    "è©": "å·¨",
    "ç¬©": "ç½š",
    "æ¹©": "åŠ¨",
    "å»˜": "è·¯",
    "æ³‚": "çª˜",
    "é´·": "è£‚",
    "é§Ÿ": "å››",
    "é†¡": "å’‹",
    "è–”": "å¼º",
    "èˆ©": "ä¼ ",
    "è†•": "å›½",
    "ç„”": "ç‡•",
    "é²°": "é‚¹",
    "ç¹¬": "è‰²",
    "ç’«": "å½“",
    "è¹": "å¹´",
    "æ«¥": "é™¤",
    "ç‹¢": "å’Œ",
    "éŒ": "çº¯",
    "ç­ˆ": "æ‰©",
    "ç’¹": "ç†Ÿ",
    "è¸¼": "å”",
    "é­Š": "ç‰",
    "æŠƒ": "å˜",
    "ç™¹": "æ‹”",
    "ç©“": "æ„",
    "é­": "ä¸¤",
    "å´ ": "ä¸œ",
    "æ“¿": "è¸¢",
    "è‘¢": "æ¦‚",
    "ä½’": "å…»",
    "ç§¨": "åš",
    "è…ƒ": "æºƒ",
    "ç­£": "ç¦»",
    "éˆ€": "é¶",
    "ç¦—": "æ€",
    "å«¿": "è¯",
    "è«¶": "é™ˆ",
    "æ­ ": "ç»°",
    "éŸ": "èµ",
    "é§¢": "å",
    "é•": "èµ",
    "å¤€": "ç˜¦",
    "èœ¾": "æœ",
    "å”™": "æ•Œ",
    "æŒ": "æ‹¿",
    "é›°": "åˆ†",
    "è¹¾": "è¹²",
    "é¤": "æ¼",
    "é…–": "é•‡",
    "å‰³": "è¾¾",
    "é´Ÿ": "åƒ",
    "é‰­": "å¦",
    "é³˜": "æ•",
    "å™€": "è®­",
    "æ„º": "è‰",
    "è«Ÿ": "æ˜¯",
    "å°": "çª˜",
    "æ‘£": "æ‰",
    "ç…š": "çª˜",
    "æ«²": "ç‰",
    "æ¤†": "æ„",
    "ç­¿": "å°",
    "é­†": "éœ€",
    "ç¶‡": "ä¿®",
    "å®¼": "æ‰£",
    "ç¥¡": "æŸ´",
    "ä¿»": "è¢«",
    "è‹": "æœ",
    "é³£": "å ",
    "ç¹›": "ç»°",
    "çŒ™": "ç",
    "çœ": "æ˜¯",
    "ç©˜": "æ¶ˆ",
    "è¢": "æŠ¥",
    "è™": "è¾¾",
    "è¾’": "æ¸©",
    "è­": "è§‰",
    "çŸ¦": "å–‰",
    "ç¸‹": "ç¼€",
    "æ—“": "çƒ§",
    "ç›¢": "ç»­",
    "æ¦": "åŸ",
    "ç€€": "ä¼˜",
    "èº¡": "è‚",
    "ç©‹": "è·¯",
    "å±‡": "ç”°",
    "å¢°": "è°ˆ",
    "æµ­": "è€•",
    "é‰¸": "è§’",
    "å¥¤": "å“ˆ",
    "å¶²": "è¥¿",
    "æ¤‘": "æ¯",
    "æ“§": "ä¸¾",
    "ç¢¡": "æ¯’",
    "è€¤": "æ",
    "å‰•": "è´¹",
    "å¦¶": "é—²",
    "è§¤": "é¬¼",
    "æ»§": "æ‘‡",
    "æœ¤": "æµª",
    "éŠ£": "å¦‚",
    "é¯±": "è™",
    "å¡": "åŸ",
    "å—¼": "å¢¨",
    "ç°£": "æºƒ",
    "é«": "æ•²",
    "ä¿¥": "è½¦",
    "å”¼": "ç…",
    "æº‚": "èœ¡",
    "ç¯‹": "å¦¾",
    "èŸŒ": "èª",
    "åŒ¸": "ç³»",
    "ç‘¸": "å½¬",
    "å–¼": "æ¥",
    "å‹—": "ç»­",
    "å”œ": "å¢¨",
    "é™‘": "è€Œ",
    "èµ¼": "å§¿",
    "ç°": "å‡",
    "æ¤ª": "ç¢°",
    "æ•‡": "å†Œ",
    "è¡": "æ³¢",
    "è ": "å¿™",
    "ç‹–": "å³",
    "ç·”": "ä¸Š",
    "æ¯‰": "ä¸€",
    "é£": "è¥",
    "è°¸": "å‰",
    "å¨§": "é€€",
    "èŠ‰": "å¹²",
    "ç‘”": "å…¨",
    "é±Ÿ": "å",
    "è‘": "è´´",
    "ç¸º": "è¿",
    "æ…¤": "ç¡®",
    "å—¢": "è¢œ",
    "è±²": "ç¯",
    "è”¿": "ä¼Ÿ",
    "è¥·": "ä¸¾",
    "æ¥¤": "è€¸",
    "æœ’": "å¥³",
    "é¥…": "ç’",
    "éº•": "å›",
    "éª¯": "è‚®",
    "ä»­": "ä»»",
    "é£ ": "çŸ³",
    "è¥œ": "æ€",
    "ç‘": "é‹",
    "å¼¬": "å®œ",
    "åµ’": "é¢œ",
    "èµ": "æºƒ",
    "è˜¤": "èŠ±",
    "å¥": "æ¢",
    "æ§¢": "ä¹ ",
    "é£¬": "å€¦",
    "èŸ£": "å‡ ",
    "é°§": "è…¾",
    "æ¾": "å«",
    "ç¤¬": "çƒ¦",
    "æŠ³": "ä½ ",
    "çšƒ": "å¸½",
    "å‰—": "é“²",
    "å… ": "å…œ",
    "ä¿¤": "åœ°",
    "å«—": "ç‰",
    "å†": "é“²",
    "è‘’": "çº¢",
    "å‡©": "æœ¨",
    "å»®": "å½±",
    "è‘": "æœ‹",
    "ç·¤": "è°¢",
    "æ™ ": "èƒœ",
    "å™£": "æ˜¼",
    "éŠ²": "æ±‰",
    "å–›": "æ¢",
    "æ¼·": "ç«",
    "ç¢•": "å…¶",
    "æ„™": "å®¢",
    "è‰": "è´¹",
    "å•‡": "åœ°",
    "æŒ€": "æ°",
    "æˆ‡": "å£®",
    "æ®¯": "å®¾",
    "ç¯¥": "åˆ©",
    "å£Œ": "åš·",
    "éª": "æœ¨",
    "è§”": "é‡‘",
    "ç¤": "ç¡®",
    "ç³µ": "è‚",
    "çŠ¨": "æŠ½",
    "å™›": "è‚",
    "é—‹": "ç¡®",
    "é³‰": "å°†",
    "ç™µ": "é¸¾",
    "æ„¼": "è‚¾",
    "è¿": "æ‰©",
    "çŒƒ": "æ˜¾",
    "å‚ƒ": "é€Ÿ",
    "çŸ–": "æ´—",
    "ç–": "å…‰",
    "åŠ‹": "è§’",
    "é‰²": "å¡",
    "è‹ƒ": "æœ‰",
    "ç‹ª": "åŒ",
    "è·”": "å±…",
    "çœ®": "åŒ",
    "ç¦†": "å¿…",
    "ç¡¾": "ç¼€",
    "æ©": "æ¾",
    "ç‰³": "æ¯",
    "ç±®": "ç½—",
    "ç–ˆ": "å±",
    "é": "åˆ˜",
    "æ™¿": "æ˜Œ",
    "å˜‡": "å±±",
    "èµ½": "è§‰",
    "ç‘Š": "é—´",
    "ä¾’": "å®‰",
    "è½‚": "å¤",
    "æ´”": "æŒ‡",
    "ç’Š": "é—¨",
    "é¹¢": "æ„",
    "è“¶": "ç»´",
    "æœ": "çœ¼",
    "éˆ®": "ä½ ",
    "é·": "å¼•",
    "è„»": "æ¥",
    "é˜¨": "æ¶",
    "å†¾": "æ°",
    "æ‡¡": "æŠ¹",
    "æ·²": "æ ‡",
    "ç ¯": "å¹³",
    "è‘»": "è“",
    "æ»’": "æ­Œ",
    "ç ": "æ¶",
    "ç–": "è‡³",
    "é¡°": "è´«",
    "æ˜…": "èŠ‚",
    "é²—": "è´¼",
    "å…Ÿ": "æ·±",
    "ç": "éª‚",
    "è‰": "é»„",
    "ç€º": "ç¼ ",
    "å†˜": "é“¶",
    "å‹¥": "é™",
    "å€ ": "è™½",
    "èº": "è°¢",
    "æ‘": "çª—",
    "ç†‡": "è´º",
    "è„´": "åŒ¹",
    "éˆ©": "èŠ¦",
    "è¹¯": "çƒ¦",
    "åœ¡": "åœŸ",
    "é£”": "æ€",
    "æ¡Ÿ": "æˆ˜",
    "ç±™": "è·¯",
    "ç¥´": "è¯¥",
    "é¬¬": "è±†",
    "è»": "æœˆ",
    "åš¤": "é­”",
    "çƒ‹": "ä¿®",
    "ç¼": "æ­¤",
    "ä» ": "æ„Ÿ",
    "ç„¾": "å¹´",
    "çŒ‚": "æ±‰",
    "ç¦": "æ ¼",
    "ç¶": "å¬",
    "å¹": "é©¾",
    "æ¬‰": "ä»",
    "è±‘": "è‡³",
    "é¹™": "ç§‹",
    "è¡˜": "é—²",
    "å½£": "æ–‡",
    "çŠœ": "è¹²",
    "æ”¼": "å¹²",
    "è—": "æ ‘",
    "ç··": "è¿",
    "ç¸³": "å€¦",
    "æª": "å",
    "ä»¾": "ä½",
    "é£±": "å­™",
    "ç•—": "è¾¾",
    "çš”": "æ±‰",
    "å’ˆ": "ç¦",
    "è—¹": "çŸ®",
    "ç¬­": "é›¶",
    "è¨": "éœ€",
    "æ®…": "ç”Ÿ",
    "å™­": "æ•™",
    "åª‚": "åœ°",
    "ç¦•": "ä¸€",
    "å¦": "è®¸",
    "é´": "æ¬§",
    "ç¡‰": "è·¯",
    "é¼•": "ä¸œ",
    "è¸¡": "å…¨",
    "å“»": "æ†¨",
    "æ«™": "æ¬§",
    "é±®": "ç»­",
    "é…": "å€Ÿ",
    "ç†¯": "æ±‰",
    "çœ¥": "å­—",
    "é¨µ": "åŸ",
    "æ½’": "è¡",
    "æ Ÿ": "å¥”",
    "æ—‰": "å¤«",
    "é«£": "è®¿",
    "å": "æ‹”",
    "å½ƒ": "å¿…",
    "è“¢": "æµª",
    "åŠ’": "è§",
    "è­ª": "çŸ®",
    "ç³¼": "å·¥",
    "è·¼": "å±€",
    "æ¦": "å•¦",
    "çŸ¼": "åˆš",
    "å–": "æ‰¯",
    "ç¨Œ": "å›¾",
    "è°": "ç»ƒ",
    "é»": "å·¨",
    "é®¡": "ç…§",
    "éº©": "å¤«",
    "æ¼¥": "æŒ–",
    "æ±«": "äº•",
    "è‹²": "çœ¨",
    "è«": "åˆ™",
    "é£ƒ": "é£˜",
    "ç–": "åˆ˜",
    "é­­": "åŸ",
    "è•¶": "é›¶",
    "æ²": "æ´—",
    "ç±": "é›•",
    "æ•Š": "å¤„",
    "æ¬": "å ",
    "æš•": "å‡",
    "å‘Œ": "æ•™",
    "çœ": "çº¢",
    "ç‹¯": "å¿«",
    "æ¨›": "çº ",
    "é‚…": "å ",
    "çŸ´": "å®š",
    "åª": "è§£",
    "å„º": "æŒª",
    "æ¹™": "æ„",
    "è–²": "è´«",
    "èŠ”": "ä¼š",
    "éŒ®": "å›º",
    "æ£“": "æ£’",
    "ç±ª": "æ–­",
    "ç·": "äº”",
    "ç•„": "åˆ˜",
    "ç£³": "å¢",
    "åª§": "æŒ–",
    "ç¿´": "è¿",
    "è§«": "é€Ÿ",
    "èŒª": "å…‰",
    "ç„": "è®¾",
    "ç‘¬": "åˆ˜",
    "å»‡": "å…­",
    "è¨Œ": "çº¢",
    "æ§±": "æœ‰",
    "ç´†": "è¿‚",
    "å…ª": "é±¼",
    "å­‹": "ç¦»",
    "æ«†": "å¥",
    "ç£¾": "ä½",
    "è«¨": "ç¦",
    "è¬”": "è¡€",
    "æª": "è§£",
    "é´€": "å¦",
    "å¤¿": "å…«",
    "çˆ«": "æ‰¾",
    "ç®‘": "ç…",
    "å”‚": "å§‘",
    "çˆ": "ç¦",
    "æ²•": "å¯†",
    "å¸Š": "æ€•",
    "ç„º": "ç”Ÿ",
    "ç³³": "åš",
    "èŠ": "æ°”",
    "é¬¶": "å½’",
    "è¢": "å®š",
    "å‚–": "ä»“",
    "è­”": "èµš",
    "çŠ¦": "åš",
    "æ·œ": "å¹³",
    "è†¤": "é›ª",
    "å†¡": "ç›Ÿ",
    "ç¯´": "æ•Œ",
    "å€": "èˆ”",
    "é­·": "ç”±",
    "å£": "ç†",
    "æ©": "çœ¨",
    "è¡–": "å‘",
    "æŒµ": "å¼„",
    "å£¼": "æ†",
    "ç®„": "æ¯”",
    "è¼—": "å°¼",
    "æ¥³": "æ¢…",
    "æ…´": "è®¾",
    "å¿º": "å…ˆ",
    "å‹¨": "å‘",
    "ç’": "å½’",
    "æ¡¯": "å¬",
    "å¹¨": "æ€",
    "ç†¦": "è§‰",
    "èµ©": "ç³»",
    "èŠ“": "å­—",
    "é–": "å—",
    "é•®": "ç¯",
    "é": "è¸",
    "è²²": "å§¿",
    "è€": "å®¶",
    "åª´": "åŸ",
    "ç¥‹": "å¯¹",
    "å˜ª": "ä¹°",
    "é‰ˆ": "è¯—",
    "æ„": "å°±",
    "æ‹¤": "æ°",
    "æ…¯": "ä¼¤",
    "æ°†": "æ™®",
    "è ": "è°¢",
    "é°": "æ¶",
    "é„®": "å¸½",
    "æ±": "å®Œ",
    "é»ª": "æƒ¨",
    "å¡‚": "å‘",
    "æ£": "æ‰",
    "è´—": "ç‡•",
    "æ¯ª": "æœ¨",
    "ç±": "æª",
    "å¸¨": "ç¡",
    "èŒ»": "å¿™",
    "ç·œ": "çœ ",
    "æ¼ƒ": "è®°",
    "èŸ—": "ç§‹",
    "æŸ¹": "æ˜¯",
    "å•ˆ": "å“¼",
    "é³“": "ä¹",
    "ç·": "æ°‘",
    "ç‹Š": "å±€",
    "åª¯": "å½’",
    "ç€…": "è¥",
    "ç¡": "å‘",
    "å¾": "è‡³",
    "å·¶": "æ‹›",
    "çŠ¤": "æ’",
    "æ¿™": "è¥",
    "è¬œ": "åŸ",
    "æ‹¸": "å®œ",
    "æ©²": "æ´—",
    "è": "è‚",
    "æ¥¿": "ç›¸",
    "æ½¥": "é€Ÿ",
    "åŸˆ": "ä¿Š",
    "ç¸Ÿ": "å…¥",
    "åš¬": "è´«",
    "ç¾¢": "å®¹",
    "æ§¾": "æ…¢",
    "èƒ˜": "é—²",
    "è©§": "èŒ¶",
    "è¢": "åš",
    "ç²‚": "æ‘˜",
    "æ¦‘": "ç¦",
    "å²¤": "å­¦",
    "å§·": "å³",
    "ç­¼": "äº‘",
    "èŸ°": "æ¶ˆ",
    "ç£¥": "å’",
    "å¨­": "å“€",
    "è¾": "å¿™",
    "å†": "å¸½",
    "éœ…": "å’‹",
    "é›Š": "å¤Ÿ",
    "çŠŒ": "å®¶",
    "å": "ä½",
    "è‡›": "æˆ–",
    "è‘„": "åš",
    "èŠ§": "ç»­",
    "ç‡": "ç»„",
    "æ°€": "é©´",
    "åš‘": "ç†",
    "æƒ": "è®°",
    "æ¢¹": "å½¬",
    "æ…": "é±¼",
    "éŒ§": "ç®¡",
    "é•´": "èœ¡",
    "å¨": "å†…",
    "é¤‘": "æ³¢",
    "é‹¯": "å‘Š",
    "æŸ‡": "å’Œ",
    "é«½": "æŠ“",
    "ç™¦": "ä¹ˆ",
    "é­": "ä»»",
    "æ–…": "ç¬‘",
    "ç˜": "é™ˆ",
    "æ”º": "ä»¥",
    "ç®Ÿ": "ä¿Š",
    "åµ¿": "é¡¶",
    "è‡«": "è§’",
    "æ˜¡": "ç‚«",
    "å¿": "çˆ¶",
    "èˆ™": "è¯",
    "å‹€": "å®¢",
    "ç‰š": "ç§°",
    "æ½»": "é¼ ",
    "è«": "æŒ£",
    "åƒ¥": "è§’",
    "è‡ˆ": "èœ¡",
    "æŸ²": "å¿…",
    "ç½£": "æŒ‚",
    "ç›": "èª",
    "ç‹“": "çš®",
    "å·—": "é¢œ",
    "é¹": "å‰",
    "æ£": "çœ ",
    "å´²": "é»„",
    "ç¢™": "è„‘",
    "æ­”": "éœ€",
    "è“": "è£¸",
    "é‹‹": "ç¼ ",
    "æ¡¼": "ä¸ƒ",
    "ç…­": "è£‚",
    "åª»": "ç›˜",
    "è•Ÿ": "å‘",
    "å…": "å‚",
    "ç˜ƒ": "ç«¹",
    "ç‘«": "æ",
    "çµ›": "æ",
    "ä¼¨": "è®­",
    "é°¾": "æ ‡",
    "æ»®": "æ ‡",
    "å»™": "æ„",
    "ç¨¾": "æ",
    "æ²´": "åˆ©",
    "å“µ": "å…«",
    "æ¸„": "é£",
    "ç’„": "äº•",
    "é¶‰": "çº¯",
    "çˆ©": "ç‰",
    "é±’": "å°Š",
    "æ¼¶": "æ¢",
    "é ¬": "å¤¹",
    "å¤£": "æ¢¦",
    "æ¸°": "çœ¼",
    "é‡”": "ä»¥",
    "è·§": "å…¨",
    "é±µ": "çœŸ",
    "åµƒ": "çœ¼",
    "æ³†": "æ„",
    "ç±¦": "ä¸­",
    "é ½": "æ¨",
    "æ£Š": "å…¶",
    "æ˜¤": "é›¶",
    "æ¯": "æ",
    "è¶‘": "å§¿",
    "è§½": "è¥¿",
    "çšš": "åŸƒ",
    "é¡“": "ä¸“",
    "è¸¦": "ä»¥",
    "æ­Ÿ": "é±¼",
    "å½®": "æ°¸",
    "æŒ„": "æ‰©",
    "é®»": "ç¼©",
    "å€": "ä¸“",
    "å“": "æ˜¯",
    "æŠ¯": "æ‰",
    "é½ª": "ç»°",
    "è¾§": "å˜",
    "å­": "å…",
    "æœ³": "å…«",
    "ç ³": "ä¹",
    "é¯¢": "å°¼",
    "å€‚": "ç—…",
    "ç…´": "æ™•",
    "çµ°": "å ",
    "çº•": "åš·",
    "ç£»": "ç›˜",
    "éµ™": "å±€",
    "åŠ¯": "æœ±",
    "æ•»": "èƒ¸",
    "éŸ": "è¿",
    "æ¬Œ": "è—",
    "çŒ¨": "åŸ",
    "ç½‰": "ç§°",
    "å¾¥": "æ˜¯",
    "æŠ": "é‹",
    "è£»": "æ¯’",
    "è®‚": "ç‚«",
    "å½Ÿ": "çº¦",
    "æ º": "æ„",
    "æ£¡": "åˆš",
    "æš¤": "å·",
    "ä¿“": "é™",
    "æ¤": "æ¶",
    "ç„…": "è£¤",
    "ç‹¶": "è¥¿",
    "è™¡": "å·¨",
    "ç®–": "æ—",
    "ç„ª": "ç©·",
    "èŒ˜": "åˆ©",
    "ç’¸": "å½¬",
    "å„©": "å››",
    "æ’": "ç°",
    "è¼Œ": "äº®",
    "ç•º": "å°†",
    "ç¤®": "ç‚®",
    "ç—œ": "çª",
    "çŠ«": "æŠ½",
    "é³€": "æ",
    "çš ": "è„†",
    "å¦œ": "æœˆ",
    "ç¬¯": "åŠª",
    "çƒ": "è§‰",
    "èƒ": "è¥¿",
    "ç†¤": "æ„",
    "è…„": "å‚",
    "å©¨": "ä¼¦",
    "æ¡®": "æ¯",
    "é“´": "æ±¤",
    "æ³™": "å¹³",
    "ç‘¥": "æ¸©",
    "ç®‚": "æ¥",
    "é°¹": "é—´",
    "çŸ°": "å¢",
    "ç«š": "åŠ©",
    "æ¥´": "æ›¿",
    "ç¥": "ä»»",
    "ç£—": "ä¸“",
    "å¡€": "å¹³",
    "é§‰": "çª˜",
    "å¸º": "å…¶",
    "é•": "ç¡¬",
    "æ‰Ÿ": "æ·±",
    "å“¬": "å’Œ",
    "è‹¿": "ä½",
    "éµ¡": "äº”",
    "ç°™": "åš",
    "éŒ†": "æª",
    "ç†": "å®œ",
    "ç¸Œ": "é€†",
    "é¥¦": "æ‹–",
    "çŸ": "é‡‘",
    "çœ–": "çŸ¿",
    "è‡’": "å§",
    "çƒ‡": "çŠ¬",
    "ç£§": "æ°”",
    "é‰½": "æ˜¯",
    "è—": "è½¦",
    "èœ": "åº•",
    "è¹†": "è…¿",
    "åµˆ": "æ¢",
    "ä¸©": "çº ",
    "ç™¶": "æ³¢",
    "é±˜": "å¯»",
    "ç”€": "ç¼€",
    "æ¿": "è¯º",
    "æ»¶": "æ•–",
    "ç¦ ": "æ€",
    "é³§": "ç¦",
    "é¯¥": "è·¯",
    "è¿Š": "åŒ",
    "å˜‚": "æ•™",
    "ç’ª": "æ—©",
    "æ˜©": "å¢¨",
    "å¾¤": "è§",
    "ç€²": "ç»ƒ",
    "å’‰": "å…»",
    "æ˜·": "æ¸©",
    "å”«": "è¿›",
    "é´–": "æ°‘",
    "ç²»": "å¼ ",
    "é©€": "å¢¨",
    "æ  ": "å¿",
    "é¿": "è¦",
    "ç™†": "åŠ³",
    "åº˜": "å‘€",
    "ç°": "å•",
    "èˆº": "ä¾ ",
    "å®¨": "æŒ‘",
    "ç¤’": "ä»¥",
    "ç‘‘": "èµš",
    "æ€³": "è°",
    "è¬": "å°",
    "éƒˆ": "å",
    "æ¹¢": "å¿…",
    "é±¥": "è´µ",
    "ç–": "å±•",
    "è’Ÿ": "ä¸¾",
    "çš¸": "å›",
    "è•¯": "é¾™",
    "å³ˆ": "è½",
    "çœ’": "æ·±",
    "é¡³": "è‚",
    "è’³": "é‚£",
    "è€‚": "è€",
    "ç˜­": "æ ‡",
    "ç›¬": "å¤",
    "æ—™": "ç¿»",
    "è•š": "æ¶",
    "æƒ·": "æ˜¥",
    "å•¨": "åº”",
    "è¼…": "å’Œ",
    "è…½": "è¢œ",
    "å«®": "äº’",
    "é®ˆ": "å±…",
    "è«²": "å› ",
    "ç‘‚": "æ¢…",
    "åƒ": "èµš",
    "æŠ": "æŠ½",
    "è¢": "é‹",
    "é†±": "å‘",
    "è“¸": "æ›¹",
    "åŸ¥": "é’",
    "è§": "äº",
    "çµŸ": "å…¨",
    "è‹…": "æ„",
    "æ¿³": "é’±",
    "å‡…": "å›º",
    "çœ†": "è®¿",
    "å– ": "ç§",
    "æ¡‹": "å®œ",
    "é ©": "å¹³",
    "ç›©": "å‘¨",
    "æ§¨": "æœ",
    "è–¾": "è€³",
    "å¶’": "å±‚",
    "æ†€": "èŠ",
    "é®¸": "å…",
    "ç¤": "é¸¾",
    "é™º": "æ˜¾",
    "å¡ˆ": "è®°",
    "è¸": "å•",
    "è±­": "å®¶",
    "ç¬²": "çƒ¦",
    "é¦": "ç§‹",
    "æŠ¦": "é¥¼",
    "æ": "å—",
    "ç¸’": "åˆº",
    "å³®": "ç¾¤",
    "æ·Ÿ": "èˆ”",
    "åµœ": "å…¶",
    "é­‹": "æ¨",
    "æ‡—": "ä¸‹",
    "è µ": "è¥¿",
    "ç³¶": "è·³",
    "å‚": "å¿™",
    "è” ": "ä¸­",
    "é…º": "è‘¡",
    "è¥š": "å²",
    "è…‚": "å’",
    "ç¢¿": "é€Ÿ",
    "é¬": "æ•²",
    "è®Œ": "ç‡•",
    "éœ¤": "å…­",
    "ç±µ": "çƒ¦",
    "æºµ": "å› ",
    "é‰“": "èµ¤",
    "éŸ¾": "å› ",
    "ç…µ": "ç”·",
    "å¡Ÿ": "è—",
    "æ¥©": "å",
    "é«": "æ¥¼",
    "èª‘": "ç‹‚",
    "ç²ƒ": "æ¯”",
    "åµš": "äº²",
    "è…": "æ’¤",
    "æ¢›": "æŒª",
    "çŠ¿": "æ¬¢",
    "ç¾˜": "è„",
    "å’": "å¿…",
    "é¹¯": "å ",
    "æ—´": "éœ€",
    "é™§": "è‚",
    "å½º": "ç‹",
    "è«": "è¿",
    "å¨": "åˆº",
    "å¨¢": "å«",
    "ç°»": "æŠ“",
    "æ³’": "å§‘",
    "è•«": "æ‡‚",
    "æ˜¸": "ä¸œ",
    "æ¥¡": "é±¼",
    "è©¨": "ç¬‘",
    "é³¡": "æ„Ÿ",
    "é¥‰": "ç´§",
    "ç±¨": "è¿",
    "é¸": "æ´’",
    "æ¸±": "çº¢",
    "é¼Œ": "æœ",
    "ç™´": "é¸¾",
    "æ‘¥": "çƒ«",
    "é¼†": "ç›Ÿ",
    "æ·¥": "è·¯",
    "ç™­": "å½±",
    "è¢": "çˆ¶",
    "æ¤‡": "ä¸¾",
    "é ¦": "å­©",
    "ä½±": "æ³•",
    "é¶„": "ç²¾",
    "å² ": "å·¨",
    "è¢¿": "å½’",
    "ç—": "å±±",
    "ç¾ ": "å®œ",
    "é¦¹": "æ—¥",
    "æ‘¼": "å‘",
    "æ³¹": "è›‹",
    "é¶": "æ¾",
    "è‹™": "åˆ©",
    "æ‡“": "çˆ±",
    "æ‘": "è·¯",
    "ç—¶": "èˆ”",
    "é†—": "ç ´",
    "ä¹¥": "äº’",
    "é‰·": "çº¢",
    "æ…’": "ä»",
    "è¶Œ": "æ",
    "å–¨": "äº®",
    "é ": "æ™®",
    "æ‡Œ": "æ„",
    "éŒ±": "çœŸ",
    "ä½ª": "å›",
    "ç£¦": "æ ‡",
    "æº™": "å¤ª",
    "è‹¨": "ä½ ",
    "èš–": "åŸ",
    "éŠ¦": "å› ",
    "é©": "å¥¥",
    "è©’": "å®œ",
    "é³¤": "ç®¡",
    "çˆŠ": "å‡¹",
    "å¦±": "æ‹›",
    "ä½«": "è´º",
    "é±¤": "æ„Ÿ",
    "ç¡": "å¤„",
    "ç‰“": "ç»‘",
    "ç¤±": "é¾™",
    "æ·ˆ": "å¤",
    "æ¦š": "å’¬",
    "ä»±": "é’±",
    "è•": "è§‰",
    "ç†ˆ": "è¥¿",
    "æ†Ÿ": "é€Ÿ",
    "æ¼°": "ç °",
    "ç‹Ÿ": "ç¯",
    "æ²˜": "æ¯”",
    "æ«": "é—¨",
    "ç¦": "å‘",
    "æšŠ": "è®¸",
    "ç¶": "å¹³",
    "å£¡": "ç‘",
    "æ¸¢": "çƒ¦",
    "æ¯±": "å±€",
    "ç²": "é†’",
    "ç¿µ": "å–‰",
    "æ±‹": "ç€",
    "èªƒ": "å®œ",
    "ç·‡": "å§¿",
    "é": "é›·",
    "ç‹º": "é“¶",
    "ç¯¶": "çƒŸ",
    "å²’": "é’±",
    "æ©·": "å…œ",
    "å·‚": "å½’",
    "å•©": "æŒ‚",
    "æ¤—": "å®š",
    "ç”½": "é•‡",
    "å•…": "ç€",
    "é¯ª": "é›¶",
    "ç¼": "å‡",
    "æ³": "é€Ÿ",
    "èº±": "æœµ",
    "é„ƒ": "ä¹¦",
    "é´¦": "å…»",
    "é‡¨": "å­",
    "è±½": "é‚£",
    "ç§": "åˆ©",
    "æ¹¼": "è‚",
    "ç ƒ": "å•",
    "é¬¾": "è®°",
    "é†–": "è¿",
    "é†‚": "æ‡’",
    "é¬‚": "å®¾",
    "æ±¯": "çº¢",
    "ç­½": "æ¬§",
    "ç’¦": "çˆ±",
    "å„«": "è±ª",
    "çŠ": "é’±",
    "ç´‰": "ä»»",
    "è–Œ": "ç›¸",
    "ç¢‚": "å®—",
    "èª”": "æŒº",
    "ç": "ç©·",
    "å ¨": "å¤œ",
    "ç¿¬": "ç°",
    "ä¾­": "ç´§",
    "è†": "æµ…",
    "è": "é©¬",
    "å¨•": "ç»°",
    "å´ˆ": "è™«",
    "è¢": "å­—",
    "é  ": "ä¼Ÿ",
    "æ‚®": "ç‰©",
    "ç¦–": "æ¢…",
    "å”": "åœ°",
    "ç®›": "å§‘",
    "é·‚": "è¦",
    "è“œ": "é…",
    "ç†‹": "å¥¶",
    "é·": "è‘¡",
    "è¶ª": "é»„",
    "æ™ˆ": "è§’",
    "è©†": "åº•",
    "ç­¶": "å‘Š",
    "èš³": "æŒ",
    "çµ¯": "è¯¥",
    "ç²": "äº’",
    "è„": "æ¢—",
    "åœ‚": "æ··",
    "ç“–": "ç›¸",
    "è“±": "å¹³",
    "æ˜¦": "å·",
    "çš·": "å¤",
    "æ©š": "é€Ÿ",
    "æ¾¦": "ç‰",
    "å³ƒ": "å­¦",
    "ä¾": "ç±³",
    "æ‘–": "æ°”",
    "ç—¡": "å¤«",
    "æ•": "é¢ ",
    "å‹¼": "çº ",
    "éº": "å¢¨",
    "æ´‰": "å",
    "çª": "æ‰",
    "å±°": "é€†",
    "é‘­": "çƒ‚",
    "ç£µ": "è§",
    "èˆ ": "åˆ€",
    "æ™¼": "ç¢—",
    "ç§¬": "å·¨",
    "é›š": "çŒ",
    "å¿¬": "ç‰",
    "é‹¹": "åœº",
    "ç‘": "ç©¿",
    "ç‰·": "å…¨",
    "ç“»": "åƒ",
    "ç¬»": "ç©·",
    "é¥": "å–„",
    "ç»¤": "ç³»",
    "ç‡¶": "å†œ",
    "å’‡": "åˆ«",
    "ç‰¥": "æ–¹",
    "é›¬": "å¦",
    "é–¶": "æ˜Œ",
    "èšƒ": "æƒ³",
    "ç·«": "èª",
    "è¤": "åœ",
    "é­¢": "å‡ ",
    "å ¼": "å“¼",
    "å©”": "é£",
    "æ˜¬": "æ˜",
    "çª‹": "ç«¹",
    "ç’š": "ç©·",
    "çˆ": "è™«",
    "è¼®": "æŸ”",
    "ç«µ": "æ­ª",
    "ç•‡": "äº‘",
    "é‘¨": "é¾™",
    "ç“•": "è¿·",
    "æ•‹": "æ ¼",
    "é®´": "ä¿®",
    "å¸‰": "åˆ†",
    "åˆ“": "å®Œ",
    "ç¥»": "å›º",
    "å€±": "æ··",
    "æ”‹": "èœ¡",
    "é„²": "å•",
    "é–‚": "æ‹´",
    "é‹Œ": "å®š",
    "å": "è®°",
    "å©–": "å¤©",
    "é«ƒ": "é±¼",
    "æ´ˆ": "ç»´",
    "è£§": "æ€",
    "ç¾‡": "æœº",
    "æ§©": "æ¦‚",
    "éŠ ": "è€",
    "è“¹": "ç‰",
    "è¹±": "ä¸­",
    "å˜“": "é”…",
    "è”•": "åœ°",
    "è†‰": "æ„",
    "è€": "è½¯",
    "å‹”": "å…",
    "å½": "é”…",
    "é€°": "ç”±",
    "è†Ÿ": "ç»¿",
    "å¸¢": "æ°",
    "åº": "å»",
    "è³¾": "åˆ™",
    "ç·­": "ä½",
    "é®ª": "ä¼Ÿ",
    "çŠ§": "è¥¿",
    "å¯½": "ç»¿",
    "é‹ƒ": "ç‹¼",
    "ä¿°": "æˆ–",
    "å°ƒ": "å¤«",
    "ç¹½": "å½¬",
    "å›€": "èµš",
    "è˜¡": "åº”",
    "ç€¦": "æœ±",
    "èŠ–": "è‡³",
    "å¨": "è°¢",
    "è«„": "å‡†",
    "å«¢": "å½’",
    "ç½ƒ": "åº”",
    "çœ˜": "è‚¾",
    "ç": "æ°‘",
    "è²­": "è‡³",
    "ç·¼": "è¿",
    "è·•": "ç‚¹",
    "å”": "è‡³",
    "é–¦": "å¤„",
    "ç£ª": "å‚¬",
    "æ‡": "ç›Ÿ",
    "åª¢": "å¸½",
    "é™": "å •",
    "å¢": "è¿›",
    "éµ¼": "ç©º",
    "ä¹Š": "ä¸€",
    "é†²": "å†œ",
    "ä¾": "ç»­",
    "æ¤»": "ç‡•",
    "ç±«": "é’»",
    "æŸ›": "æ·±",
    "æ½": "å±",
    "é—š": "äº",
    "ç®": "æŒ",
    "é›": "åš",
    "çˆ§": "é›¶",
    "æ­­": "æŒ",
    "æ¢¿": "è¿",
    "ç¥²": "è¿›",
    "è•œ": "åŒª",
    "å¶Œ": "å¯¼",
    "å°«": "æ±ª",
    "è²š": "è°ˆ",
    "æ•¯": "æ•",
    "å½": "å®œ",
    "æª·": "ä½ ",
    "æ„´": "åˆ›",
    "ç›«": "å®‰",
    "æ’": "æ£’",
    "ç¨°": "è®¸",
    "çµ¤": "ç°",
    "ç¡": "è“",
    "åµ": "åŸ",
    "ç’²": "å²",
    "çœ": "æ¥",
    "è’…": "æŸ“",
    "ç": "å¤«",
    "ä¼®": "åŠª",
    "å·ƒ": "é¾™",
    "ç»¹": "æ¡ƒ",
    "åµ¶": "å¼±",
    "ç„½": "èƒ¸",
    "çœ½": "å¢¨",
    "å¾«": "ä¼Ÿ",
    "ç¾¬": "é’±",
    "çœ¡": "æ˜¯",
    "è¿": "æ¬¡",
    "è „": "ç´",
    "å¾": "è‡³",
    "é¨": "å¤œ",
    "ç«‚": "èŠ",
    "æ‘": "çº ",
    "é¨²": "è‰",
    "è‹‚": "é“¶",
    "å®»": "å¯†",
    "ç“": "å§",
    "å°Ÿ": "æ˜¾",
    "éº¨": "åµ",
    "ç‚": "å¼€",
    "å¢­": "èƒœ",
    "éª»": "è·¨",
    "è¤©": "ç­",
    "è“´": "çº¯",
    "è»¾": "æ˜¯",
    "æ•¶": "é•‡",
    "è§¿": "è¥¿",
    "é¨¾": "ç½—",
    "ç´¾": "æ•",
    "é‘ ": "ç¡•",
    "æ·": "æ˜Œ",
    "ç¨Š": "æ",
    "éš¤": "æ¨",
    "èŒ": "é™ˆ",
    "æ«": "ç›˜",
    "å¦‘": "è¶´",
    "çš˜": "æ¬ ",
    "å’Š": "å’Œ",
    "æ¤š": "ä»¬",
    "æ¥†": "é‚€",
    "æ£¯": "å¿",
    "é˜¾": "é¢†",
    "å": "æ",
    "æ§«": "å›¢",
    "é®„": "ç¦",
    "æ€‘": "åŠ",
    "å»µ": "å¯»",
    "åˆ": "å¸¦",
    "å«": "æ",
    "æ™„": "è°",
    "ç¢¦": "å®¢",
    "å·¼": "å…«",
    "æ¢²": "ç€",
    "çŠ–": "è½",
    "åƒ¤": "è›‹",
    "é¢°": "æ‹”",
    "æ‡ƒ": "ç´",
    "çŠ": "ä»“",
    "æ¤¥": "çŸ¥",
    "è›§": "ç½‘",
    "è‡": "å–",
    "é† ": "ç›",
    "å¿³": "å",
    "ç¥": "å¯»",
    "è‡": "è£¸",
    "ç¤‚": "è¥¿",
    "å¡°": "æµ·",
    "ç“¥": "åˆ©",
    "ç¡‹": "çˆ±",
    "æ²‹": "ç”±",
    "ç¹’": "å¢",
    "é´´": "æ¨ª",
    "é¤­": "é»„",
    "é½¬": "é›¨",
    "æŠ†": "ç¨³",
    "é…™": "çœŸ",
    "é¼’": "å§¿",
    "å•´": "é“²",
    "ç‘¡": "è¯—",
    "æ€": "é€¢",
    "çš": "é»„",
    "è£": "å°¼",
    "ç¢ª": "çœŸ",
    "é·ƒ": "ç‡•",
    "åª": "å®œ",
    "æ£": "å›º",
    "é‹¦": "å±…",
    "å–©": "ç‰",
    "é‹¶": "æŸ³",
    "æ±˜": "å‰",
    "å†£": "å·¨",
    "çšŒ": "å¢¨",
    "é–¸": "æ†",
    "ä½¡": "å…ˆ",
    "è¸": "è°‹",
    "æŒ°": "æˆ",
    "å¤": "å€Ÿ",
    "å‹¦": "è¶…",
    "åº¿": "åº™",
    "æ«": "å‘¨",
    "ä¾•": "è€Œ",
    "æ¤„": "æ¥",
    "æ˜": "é¥¼",
    "æ„": "ä¼¦",
    "èª©": "é™",
    "é°†": "æ˜¥",
    "å‰˜": "å…¶",
    "è„µ": "å¤",
    "ç•·": "ç¼€",
    "è¨": "èŠ‚",
    "è³’": "èˆ",
    "é‘–": "ç­",
    "åšµ": "ç¼ ",
    "å«¼": "å¢¨",
    "ç¡¤": "ä¾ ",
    "é‰®": "ç¯",
    "æƒ»": "å†Œ",
    "é°¥": "å…³",
    "åª”": "çœ ",
    "ç£‘": "ç»´",
    "è™€": "æœº",
    "æ¹": "æ¥",
    "è±¿": "ç‹—",
    "åº": "å¤«",
    "ç–ª": "å¿…",
    "éº": "ç²—",
    "è©˜": "åŒº",
    "ç¡”": "çº¢",
    "æ—": "ä¸",
    "çµ‹": "çŸ¿",
    "åŸ¢": "å…¨",
    "é°°": "ç¥",
    "éŸ¤": "è¢œ",
    "è¥¾": "äºš",
    "åš›": "äº’",
    "æ«˜": "ä¼š",
    "è–": "ä¸‡",
    "ç¢½": "å·¥",
    "æ•": "åœ°",
    "é¶¡": "å’Œ",
    "ç¶": "è„‘",
    "æ­œ": "å¤„",
    "å°—": "ä¹¦",
    "é¤": "è§",
    "çŸ™": "çœ‹",
    "æ¤“": "ç€",
    "é£®": "å¼•",
    "æ¸¿": "è€",
    "æ†¯": "æƒ¨",
    "æ©ƒ": "ç½š",
    "è¨½": "å¤Ÿ",
    "æ¶­": "ç˜¦",
    "æ…†": "æ",
    "åª": "é¢",
    "é£¡": "å‚",
    "è©º": "å‘½",
    "é—“": "å‡¯",
    "é¨ƒ": "åŸƒ",
    "é¯€": "æ»š",
    "ç¦´": "æœˆ",
    "ç¥¾": "é›¶",
    "æ»": "è¡Œ",
    "è¡œ": "åˆ°",
    "è¬„": "è…¾",
    "è¡­": "å¤«",
    "ç‰": "ç§‘",
    "é°‹": "çœ¼",
    "è½¤": "èŠ¦",
    "æŠ": "å‘¢",
    "ç­»": "æ ",
    "èŠ›": "ä¼Ÿ",
    "æƒ’": "å’Œ",
    "ç¤²": "é¾™",
    "ç¯ˆ": "é£",
    "è §": "åº¦",
    "æ’¦": "æ‰¯",
    "æ…­": "å°",
    "æ›“": "æŠ¥",
    "ä¸±": "çŒ",
    "ç–¢": "è¡¬",
    "èš": "æ‹“",
    "æ£­": "æ„",
    "å¥¿": "é¥­",
    "è†‡": "ç¼€",
    "é–§": "çº¢",
    "è¢ ": "è‡³",
    "ç³¤": "ä¸‰",
    "é°£": "çŸ³",
    "è“‡": "å¤",
    "ç¤": "æ£’",
    "æ´": "å¾·",
    "é§¸": "äº²",
    "å‚«": "å’",
    "å§„": "æ°‘",
    "åŒ±": "è´µ",
    "è¾·": "ä¸€",
    "æ»¸": "è™",
    "ç‘µ": "æ‰¾",
    "è’": "åŸ",
    "éŒ¸": "æ¥",
    "åƒ¸": "è¿›",
    "æ¶·": "ä¸œ",
    "é‹½": "æ‰",
    "æ¯": "è‚¯",
    "éœ": "é›¶",
    "ç†œ": "èª",
    "çœœ": "å¢¨",
    "å¼…": "æ„¤",
    "èœ¼": "ä½",
    "ç„‘": "çƒŸ",
    "æ¿ª": "åº†",
    "é¦": "å¼€",
    "é³›": "ä¹ ",
    "å´°": "å§¿",
    "ç¡º": "ç€",
    "æ€£": "ç”±",
    "è—Ÿ": "å’",
    "ç·¿": "å¸¦",
    "é«•": "å®¾",
    "ç•“": "å¤š",
    "è¹": "æ",
    "å£»": "ç»­",
    "åº": "å°º",
    "æœ¸": "åˆ©",
    "æ¯‘": "è§£",
    "ç³‰": "å®—",
    "è¥´": "è“",
    "é‰": "ç”Ÿ",
    "ç­¸": "å¹²",
    "æ”": "è§‰",
    "ç´": "æ„",
    "ç¹¾": "æµ…",
    "è—¦": "å¢¨",
    "éºƒ": "è¢",
    "å°Œ": "æ ‘",
    "å§º": "æ·±",
    "åŒ¨": "è„",
    "æ®°": "æ¯’",
    "è¸ˆ": "ä¹¦",
    "å»": "å¿ƒ",
    "é": "æ•™",
    "ç¸²": "é›·",
    "ç´¨": "å¤«",
    "è¨§": "ç”±",
    "çƒ•": "ç­",
    "æœš": "è’",
    "è™–": "å‘¼",
    "å´¼": "æ˜¯",
    "æ·´": "å‘¼",
    "ç·": "ç¡®",
    "åš±": "ç³»",
    "é½¾": "äºš",
    "æ ”": "æ°”",
    "ç½‡": "å°Š",
    "ç±¿": "å¯¸",
    "æ·": "å‘¨",
    "äº¯": "æƒ³",
    "é‰": "çˆ±",
    "é„©": "å¯»",
    "è¬†": "å–„",
    "æ—": "ç…§",
    "çº†": "å¢¨",
    "ç‰": "çº ",
    "æ‰µ": "é±¼",
    "é¢•": "å½±",
    "åƒ©": "ç°",
    "å½½": "ä½",
    "éŠ": "ä¿®",
    "æ¿": "è‚",
    "å‚": "é¢ ",
    "è¹½": "äº†",
    "æ€º": "æ°¸",
    "æ¹": "çœŸ",
    "å•º": "å”",
    "è¿": "è¾¾",
    "çµ": "è¯—",
    "ç——": "å¦¹",
    "æ§¤": "è¿",
    "ç‘ƒ": "æ˜¥",
    "ç†": "è·¯",
    "å´¿": "æ¶",
    "ç©¡": "è‰²",
    "éŸ‚": "é¢¤",
    "å®­": "ç¾¤",
    "å˜¥": "å¡",
    "ç…£": "æŸ”",
    "å¿ˆ": "äºº",
    "è‚": "æ¿",
    "é³‹": "éªš",
    "ç¸": "ç¦",
    "ç‹”": "ä½ ",
    "ç´": "è½°",
    "ç¥’": "æ¡",
    "å´’": "æ—",
    "èƒ": "åŒª",
    "èœ": "å†¤",
    "å„™": "æ¬ ",
    "çˆ”": "è¥¿",
    "é‰¥": "æ ‘",
    "æ¾Š": "æ‘",
    "ç”¿": "ç›Ÿ",
    "éŠ¶": "æ±‚",
    "ç ½": "ç”¨",
    "é‹‚": "æ¢…",
    "é¨–": "ç‰©",
    "æŸ€": "æ¯”",
    "æ«¸": "ä¸¾",
    "é¨¶": "é‚¹",
    "è€­": "æœº",
    "éŸº": "åº”",
    "ç³±": "è‚",
    "é´µ": "æ¶ˆ",
    "æ¸˜": "æŸ”",
    "è”¾": "ç¦»",
    "å¬": "ç¯",
    "é…³": "å°",
    "æ‘œ": "çŒ",
    "é½›": "è°¢",
    "è­": "æºƒ",
    "çŒ§": "çª",
    "é« ": "æ˜†",
    "é½™": "åŒ…",
    "ç›": "è·¯",
    "ç¬—": "ä¸œ",
    "éµ¾": "æ˜†",
    "å ": "å®¢",
    "å•¿": "è›‹",
    "çŒº": "æ‘‡",
    "ç©•": "å¦¾",
    "å¨": "å‰–",
    "è©Š": "åˆ¤",
    "æ—²": "å°",
    "æ¦¶": "å”",
    "æ´¤": "å…¨",
    "è¢”": "è´º",
    "ä¸’": "ä¸‘",
    "æ‘ƒ": "åº·",
    "ç°±": "å…¶",
    "æ˜¢": "ç ´",
    "æŸ": "å¤«",
    "ç£†": "å",
    "å¡¡": "ç”°",
    "å³": "æ©",
    "æ¯š": "ç¼ ",
    "ç²": "ç±³",
    "å›¼": "èƒ",
    "æ´¦": "ç ´",
    "ç¯¯": "é—´",
    "æ¡": "èŠ‚",
    "è€": "å°†",
    "é•¸": "é•¿",
    "è¢—": "æ•",
    "å¹°": "æ˜¾",
    "çˆ“": "ç‡•",
    "é¤—": "é€Ÿ",
    "ç¹¶": "æ„",
    "å½š": "ä¼š",
    "è ": "å¹³",
    "é·Ÿ": "ç€",
    "é½": "æœº",
    "é±¯": "äº’",
    "é­™": "å ",
    "è": "å½’",
    "æ©•": "ç§°",
    "é ": "é£",
    "ä¼¡": "è½¦",
    "æ¼š": "æ¬§",
    "å¾°": "ç",
    "ç·": "èµš",
    "åª®": "å·",
    "æµ°": "ç»ƒ",
    "å¹ƒ": "ç»´",
    "èœ": "çˆ¶",
    "è¼": "å¦",
    "èœº": "å°¼",
    "æ¦": "æ›¿",
    "ç³­": "å®—",
    "ä¼·": "æ˜¼",
    "é±‡": "åº·",
    "è†¥": "æ‘",
    "äº": "é±¼",
    "å¼–": "äº’",
    "èƒŠ": "å–",
    "æ­¾": "å¢¨",
    "ç¯”": "äº‘",
    "é´†": "é•‡",
    "ç¾«": "æª",
    "å·": "è¡€",
    "ç¬": "èƒ†",
    "åŸ‘": "å“²",
    "è¡’": "ç‚«",
    "æ©´": "å­",
    "ç“": "æ•–",
    "å¦š": "å¦",
    "ç¯¢": "å„",
    "è¿µ": "åŠ¨",
    "ç©": "ç§‹",
    "è£ ": "ç¾¤",
    "æ‘µ": "è®¾",
    "é„¦": "è®¸",
    "å¯¯": "ä¿Š",
    "æªµ": "è®°",
    "é¨„": "è·¯",
    "æ¾«": "ä¸‡",
    "å“«": "æ—",
    "é¹¡": "æ",
    "å¢¤": "å¿«",
    "é»–": "ç³»",
    "æ¥‡": "æˆ–",
    "è«›": "é±¼",
    "è–‰": "ä¼š",
    "é»«": "çƒŸ",
    "è¿": "å‡†",
    "æ›": "æƒ³",
    "è€": "è€",
    "ç”": "ä¸²",
    "åƒ‚": "æ¥¼",
    "é¸§": "ä»“",
    "å‰¸": "å›¢",
    "è·…": "æ‹“",
    "é ": "å…¶",
    "ç£": "å´©",
    "ç¤µ": "åŒ",
    "çš": "çƒŸ",
    "é Š": "éœ€",
    "æ±ˆ": "é›•",
    "ç¬´": "æ„Ÿ",
    "å«¬": "é®",
    "åƒ¿": "èµ›",
    "æŸ†": "æ‹‰",
    "è”³": "æ¬ ",
    "åˆ…": "çª—",
    "éª": "äº²",
    "é¸œ": "å–",
    "å¬§": "è¿›",
    "é¢¿": "ç¿»",
    "è¨": "é‚€",
    "çºš": "ç¦»",
    "ç¶³": "å´©",
    "é¬’": "æ•",
    "ç¹·": "å†œ",
    "æŠ‚": "ç‹‚",
    "å‡¢": "çƒ¦",
    "è¦”": "å¯†",
    "ç ": "å½¬",
    "é –": "åˆ¤",
    "ç©ˆ": "æ¢…",
    "è©·": "åŒ",
    "é ²": "æŒº",
    "ç£±": "åŠ³",
    "é¡™": "æ¡‘",
    "è¾¡": "å˜",
    "è•‡": "ç‚¹",
    "æˆ±": "ç³»",
    "é»½": "å…",
    "è £": "åˆ©",
    "æš™": "æ˜¥",
    "åˆ¦": "èŠ‚",
    "çŠ›": "æ¯›",
    "é¶©": "ç‰©",
    "æœŒ": "åŸ",
    "æ¤®": "æ£®",
    "é°½": "æ±‚",
    "å¿²": "å¤ª",
    "èª‹": "è®°",
    "å«Š": "é€Ÿ",
    "ç¨­": "æ¥",
    "æ¼»": "èŠ",
    "é¤‚": "èˆ”",
    "åƒ¡": "ä¼š",
    "ç”§": "æ·±",
    "çŒ ": "ç‚¹",
    "é¬›": "è£‚",
    "ç·¡": "æ°‘",
    "è§©": "æ±‚",
    "æ™†": "å¥",
    "ç›½": "é£",
    "é«†": "åš",
    "æ¡’": "ä¸§",
    "è—¼": "å®£",
    "é—": "çœ‹",
    "ç‘´": "è§‰",
    "è±–": "å¤„",
    "çµ…": "çª˜",
    "ç·¦": "æ€",
    "éµ½": "å •",
    "è­´": "æµ…",
    "é½²": "æ›²",
    "æ¹Œ": "å‚",
    "ç¼": "å‚¬",
    "æ™¬": "æœ€",
    "ç­¨": "å«",
    "ç—Ÿ": "æ¶ˆ",
    "è“¨": "æ¡",
    "ç¶": "åœ°",
    "ç": "å…«",
    "å›¶": "å›½",
    "ç§—": "ç‰",
    "æ«¹": "æ¶ˆ",
    "ç¡Š": "ä¼Ÿ",
    "ç®’": "è‚˜",
    "æ’¯": "ç€",
    "ç‘‡": "å¸¦",
    "åªŸ": "è°¢",
    "å¿¼": "åº·",
    "æ±¼": "ç‰›",
    "å¥º": "ä¹…",
    "ç½Œ": "åº”",
    "è¬³": "æ¬§",
    "æ©¿": "å°†",
    "ä¼‡": "æ„",
    "é £": "å®¡",
    "è®": "é±¼",
    "è˜„": "å…¶",
    "ç«•": "åˆ†",
    "è·±": "è‡³",
    "å¤‘": "è°¢",
    "æ¦": "ä¹…",
    "é§‘": "åŠª",
    "è–¡": "é¡¶",
    "æ´´": "å¹³",
    "åƒ·": "å¤œ",
    "ç…": "è§’",
    "æŠ": "è¢„",
    "åª£": "æŸ“",
    "åª¼": "è¢„",
    "éº": "æ—",
    "ç¾—": "æª",
    "æ©†": "äº”",
    "æŸ": "å…»",
    "ç…ˆ": "å¥‰",
    "åŠ": "å“‘",
    "æ˜š": "è‚¾",
    "è™‚": "è·¯",
    "ç¤ ": "ç“·",
    "ç²·": "å±€",
    "é¬°": "ç‰",
    "ç—": "ä¼Ÿ",
    "éˆ°": "æ˜¯",
    "ç©½": "äº•",
    "è–": "å®£",
    "è©Œ": "å¹²",
    "é": "è§‰",
    "éŸ™": "ä¼Ÿ",
    "è–±": "å¯¹",
    "ä¾»": "é€€",
    "ç‹¤": "æ",
    "å¸¤": "å¦‚",
    "æ®«": "å•",
    "ç’": "ä½",
    "æƒ„": "é€†",
    "é¼«": "çŸ³",
    "æƒ": "è¥¿",
    "è¡¯": "åˆ†",
    "ç´": "è°‹",
    "è—¯": "ä½",
    "è": "ç¯",
    "å—‡": "è‰²",
    "ç  ": "å±…",
    "æ’": "æ ‘",
    "é­–": "éœ€",
    "ç¯": "å§‘",
    "éŠ›": "å…ˆ",
    "ç‰": "æ‰£",
    "é«³": "æ¯›",
    "ç¼": "è¢«",
    "å‘ ": "å–·",
    "æ¥§": "å…»",
    "ç¢»": "ç¡®",
    "ç¢ ": "å®š",
    "é›": "åº¸",
    "çœ¿": "å¢¨",
    "å¹": "æŸ”",
    "è¦¡": "ä¹ ",
    "çšœ": "å·",
    "ç¯—": "æœˆ",
    "æ±®": "å›",
    "çŒ”": "å®—",
    "è†…": "å”",
    "å•™": "å­",
    "å„¥": "ç‰",
    "é¥ˆ": "ä¿®",
    "ä¾±": "æˆ",
    "çªµ": "æ‰",
    "å“·": "è£‚",
    "å®§": "å®œ",
    "é¯’": "æ°¸",
    "è‡": "åŠ©",
    "èƒ¾": "å­—",
    "éŒ©": "æ˜Œ",
    "è©": "ä¸»",
    "é«¢": "æ•Œ",
    "ç½«": "æŒ‚",
    "é¨”": "æ ¼",
    "ç£": "æ•–",
    "ä¾¼": "åš",
    "çŒ˜": "è‡³",
    "è¿¯": "æ¡ƒ",
    "æ¼™": "å›¢",
    "ç©": "ç‡•",
    "å›ˆ": "æ„",
    "å³³": "ç”±",
    "æ±µ": "å¹²",
    "ç¶ˆ": "æ",
    "é‰‚": "ä½¿",
    "è††": "é€Ÿ",
    "è‚¨": "èƒ–",
    "è¡§": "é±¼",
    "ç¶Ÿ": "åˆ©",
    "ä¼£": "æ¬ ",
    "è‡œ": "åŒ",
    "å‘": "ç‰™",
    "çª¡": "ç€",
    "ç´": "å·®",
    "ç": "äº”",
    "ç¦": "æ˜¾",
    "å•š": "æ¯”",
    "ç‚": "åŸ",
    "ç£€": "é¢",
    "æª¨": "èˆ",
    "æ¿˜": "å®",
    "æ­¨": "ä¸",
    "åŸ»": "å‡†",
    "å¤»": "è¯",
    "ç¢­": "è¡",
    "ç°¤": "å¸¦",
    "çƒ¼": "ç»­",
    "æ£": "å¾·",
    "æŸ‚": "å®œ",
    "ç ": "å€¦",
    "é½·": "å§",
    "åˆµ": "äºŒ",
    "å±œ": "æ›¿",
    "éœ—": "é›¶",
    "é®Ÿ": "æŒ‰",
    "ç•©": "ä¸€",
    "æªª": "åˆ©",
    "é‹": "å¤¹",
    "å¦€": "å‡ ",
    "åƒœ": "ç§°",
    "æ©ª": "æŸ“",
    "è›¶": "å€Ÿ",
    "è™°": "ä¸",
    "æ¼´": "å£®",
    "å¬‘": "æ„",
    "å«•": "æ„",
    "è”": "è®¾",
    "çŸ»": "å“­",
    "è§®": "è·¯",
    "å“´": "äº®",
    "ç’": "èµ",
    "æŒ•": "å ",
    "å‚": "æœ",
    "æ£ƒ": "ç¦»",
    "ç ¡": "ç‰",
    "æ ’": "å¯»",
    "æº": "å¢¨",
    "æ›": "å›",
    "é´•": "é©®",
    "ç„": "ç‚«",
    "æ™Š": "è‡³",
    "çŸ": "ç»´",
    "ç®": "å…‰",
    "è¨¸": "å’Œ",
    "æ´": "æ€»",
    "ç°¥": "æ•™",
    "åµ•": "å®—",
    "å¯—": "å‡",
    "æˆ¨": "æ­Œ",
    "è¡ ": "å‡†",
    "æš†": "å®œ",
    "è«¤": "æ¶",
    "æ„": "å…",
    "ç­ƒ": "å› ",
    "å¡‹": "è¥",
    "æ½§": "çœŸ",
    "æœ": "æœº",
    "é¨³": "æ¯’",
    "æ¡¸": "è¥¿",
    "é†™": "æœ",
    "çœ¤": "é€†",
    "çŸ": "å²",
    "ç¯˜": "æŠ½",
    "é—": "ç”°",
    "æ¼¼": "è„†",
    "ä¾°": "çª˜",
    "ä»¦": "åµ",
    "é­¡": "æ‰",
    "è": "æ¡¥",
    "å‘‡": "èµ·",
    "è‚§": "åŸ¹",
    "æ": "æ‹–",
    "é°¤": "è¯—",
    "è­Œ": "é¢",
    "å©": "çˆ¶",
    "æª°": "çœ ",
    "é´‚": "è§‰",
    "æ£´": "ç¦",
    "é±": "å¤œ",
    "å²¨": "åŒº",
    "ç„«": "å¼±",
    "å‰": "è¢",
    "è †": "æ‹†",
    "è¼ˆ": "å‘¨",
    "æ¢‚": "æ±‚",
    "ä¿½": "å¿ƒ",
    "éƒ³": "å°¼",
    "éˆ‘": "æ¿",
    "è™": "å¥",
    "å„ˆ": "å¿«",
    "è«‚": "é“²",
    "è©›": "ç»„",
    "é‘": "å®—",
    "è¡µ": "æ„",
    "æ·›": "è¿™",
    "é˜¸": "æ¶",
    "æ˜®": "å®—",
    "æ": "æˆ",
    "é‰¿": "å®¶",
    "ç“«": "ç›†",
    "å¾": "æˆ",
    "éƒ‰": "è¡Œ",
    "é£‚": "å…­",
    "å†": "æ—",
    "è¹": "æ‰",
    "å‡": "å› ",
    "å³¸": "æˆ",
    "è™µ": "è›‡",
    "ç¶£": "çŠ¬",
    "æ‰¢": "å¤",
    "è©–": "å¿…",
    "è±—": "ç°",
    "éŸ": "å‹¾",
    "è—‹": "æ‰",
    "æ¨†": "ç¦»",
    "é——": "å…³",
    "ç“ƒ": "é›·",
    "è„": "æŒ‘",
    "ç˜": "æ„",
    "èŸŸ": "èŠ",
    "åšš": "é“¶",
    "è†Œ": "æ",
    "ç³´": "æ•Œ",
    "ç‡¨": "è¥¿",
    "éµ·": "å†¤",
    "é§": "å¼ ",
    "è¡": "å–",
    "æ—": "å¿™",
    "å´³": "é±¼",
    "æ„’": "å¼€",
    "ç¯›": "å¼±",
    "æ": "å­",
    "æ¬…": "ä¸¾",
    "é·": "æ„",
    "åœ‘": "æ™®",
    "é—¥": "è¸",
    "çŒ»": "å­™",
    "ç¹¦": "æŠ¢",
    "éµ": "æ— ",
    "é ³": "ç§°",
    "æ‚ˆ": "å€Ÿ",
    "è¬‘": "æ´—",
    "æ„¢": "å¡",
    "æ—µ": "é“²",
    "ç˜": "ç¾Š",
    "æ¢º": "ä¸‹",
    "é¡ ": "æ¼‚",
    "çœƒ": "äº‘",
    "æ¦£": "æ‘‡",
    "ç‚›": "å…‰",
    "å¸": "æ—",
    "å‹": "ç¬¨",
    "æŒ": "åŠ¨",
    "æ–¶": "å¤„",
    "æ¦": "å±•",
    "å¢£": "è‘¡",
    "æ¤ˆ": "å±€",
    "å´": "è·¯",
    "éƒš": "æ— ",
    "é»¿": "åŸ",
    "åœ¥": "è·¯",
    "çº—": "å˜´",
    "ç§®": "æ´»",
    "ç•": "å€Ÿ",
    "å™§": "è°¢",
    "æ”ƒ": "æ“¦",
    "é›˜": "å§",
    "æ³": "äº›",
    "ç«‰": "å„",
    "èƒ»": "æ¨ª",
    "æ‚‘": "ä¸",
    "ç”¹": "å¹³",
    "è·“": "åŠ©",
    "é¤’": "å†…",
    "ç¹": "å²",
    "ç¸": "æ— ",
    "æ¥Ÿ": "åœ",
    "æ ¯": "æœ‰",
    "æ§–": "é©®",
    "éº·": "é£",
    "ç•": "ç¦»",
    "é…“": "çœ¼",
    "é°Œ": "ç§‹",
    "èƒµ": "åƒ",
    "è": "é”™",
    "è¿": "è¥",
    "ç†°": "æ¬§",
    "è—³": "æ",
    "ç¸—": "å‚¬",
    "é„": "è¢",
    "å–": "æœº",
    "æ”¨": "æŒ–",
    "æ˜¹": "çŸ®",
    "æ®": "æ±‚",
    "ç“‚": "æ¦‚",
    "ç–": "æ„£",
    "é††": "å±•",
    "å‚†": "é™¢",
    "è‡": "å®¾",
    "ç°ˆ": "å¹³",
    "å‡˜": "æ€",
    "è†™": "è®²",
    "çŠ†": "ç›´",
    "æ¾¼": "å±",
    "èˆ‹": "ä¿¡",
    "ç´¤": "ä¹…",
    "èˆ": "ä¾ ",
    "é²": "èºº",
    "æ……": "éªš",
    "ç¶¶": "æœ",
    "é‰¦": "ç",
    "å½„": "å£",
    "ç¾‰": "é¸¾",
    "è‘€": "æ‰©",
    "æ®—": "å¤œ",
    "è¬": "ç“®",
    "å„ƒ": "ç¼ ",
    "åŸ": "ä½",
    "è±˜": "å",
    "é•µ": "ç¼ ",
    "ç…†": "è™¾",
    "ç®®": "å®£",
    "è¶†": "ä½",
    "æš°": "èª",
    "æµ»": "çª˜",
    "ç¹¯": "ç¯",
    "ç”‚": "ç¼–",
    "æ”§": "é¢ ",
    "é«‡": "æ¶ˆ",
    "ç­°": "åš",
    "å»¹": "æ‹",
    "ç®¥": "è·›",
    "éµ©": "ç¦",
    "æˆ": "è½°",
    "æ‡ ": "å…¶",
    "å‡£": "çƒ¦",
    "ç¸": "å¦¹",
    "è¢¥": "æ‹–",
    "è¼™": "å“²",
    "è’›": "ç¼º",
    "æ¢˜": "å‡",
    "æ¡µ": "ç‘",
    "ç£": "å˜",
    "ä¹²": "å§¿",
    "éµ": "æ•™",
    "èš«": "æŠ¥",
    "è§¶": "è‡³",
    "ç’®": "å¦",
    "ç«¼": "æœ‹",
    "å“¹": "ç¦",
    "ç¦£": "çˆ¶",
    "æ¢„": "æœ‰",
    "é®’": "çˆ¶",
    "å­»": "å¥¶",
    "è³‚": "è·¯",
    "ç¯°": "ä¸",
    "å¶°": "è°¢",
    "ç·‰": "ä¸¤",
    "ç³’": "è¢«",
    "è¤»": "è°¢",
    "é˜“": "ä¼š",
    "é£œ": "ç¿»",
    "ç—Œ": "é€š",
    "æœ˜": "å˜´",
    "æ…²": "ç’",
    "æº°": "åŸƒ",
    "ç‘": "é”",
    "çœ£": "å ",
    "éš©": "å¥¥",
    "æ»­": "å¿…",
    "çš§": "çˆ±",
    "æ·¿": "åš",
    "é˜°": "çš®",
    "é€ˆ": "çª˜",
    "ä¹·": "æ²™",
    "å™ˆ": "ä¿ƒ",
    "æ¯¶": "ä¸‰",
    "å²¦": "åˆ©",
    "å“¸": "è™½",
    "ç ±": "é›¶",
    "å¡œ": "ç§",
    "å–": "ç‰",
    "æ”": "æ€»",
    "æ±ƒ": "å½¬",
    "å¤—": "é™¢",
    "è†´": "å‘¼",
    "å‚‹": "è®²",
    "é…": "æ³•",
    "ä¾´": "ä¸‘",
    "åºˆ": "ç´",
    "ç§±": "åŒ",
    "é¬‹": "å‡",
    "æ": "è§’",
    "æ§®": "æ£®",
    "è‹": "èœ¡",
    "é¶": "ç›˜",
    "æ…¿": "å¹³",
    "ç«›": "é›¶",
    "èŠ¢": "äºº",
    "å±§": "è°¢",
    "è¢š": "æ³¢",
    "å¬€": "å½’",
    "ç’“": "ç§€",
    "å¡¼": "ä¸“",
    "è¥": "ä¸¾",
    "éŸ†": "å‰",
    "å´¬": "ä¸œ",
    "æ—‘": "ä»¥",
    "æ•¨": "å·",
    "ç‹¦": "å±±",
    "ç™": "é—²",
    "è’µ": "ä¹ ",
    "ç½“": "åˆš",
    "ç«": "å´©",
    "è»¿": "å¹³",
    "é± ": "å¿«",
    "å‚Œ": "éª‚",
    "ç¹": "è§",
    "è›": "é›•",
    "å¬": "æ‡‚",
    "å§¶": "æ¶",
    "è«‰": "ä¼Ÿ",
    "è€": "ä¸œ",
    "è†": "çš®",
    "è‘…": "ç§Ÿ",
    "åœ«": "ç‰",
    "æ– ": "æ•™",
    "ç¤‹": "åˆ™",
    "ç…‘": "ä¸»",
    "å«": "æ•–",
    "è€‡": "ç‹—",
    "æ”±": "é¬¼",
    "è„": "æ€",
    "ç®·": "å®œ",
    "é¹": "é‚£",
    "æ¨Œ": "çŒ",
    "è’«": "æ“",
    "ç¸€": "ä¾ ",
    "é¡š": "é¢ ",
    "é¢¸": "æ€",
    "å›Œ": "è‹",
    "ç£ˆ": "ä¼Ÿ",
    "å¢²": "æœ¨",
    "å¦Ÿ": "ç‡•",
    "è˜¹": "æ€€",
    "ç†¿": "é»„",
    "å½": "å°",
    "å­Œ": "é¸¾",
    "æ¤¯": "æœµ",
    "è©‚": "çˆ¶",
    "çŒŒ": "å°",
    "æ–": "å®Œ",
    "å°­": "æ‘‡",
    "äº—": "å²",
    "è™³": "è§‰",
    "çª": "é—´",
    "ç°“": "é›•",
    "çœ": "ç¾Š",
    "å®": "æ",
    "å¬": "å°¿",
    "é¹": "æœ",
    "æ¹¦": "ç”Ÿ",
    "é‡¤": "å–„",
    "èš‘": "å…¶",
    "éˆ¼": "åš",
    "æµ‚": "æ„",
    "å†“": "å¤Ÿ",
    "æšƒ": "é£",
    "æ—Ÿ": "é±¼",
    "æ¡½": "ç¨³",
    "é¶²": "ç“®",
    "æ¬‹": "å–",
    "ç§¡": "åš",
    "æ‡": "å",
    "å­": "ç©·",
    "é¤ ": "é¥¼",
    "å¼¨": "è¶…",
    "ä½–": "å¿…",
    "æŸ­": "å…«",
    "å “": "æŒ‰",
    "æ‹»": "ç°",
    "é¹": "æŠ¢",
    "æ¼„": "ç‰™",
    "æŒ": "æŒ‘",
    "è©": "è¦",
    "èº¹": "å±€",
    "éº€": "ä¼˜",
    "ç¦’": "æ˜¾",
    "ç£“": "å †",
    "é¬¿": "å…¶",
    "ç€¸": "é—´",
    "é¹²": "ç›Ÿ",
    "å´¶": "é£",
    "é½": "è¾¾",
    "é…«": "ç»°",
    "è¹”": "èµ",
    "ç¢¸": "é£",
    "çŒ": "æ±‚",
    "é¡": "æ‹‰",
    "å–¿": "é€ ",
    "æ°": "å†²",
    "è©¶": "æ˜¼",
    "å‡´": "å¹³",
    "è„º": "ç¿ ",
    "ç¸­": "ç¦»",
    "çµš": "è€•",
    "çŒ¯": "å›¢",
    "è¹£": "ç›˜",
    "è¶": "æŒ",
    "æš²": "å¼ ",
    "ç˜“": "æ¢",
    "é˜": "åº†",
    "æ˜": "å®£",
    "æ·¶": "æ¥",
    "å¥µ": "é¡¶",
    "èª": "æµª",
    "ç‰‚": "è„",
    "æª®": "æ¡ƒ",
    "æŸ¶": "å››",
    "æ¬¦": "å‰",
    "è€¹": "ç´",
    "åµ“": "é¢œ",
    "æ“¸": "è£‚",
    "å¡³": "æœ‹",
    "æ™": "åŒ",
    "é‹—": "å®£",
    "èŒ¢": "è£‚",
    "è”‚": "é›·",
    "çŸª": "å‘¨",
    "ç±": "å¤œ",
    "æ”™": "æ€",
    "è¿ ": "æ’¤",
    "å«": "ç‹¼",
    "æ‘‹": "è¨",
    "è‘´": "çœŸ",
    "èª„": "å’",
    "é³½": "é—´",
    "æ¢’": "å«",
    "ç©„": "è®°",
    "åµ–": "èŒ¶",
    "é—¬": "æ±‰",
    "æ¢”": "çŸ¥",
    "é€Œ": "ä¼˜",
    "é†©": "ç³Ÿ",
    "é¨­": "è‡³",
    "å¾–": "ä»",
    "è˜»": "è®°",
    "ç¡¸": "è™",
    "ç†": "è“",
    "ç­ª": "ä¾ ",
    "çª‡": "è–„",
    "æ¬—": "è“",
    "æ¡¹": "ç‹¼",
    "é¥": "æ„",
    "çš¾": "æ¯’",
    "å¡¯": "å…­",
    "ç·›": "è½¯",
    "åª­": "éœ€",
    "ç§": "æ¶",
    "é¯µ": "æ·±",
    "å¢": "æŠ¢",
    "é«¤": "ä¿®",
    "é¾": "æ—",
    "ç¦Ÿ": "å”",
    "æºˆ": "ç»´",
    "æ ": "æ°¸",
    "æŒ‹": "é•‡",
    "å·µ": "çŸ¥",
    "è†¶": "æ¶¦",
    "è­®": "è¯",
    "ç°”": "ç¼©",
    "å€²": "ä¸œ",
    "ç„": "å",
    "å©‘": "ç‘",
    "å—¹": "è¿",
    "æ¨’": "å¯†",
    "æ©‘": "è€",
    "æ” ": "è¿·",
    "é´ ": "è›‹",
    "è¶": "é“¶",
    "ä¸¯": "å€Ÿ",
    "å­¨": "è½¬",
    "è˜¶": "ä½",
    "é´£": "å§‘",
    "ç¿": "å‹º",
    "å®·": "å®¡",
    "é¼¢": "åŸ",
    "å‰ ": "æƒ…",
    "å©¤": "æŠ½",
    "è—": "è¿›",
    "éšƒ": "æ ‘",
    "è²": "æ¯”",
    "éº": "å¼ ",
    "è¥": "è„¸",
    "è¼–": "å‘¨",
    "æ—": "ç²¾",
    "é®": "å·",
    "è¨‰": "é¥­",
    "éˆœ": "çº¢",
    "ç³·": "çƒ‚",
    "æƒ™": "ç»°",
    "å½¾": "é›¶",
    "è¦˜": "æ€",
    "ä¼†": "ç‰©",
    "è–¸": "æ¼‚",
    "è¾¬": "ç­",
    "èª¶": "å²",
    "æ‹µ": "å­˜",
    "è§µ": "å·¥",
    "éŠ©": "ä¸¢",
    "é¶¥": "æ¢…",
    "è­©": "ä¸€",
    "ç¸": "æ˜¼",
    "ç«¢": "å››",
    "é¶": "ç¯",
    "éŸ¨": "ç¦",
    "æˆ": "å›¾",
    "é±š": "æ´—",
    "è¾ ": "æœ€",
    "å¥©": "è¿",
    "è—": "æ„",
    "é£": "æ ‘",
    "è®‰": "å®œ",
    "æ€—": "è´´",
    "çƒ†": "æ¨ª",
    "å™‡": "åºŠ",
    "æ“£": "å¯¼",
    "å¾›": "è®°",
    "é»¦": "æœˆ",
    "å™²": "å¿«",
    "å¦›": "åƒ",
    "éŠ•": "é“",
    "æ•¿": "è§’",
    "ç˜": "å›¾",
    "é¦œ": "ä½ ",
    "é§‚": "å®",
    "è£": "ç¡",
    "ç³": "ç¦»",
    "è˜¥": "æœˆ",
    "åŠ™": "ç¦»",
    "å­": "ä¿¡",
    "é¾": "å·¥",
    "èŸ‡": "éº»",
    "é–—": "è±†",
    "æ·": "æ§",
    "é¬®": "çº ",
    "æ¼º": "åŒ",
    "é‰": "çœŸ",
    "å•¹": "å±€",
    "å«Ÿ": "é€†",
    "é¢­": "å±•",
    "ç¾‚": "å€¦",
    "é€¨": "æ¥",
    "é¬": "å…¶",
    "é‡": "å‡¯",
    "ç”›": "ç”°",
    "åƒ": "ç¼ ",
    "é£Š": "æ ‡",
    "å¨Š": "ç°",
    "èŸ¢": "æ´—",
    "ç”–": "åº”",
    "é¥Ÿ": "æƒ³",
    "è“": "å®œ",
    "ç†–": "ç‡•",
    "æ™": "æ€’",
    "ç¿†": "ç¿ ",
    "é‹©": "å¿™",
    "æ†“": "ä¼š",
    "æŸ": "æ˜¾",
    "æ–µ": "ç€",
    "è•¬": "æ€",
    "è†–": "æ—",
    "ç¤­": "ç¡®",
    "è¿‰": "ä¸ƒ",
    "ç ˆ": "é¥¿",
    "å’¼": "é”…",
    "æˆ©": "å‡",
    "ç’": "é£",
    "æ…": "è¿",
    "ç©œ": "ç§",
    "é¦Œ": "å¤œ",
    "æ©’": "äº‘",
    "æŒœ": "äºš",
    "è ™": "è´«",
    "é¦º": "è¨",
    "çº‡": "æ³ª",
    "å¤˜": "å¸½",
    "ä½": "ä»¥",
    "æ£·": "é‚¹",
    "å™³": "é›¨",
    "æ„Š": "å¿…",
    "ç‚": "ç€",
    "æ¥²": "å¾®",
    "ç™°": "åº¸",
    "ç«": "çˆ¶",
    "æ‚": "å¦¾",
    "æ–˜": "ç”Ÿ",
    "èº‚": "è¾¾",
    "æ¸": "æ±‚",
    "æ˜‘": "å¯",
    "é„¾": "ä¼˜",
    "é©Œ": "é€Ÿ",
    "èº§": "æ´—",
    "é¥¤": "å®š",
    "é½„": "æ‰",
    "å¶®": "æ˜¾",
    "æ…": "è€",
    "æ½£": "æ•",
    "å½‘": "è®°",
    "å®Š": "çª",
    "ä¿´": "è§",
    "é¥¾": "è±†",
    "é¯®": "å®—",
    "è¾¢": "èœ¡",
    "åªŒ": "æ",
    "å¸„": "ä¸",
    "é": "è¢«",
    "è­": "é›¨",
    "è¥ƒ": "åŒ…",
    "çµ": "è¡Œ",
    "å³": "åŒ",
    "è«": "éœ€",
    "å™ƒ": "ç¿»",
    "æ¢€": "é€Ÿ",
    "ç£Œ": "ç”°",
    "é¯ˆ": "æ¡",
    "ç•®": "æ¯",
    "æ‰": "æŒ‡",
    "æ«": "å››",
    "å€›": "ä¸ƒ",
    "çµ ": "æ”¹",
    "é±": "è§",
    "åˆ£": "ä¸­",
    "å›‡": "åˆ©",
    "åƒ ": "æ³¢",
    "è¦œ": "è·³",
    "å’®": "æ˜¼",
    "ç‰°": "å³",
    "æ…™": "æ®‹",
    "æ§€": "æ",
    "è¿’": "è¡Œ",
    "é¼˜": "å†¤",
    "é¡£": "ä¿ƒ",
    "ç˜”": "è£¤",
    "å‰®": "å¯¡",
    "è¡¶": "é‡",
    "èµ—": "å¥‰",
    "æŒ­": "æ¢—",
    "ç¦": "è®°",
    "é»·": "æ¯’",
    "å¹‘": "ç°",
    "æ–¸": "ä¸»",
    "è²†": "ç¯",
    "ç“": "æ•™",
    "å•›": "ç¿ ",
    "è¨†": "æ•™",
    "é±Š": "ç‰",
    "ç¢ˆ": "æ°‘",
    "å’·": "æ¡ƒ",
    "ç´•": "æ‰¹",
    "ç¬‡": "ç®—",
    "è—‚": "ä»",
    "åœ”": "äºš",
    "è¸¨": "å®—",
    "è‘¹": "è¯—",
    "é±¬": "å¦‚",
    "æˆ…": "æ ",
    "è‰»": "ä¹",
    "æ†": "å¯¹",
    "è§": "ç°",
    "æ³": "å¥”",
    "ç‡€": "é“²",
    "æ¥»": "é»„",
    "ç…¾": "æ©",
    "åƒ”": "å°Š",
    "è¥­": "é‹",
    "å°·": "å¹²",
    "æ·½": "æŒ‡",
    "è¨ƒ": "çˆ¶",
    "ç¢¨": "ä½",
    "æ¤·": "é—´",
    "åµ": "å®—",
    "æµ€": "åŒº",
    "å¹š": "å¸®",
    "é²„": "å’Œ",
    "è¾Œ": "è‰¯",
    "ç­œ": "å½“",
    "è‹¸": "å‘¼",
    "æ»«": "ä¿®",
    "å½¸": "ä¸­",
    "å¦·": "ç›´",
    "ç—“": "èµ¤",
    "ç¡°": "æ²™",
    "é¤¿": "æœ",
    "å¨¬": "äº”",
    "é‹®": "æˆ",
    "æ™±": "é—ª",
    "ç§Œ": "ç§‹",
    "ç†›": "æ ‡",
    "ç®¤": "æ—",
    "èƒ¹": "è€Œ",
    "å£–": "è½¯",
    "éœ¥": "æ¢¦",
    "æ®¸": "åº†",
    "ç¡¥": "å¿™",
    "æƒƒ": "æ»š",
    "å½…": "å‡",
    "ç©Š": "è®°",
    "å¹˜": "åˆ™",
    "è†§": "åŒ",
    "æ„¾": "å¼€",
    "å”•": "é€ ",
    "é­½": "å«",
    "è•": "æ€§",
    "é†": "é¥¼",
    "åª¬": "å®",
    "è‹š": "ç”¨",
    "æ¤Œ": "æª",
    "è»¨": "é›¶",
    "æ¨•": "é€Ÿ",
    "å±´": "åˆ©",
    "ç ": "æ‡’",
    "æ‡": "æˆ–",
    "å©˜": "å…¨",
    "å£‚": "ç”µ",
    "ç¦¡": "éª‚",
    "å‹„": "æ•",
    "æ¾¨": "æ˜¯",
    "æ¼": "å›½",
    "å° ": "æ˜¾",
    "å²¼": "å¹³",
    "éˆ«": "ç´",
    "è±µ": "å®—",
    "çº‹": "ä¼˜",
    "æ€": "æ„",
    "æ³œ": "çŸ¥",
    "é€¥": "å›",
    "æ¹µ": "æœ‰",
    "é›®": "æœ¨",
    "é£‡": "æ ‡",
    "é¬": "é’±",
    "ç±§": "å–",
    "çŒ³": "å®¶",
    "é¹¥": "ä¸€",
    "æº¹": "æ‰€",
    "æ­": "ç ",
    "è³–": "èˆ",
    "çª‚": "åŠ³",
    "è­•": "æ— ",
    "å¦¦": "é£",
    "æµ¹": "å®¶",
    "ç¹®": "å°†",
    "é·˜": "èµ¤",
    "åˆœ": "ç¦",
    "åš ": "åˆ˜",
    "æ¶¥": "å“¼",
    "ç¸¡": "åœ¨",
    "é²‰": "ç”±",
    "ç¬·": "å¸½",
    "éŠª": "æœ‰",
    "é·š": "å…­",
    "è˜": "ä»¥",
    "å™": "æ",
    "ä½Œ": "æ­¤",
    "çœ²": "å‘¢",
    "åŸ¿": "å°¼",
    "ä¿¹": "äºš",
    "ç¾": "å‡",
    "å¶´": "å¥¥",
    "è™‹": "é—¨",
    "ç¾": "å€Ÿ",
    "é†": "è·¯",
    "ç’’": "ç¯",
    "è•¼": "å››",
    "æ²": "äº’",
    "å”¡": "ä¸¤",
    "é¤³": "å”",
    "æ¼˜": "çº¯",
    "é¶»": "å¤",
    "æ®§": "å°±",
    "ç²“": "å¹²",
    "å„¤": "æŠ¥",
    "æŸ§": "å§‘",
    "å¸ª": "çœŸ",
    "å™Ÿ": "ç¡¬",
    "ä¼­": "é—²",
    "é¹”": "é€Ÿ",
    "åœ¤": "è‘¡",
    "èµ’": "å‘¨",
    "é·”": "æ•–",
    "ç™‡": "é—²",
    "é²¿": "é•¿",
    "ç’µ": "é±¼",
    "æ‚™": "å“¼",
    "æ³ˆ": "ä¸­",
    "è»‘": "å¸¦",
    "å¼š": "æ¨",
    "æ…": "é¢œ",
    "ç„": "ç»",
    "çƒ®": "è£‚",
    "ç½¶": "æŸ³",
    "è«¢": "æ··",
    "éº›": "è¿·",
    "è¾ª": "é´",
    "è¿Œ": "å…”",
    "è¢™": "æ€•",
    "èº“": "è‡³",
    "è«ˆ": "ç¼€",
    "è¯": "åŸ",
    "ç¶¼": "å¿…",
    "æ´‚": "å¤œ",
    "è¤—": "çœ¼",
    "ç¹ˆ": "æŠ¢",
    "çœ§": "åµ",
    "æ¸‚": "é—®",
    "ç¿§": "å®£",
    "é‰°": "æ€",
    "ç©¬": "çŸ¿",
    "é¸": "ç¦»",
    "æµ±": "çº¯",
    "å”": "å› ",
    "æ„–": "é™ˆ",
    "é¨¨": "é©®",
    "èœ¨": "å ",
    "å§°": "å›",
    "å›œ": "æ‚¨",
    "æŠ¶": "èµ¤",
    "è¤³": "è¿",
    "éª­": "å¹²",
    "éŒ‡": "å‰–",
    "æ¶†": "æ±‰",
    "å‘Ÿ": "å·",
    "é½": "æœº",
    "å³•": "çŸ³",
    "èµ": "çª",
    "å §": "è½¯",
    "è’ª": "ç ´",
    "ç¨": "å¹²",
    "è­¶": "è¸",
    "é±“": "å–„",
    "è—§": "æ¢",
    "å‚´": "é›¨",
    "è¢Œ": "æŠ¥",
    "èº¥": "çªœ",
    "å”‹": "çª",
    "å”­": "æ°”",
    "é†": "è¿",
    "æœ¾": "æˆ",
    "è»ƒ": "æœµ",
    "é¾‚": "é“¶",
    "è–¶": "ä¹°",
    "é´½": "å¦‚",
    "éµ¯": "æ¯",
    "çŒ": "æ…¢",
    "å±©": "ç»",
    "å¨¯": "é±¼",
    "å¶¢": "æ‘‡",
    "éƒ”": "é¢œ",
    "æ¦¯": "çŸ³",
    "è†µ": "ç¿ ",
    "è—…": "ç½š",
    "è©µ": "æ·±",
    "ç·": "ç‰",
    "ä¹»": "é±¼",
    "é·–": "ä¸€",
    "è¶«": "æ¡¥",
    "ä¿€": "è…¿",
    "è²º": "çŸ¿",
    "è£Œ": "å¤¹",
    "çˆ": "çƒ‚",
    "ç²©": "æ",
    "éŸˆ": "è¢œ",
    "åŠ˜": "é­”",
    "è¼¹": "çˆ¶",
    "å–£": "è®¸",
    "èŒ·": "ç½š",
    "è™Œ": "æ†‹",
    "ç´Œ": "æ±‚",
    "å¡¤": "ç†",
    "é°¼": "ä¹ ",
    "è€¼": "å•",
    "æ®™": "æ˜",
    "ç§”": "ç²¾",
    "æ“": "è¥¿",
    "è…·": "å¿…",
    "é§ƒ": "è§‰",
    "åœ": "é“¶",
    "å£„": "ä¹Ÿ",
    "æ¦™": "ä»–",
    "é˜›": "ç¯",
    "é†¶": "ç‡•",
    "é®¶": "å›",
    "å¯": "ç—…",
    "åŸ°": "èœ",
    "å™¦": "æœˆ",
    "ç¬": "åŸ",
    "åš»": "æ¶ˆ",
    "ç”®": "å¥‰",
    "å”¹": "è¿‚",
    "é¿": "æ„",
    "å£‹": "è¡",
    "ç­«": "è‡³",
    "é‡º": "å‰",
    "æ®œ": "å ",
    "éŠ§": "å…‰",
    "æ¡±": "é™",
    "å“¶": "ç­",
    "å³": "è€Œ",
    "ç»": "æ¢…",
    "åŒ´": "ç®—",
    "çˆ": "èŠ¦",
    "åŒ’": "è¾¾",
    "é„…": "é›¨",
    "æ¤": "å‘€",
    "èµ": "å…”",
    "å”¶": "åˆ™",
    "å³": "ç«¯",
    "æ«œ": "é«˜",
    "ç¡": "ç¡®",
    "è•£": "é¡º",
    "æ¥¥": "ç‚«",
    "è¶‰": "è§‰",
    "æƒŒ": "å†¤",
    "å†¸": "åˆ¤",
    "æŠ´": "å¤œ",
    "è¬ƒ": "æ˜Ÿ",
    "ç˜’": "æ–‡",
    "å­§": "å³",
    "å…¦": "ç‹",
    "é†³": "æ„",
    "çµ¸": "å‡",
    "é˜¤": "è‡³",
    "è·": "å“­",
    "ç°—": "åŠ©",
    "æ”½": "ç­",
    "ç±ˆ": "çœŸ",
    "è›º": "å¤¹",
    "é‡‚": "æ•™",
    "è‘²": "å…¨",
    "ç‡‘": "åŒ",
    "æ…›": "å‚¬",
    "é‡•": "äº†",
    "å†": "è¿·",
    "çºˆ": "é‹",
    "æ²°": "æ‹–",
    "å¥†": "å€¦",
    "é´«": "ç”°",
    "ç¾±": "åŸ",
    "å²‡": "æ˜‚",
    "æ„“": "è¡",
    "æ£": "ç¼ ",
    "éŸƒ": "è¾¾",
    "è¸„": "ä¸",
    "ç˜ˆ": "èµ¤",
    "éšº": "èƒ¡",
    "éƒº": "åº¸",
    "é±€": "è®°",
    "é·¦": "æ•™",
    "å€´": "ç¬¨",
    "ç–‰": "å ",
    "åŸ²": "å´©",
    "çµ“": "æŒ‚",
    "å¶": "æ¥¼",
    "å³¢": "æ",
    "æ¾‘": "å…­",
    "æ‚": "çº ",
    "æ¬€": "ç›¸",
    "æ•€": "ç ´",
    "åœ—": "å›¾",
    "åƒˆ": "ç’",
    "æ”†": "å¹´",
    "æ²‘": "å¥³",
    "ç½§": "è‚¾",
    "æª£": "å¼º",
    "ç¡": "ç°",
    "ç“”": "åº”",
    "å„®": "åˆ©",
    "è š": "å–",
    "ç’–": "å–",
    "æ±¬": "äº•",
    "ç®‰": "æ‹",
    "ç¸©": "èœ",
    "ç‘": "å‡",
    "åª¥": "å",
    "æ‘ ": "æ€»",
    "é‘Œ": "å½¬",
    "æ‰¤": "ç‰©",
    "é°‚": "è´¼",
    "è’¶": "åŸ",
    "é¨": "èµ",
    "é¸‘": "æœˆ",
    "ç£¹": "è°ˆ",
    "çŸ±": "çº¦",
    "é–º": "æ–‡",
    "å˜·": "è±ª",
    "æ³˜": "å‘¼",
    "å¢": "åŠ",
    "ç’¡": "è¿›",
    "è±": "ç›Ÿ",
    "æ™œ": "æ˜†",
    "ç—": "æ˜¯",
    "ç¼»": "å¦",
    "æ»": "è‡³",
    "é–´": "å»",
    "é¯¿": "ç¼–",
    "ç“ˆ": "ç¦»",
    "å£ƒ": "å°†",
    "å‰": "åˆ›",
    "æ™€": "æŒ‘",
    "å–¤": "é»„",
    "éŠš": "æ‘‡",
    "æ©“": "é¡º",
    "ç‚‹": "æ‰¹",
    "æ®": "å…ˆ",
    "çˆ": "å–",
    "è˜ª": "æ¢…",
    "æ¢¡": "é­‚",
    "çŸ‰": "è´«",
    "è¤¾": "è¡¨",
    "æ·”": "ç›´",
    "é‰": "åˆ©",
    "é……": "è¥¿",
    "ç¦¬": "è´µ",
    "é¢£": "æ³ª",
    "è½˜": "ç¯",
    "é´": "å¸¦",
    "ç€™": "äº²",
    "è«": "é‚¹",
    "é´³": "ç‡•",
    "é¥˜": "å ",
    "é¸Œ": "äº’",
    "é¶–": "ç§‹",
    "èš”": "å…¶",
    "ç¤‰": "å’Œ",
    "æ½–": "çˆ¬",
    "è—„": "å…¶",
    "å¨": "å¤«",
    "ç¹‘": "æ•²",
    "éŸ½": "å®‰",
    "èŒ½": "é‡",
    "é§”": "è„",
    "æ‚¢": "äº®",
    "è¶½": "æ”¾",
    "ç‰•": "çª—",
    "æ»µ": "å¯†",
    "ç¯½": "ç‰",
    "çš¡": "å·",
    "è‚": "å’Œ",
    "ç¹‚": "ç»¿",
    "ç¬°": "ç¦",
    "å’“": "ç“¦",
    "è´‹": "ç‡•",
    "çš»": "æ‰",
    "è¾": "å¼•",
    "æƒ": "å¿ƒ",
    "çŒ°": "äºš",
    "æ‡œ": "çŒ›",
    "ä¾…": "è¯¥",
    "ç…»": "å”",
    "è¥": "çº½",
    "é": "æ€",
    "å¥¾": "å…ˆ",
    "ç¦œ": "æ°¸",
    "é¼‘": "é¡¶",
    "é": "ç­”",
    "é•": "å”",
    "æ›": "æ˜Ÿ",
    "è™£": "æŠ¥",
    "ç§¹": "å¿",
    "æ¢½": "è‡³",
    "é‹¨": "é“",
    "è‘": "ç¦",
    "çœ«": "ç±³",
    "æ¬": "é›¶",
    "æŠŒ": "èƒ†",
    "æ": "èµ",
    "è”¢": "å©†",
    "å©ˆ": "é›¶",
    "æ„œ": "å¦¾",
    "å ¾": "æ˜¥",
    "ä½„": "æ†¨",
    "å‰º": "ç¦»",
    "è²¹": "èƒœ",
    "é®—": "ä¸œ",
    "é—‘": "è‚",
    "è¹•": "å¿…",
    "å²Š": "èŠ‚",
    "é‘¢": "ç»¿",
    "ç…ƒ": "é­",
    "å£š": "èŠ¦",
    "è³": "è§",
    "æ‘": "å†²",
    "é­£": "ç»­",
    "é“": "å¬",
    "åŸŠ": "åœ°",
    "é‹•": "è‡³",
    "é±£": "å ",
    "åµ§": "åˆ˜",
    "é©": "æ²™",
    "ç­“": "æœº",
    "å¼": "ç‰©",
    "åª‘": "é‡",
    "æ¦…": "æ¸©",
    "ç²†": "æ²™",
    "éŒ¤": "æœº",
    "è ˜": "èŠ‚",
    "æ¦¾": "å¤",
    "ç”": "é•¿",
    "é¨©": "å½’",
    "å¹‰": "å ",
    "å·„": "é¾™",
    "é°±": "è¿",
    "è…¯": "å›¾",
    "ç¿‹": "æ‹‰",
    "è¦Š": "æœº",
    "æ¡¾": "å›",
    "æŸŒ": "ç“·",
    "ç¤Ÿ": "ç‚®",
    "ç¢”": "äº”",
    "å€µ": "äº”",
    "æ«š": "é©´",
    "å¦Œ": "é™",
    "ä¿‰": "äº”",
    "æ¾­": "åº¸",
    "è‚‚": "å››",
    "éª™": "å¥",
    "çˆ•": "è°¢",
    "èª¹": "åŒª",
    "èƒ·": "èƒ¸",
    "ç­€": "è´µ",
    "å¤½": "å…",
    "æ¡º": "æŸ³",
    "å¦”": "å‘",
    "æ…“": "é£˜",
    "çª»": "çª—",
    "åˆ´": "å •",
    "å‰¼": "å±±",
    "è²": "å³",
    "æ¦³": "åœ",
    "ç¹Ÿ": "é“²",
    "é³¦": "ä»¥",
    "ç·„": "æ»š",
    "è™™": "ç¦",
    "å¢¡": "å–„",
    "è»": "æ¥¼",
    "åƒº": "å·§",
    "é— ": "ä¼š",
    "æ—½": "å",
    "ç°‰": "é€ ",
    "ç™": "å",
    "æŒ†": "æœµ",
    "æŒ": "å¡",
    "ç«ƒ": "é€ ",
    "å›‹": "æ‚",
    "èŸ": "å–",
    "å‚‚": "è‡³",
    "ç©Ÿ": "å²",
    "èµ": "å¿",
    "å¿´": "é’±",
    "æ¥•": "å¦¥",
    "ç€“": "æˆ",
    "æ·­": "å–",
    "è­­": "æ¯",
    "æ‘®": "æ•–",
    "ç††": "å’Œ",
    "åƒ¯": "å",
    "è“": "å”",
    "æ±’": "å¿™",
    "æ¤«": "å–„",
    "ç·": "é—´",
    "å³–": "å®‰",
    "çª": "å¿«",
    "è‰›": "æ¥¼",
    "é¤§": "ä½",
    "èª": "ç§‘",
    "ç„¿": "è€•",
    "å´": "æ¥",
    "èŸˆ": "é”…",
    "å†": "æ¢…",
    "é€´": "æˆ³",
    "é•ƒ": "å§¿",
    "å™†": "èµ",
    "è¼§": "å¹³",
    "çµ‡": "å–",
    "æ½¨": "ä»",
    "ç€¡": "é«“",
    "ç”": "è§",
    "èšš": "å…¶",
    "é¥™": "åˆ†",
    "ç™": "æŒ¡",
    "è˜¯": "è¡",
    "æ³Ÿ": "ç§°",
    "è³™": "å‘¨",
    "åƒ´": "ç°",
    "æ¸»": "çœ",
    "è—¨": "æ ‡",
    "æ‚¤": "èª",
    "éˆ¢": "æ´—",
    "ç±‚": "è¯—",
    "èš": "æœ¨",
    "é­’": "é£˜",
    "è–µ": "æ„",
    "é—": "é£",
    "æ¨…": "èª",
    "æ¦¿": "ä¸ƒ",
    "ç‘²": "æª",
    "åŒŠ": "å±…",
    "è›": "å¾®",
    "æ": "å…¥",
    "åš‚": "çƒ‚",
    "æ°»": "ä¹",
    "è¡": "å¦‚",
    "å”ˆ": "æ„",
    "æµ§": "å½±",
    "é«²": "å¿…",
    "å€": "å‘¼",
    "ç¥‘": "è‡³",
    "èŠ¿": "ä»",
    "å¼": "èŠ‚",
    "é¥": "èµ¤",
    "è¤¸": "æ—…",
    "é˜£": "æ¦‚",
    "è¼·": "è½°",
    "ä¹•": "è™",
    "æ««": "æœ±",
    "ç³": "ä¸‰",
    "åš«": "è¡¬",
    "å¸Ÿ": "æ„",
    "æ¥–": "è‡³",
    "éˆ¶": "å››",
    "ç‰ª": "ç‡•",
    "çŸ ": "åˆ™",
    "é¬º": "ä¼¤",
    "é·¯": "èŠ",
    "ç…": "é™ˆ",
    "é„‘": "å§¿",
    "æ–„": "ç¦»",
    "è¥": "æ‚",
    "çª¼": "æ‹›",
    "åœ": "å“­",
    "æµ¢": "è±†",
    "è¢‡": "ç„¶",
    "ç¡²": "ç‰",
    "çŠ˜": "éº»",
    "è£ª": "æ¡ƒ",
    "è®": "è¿",
    "å¶“": "æ³¢",
    "å‰¾": "å£",
    "åª": "èŠ‚",
    "å¾†": "è¥¿",
    "æ•œ": "è‚",
    "ç€¿": "çƒ¦",
    "éœ£": "å…",
    "é·‰": "è¸¢",
    "è‹": "è€Œ",
    "å—º": "å˜´",
    "ç¶€": "ä¹¦",
    "å›ƒ": "æ“¦",
    "é¨‚": "æ˜Ÿ",
    "èª½": "é€†",
    "éš": "æ•–",
    "å¿¦": "å¤¹",
    "æ¿¼": "è½",
    "ç—½": "å †",
    "æº ": "å’‹",
    "åŠ”": "è§",
    "å»”": "æ¥¼",
    "å„¦": "æ ‡",
    "è·²": "å¤¹",
    "è™·": "å«",
    "èš»": "ç‚¸",
    "éª´": "åˆº",
    "å¢‘": "åœ°",
    "éŸ ": "å¿…",
    "è¼´": "æ˜¥",
    "åƒ": "å–„",
    "é²ª": "å›",
    "å‰«": "å¤º",
    "è•‹": "ç‘",
    "ç½³": "æ€",
    "éŒ‹": "æœ‹",
    "é±": "æ„¤",
    "æŠˆ": "æœˆ",
    "é£": "ä¿®",
    "æ¦¬": "åŸ",
    "æ‡£": "é—·",
    "æ¾®": "ä¼š",
    "æ†ˆ": "åŒº",
    "é‹š": "æ¡",
    "åšˆ": "å¤œ",
    "æœœ": "å",
    "çŠ‰": "çº¯",
    "é¦‰": "å¤",
    "é§˜": "å°",
    "å£ª": "å¼¯",
    "ç«": "å‰",
    "è“™": "åš",
    "ä¼»": "å´©",
    "å¿": "ç»­",
    "èˆ•": "æ¢",
    "æªŸ": "å‡",
    "ç¨§": "ç³»",
    "æª‡": "æœ€",
    "è• ": "å¦‚",
    "èŒ ": "è’¿",
    "å­¼": "è‚",
    "çŸº": "å“²",
    "ç€ ": "è¥",
    "æ°Š": "å ",
    "å¨»": "ä¸œ",
    "æš¯": "å¢¨",
    "éš¡": "è¨",
    "å³": "è¡Œ",
    "æ†œ": "å •",
    "é§¹": "å¿™",
    "å´¹": "æ",
    "è¹": "æ",
    "å–´": "å¾®",
    "é³¼": "æ–‡",
    "æ£†": "ä¼¦",
    "å–•": "å…",
    "çƒ¾": "èµ¤",
    "ç¦‘": "æ— ",
    "æŒ¦": "é—²",
    "æ¾¸": "èƒ†",
    "ä¼”": "èƒ†",
    "é¯»": "èœ¡",
    "éˆ’": "è¨",
    "æ¥¶": "èŠ‚",
    "é¼ª": "ç”Ÿ",
    "è–½": "çœŸ",
    "å±¨": "å·¨",
    "ç–»": "æŒ‡",
    "è§": "èˆ”",
    "å¦": "å®³",
    "å¥…": "ç‚®",
    "æ¹¬": "è§’",
    "ä¸£": "æœ‰",
    "è»°": "è¢«",
    "å‘": "ä¿Š",
    "å¿Ÿ": "ç¨³",
    "å‹š": "æ„",
    "é½¨": "å°±",
    "çª´": "ç”°",
    "æ¼®": "åº·",
    "æ¨€": "æ•Œ",
    "é££": "å®š",
    "å¼‰": "è—",
    "ç¿": "å…­",
    "å°£": "æ±ª",
    "ç¾„": "ç…§",
    "ç²¸": "å…¶",
    "è¢º": "èŠ‚",
    "é‰": "åœˆ",
    "åˆ": "ç½•",
    "è…¶": "æ–­",
    "é§": "é©®",
    "è«Œ": "æ‡‚",
    "æ±": "è‡³",
    "è¢": "å¿…",
    "é­ ": "æ‹–",
    "è¿»": "å®œ",
    "å’º": "é€‰",
    "èŸ": "æ±‰",
    "æšœ": "æ™®",
    "è©ƒ": "å‡",
    "é±„": "ä¸“",
    "éµ³": "é—´",
    "è›": "çº ",
    "é ‹": "é¥¿",
    "æ“•": "é‹",
    "æ°": "èƒ¡",
    "ç†": "è±ª",
    "å˜º": "æ¡¥",
    "çˆ–": "é¾™",
    "ç¢«": "æ–­",
    "æ¨ƒ": "æµª",
    "ç¾": "ç¾Š",
    "è¤­": "å°¿",
    "æ©…": "é­”",
    "è‚": "è‚¯",
    "ç¦¥": "å…¶",
    "å¯™": "é›¨",
    "æ—¹": "çŸ³",
    "éˆ": "æ–¹",
    "è˜³": "ç°",
    "ä¹­": "çŸ³",
    "å‡™": "å¤º",
    "è¦°": "åŒº",
    "ç²": "å‰",
    "å ": "ç‰™",
    "å€¸": "é‡‡",
    "æŒ®": "ä½“",
    "ç±±": "ç€",
    "è»˜": "å",
    "ç½": "è°ˆ",
    "è”": "è·¯",
    "çƒ–": "ç¾",
    "è‡·": "å ",
    "æ²œ": "åˆ¤",
    "æ¹": "ä¼š",
    "å„½": "é›·",
    "è’€": "æ™•",
    "é»Œ": "çº¢",
    "ç¶": "æœµ",
    "è½": "ç‰",
    "é„": "å¤„",
    "å³‚": "åŒ",
    "éª¦": "åŒ",
    "çª¶": "å·¨",
    "é¤±": "å–‰",
    "è¼": "ç½‘",
    "çŒ¤": "è´µ",
    "è¶": "å´©",
    "ç’´": "æ¥š",
    "ç«": "çŸ³",
    "è†¹": "æ„¤",
    "ç®ˆ": "å°",
    "é»ˆ": "å·",
    "è—¢": "æŒ‡",
    "è‡ ": "é¸¾",
    "æ¿": "ä¸ƒ",
    "è£¬": "é›¶",
    "èœ…": "è¾…",
    "æ›¶": "å‘¼",
    "ç©›": "æ‰",
    "çˆ": "æ„",
    "å´ª": "æ—",
    "èªœ": "åˆ·",
    "å«¸": "å±•",
    "è¡‚": "å¥³",
    "åµ„": "ç¾",
    "è¯": "çˆ¶",
    "èƒ": "å­",
    "é¦": "å¿…",
    "è– ": "çƒ¦",
    "éƒ£": "åš",
    "é‡´": "æ„",
    "ç•": "è´«",
    "è¬©": "é­”",
    "å¸µ": "å¼¯",
    "ä¾‡": "å®œ",
    "æ½«": "å¼¯",
    "ç› ": "ç¦»",
    "å‰Ÿ": "å¤š",
    "èŠµ": "è§‰",
    "æ½ ": "è®­",
    "çš…": "è¶´",
    "å«§": "åˆ™",
    "ç•£": "è¾¾",
    "é‡": "é¾™",
    "è": "æ˜†",
    "å‰š": "å­—",
    "ç±©": "ç¼–",
    "çŠƒ": "å‰–",
    "è³«": "æœº",
    "è“š": "æ¡",
    "å˜¨": "ç¬‘",
    "è¹…": "å·®",
    "çš°": "ç‚®",
    "è«•": "è±ª",
    "è—·": "é¼ ",
    "é‹˜": "å",
    "ç±¯": "è¥",
    "æ«ª": "åˆ©",
    "é´Š": "æŒ£",
    "å¾»": "ä¼š",
    "æ˜£": "æ•",
    "è³…": "è¯¥",
    "è€¡": "é™¤",
    "ç€œ": "å®¹",
    "å¨€": "æ¾",
    "å©«": "æ˜†",
    "ç·ª": "è€•",
    "æ•": "ç§‘",
    "å¿¢": "ç‰©",
    "èš˜": "å›",
    "æ€¤": "å¤«",
    "è²": "é›¨",
    "å‹±": "å–",
    "è™¤": "é¢œ",
    "éº„": "ç²—",
    "ç¹": "èŠ¦",
    "ä¾¸": "æ ‘",
    "ç³²": "åˆ©",
    "é›": "é•¿",
    "åŒ¤": "åŒº",
    "è ­": "é£",
    "å·‹": "äº",
    "è­“": "ä¼š",
    "ç€‡": "ç½‘",
    "é´": "æ–‡",
    "æ’¢": "èƒ†",
    "ç¹¸": "å²",
    "åŸ¨": "ä¼¦",
    "å±": "å…»",
    "æ½±": "æ¤°",
    "å¿£": "æ",
    "ç®": "æ˜¾",
    "ç¤¶": "çŒ",
    "æ“¯": "å®¾",
    "åŸ€": "å‚",
    "é…‡": "èµ",
    "å²¥": "å¡",
    "æ¦¥": "è’",
    "é¡": "å®š",
    "æ¤™": "æ˜Œ",
    "åº": "å…œ",
    "éµ": "å",
    "æ¼Ÿ": "å”",
    "è’¢": "é™¤",
    "ä¿‡": "é€›",
    "çŠ ": "è¥¿",
    "ç¿": "å¤„",
    "ç”¤": "ç‘",
    "å¥±": "é¸¾",
    "é‰‹": "æŠ¥",
    "çŠ™": "ä¸‰",
    "å¹¤": "å¿…",
    "å±ª": "èŠ",
    "ç¶Š": "é‹",
    "å¡‡": "å®£",
    "æ·—": "å±€",
    "æ²—": "æ—",
    "é«§": "è›‹",
    "è©‘": "å®œ",
    "å‡·": "å¿«",
    "æ¼¦": "æŒ",
    "å‡®": "é£",
    "åƒŸ": "æœº",
    "ç§„": "å­",
    "æ®€": "é‚€",
    "çª": "å½’",
    "é‚†": "è…¾",
    "æ†’": "æºƒ",
    "æ±»": "è™",
    "èŠ‡": "çœ ",
    "å‡": "å¸¦",
    "æ‹•": "æ‹–",
    "æµŸ": "ç”±",
    "ç¹µ": "è›‹",
    "é±…": "åº¸",
    "é¡": "å§¿",
    "è“­": "å®‰",
    "è„„": "æ¢…",
    "é¹º": "æ“",
    "æ¬": "è¾…",
    "å¹œ": "äº•",
    "æ£”": "æ˜",
    "éŸ‘": "ä¼Ÿ",
    "æ²¶": "å®œ",
    "æµ²": "é€¢",
    "ç›‡": "å’Œ",
    "è’ƒ": "èµš",
    "å›": "å¸…",
    "çœ»": "ç¾Š",
    "ç®¯": "ç¼–",
    "è“½": "å¿…",
    "åˆ": "èƒ†",
    "è¶š": "é€Ÿ",
    "è¸§": "ä¿ƒ",
    "å¨·": "ç¼€",
    "ç…Ÿ": "ä½",
    "ç˜²": "å®—",
    "ç§¼": "æœ±",
    "ç´·": "é›¶",
    "æ«": "çº ",
    "ä¿Œ": "è¾…",
    "æ»Š": "ç³»",
    "åŒ›": "å°±",
    "è˜¢": "é¾™",
    "å§¯": "å…‰",
    "è™…": "è…¾",
    "ç«¸": "é™",
    "è‡¶": "è§",
    "å€¯": "æ¾",
    "ç¤Š": "å®¢",
    "ä½Š": "æ¯”",
    "é‚": "åˆ©",
    "é©„": "èª",
    "çˆš": "æœˆ",
    "è¿¼": "èŠ‚",
    "æ¢ª": "è±†",
    "ç¦¢": "è¸",
    "é¢»": "æ‘‡",
    "é£©": "å",
    "ç…¯": "æ¥",
    "è¼¤": "æ¬ ",
    "æ°": "å ",
    "è": "äºš",
    "ç†•": "å·¥",
    "å©¯": "åˆ©",
    "è‚™": "é™¢",
    "ç¸¼": "ç‚«",
    "æ¿": "é¡¶",
    "éµ": "é‹",
    "æŸ": "èƒ¸",
    "ç„›": "å",
    "ç·": "å¹¿",
    "é¯": "æ¢—",
    "é­¸": "ç‰‡",
    "ç¿œ": "ç…",
    "ç¸§": "æ",
    "èŠ…": "æ„",
    "ç ›": "é‡‘",
    "æ›—": "å¤œ",
    "åƒ½": "æ˜¼",
    "è": "èŠ‚",
    "é ": "é­",
    "ç„Ÿ": "è¥¿",
    "æ¬¶": "ç¡•",
    "å·ª": "å·¨",
    "ç½": "æ¯›",
    "é¶•": "å®‰",
    "æ‰": "ç¡®",
    "æª²": "å›¢",
    "å·": "è„‘",
    "åœ": "ä¸‘",
    "éŒ": "å®",
    "å£": "ä¼Ÿ",
    "æ´": "å››",
    "è§²": "æ˜Ÿ",
    "å®‚": "å®¹",
    "ç½": "èŠ¦",
    "è¬–": "é€Ÿ",
    "æ«±": "è‚",
    "å»±": "åº¸",
    "é«": "èµ¤",
    "æ¤Š": "åš",
    "çŸ": "é€Ÿ",
    "å¿¹": "ç‹‚",
    "è¬‰": "æºƒ",
    "éµ": "é¢",
    "ç’”": "å¢",
    "çˆ®": "è¢",
    "çˆ¯": "ç§°",
    "æµ–": "è£‚",
    "ç€": "åŒ",
    "å¢ª": "è¹²",
    "è›“": "æ¬¡",
    "è²Ÿ": "åŸ",
    "è‘¼": "å®—",
    "è§¬": "å°¼",
    "è¨¡": "é“¶",
    "æ¼¹": "çƒŸ",
    "è“·": "æ¨",
    "å³—": "ç»´",
    "æ‰²": "é’±",
    "ç‹½": "è¢«",
    "ä¹¹": "å¹²",
    "é·Š": "æ„",
    "åªŠ": "é’±",
    "é¨‹": "æ¥",
    "è¿¡": "é€†",
    "é„": "é»„",
    "ç¢": "è½¯",
    "èµ¸": "å–„",
    "åª ": "å¦¥",
    "ç·º": "ç“œ",
    "é¬": "ä¼š",
    "å…º": "åˆ†",
    "ç¯": "é—²",
    "éº³": "æ¥",
    "ç†": "é‹",
    "æ¾•": "å’Œ",
    "è†“": "é•¿",
    "æ¢‰": "è£…",
    "é€": "æºƒ",
    "é·†": "ç”°",
    "å‡§": "ç",
    "ç·": "å¦¾",
    "å’¶": "å",
    "çŸ‚": "æ‡†",
    "æ´¬": "é€Ÿ",
    "æ¥º": "æŸ”",
    "é³‘": "æ—",
    "ç·¸": "å› ",
    "è­–": "æ€",
    "é¢·": "æ ‡",
    "ç‚“": "æ–™",
    "ç±°": "æœˆ",
    "å·": "åˆ©",
    "å€": "è¥¿",
    "å˜’": "ä¼š",
    "é¥€": "æ¡ƒ",
    "è": "å¸½",
    "é—¤": "ç¯",
    "é‰º": "äºŒ",
    "ç¾€": "æŸ³",
    "å”€": "å³",
    "è": "æ— ",
    "ç¸†": "è€•",
    "å„§": "èµ",
    "é£¤": "å››",
    "æ¬": "ç‰",
    "é½Ÿ": "ä¸¾",
    "åœ·": "ä¸‹",
    "å–—": "å…",
    "è…": "æ˜¥",
    "æ„„": "å¾®",
    "æ»ª": "ç‰",
    "æ™": "ç¼–",
    "æ… ": "å¥¥",
    "è": "å®¹",
    "ç­º": "æ¡†",
    "åƒ": "ç†",
    "å™…": "ç°",
    "æ´": "æ‹",
    "ç¥¬": "çŸ¥",
    "ç‰£": "ä»»",
    "çŸ‘": "èŠ¦",
    "æ±³": "å˜",
    "è¬ˆ": "å©†",
    "é¾": "ä¸‰",
    "å‰¨": "éœ",
    "è’": "æ ‘",
    "æƒ": "é‡‘",
    "åµµ": "çŸ³",
    "é·": "å¿…",
    "è¬": "å®—",
    "èƒ“": "å¹³",
    "å³Š": "çˆ¶",
    "ç¢‹": "è´º",
    "é°": "åœ°",
    "é­“": "å¿…",
    "èˆ¤": "çƒ¦",
    "æ´­": "æ¡†",
    "é˜¹": "åŒº",
    "å¤…": "é™",
    "æ•": "é•‡",
    "ç¹²": "è°¢",
    "å¨«": "é¢œ",
    "å¬Ÿ": "æ„",
    "éŒ½": "ä¸‡",
    "æ¢´": "æ€",
    "æ©": "ç”µ",
    "çª½": "æ¬¾",
    "ç‘½": "èª",
    "çˆŒ": "çŸ¿",
    "å®’": "å‡†",
    "è¬“": "é™ˆ",
    "ç¢": "æ¶ˆ",
    "éƒ˜": "æ—…",
    "ç‡–": "å¯»",
    "è": "å¦¹",
    "éŒ‘": "æ³ª",
    "çŒ½": "æ˜",
    "è½ƒ": "çœŸ",
    "èºƒ": "å¿…",
    "å†": "å®œ",
    "æ®•": "å¦",
    "è¶–": "ç¼©",
    "é£«": "ç‰",
    "æ¶°": "ç»°",
    "ç¿‚": "åˆ†",
    "å©£": "å› ",
    "æ©–": "å”",
    "æ±¦": "æŒ‡",
    "æ‰œ": "è¿‚",
    "å’Ÿ": "æˆ–",
    "æ‡•": "çƒŸ",
    "çº‰": "é’»",
    "é¬·": "å®—",
    "æ‚º": "çŒ",
    "éƒ²": "æ¥",
    "é±«": "çˆ±",
    "æ¤±": "çˆ¶",
    "é¢¼": "æœ",
    "ç¹¤": "é’»",
    "è¤±": "æ€€",
    "æ›¨": "é¾™",
    "çˆƒ": "å®¹",
    "é»µ": "å±•",
    "ä¼¿": "æ„",
    "é‡°": "æ—¥",
    "è‰¤": "ä»¥",
    "èŠ€": "æ¡",
    "ç••": "å°†",
    "é«": "èŒ¶",
    "é½": "åƒ",
    "ç¸¿": "å±±",
    "èº": "å…",
    "æ­®": "è‰²",
    "éºŒ": "é›¨",
    "ç…": "å¦¹",
    "é˜­": "å…",
    "é™¾": "ä»",
    "è¸†": "æ‘",
    "å³†": "å’Œ",
    "åŠ¤": "è¿›",
    "éœ¨": "ä½",
    "çµ": "å²¸",
    "é‰": "ä½",
    "å•’": "å¤",
    "ç´²": "è°¢",
    "è´": "å†Œ",
    "ç±‹": "è‚",
    "ç§": "å€¦",
    "é‰«": "å®¶",
    "å¢Œ": "ç›´",
    "é‰": "çŸ³",
    "æ–¨": "æª",
    "èŸ¡": "é¬¼",
    "ç£": "è¥¿",
    "æ": "ç¦»",
    "æ¸": "ä¹¦",
    "ç–•": "æ¯”",
    "é‰”": "åŒ",
    "èš": "æŸ”",
    "èº£": "å–",
    "å†": "å¯¡",
    "ç¤": "å®",
    "èš†": "å…«",
    "ç¯µ": "èª",
    "æ¹•": "å‡",
    "æœ»": "çº ",
    "è°": "åº¦",
    "ç©»": "è¿‚",
    "åƒƒ": "è¢«",
    "åš–": "ä¼š",
    "é¿": "æœº",
    "æ‘¬": "å½±",
    "æª¦": "è¡¨",
    "å¥°": "å¿…",
    "å˜µ": "æ¶ˆ",
    "çˆ…": "å¢¨",
    "ç‡¡": "æ„",
    "åŠ¥": "å‘",
    "æ¶": "æ˜¥",
    "è‘¿": "æ¢…",
    "è•³": "é—´",
    "é€": "æ±‚",
    "é°«": "åº¸",
    "è’†": "é´",
    "é¨£": "å®—",
    "åŒ¬": "é›¨",
    "é©‚": "å‚",
    "å•‘": "ç…",
    "ç¶¹": "æŸ³",
    "å»": "å°±",
    "é¯¾": "ç¼–",
    "è©»": "æ¶",
    "æ«¬": "è¡¬",
    "é„‹": "æœ",
    "é€": "æ¡ƒ",
    "é‹¡": "å«",
    "åª": "ç»´",
    "éŸ": "æ‰©",
    "é¤œ": "æœ",
    "éŒ¼": "è€",
    "ç›³": "å¿˜",
    "è™˜": "æ“",
    "è‘ƒ": "åš",
    "ç°": "ç­›",
    "è¼³": "å‡‘",
    "ç›‹": "æ³¢",
    "é”": "å¤©",
    "è²’": "å›¢",
    "æˆ§": "æª",
    "ç•Ÿ": "å†Œ",
    "ç¦¼": "è°¢",
    "é¹´": "åŒ",
    "å»¦": "å¿…",
    "é‡–": "åˆ€",
    "å¹": "å¸®",
    "ç©µ": "æŒ–",
    "è¶ ": "ç»°",
    "æ’§": "ç»",
    "è„¼": "ä¸¤",
    "å€": "åº”",
    "å©¾": "å·",
    "ç¨‰": "ç²¾",
    "å¬": "ç¾",
    "è©": "æ„",
    "è›£": "ä¸ƒ",
    "å¶”": "äº²",
    "é…˜": "è±†",
    "æ“µ": "é­”",
    "æ†­": "äº†",
    "ç‘¿": "ä¸€",
    "è²€": "é‚£",
    "é–…": "é—¨",
    "ç…¢": "ç©·",
    "é†": "é—²",
    "è³": "ç´",
    "è–‹": "ç“·",
    "åŠ–": "ç¼ ",
    "é·": "é™ˆ",
    "ç­„": "è¦",
    "æ¯¿": "ä¸‰",
    "è¦‚": "å¥‰",
    "æª": "æ„",
    "å¯·": "é£",
    "é‡ª": "å",
    "æ¹¹": "ç¼ ",
    "æ†ƒ": "å†²",
    "æ§•": "æ‰",
    "è½ª": "å¸¦",
    "ç…±": "ç“œ",
    "é´œ": "ç“·",
    "æ­Š": "æ¶ˆ",
    "é°¯": "å¼±",
    "é™ƒ": "é¥¼",
    "èˆ‘": "è´ª",
    "å¤’": "è„‘",
    "æˆº": "æ˜¯",
    "æ½³": "å›¾",
    "è’§": "ç‚¹",
    "é²¾": "é€¼",
    "åª‹": "æ˜¥",
    "é¬™": "åƒ§",
    "é½•": "å’Œ",
    "èŸ©": "è§‰",
    "è¢µ": "ä»»",
    "è…¤": "å®‰",
    "é®¹": "çƒ§",
    "å¥œ": "åŒª",
    "é¯": "åº¦",
    "æˆ„": "è§‰",
    "å¡¨": "å·¥",
    "æ³€": "æ€",
    "é¯": "å›º",
    "å¯Š": "çœŸ",
    "è€´": "æ„",
    "é­Œ": "ä¸ƒ",
    "é±¦": "ç¡¬",
    "è¸ ": "ç¢—",
    "ç¬": "ç¼€",
    "ç—": "å¿™",
    "é¯¹": "æ˜Ÿ",
    "è´": "è´¥",
    "éŠ—": "å‘",
    "æ–†": "ç¬‘",
    "å¯‹": "è§",
    "å³Ÿ": "å³",
    "ç§": "ç‡•",
    "é‹„": "ä¸‡",
    "ç¶Œ": "ç³»",
    "è—‡": "ç»­",
    "é·™": "è‡³",
    "è¸˜": "å±…",
    "é „": "å¥",
    "åš²": "æœµ",
    "æ€": "æ˜¯",
    "å¬“": "æ•™",
    "å¬œ": "å¿ƒ",
    "é¼€": "ä¿ƒ",
    "ä¿‹": "æ„",
    "æ": "æ„",
    "å«": "åº·",
    "ç´ƒ": "å¯»",
    "é†“": "å¦",
    "æ¦¤": "èŠ‚",
    "æ—¿": "äº”",
    "å »": "é‡‘",
    "æ‡°": "åˆ˜",
    "æ—": "å¯»",
    "å¨ª": "æ— ",
    "ç°‚": "è´µ",
    "èš¼": "ç‹—",
    "æ–¢": "æŒ‘",
    "ä¹¤": "ä¸‹",
    "é„¨": "å¿…",
    "è§": "åº•",
    "è†": "æ¬¡",
    "å¿Š": "å®š",
    "æ¨ ": "ç’",
    "ç¢ƒ": "åº†",
    "çƒ°": "ç¦",
    "é¸‡": "å ",
    "éµ»": "è¿½",
    "ç¸•": "è¿",
    "é‰¼": "é¥¼",
    "è£ƒ": "å¡",
    "é§ˆ": "åŒº",
    "é»": "æ¥¼",
    "åŠœ": "äºš",
    "ç¦": "é™¢",
    "å¼¿": "å‡",
    "é…›": "åŸ",
    "è¡": "é™¢",
    "é¯": "æˆ",
    "è›–": "å¿™",
    "æ½¹": "ç¼ ",
    "å ˜": "æˆ",
    "è£©": "æ˜†",
    "å«¾": "è¿",
    "ç‚¡": "ç",
    "è‘Œ": "é—´",
    "ç§³": "æ´»",
    "è¢¨": "ç‚«",
    "è¶·": "ç§‘",
    "é¥ ": "ç½—",
    "ç¸š": "æ",
    "è•": "å·",
    "è¼Š": "è‡³",
    "å‡": "è¥¿",
    "çŒ’": "ç‡•",
    "ç½–": "ç½—",
    "æ¶½": "æ˜",
    "è¤§": "çª˜",
    "æ›®": "çœ¼",
    "è»": "æ¯›",
    "è¶œ": "å±€",
    "ç‡³": "ç…§",
    "åŸ“": "è£‚",
    "é¨›": "é£",
    "ç—µ": "è®°",
    "ç…¹": "å¤Ÿ",
    "æ«": "åŸ",
    "ç’": "å¤œ",
    "ç©š": "æ•™",
    "æ¤¶": "å®—",
    "çœ": "æ—",
    "é¦": "èª",
    "è¶®": "é€ ",
    "è§•": "ç²—",
    "è€°": "ä¼˜",
    "è¡Ÿ": "åˆ°",
    "ç¤¹": "é¢œ",
    "å·“": "é¢ ",
    "ç·…": "é‚¹",
    "è±»": "æŒ‰",
    "é„·": "é£",
    "é¼ƒ": "æŒ–",
    "ç±": "è¥",
    "ä¸®": "å‡ ",
    "å ¹": "é‡",
    "ç’…": "é”",
    "ç¥³": "è‚¾",
    "ç¹": "ç‚«",
    "é²": "çš®",
    "å§¼": "çŸ³",
    "æ»": "å¾®",
    "ç¹…": "éªš",
    "é¶’": "èµ¤",
    "å¹ ": "å‘¼",
    "å¬£": "å‡",
    "èº": "è“",
    "é™¼": "ä¸»",
    "é´“": "ç­",
    "é¼³": "å±€",
    "è‡©": "å¹¿",
    "çŠ•": "è¢«",
    "ç¤¨": "å’",
    "å¼†": "ä¸¾",
    "è‘§": "åš",
    "é¤": "æŠ¥",
    "è¼‘": "å¼•",
    "é©˜": "ç½—",
    "çµ»": "å…",
    "é§": "ç§‹",
    "é¨": "å’Œ",
    "é¦ ": "æ†¨",
    "ç‰¨": "åˆš",
    "è­¤": "æœº",
    "ç¡": "ç¡•",
    "å€½": "ç…",
    "ç ¨": "æ¶",
    "æ¬¿": "ç ",
    "ç±›": "å‡",
    "å·°": "æ±‚",
    "ç†": "æºƒ",
    "é¼‚": "æœ",
    "ç²‡": "åº·",
    "è¥ˆ": "èµš",
    "ç¢¢": "é©®",
    "çœ": "åŠ©",
    "çŒµ": "ç¼–",
    "æ•’": "æ·±",
    "ç°„": "äº’",
    "è§¹": "è¥¿",
    "åŒ": "æ–‡",
    "å¨¾": "çŸ®",
    "ç¦‚": "å¯¼",
    "è‚": "ç…§",
    "é°¶": "è®°",
    "çˆ¢": "è¿·",
    "ç˜»": "æ¼",
    "æ›": "ç…§",
    "é†½": "é›¶",
    "åœ¿": "å¤¹",
    "è©™": "æ‹”",
    "ç‘¦": "äº”",
    "æ¤¸": "å®œ",
    "é°": "é€¼",
    "å°¶": "å¹²",
    "æ–•": "è“",
    "æº“": "è¿",
    "è§±": "å¿…",
    "èš„": "æ–¹",
    "åœ": "åŸ",
    "æ«´": "èµ–",
    "åªˆ": "ç°",
    "ä»œ": "çº¢",
    "åˆ±": "åˆ›",
    "æ«‘": "é›·",
    "è½¢": "åˆ©",
    "é›¥": "æ‚",
    "å§": "çŸ¥",
    "æ£–": "æˆ",
    "è ": "æ„",
    "çª¢": "éœ€",
    "é¶½": "æŸ",
    "ç …": "åˆ©",
    "ç–·": "çŸ¥",
    "æŒ¸": "å‡",
    "å ·": "å°",
    "å¦‰": "å•",
    "ç” ": "æƒ…",
    "è˜·": "å¥",
    "æœ": "æƒ¨",
    "é¾": "é´",
    "çƒ’": "æ˜¯",
    "æ¾’": "çº¢",
    "æ­°": "è‰²",
    "æ…": "æœº",
    "ç™": "ç¦",
    "å®±": "å’‹",
    "çŸ“": "é¾™",
    "æ´·": "è‡³",
    "ç°¶": "è·¯",
    "æ©‰": "å",
    "ç½›": "å§‘",
    "å¡¿": "æ¥¼",
    "é™˜": "è¡Œ",
    "ç¤œ": "ç‰",
    "æ•‚": "æ‰£",
    "åƒ¼": "é£",
    "èœ¯": "æ£’",
    "é†°": "è°ˆ",
    "è¬·": "æ•–",
    "å²": "å®Œ",
    "é¤·": "æ’",
    "å¡®": "è°¢",
    "é½œ": "æŸ´",
    "è†ƒ": "è¢œ",
    "æ‰¸": "è¥¿",
    "å‚": "è®­",
    "è‡¦": "é€›",
    "è—¾": "èµ–",
    "é±¶": "æƒ³",
    "ç¼¹": "å¦",
    "å§": "é¢ ",
    "åŒƒ": "æ¦‚",
    "å‘©": "æ˜¯",
    "è¼­": "è½¯",
    "æ»»": "é“²",
    "è¾¤": "ç“·",
    "å‰¦": "çƒŸ",
    "æ˜": "çŸ¥",
    "å¸—": "æ³¢",
    "çªŒ": "æ•™",
    "è´‘": "å¹²",
    "ç”": "å«",
    "ç”ˆ": "æ°”",
    "ç¤¸": "æ“¦",
    "è¾³": "å†œ",
    "é¨œ": "é»„",
    "éº–": "ç²¾",
    "ä¹¶": "è¾…",
    "é˜˜": "è¾¾",
    "é€“": "åœ°",
    "ç©…": "åº·",
    "æ§°": "æœ‹",
    "å‰¢": "ç£",
    "æ¥": "æ›¿",
    "é‡¡": "è¾…",
    "é›¼": "è¡",
    "è­": "å°",
    "é´—": "åˆ©",
    "åª™": "å¾®",
    "é§“": "æ‰¹",
    "é´„": "åŒ¹",
    "è¦¿": "æ•Œ",
    "ç¶¡": "è‰¯",
    "æ‡¯": "å¤«",
    "ç…": "å¿…",
    "è‰‘": "å˜",
    "è¢§": "å‹¾",
    "é˜ ": "ä¿¡",
    "ç©§": "è®°",
    "èš‡": "å°º",
    "æ¯£": "æœ¨",
    "èœ§": "åˆ©",
    "éŸ": "ç¦",
    "è ¦": "èŠ¦",
    "é½º": "é‚¹",
    "é–": "éª‚",
    "æ¦Ÿ": "å­",
    "é¤": "æ’",
    "æ›‚": "è’",
    "é¹’": "è€•",
    "ç£‚": "åˆ˜",
    "è’­": "é™¤",
    "å³±": "è„‘",
    "ç£": "è¿",
    "è‚": "å¤«",
    "æ‹º": "å†Œ",
    "æ¢–": "è¢«",
    "è€¬": "æ¥¼",
    "è‘”": "å–‰",
    "æŸ•": "å¸½",
    "ç¤‡": "ç‰",
    "é€½": "è¯º",
    "ä¹¨": "ä½¿",
    "é®": "è€Œ",
    "æšµ": "æ±‰",
    "é‚": "åŸ",
    "æ€": "æ¾",
    "é°ƒ": "å¾®",
    "å±“": "è°¢",
    "è¢¡": "ç„¶",
    "åœ‡": "ä¼¦",
    "è™—": "éœ€",
    "é": "å¾ˆ",
    "å´Š": "æ—",
    "çŸ": "ä¸‘",
    "ä¿ˆ": "è£¤",
    "é¦": "åš",
    "æŠ­": "å’¬",
    "ç¯¨": "é™¤",
    "è•Œ": "å’",
    "ç®¶": "èƒ¡",
    "é¤ˆ": "ç“·",
    "è™ª": "æ ‘",
    "å›•": "æ‡’",
    "è¸³": "å–˜",
    "è²": "èœ¡",
    "åª¹": "åˆ˜",
    "å¶¤": "æ‘‡",
    "çœ‘": "å’¬",
    "æ¿¨": "ç“·",
    "æª¿": "çœ¼",
    "æª": "æ™®",
    "åŠŒ": "è´µ",
    "æ…": "æ··",
    "å¶—": "åŠ³",
    "åœ¢": "æŒº",
    "éœ³": "é¾™",
    "è‰ˆ": "ç‰",
    "é†¹": "å¦‚",
    "åº¼": "è¯·",
    "æ¬‚": "åš",
    "é…§": "æ„",
    "é†„": "æ¡ƒ",
    "å“£": "å‰–",
    "é†•": "çº¯",
    "çª«": "äºš",
    "ç”’": "äº”",
    "å³¬": "ä¸",
    "ç¯Š": "é»„",
    "å…¤": "è°",
    "æ©Š": "åˆ˜",
    "ç¹¢": "ä¼š",
    "ç½": "åš·",
    "èŒ©": "å¤Ÿ",
    "é‚¿": "è¯—",
    "é†·": "æ„",
    "å­´": "ä½ ",
    "éƒ": "è„‘",
    "ç¤§": "é›·",
    "å": "è´¹",
    "åš": "æ•Œ",
    "æ¤–": "æœ‹",
    "è–ƒ": "å·",
    "åˆŒ": "å­˜",
    "ç¿½": "ä¼š",
    "å™¥": "å†œ",
    "ç…ª": "æ±‚",
    "ç¡ ": "ç‹¼",
    "ç§™": "è£¤",
    "è––": "ç§‘",
    "è¥†": "ç¦",
    "æ¤ƒ": "è±ª",
    "ç“‰": "èµ",
    "å ­": "é»„",
    "è¸¶": "åœ°",
    "æ‡™": "é›¨",
    "å¢¸": "åŠ©",
    "å‰„": "äº•",
    "é‡™": "ç ´",
    "ç•–": "æŒ–",
    "ç„¸": "èƒ¸",
    "æŠ": "å®Œ",
    "ç£¿": "åˆ©",
    "ç‰¶": "åŠ",
    "è¥©": "é¼ ",
    "æ¤˜": "æ¥š",
    "è™¨": "å½¬",
    "å¯‰": "è´º",
    "å“ ": "å·",
    "ç·™": "å®¢",
    "é§": "ä¼š",
    "è¢½": "å¦‚",
    "æ‰š": "æ‰",
    "æ¶º": "å±…",
    "è«˜": "è¡¨",
    "å³": "è·¯",
    "é³": "ç§°",
    "çš©": "è’",
    "è¸¤": "æ—",
    "é„†": "è¿",
    "æ…¹": "ç›´",
    "æ€Œ": "åŸ¹",
    "æ‘´": "å‡º",
    "æ±“": "æ±‚",
    "å‘¥": "ç„¶",
    "æŠ§": "æŒ‡",
    "ç¢¬": "ä¾ ",
    "é´¸": "æœ±",
    "æ¿µ": "å½¬",
    "è¤ˆ": "è™«",
    "é¶ª": "å±€",
    "å«": "æ—",
    "é¸Š": "å±",
    "éŸŠ": "è“",
    "é·¡": "æ— ",
    "æ—•": "é±¼",
    "èœ™": "æ¾",
    "æ¸": "çª",
    "è¹¢": "æ•Œ",
    "çš‰": "æ­¤",
    "è‡½": "ç°",
    "éœ": "çº¢",
    "ç‡£": "è“",
    "ç…—": "æš–",
    "å¸¿": "å–‰",
    "é„³": "ç›Ÿ",
    "æ¼’": "å¼º",
    "ç²«": "è€Œ",
    "å²¹": "æ¡",
    "éŠ": "è‡³",
    "åœ…": "å«",
    "ä¿’": "æ··",
    "è›ƒ": "é¥¼",
    "å¥«": "æ™•",
    "å¤µ": "çœ¼",
    "ç‚": "ç½š",
    "åŠ¸": "æŒ–",
    "ä¾³": "åš",
    "é²–": "åŒ",
    "æ¡™": "é±¼",
    "é½Œ": "è®°",
    "é‘": "èœ¡",
    "å·¹": "ç´§",
    "è£­": "å°º",
    "è±¦": "å·¨",
    "å•Œ": "ç›¸",
    "èš": "é¢œ",
    "éƒ±": "å¹³",
    "æº": "ç‡•",
    "ç’¬": "è§’",
    "éŒ‚": "é›¶",
    "è–¢": "è°¢",
    "ç¨¬": "è¯º",
    "ç‹‡": "æœ¨",
    "é¸“": "å’",
    "æ¨": "å’",
    "èº©": "è§‰",
    "è›’": "æ ¼",
    "å¡“": "å¯†",
    "å‰±": "è§",
    "è’©": "ç§Ÿ",
    "åœš": "ä¼š",
    "é³": "è§",
    "å³š": "å¯†",
    "èš²": "å¹³",
    "æ…–": "å›½",
    "ç‘“": "ç»ƒ",
    "å¡": "å‡¯",
    "è¥¡": "é¼ ",
    "è¹": "ä¸",
    "æ°œ": "ç¾Š",
    "çˆŸ": "çŒ",
    "ç®†": "å¿…",
    "èµ¬": "ç§°",
    "é½†": "ç“®",
    "é¤‹": "å€¦",
    "å™®": "é™¢",
    "ç³‚": "ä¸‰",
    "é¶ ": "çœ¼",
    "æ°’": "è§‰",
    "è‘®": "æ–­",
    "è ": "å…¶",
    "éŒ": "å‘€",
    "è¶’": "æ¡",
    "æ‘š": "ç§°",
    "ç‹": "é©®",
    "é‘“": "æµ…",
    "æ¹¤": "è¯—",
    "æª¾": "è¯·",
    "ç¶’": "å¤«",
    "ç½¯": "ä¿º",
    "ç¨‘": "è·¯",
    "æ›…": "å¤œ",
    "æ¾Ÿ": "å",
    "ç‰": "çœŸ",
    "é‘¥": "é²",
    "æš¶": "æ—‹",
    "å ©": "æ›´",
    "é¸‚": "è¥¿",
    "æ–³": "ç´",
    "èœ‹": "ç‹¼",
    "å˜‘": "å‘¼",
    "ç“¬": "è®¿",
    "èª–": "è¢«",
    "ç‡µ": "è¾¾",
    "ä¹µ": "çœ¼",
    "è‡™": "çƒŸ",
    "é›Ÿ": "è¥¿",
    "è¢€": "å›",
    "å±": "è“",
    "è¹¹": "ä»–",
    "æ‘™": "è„¸",
    "å ’": "æ˜†",
    "è±€": "è¥¿",
    "æŒ³": "å‘",
    "ç¡": "å½’",
    "è«ª": "åœ",
    "çŠ": "é£",
    "é": "æ…¢",
    "çº": "å…ˆ",
    "çˆ": "èŠ",
    "è¥‚": "æ£®",
    "æ¸": "çœŸ",
    "æ¥˜": "æœ¨",
    "æ¼": "ä»",
    "å¹": "è¡€",
    "é„¤": "æ…¢",
    "éŒ": "å®‰",
    "æ¸œ": "æš–",
    "é¡´": "å…¨",
    "åƒ": "ä¼Ÿ",
    "è“ª": "é€š",
    "æ¬˜": "ç«¹",
    "ç©‡": "æƒ¨",
    "é¡‘": "ç ",
    "éœ›": "é›¶",
    "ç´": "å·",
    "ç¸‘": "é—´",
    "è³»": "çˆ¶",
    "æˆ­": "çœ¼",
    "äº": "å…¶",
    "æ‰´": "å¤¹",
    "è†": "æ—…",
    "éˆ": "å¼•",
    "é´ƒ": "è§‰",
    "ç¤‘": "è¡",
    "æ«‰": "é™¤",
    "å¬½": "å†¤",
    "çšª": "åˆ©",
    "é§¶": "å±€",
    "è•¸": "ä¾ ",
    "çºŠ": "çŸ¿",
    "ç‰‰": "åˆ¤",
    "ç”†": "ç“·",
    "æˆ": "ç”·",
    "æƒ¸": "ç©·",
    "é‚§": "åŸ",
    "çŠ—": "å€Ÿ",
    "é–ˆ": "æ±‰",
    "å„Œ": "è§’",
    "çš": "è§’",
    "é†»": "æ„",
    "ç•’": "æ¯",
    "æŒ©": "æ‹–",
    "è": "ç¼©",
    "æ¬­": "æ„",
    "é½±": "é‚¹",
    "è¦¤": "ç³»",
    "èªš": "å·§",
    "ç‰»": "å¿™",
    "æ®¾": "è®­",
    "é¶¤": "æ˜†",
    "é‰¶": "è¡Œ",
    "é„¡": "æ•²",
    "èˆ®": "èŠ¦",
    "æ†®": "äº”",
    "å¢´": "é»„",
    "è­": "å°Š",
    "è": "è€Œ",
    "æ °": "ç½š",
    "å¸¾": "å µ",
    "åª": "ç´§",
    "æ˜›": "å·¨",
    "è¸›": "è·¯",
    "é–µ": "å",
    "ç¤˜": "æ¶",
    "æ‚¥": "æ„",
    "ç¤„": "æ¡¥",
    "é¡¬": "å¦‚",
    "é ™": "æ’¤",
    "ç‡—": "çƒ‚",
    "éƒ°": "é‚¹",
    "åµ¡": "ç“®",
    "ç„´": "ç‰",
    "è‡": "ç†",
    "é‚": "æ",
    "è¥¢": "å¦",
    "è’¥": "åˆ˜",
    "éœ”": "åŠ©",
    "ç¹œ": "å°Š",
    "å²…": "æ¿",
    "ç ": "å€Ÿ",
    "ç£Ÿ": "å…­",
    "å‚”": "æ¬ ",
    "å–’": "æ‚",
    "æ¿¹": "ä¹ˆ",
    "é·œ": "é©´",
    "èŠ": "æ•™",
    "æ’ ": "å‡ ",
    "éª¾": "æ¢—",
    "æ›¤": "æˆ–",
    "è«°": "æ´—",
    "ç²": "æ¶ˆ",
    "å¾¶": "åˆ«",
    "éµ¹": "ç¦»",
    "æ¬´": "ç‹¼",
    "èª·": "ç½‘",
    "é¦š": "åŸ",
    "é°”": "æ„Ÿ",
    "éœ": "è¸",
    "é¼œ": "æ°”",
    "æ‘«": "å½’",
    "æ¼‘": "æ¦‚",
    "å¶µ": "å˜´",
    "æª‹": "å±€",
    "é¸": "ä¼š",
    "è¬": "è¥",
    "å…›": "å‰",
    "åƒ„": "ç¥¨",
    "è¬¸": "æ•–",
    "é±": "å…³",
    "ç¶§": "å‡†",
    "è¾¸": "ä»",
    "è·°": "å",
    "é™¿": "ä¾ ",
    "ç©¾": "è¦",
    "ç±’": "æ˜¼",
    "æ©¸": "ç²¾",
    "ç°†": "æ‰£",
    "å¹": "è¢«",
    "ç¨«": "å±",
    "å»§": "å¼º",
    "é¹ ": "åˆ˜",
    "å³": "ç»´",
    "æ•š": "å¤º",
    "è¡¦": "æ„Ÿ",
    "ç®¾": "ç¡•",
    "èº": "é—´",
    "éµ¿": "ç”Ÿ",
    "å–­": "ç‡•",
    "é¸´": "å­¦",
    "è›«": "é¬¼",
    "å‚¤": "åœ¨",
    "ç˜": "æ°‘",
    "è‰­": "åŒ",
    "éµ¦": "è·¯",
    "è¶¦": "å§¿",
    "æ§œ": "æœ€",
    "ç©–": "å‡ ",
    "èƒ®": "æ—",
    "æ€°": "ç‚«",
    "åƒ—": "åŠ³",
    "ç˜": "å‘¨",
    "é˜º": "åº•",
    "åŸ®": "æ¢",
    "æ¹º": "é—²",
    "åµ…": "å«",
    "æ±±": "çŠ¬",
    "å°©": "æ±ª",
    "è¶›": "å¼•",
    "æ˜–": "é¢œ",
    "æ˜˜": "è®¿",
    "åŠ½": "è£‚",
    "éŒ™": "å§¿",
    "ç•¡": "è¯¥",
    "æ¥œ": "èƒ¡",
    "å±µ": "æ¶",
    "è“¡": "æ·±",
    "è°¼": "çº¢",
    "ç¹‰": "é­‚",
    "è†¡": "ç¡¬",
    "éƒŸ": "å¤¹",
    "é·›": "åº¸",
    "å­†": "åº”",
    "å†º": "æ•",
    "æ·Š": "çƒŸ",
    "å‚¿": "ç‡•",
    "å¬ƒ": "éœ€",
    "é¬Š": "é¡º",
    "è®‹": "å“²",
    "é°": "å‡",
    "åœ¼": "è‚",
    "é·“": "è¿™",
    "é‡": "çƒ¦",
    "ç§": "æ„¤",
    "é¢²": "è£‚",
    "èƒˆ": "æ‹”",
    "é¼£": "è´¹",
    "ç«": "å†²",
    "å¥": "è¥¿",
    "å½": "é›¶",
    "ç¬¡": "å¦¾",
    "é‘—": "ç¦»",
    "éš‰": "è‚",
    "ç†£": "è™½",
    "ç—š": "æ¶ˆ",
    "æ±¥": "çŸ¥",
    "åš¾": "æ¬¢",
    "è¸–": "æ",
    "æ•ƒ": "æ•",
    "èŒ": "èª",
    "éº°": "è°‹",
    "è‹¢": "ä»¥",
    "é´Œ": "å¥‰",
    "å·™": "å¥",
    "å —": "çª",
    "é‚«": "å¸®",
    "è½“": "ç¿»",
    "æ‚‹": "å",
    "éŸ£": "æ¯’",
    "ç–": "éœ€",
    "æ¡ª": "å¯»",
    "å©¡": "æ¥",
    "çŠ": "åŒ",
    "æ›": "å·¥",
    "æ¹¨": "å±€",
    "è¨’": "ä»»",
    "æ‚·": "åˆ©",
    "è¹–": "å†²",
    "å¥’": "å¼€",
    "è¼‚": "å±€",
    "æ€š": "å·¨",
    "æ¹†": "æ°”",
    "è‘Š": "å®‰",
    "å‡": "æœ",
    "æ¾½": "å·¨",
    "æª¡": "å®…",
    "åµ‰": "åœ",
    "ç•‰": "ç¦",
    "ç–˜": "åˆš",
    "è˜ ": "å¼º",
    "æ«": "æ¯’",
    "è¼": "ç‰",
    "æ¥°": "é±¼",
    "æ¤µ": "å‡",
    "çš­": "æ•™",
    "ç¤·": "è“",
    "ç†…": "è¿",
    "é›”": "æ„",
    "å‰’": "é”™",
    "è’°": "ç›˜",
    "ç¥®": "å‘Š",
    "ç’•": "å¯»",
    "ä¾œ": "å‘¨",
    "å²§": "æ¡",
    "è¥µ": "è€…",
    "ç®¼": "ä¹Œ",
    "çµ": "è°¢",
    "å‡¾": "å«",
    "å–": "å †",
    "é¢¶": "å·¨",
    "é š": "äº•",
    "é¯¸": "å–‰",
    "å·š": "çœ¼",
    "é½ ": "æ¡",
    "å¡»": "å¢¨",
    "ç«“": "è±ª",
    "åµ£": "è¡",
    "èƒ£": "å°º",
    "åª¦": "ä½",
    "å‚±": "è€¸",
    "æ¯¸": "å¡",
    "æ¬ª": "å¤„",
    "çº": "è§’",
    "è‰": "ç“®",
    "æ‘ª": "å°†",
    "æ® ": "è‡­",
    "ç£’": "å…",
    "èª": "ä¿ƒ",
    "é¶µ": "é™¤",
    "è’–": "çœŸ",
    "ç’¼": "è“",
    "ç‹": "é¢",
    "ç§": "äº‘",
    "ç­": "å†Œ",
    "éµª": "å®‰",
    "è¿¬": "åŠ©",
    "å‹Œ": "å€¦",
    "å“¤": "å¿™",
    "è‹‰": "åŒ¹",
    "æ„‹": "å®£",
    "ç˜": "å¿…",
    "é¢µ": "çƒ§",
    "ç€´": "è¥",
    "æ­": "è¿›",
    "å­¶": "å§¿",
    "æ“¥": "æ‡’",
    "ç‰¼": "å‘",
    "èºŒ": "äº”",
    "ç¥±": "ç¡",
    "ç³¿": "å³",
    "æ´•": "å°",
    "è±°": "åš",
    "é¤¤": "è°ˆ",
    "æ¢©": "ç¦»",
    "è’ ": "è¥¿",
    "è„€": "æˆ",
    "èª†": "æ¡†",
    "ç¦™": "è¢«",
    "é¸•": "èŠ¦",
    "é¦½": "ç›´",
    "ç¬œ": "ç«¹",
    "é˜": "ä¹",
    "è¤µ": "ç¦»",
    "å€ˆ": "æ¥",
    "å¢±": "å‡³",
    "ä¾¾": "æ¶ˆ",
    "èº¦": "æ“",
    "è©¼": "ç°",
    "å¦¢": "åŸ",
    "è–‚": "ä¹ ",
    "é…¦": "ç ´",
    "è‘–": "çª",
    "è—¸": "é™¤",
    "è…": "ç»°",
    "è¬": "è´º",
    "èŒ‹": "æŒ‡",
    "é‹€": "å·",
    "æ¸³": "ç±³",
    "çµ­": "å€¦",
    "æ£¦": "ç§°",
    "èš—": "è§‰",
    "å¯´": "äº²",
    "èŒ¡": "å­—",
    "è§»": "åˆ©",
    "è½–": "è‰²",
    "æ¤º": "ä¹ ",
    "é°‰": "é»„",
    "æ¼": "å¼±",
    "é·°": "ç‡•",
    "éˆ™": "ç´",
    "åº": "æ‚",
    "æ†": "æ¯›",
    "å ³": "æ¢…",
    "ç™¤": "æ¥",
    "åº": "æ‹”",
    "ç¢": "é“¶",
    "æ¶¾": "è¸",
    "ç¸¸": "æœ¨",
    "ç±¸": "æ·±",
    "ç““": "çƒ‚",
    "ç±­": "æ€",
    "å©®": "å±…",
    "åš³": "è£¤",
    "çˆ‚": "æ ‡",
    "èŒ£": "æ— ",
    "é¡": "ä¼Ÿ",
    "å±³": "å…ˆ",
    "è­±": "å–„",
    "ç€»": "å¸¦",
    "åŒ‡": "æ„",
    "å³œ": "è®°",
    "é®¢": "æœ±",
    "ç°": "éª‚",
    "æ†–": "å°",
    "èŸ­": "æ•™",
    "æ–": "é›¨",
    "ç·“": "åº”",
    "è¬": "æ¡¥",
    "å¶£": "æ•™",
    "æ«°": "æ€€",
    "éµ¶": "å‘€",
    "æŸ£": "è‡³",
    "ç¬¿": "è½",
    "éŸ‰": "é—´",
    "éŠ¾": "çº¢",
    "çœ": "ç",
    "æ’œ": "æ•´",
    "é®¦": "åŒ",
    "é»³": "ä¸€",
    "æ«•": "æ”’",
    "è¶‚": "è¡¬",
    "é¦": "åˆ˜",
    "èš¹": "çˆ¶",
    "ç—‘": "è´ª",
    "æ‘": "äº’",
    "å±·": "ä¼š",
    "èŠ†": "æ‹†",
    "æ¨": "åˆ™",
    "é…•": "æ¯›",
    "è‡®": "è®°",
    "é­—": "ä¸‘",
    "é¨ ": "æ",
    "æŸ—": "æ¾",
    "å¡ ": "å †",
    "æ‘—": "æœ",
    "åŠ€": "ç“œ",
    "ç¥©": "åŠ©",
    "é¼": "è´µ",
    "è„": "ä¸œ",
    "æ’": "å‰",
    "ç¤": "è½¯",
    "å•½": "å²¸",
    "é³¬": "è¾…",
    "è˜“": "è‹",
    "çª·": "æ–™",
    "è–»": "æ—©",
    "é–„": "æˆ–",
    "æ˜": "ç§‹",
    "èˆš": "å¤©",
    "å¢‹": "å°˜",
    "æ¹—": "å¥‰",
    "åœ¶": "æ°",
    "å¹": "è…¾",
    "è™­": "é›•",
    "é‚": "å¤«",
    "éœš": "ç‰©",
    "ç©": "è·¯",
    "é´”": "ç¦",
    "æ©§": "å¢",
    "è¶ˆ": "å ",
    "é¤²": "çˆ±",
    "é†": "é—¨",
    "éŒ¿": "è™",
    "æƒ¿": "æ",
    "è†": "é‚¹",
    "è¾¥": "é´",
    "åƒ¶": "æ•",
    "è”": "æ¥¼",
    "å¬™": "å¼º",
    "è­Š": "è„‘",
    "ç•½": "å",
    "çµ–": "çŸ¿",
    "è›ˆ": "é“",
    "å˜¾": "è›‹",
    "é™“": "è¿‚",
    "è¡": "è’",
    "ç½": "ç‹—",
    "é°—": "èƒ¡",
    "ç¦¨": "æœº",
    "æ˜": "èµ¤",
    "æ´ ": "è°‹",
    "ç†—": "å‘›",
    "æ…”": "æœ¨",
    "æª¶": "å‰",
    "å‡Ÿ": "æ¯’",
    "å¢·": "å¤œ",
    "ç¦": "è¥¿",
    "è±": "å",
    "ç¨¡": "æœ€",
    "é¦»": "å…",
    "éƒ¥": "è¢«",
    "è¿¶": "å³",
    "æ€¶": "å¿…",
    "è“": "å¹³",
    "é¥‚": "æ¸©",
    "å…¾": "è®°",
    "å¢¯": "å •",
    "å¾¾": "æ¢…",
    "é": "è´º",
    "ç·®": "çˆ¶",
    "èŸ¦": "è‚¥",
    "é¾": "ç§‹",
    "æ§§": "æ¬ ",
    "ç“": "éš",
    "é‹»": "è§",
    "å¤": "æƒ…",
    "é¯©": "ä¼¦",
    "å‰™": "åˆ›",
    "åŒ³": "è¿",
    "å‚Š": "è¿",
    "æ«¡": "ç€",
    "ç°": "èŠ",
    "èŸ¶": "ç§°",
    "æ¹·": "è£…",
    "è–¿": "ä½ ",
    "è†²": "æ•™",
    "çŸ’": "ç›Ÿ",
    "é•": "è·¯",
    "é¼Ÿ": "è…¾",
    "å€¿": "å®",
    "éˆ–": "åˆ†",
    "åš¹": "å•¦",
    "å§•": "å§¿",
    "ç‚—": "å…‰",
    "é³": "è¸",
    "æ´": "å…ˆ",
    "ç…“": "å›¢",
    "å‚®": "ç³Ÿ",
    "ç’—": "è¡",
    "è‰©": "å…¶",
    "é±": "ä»»",
    "æ‹¹": "é‹",
    "é¥œ": "ç‡•",
    "èµ¯": "å”",
    "è¨¨": "æŒ‡",
    "éœº": "ç»´",
    "è•": "ä¼¦",
    "è‘ ": "æ·±",
    "ç¥°": "å‘Š",
    "å¾²": "æ",
    "æ€¢": "çª",
    "å¿¯": "å…¶",
    "èšˆ": "å‰",
    "é£€": "åˆ˜",
    "å‰´": "å‡¯",
    "æª–": "å²",
    "å‘„": "æ ¼",
    "è¥˜": "è´µ",
    "è£‹": "æ ‘",
    "é¡¡": "å¤–",
    "é‹": "åˆ©",
    "è›•": "å›",
    "åˆ¡": "æ•",
    "é»®": "èƒ†",
    "é½": "çœ¼",
    "å«": "è‡³",
    "çƒ³": "æ™®",
    "ç•ƒ": "å¯»",
    "æ€‰": "å®",
    "æŒ¬": "åš",
    "çš½": "æ‹›",
    "é€·": "æ›¿",
    "åŠ—": "å‡",
    "è": "ç…",
    "é¶†": "æ¥",
    "ç©­": "æ—…",
    "ä¹¯": "å‘¼",
    "å«“": "å±",
    "ç«": "ç¿ ",
    "æœ„": "å¼•",
    "ç¯": "æ˜",
    "è•": "é’±",
    "é½­": "æ¥š",
    "è": "æŒ",
    "æ†¸": "å…ˆ",
    "ç›°": "å¹²",
    "æ£œ": "ç‰",
    "å£¨": "é›·",
    "ç¸¶": "ç›´",
    "è½Œ": "é›ª",
    "è½”": "æ—",
    "é¨±": "ä¹ ",
    "ç€«": "èƒ¡",
    "åµ": "å›¾",
    "è¸’": "çª",
    "æ¦“": "å¯†",
    "ç„»": "å”±",
    "è³": "æ¯’",
    "ç©±": "æ‰",
    "é´‹": "æ–¹",
    "è¢©": "å“²",
    "çŒ¼": "åš",
    "ç§¥": "å¹´",
    "ç°¼": "å‹¾",
    "çµ¿": "æ±‚",
    "é¤€": "å®³",
    "æ¨²": "äºŒ",
    "æ¾“": "ç¦",
    "çŒ²": "äº›",
    "å¥™": "æœ¬",
    "æ—¾": "æ˜¥",
    "é¬¹": "å½’",
    "èƒŸ": "æ¯",
    "è²¤": "å®œ",
    "è¾": "ç”°",
    "è“²": "ç§‹",
    "è€®": "çƒ™",
    "èœª": "æ¡ƒ",
    "å¶¦": "å ",
    "æ¼": "å¯†",
    "é¡¢": "ç’",
    "æ´¡": "æ³ª",
    "æ”„": "ä¹¦",
    "å²»": "æŒ",
    "ç–¶": "é´",
    "é°": "ç¯",
    "é™Š": "å •",
    "ç¯…": "ä¼ ",
    "å¬¥": "æŒ‘",
    "è©…": "é›¶",
    "å¯ˆ": "é’",
    "éŸŸ": "é«˜",
    "æ¸®": "å’Œ",
    "å²°": "å¥¥",
    "æ ®": "è€³",
    "ç‹": "è±ª",
    "ç„‚": "ä¹¦",
    "å±˜": "æ»¡",
    "çª”": "è¦",
    "é‰§": "æ¯",
    "æ¥": "æœ¬",
    "è—™": "æ„",
    "é‰©": "æ´—",
    "é«¾": "çƒ§",
    "æ…½": "ä¸ƒ",
    "ç¡¦": "è½",
    "åª•": "å®‰",
    "éŸ": "åŠ",
    "è¹›": "å¸¦",
    "ç§…": "èŒ¶",
    "é‰³": "åŒ—",
    "ç«¬": "æ›²",
    "æ¿¦": "å¼•",
    "é½”": "è¡¬",
    "å‘": "è½°",
    "ç‡¸": "å¦‚",
    "æ° ": "æ·±",
    "å”©": "çª",
    "æ¹ ": "æ¢",
    "èƒ’": "é€†",
    "çˆ‰": "èœ¡",
    "è­£": "æ˜¾",
    "å¯ª": "ä¼Ÿ",
    "å¶•": "æ•™",
    "æ·¾": "å¼•",
    "è±": "ä¼Ÿ",
    "ç¤©": "è‡³",
    "å…": "å¯",
    "ç¯": "ç§‹",
    "å—": "æ",
    "ç©¼": "æ·±",
    "é½¯": "å°¼",
    "å‚¯": "æ€»",
    "è³": "åº”",
    "æ”…": "èµ",
    "æ›§": "å®¹",
    "éƒ©": "è‚–",
    "çŸŠ": "çœ ",
    "é ¥": "å®œ",
    "æ³": "å",
    "ç°œ": "è¡",
    "é¨¤": "å¥",
    "è«µ": "å—",
    "éˆ": "æ ¼",
    "é‚”": "èµ·",
    "å¿‚": "å–",
    "è¾": "è™½",
    "è—½": "äº²",
    "ç²€": "å¸",
    "å©Ÿ": "äº’",
    "æ¿²": "å¤",
    "èŒ®": "æ•™",
    "é¶¬": "ä»“",
    "é½°": "åˆ™",
    "å¶ª": "å¤œ",
    "æ‡¹": "è®©",
    "è¿¾": "è£‚",
    "åŠ°": "å¢¨",
    "æ­‚": "å–˜",
    "æ»–": "è™½",
    "é¼²": "é­‚",
    "é¨‰": "æ˜†",
    "çš¶": "æ‰",
    "å§‚": "ç½š",
    "é‹²": "å†°",
    "æ·§": "å¯†",
    "é°³": "ä¹",
    "ç„¬": "è¥¿",
    "éŒ": "å‘",
    "ç™": "è´µ",
    "éº": "å›",
    "æ¿§": "å¯¹",
    "ç™™": "é¼ ",
    "æ–": "ç‹¼",
    "ç¯»": "æ¼‚",
    "èº†": "å·¨",
    "æ±‘": "æ‹–",
    "å©°": "ç‚¹",
    "å˜„": "æ•™",
    "æ›": "æ—…",
    "æŒ§": "é›¨",
    "æŠ©": "å—",
    "é«¥": "ç„¶",
    "æ‡": "ç”·",
    "æ¡‡": "å¦‚",
    "è ¿": "æ‰",
    "è¹œ": "é€Ÿ",
    "æ«": "å’",
    "é—’": "è¸",
    "å ˆ": "åˆš",
    "éœ¿": "ç›Ÿ",
    "æ†•": "æˆ",
    "æ–": "é‚€",
    "è¼ ": "æœ",
    "è“’": "å®£",
    "é„ª": "å¿…",
    "è•€": "æ",
    "å²¶": "ç ´",
    "è¹µ": "ä¿ƒ",
    "ç—†": "è‚",
    "åµº": "èŠ",
    "ç¤”": "æ‰¹",
    "é›¡": "å…­",
    "è†¸": "é«“",
    "æ": "ç¦",
    "æ»±": "æ‰£",
    "å‡“": "åˆ©",
    "å©©": "æŒ‰",
    "è€ˆ": "ç‹—",
    "ç·€": "ä¸ƒ",
    "èº": "æ›²",
    "é­": "å–‰",
    "å‚¡": "ç—…",
    "è": "åœ",
    "é¯ƒ": "æ— ",
    "è¥®": "åš",
    "è­…": "è‰²",
    "éº«": "é¢",
    "ç‹š": "è›‹",
    "ç¶•": "çŸ¥",
    "é‰ƒ": "æ˜¯",
    "æ«½": "å¼•",
    "å«›": "ä¸€",
    "å­¾": "åº”",
    "å´": "çœ¼",
    "å—¿": "å¦",
    "è”": "å¼ ",
    "éµ®": "å‰",
    "å¯": "ä»¥",
    "è±¯": "è¥¿",
    "æ•„": "ç‰©",
    "ç¶©": "ç¢—",
    "æ‰": "æ±‚",
    "æ”¦": "åˆ©",
    "ç": "ç»­",
    "åˆ¾": "æ¬¡",
    "é¸¼": "å‘¨",
    "æœ°": "æœ¨",
    "æ½": "æ•™",
    "é ‡": "æ†¨",
    "å¨™": "è¡Œ",
    "çŠš": "ä½",
    "å­": "é¢œ",
    "æ¤¢": "è´µ",
    "è¥€": "æœº",
    "å§²": "ç‡•",
    "æ‰»": "è‡³",
    "åª‰": "å§",
    "è£¿": "ä»¥",
    "æ‚": "å†¤",
    "å¶¡": "è´µ",
    "è¡š": "èƒ¡",
    "ç¿„": "èµ¤",
    "éº®": "å»",
    "æ¼‹": "é¾™",
    "ç¾¾": "å…±",
    "ç½¸": "ç½š",
    "é†¦": "å°˜",
    "è": "æ¶",
    "é¬¦": "è±†",
    "æ“ª": "å¤œ",
    "é": "é”",
    "å–": "æŸ´",
    "èš…": "æ¶",
    "é¢": "è°¢",
    "é°›": "æ¸©",
    "æ‰¨": "ä»»",
    "æ˜ˆ": "äº’",
    "æ†Š": "è¢«",
    "æ¢»": "ä½›",
    "é": "å¤©",
    "è¤…": "æ›¿",
    "ç°´": "å·¨",
    "é½´": "çœ¼",
    "è»„": "ç›´",
    "éƒ‚": "è¯¥",
    "åº´": "æ",
    "å¶": "å •",
    "æŸ¡": "æ°¸",
    "æˆ": "è°‹",
    "æ ™": "ç¿”",
    "æ¿½": "èµ",
    "æ¢«": "å¯",
    "çƒ“": "å¾®",
    "é·«": "é€Ÿ",
    "ç¨º": "è‡³",
    "åª†": "è½¯",
    "è‰’": "æœ¨",
    "ç”": "ä¸“",
    "é®Š": "çˆ¸",
    "éˆƒ": "è¡Œ",
    "éª½": "è…¿",
    "æ·•": "è·¯",
    "çŠ“": "é™¤",
    "è™": "å‚",
    "è‘Ÿ": "é»„",
    "é¶›": "æ¥",
    "å•": "äº”",
    "å¥›": "è°",
    "è½œ": "è€Œ",
    "å¸": "è›‹",
    "è­": "èŠ‚",
    "è˜œ": "å±€",
    "é€¿": "è¡",
    "æ€­": "å¿…",
    "æ ¶": "å› ",
    "åƒ«": "æ¶",
    "è“ƒ": "æœ",
    "å¹¦": "å¯†",
    "ç›­": "åˆ©",
    "æ¦¡": "é€Ÿ",
    "éŸ›": "è´¥",
    "é´": "æ­Œ",
    "é‡¾": "çˆ·",
    "å¹": "æ",
    "æ†±": "ä¿ƒ",
    "å°": "çº¢",
    "æ©”": "è¹²",
    "è¼˜": "æ£±",
    "ç¡": "ç´",
    "ç¯": "æ¬ ",
    "è©": "è™«",
    "é¨": "éœ",
    "é£º": "ç“·",
    "ç ª": "æ¯",
    "å—Š": "çº¢",
    "ç¸¨": "è°",
    "æš“": "å¸½",
    "åƒ“": "è…¿",
    "ç˜¹": "æ‰",
    "æ‡€": "ä½",
    "å¤ƒ": "å¤",
    "é°„": "å¾®",
    "å‹†": "ç‹¼",
    "æƒ": "è“",
    "ç¬§": "å†Œ",
    "æ¦": "èŠ¦",
    "åŒ·": "è§‰",
    "æ¢": "å¿…",
    "èª€": "äºŒ",
    "å—": "ç…§",
    "å£”": "å¯¼",
    "é„": "äº®",
    "å¸": "å¢¨",
    "è‚": "æ­Œ",
    "æ›”": "é™",
    "æ£³": "æ‰",
    "æ®": "åœˆ",
    "æ¥Œ": "é¢œ",
    "é‡…": "ç‡•",
    "æ«‹": "çœ ",
    "è‡‡": "å·",
    "å±­": "ç³»",
    "å«¯": "å¥¥",
    "è“¤": "é›¶",
    "é™»": "å› ",
    "æ‹": "å®¶",
    "ç¾œ": "åŠ©",
    "è†«": "èŠ",
    "è†¯": "è…¾",
    "å…£": "æ",
    "ä¾™": "åƒ",
    "æ›’": "è§’",
    "ç²Š": "å¿…",
    "è¥§": "æŒ‡",
    "ç±·": "å“²",
    "ç–º": "ç½š",
    "éŒ": "å®—",
    "çŒš": "ç‰™",
    "ç£›": "ç¼ ",
    "é‰’": "åŠ©",
    "æ£™": "åˆ©",
    "è¶": "å–",
    "å³”": "æ¯",
    "ç†¢": "æœ‹",
    "é¶¼": "é—´",
    "é½—": "é“¶",
    "æ¨§": "æ²™",
    "é§Š": "å¡",
    "ç€Œ": "æ ‡",
    "é­°": "æ–‡",
    "è…Ÿ": "èµ¤",
    "è’¬": "å†¤",
    "çƒ¿": "å®¹",
    "é±­": "è®°",
    "æ¹»": "çº¯",
    "æ¤¡": "åˆ°",
    "èŸ•": "å˜´",
    "éŠ±": "æ‰",
    "ç†©": "äº’",
    "ç™³": "è£¸",
    "å¨—": "æŒº",
    "è·¦": "æœ±",
    "æ–¦": "é“¶",
    "é¬´": "è¾…",
    "éŠ¤": "ç±³",
    "ç‚¾": "è°",
    "è…¢": "å¶",
    "ç”": "ç”µ",
    "è¹": "è§",
    "è²›": "æ¬¢",
    "æ˜": "å°Š",
    "å£œ": "è°ˆ",
    "ç¯‚": "æ˜Ÿ",
    "æŸ‹": "å¸¦",
    "é¹¸": "å‡",
    "ç””": "å•",
    "è´œ": "è„",
    "ç‚": "æ¢…",
    "é›ƒ": "å‰",
    "å®": "å’¬",
    "æŒ…": "æœµ",
    "éœ¦": "å½¬",
    "è“«": "å¤„",
    "å‰¹": "è·¯",
    "é‚": "å",
    "ç‘¹": "ä¹¦",
    "è–": "å ",
    "é¬„": "æ•Œ",
    "é": "é€¼",
    "è§›": "è›‹",
    "é¼±": "ç²¾",
    "ç¸“": "å…¨",
    "æ ": "ç”Ÿ",
    "éºŠ": "è¿·",
    "é«·": "åŒº",
    "è¨‘": "å®œ",
    "è¢¯": "åš",
    "ç‰…": "åº¸",
    "ç¬š": "ç­”",
    "æ°": "å†…",
    "é­€": "å°¬",
    "è¼": "ä¸‡",
    "è¶¤": "è¡",
    "è": "æŒ‘",
    "æ—": "å¿«",
    "é¢¾": "æœ",
    "è±œ": "é—´",
    "å­Š": "ç±³",
    "é‘”": "å·®",
    "è·´": "é‡‡",
    "æš¿": "æ´—",
    "ç¾": "è¾¾",
    "é¼": "å¯†",
    "å™½": "åŒ¹",
    "ç€­": "ä¹¦",
    "æ²¯": "æ‚",
    "æ‡¨": "çƒŸ",
    "çºƒ": "å…¶",
    "èœµ": "å†¤",
    "è¢£": "æ„",
    "ç®Š": "è¿‚",
    "å¿‡": "ä¹",
    "å…": "æ¯›",
    "è•¢": "æºƒ",
    "é°‡": "æŸ”",
    "é­¼": "åŒº",
    "èµº": "å¼•",
    "è‚•": "ä»»",
    "ç¨¯": "å®—",
    "è¦¯": "å¤Ÿ",
    "é„¶": "å¿«",
    "ç¶—": "çª˜",
    "ç¦ˆ": "ç°",
    "åœ ": "äºš",
    "è½ ": "é›·",
    "æ…¦": "å°±",
    "çŸ„": "ç†",
    "è‡": "è¥¿",
    "ç¦“": "ç¾Š",
    "çƒ": "å§",
    "æ€‹": "æ°‘",
    "ç Š": "åº·",
    "å‚": "å·",
    "åšŠ": "å±",
    "ä¿”": "æ¬ ",
    "ç†‚": "ç³»",
    "ç’¯": "ä¼š",
    "åŒ": "é™",
    "é¨…": "è¿½",
    "é¯—": "æƒ³",
    "å ¸": "é€¢",
    "è­": "ç”Ÿ",
    "é¼": "åŸ",
    "æ¹": "å¢¨",
    "å£€": "çš®",
    "é¡–": "ä¿¡",
    "ç¡£": "æ¶ˆ",
    "å¦•": "é‡",
    "å— ": "çƒ™",
    "ç’‘": "æ— ",
    "è ¸": "å…¨",
    "è™": "åº¦",
    "è—Š": "æ‰",
    "ç": "ç“·",
    "ç„‡": "æ¶ˆ",
    "é©£": "è…¾",
    "è±®": "åŸ",
    "çµ´": "ç¿”",
    "å¬": "é¬¼",
    "å ¶": "é©®",
    "çµ¥": "ç¦",
    "é„": "æ˜",
    "ç…¼": "åµ",
    "é‰µ": "åŒ",
    "è”†": "é›¶",
    "æ ": "å¸®",
    "ç”": "å",
    "è‡–": "æ€§",
    "å¬„": "ä¸€",
    "å‚¸": "é—¯",
    "è™©": "ç³»",
    "é¯": "ç€",
    "ç—»": "æ°‘",
    "æ£…": "é¥¼",
    "æ”–": "åº”",
    "æ§µ": "æ¢",
    "è˜°": "æ…¢",
    "å·‘": "æ”’",
    "ç¹“": "å·¦",
    "é¥„": "å”",
    "é»š": "é’±",
    "ç‡“": "åŸ",
    "æ¦": "ä¸Š",
    "åƒ›": "ä¸ƒ",
    "çµ½": "æ—…",
    "è˜±": "æ³ª",
    "æ¦—": "è§",
    "æ„ƒ": "å®£",
    "ç©ª": "ç§°",
    "é—": "æ¼‚",
    "æ¨": "æ‰",
    "è¡º": "é‹",
    "é‘•": "è‡³",
    "è•˜": "æ‰°",
    "é±†": "å¼ ",
    "é…Ÿ": "å¤©",
    "é®": "åº·",
    "æ‹²": "æ‹±",
    "æ®": "æ°",
    "æ«©": "é¢œ",
    "é¬…": "æœ‹",
    "å·": "ç°",
    "å·Š": "å½±",
    "æ‡­": "æ—·",
    "ç±„": "æºƒ",
    "ç–§": "å…¶",
    "éƒ¼": "ä¸€",
    "ç¥¶": "åœ°",
    "è«": "å",
    "çš¨": "æ˜Ÿ",
    "ç«˜": "æ›²",
    "åŸ±": "å¤„",
    "èœ²": "å¾®",
    "è½—": "ç ",
    "ç½": "åˆš",
    "é°¢": "é©¬",
    "ç³": "è°¢",
    "è£“": "æ ¼",
    "å¨‹": "ç»",
    "ç„€": "èƒ¡",
    "ç•": "ç¢—",
    "æ¡š": "èµ",
    "è¬ª": "ä¼¤",
    "é±¾": "å‡ ",
    "çƒ£": "ç°",
    "æ³‹": "ä¼š",
    "é‹Ÿ": "å¯",
    "çˆ„": "åˆ©",
    "è©—": "èƒ¸",
    "å¬‡": "æºƒ",
    "æ»¼": "é¥­",
    "æˆ": "å¿…",
    "æ¾™": "ç³»",
    "è¥«": "æ˜¯",
    "èŸ": "æ–‡",
    "è›‚": "åˆ«",
    "å¨”": "å®¢",
    "æ¢‘": "æ•Œ",
    "æŠ‹": "äº²",
    "èµ²": "åˆ©",
    "ç§¶": "å§¿",
    "éœ¡": "å–",
    "æ½Œ": "è‡³",
    "å¹": "éœ€",
    "è²£": "ç‰¹",
    "èŸ±": "æ— ",
    "é°·": "æ¡",
    "ç¶‚": "ç»Ÿ",
    "èœ›": "å±…",
    "è’’": "è¯—",
    "éŸ": "æŒ",
    "ç˜„": "ä¿ƒ",
    "ç•": "å¿…",
    "é¦¦": "å…ˆ",
    "é«´": "ç¦",
    "å¡‰": "æ",
    "éŠ‹": "äºº",
    "æ—”": "è§",
    "ç³“": "å¤",
    "è”©": "é“¶",
    "ç½‹": "ç“®",
    "æ¢®": "å±…",
    "çº‘": "èŠ¦",
    "æ¡": "å",
    "é‘©": "æ¶",
    "è–": "ç­",
    "éº¬": "å¤«",
    "å¥¯": "æˆ–",
    "è‘": "éœ€",
    "å´¸": "ç¾Š",
    "æ„»": "è®­",
    "é±©": "é›·",
    "è¿": "å°†",
    "è ’": "å‡",
    "å¨¨": "ç°",
    "ç±‰": "å°",
    "ç³": "ç°",
    "å”¨": "ç»„",
    "åª¶": "å®¹",
    "æ‘¨": "å¥¶",
    "ç¨": "äºš",
    "ç¨¦": "ä¸€",
    "é†˜": "ç§‘",
    "åµ¤": "å®¹",
    "ä¸³": "é“²",
    "é§œ": "å¿…",
    "çŠ": "ç§‘",
    "ç¡œ": "å‘",
    "è©š": "è¾¾",
    "æ¹": "æ¶",
    "éˆ": "åº”",
    "è”ƒ": "å¼º",
    "æ‚¡": "ç¦»",
    "èµ¹": "ç©·",
    "è’®": "ç‰",
    "å…“": "é‡‘",
    "é½…": "ç§€",
    "æ‡…": "å·¨",
    "å´µ": "ç¾Š",
    "æ­–": "æ´—",
    "é½¼": "æ¥š",
    "æ…": "å¼ ",
    "å¿¶": "é­‚",
    "é¼": "å»",
    "ç¤†": "å‡",
    "éµ„": "åƒ",
    "é¸˜": "åŒ",
    "ç“¡": "ç›´",
    "ç°": "å¹´",
    "é›¦": "æ",
    "å˜‹": "ç¬‘",
    "ç»": "å®£",
    "ç·": "çƒ¦",
    "è¤": "å·",
    "é¸": "æ•Œ",
    "é®": "æ˜Ÿ",
    "å²": "åš",
    "é…»": "æœ€",
    "æ±„": "è´£",
    "è§Ÿ": "è¯",
    "è¬§": "ç¦»",
    "ç€¤": "æ€€",
    "å±¶": "ä¼š",
    "è¦´": "ç¯",
    "æœ†": "åˆ†",
    "è©¯": "ä¼š",
    "çŸ”": "çŒ",
    "è§¡": "æ ¼",
    "è": "æ±‚",
    "æ©—": "ç›Ÿ",
    "ç”‹": "åœ°",
    "éš¥": "å‡³",
    "ç¯²": "ä¼š",
    "æ¼Œ": "ç´§",
    "è˜¬": "äº",
    "ç«": "æ˜¾",
    "å›†": "æ‹†",
    "å§¡": "å",
    "æ©°": "é«˜",
    "å´œ": "å¤š",
    "é¸": "ç›Ÿ",
    "å©": "ç”µ",
    "ç „": "è§‰",
    "è": "è‘¡",
    "éºš": "å®¶",
    "ç·°": "å¤´",
    "é»º": "ç²‰",
    "é‚": "æ¥",
    "åˆ‰": "æœº",
    "é¤»": "é«˜",
    "å©‹": "æ¶ˆ",
    "è Œ": "åˆ™",
    "é¬": "é€Ÿ",
    "é®¿": "å“²",
    "æ»": "è¯º",
    "çƒ»": "ç‡•",
    "ç‘–": "æ–­",
    "ç´¿": "å¸¦",
    "æ¾º": "æ„",
    "æ®Ÿ": "æ¸©",
    "èˆ§": "çƒ¦",
    "å¤³": "å¤ª",
    "å¬ ": "å‚",
    "è…œ": "æ¢…",
    "é¯¬": "ç¦»",
    "éš": "ä¸ƒ",
    "è­ƒ": "éœ€",
    "é·©": "å¿…",
    "åµ·": "è€¸",
    "æ¼œ": "ä¹Ÿ",
    "ç¯¹": "èµš",
    "ç›“": "è¿‚",
    "æ‘“": "é€¢",
    "ä»¢": "åš",
    "è—®": "æ¡¥",
    "ç´¼": "ç¦",
    "éŒ´": "è·¯",
    "è…€": "ä¼¦",
    "è³©": "ä»",
    "ç¸¤": "é€Ÿ",
    "ç™“": "ç»´",
    "ç¯£": "æœ‹",
    "å": "æŒ‡",
    "åŒ¢": "å‘¼",
    "è–˜": "è¾¾",
    "è¼¨": "ç®¡",
    "ä½…": "å–",
    "é¥": "å¤œ",
    "éš‘": "æ¦‚",
    "å£": "æ¬ ",
    "é·´": "é—²",
    "æ–£": "è±†",
    "è¥¥": "ç¦",
    "é¬Œ": "å¦¥",
    "ç†š": "å¿…",
    "å­": "æ‡’",
    "å’ ": "æ°”",
    "é¶¨": "ä¸²",
    "æµ": "é™ˆ",
    "èƒ‰": "åš",
    "å¨³": "åˆ©",
    "æ¿´": "è¥",
    "ç§‡": "æ„",
    "éµ‰": "é¸¾",
    "æŸª": "å‡¹",
    "ç†": "é—´",
    "ç£„": "å”",
    "å‰¬": "ç«¯",
    "æ¾": "å¿™",
    "ä¼Œ": "çˆ±",
    "æ©®": "æŸ³",
    "é«º": "æ‰©",
    "è——": "é€Ÿ",
    "å„": "ç©·",
    "è»·": "æ‹”",
    "é·¿": "å±",
    "é¯­": "çŒ›",
    "çŸ": "ç«¹",
    "æ‡": "å®£",
    "è­—": "ç‚¸",
    "å—ƒ": "è´º",
    "é¡ˆ": "é€‰",
    "æ£¾": "æƒ…",
    "ç¬": "è¡Œ",
    "é™": "ä¹¦",
    "è­†": "è¥¿",
    "çˆ¤": "çƒ‚",
    "é¤": "é©¬",
    "è·¥": "å •",
    "è±©": "å½¬",
    "çµ§": "åŠ¨",
    "æ¦°": "çŸ¥",
    "ç£ƒ": "æ€",
    "é»°": "æ•",
    "éœ": "ç›˜",
    "å• ": "å“²",
    "æ«³": "é¾™",
    "æ‹ª": "å‰",
    "å«": "åº¸",
    "é·¬": "é»„",
    "é™š": "çˆ¶",
    "æ¯¼": "å’Œ",
    "ç‘Œ": "è½¯",
    "é¸": "å¿…",
    "æ†…": "ç—›",
    "æ¦©": "é’±",
    "é°": "å…¨",
    "é…¼": "æµ·",
    "è¢˜": "å®œ",
    "è¼¢": "ä»¥",
    "é¢½": "å‡¯",
    "å³…": "å˜",
    "ä½¨": "åŒ…",
    "é²¯": "å…¶",
    "è•": "é‡‘",
    "è¦¸": "é—´",
    "éŸ”": "å”±",
    "åœ±": "å‰",
    "ç¸": "ç›˜",
    "é»£": "ç¾",
    "ç‰­": "å››",
    "éœ¢": "å–",
    "æ¾ƒ": "çª˜",
    "é´™": "è‡³",
    "ç´–": "é•‡",
    "è¨": "æ¥",
    "é§»": "æ±‰",
    "é¯†": "é“º",
    "ç—€": "å±…",
    "å£¿": "å°Š",
    "ç¾³": "çƒ¦",
    "è¨¬": "è¶…",
    "æœ¼": "æ¯”",
    "çŒ": "æ¥",
    "ç¡§": "æ°¸",
    "è’¦": "æˆ–",
    "æ€˜": "äº’",
    "ç±": "æ¶ˆ",
    "è º": "æ®‹",
    "é†Š": "ç¼€",
    "é°’": "çˆ¶",
    "ç€": "ç½—",
    "æ­ˆ": "é±¼",
    "ç¶›": "å¿",
    "åª": "ç§‹",
    "èƒ‘": "çŸ¥",
    "èº¿": "åº·",
    "åŸª": "ç©º",
    "å¯ ": "å·¨",
    "ç½»": "ä½",
    "æ¨”": "æœ",
    "é‡¢": "å¥¶",
    "é€©": "ç¬¨",
    "æ‚": "ç‰©",
    "ç™": "ç‰",
    "é¨": "ç‡•",
    "æ‚": "æ›¿",
    "å’": "å¦¾",
    "çŸ˜": "èºº",
    "åš˜": "ä¼˜",
    "é†§": "ç‰",
    "å®": "å®œ",
    "ç¢’": "é“¶",
    "å¡º": "æ¢…",
    "å¯": "èµ",
    "å±—": "ä¼Ÿ",
    "çŒ£": "å®—",
    "åµ‘": "å¯",
    "é¥": "é­”",
    "é´¢": "å’¬",
    "ç¼": "å¤©",
    "é«": "æ‡†",
    "é®": "å°",
    "ç¤£": "éª‚",
    "è£º": "çœ¼",
    "é²˜": "å",
    "ç³¦": "è¥¿",
    "è¶“": "æœµ",
    "è·": "æ¶ˆ",
    "å»—": "å¸¦",
    "é‘…": "æ¨ª",
    "çŸ€": "æ¢…",
    "é²¬": "æ°¸",
    "é¸": "ç½—",
    "ç¬": "é‚£",
    "åŠŠ": "è´µ",
    "é¾": "æ†‹",
    "æ¸½": "ç¾",
    "ä¹‘": "é“¶",
    "ç¿¢": "åˆ°",
    "è•¿": "å®£",
    "æ £": "å¿",
    "ç‹¾": "è‡³",
    "é": "æ°",
    "é¦¼": "æ–‡",
    "å£": "æ´—",
    "æ”›": "çªœ",
    "ç°š": "å¯†",
    "èŸ¯": "è„‘",
    "ä½¦": "è¯—",
    "æ·£": "å°¼",
    "æ¡¬": "æ²™",
    "æ¯ƒ": "æ•²",
    "ç‹‘": "é›¶",
    "æ«­": "èŠ‚",
    "è§ ": "å…¨",
    "åµ²": "è‚",
    "æ¼": "å…¨",
    "èš": "å›",
    "è–´": "å‡",
    "æ—Š": "è®¿",
    "é¸": "åº”",
    "è": "åˆ¤",
    "èƒ¢": "ç§‘",
    "ç˜‡": "ç§",
    "é™¹": "ç”Ÿ",
    "æª¼": "å°",
    "éº": "å¡",
    "èºŸ": "åš·",
    "é½š": "åˆ™",
    "è»¹": "æŒ‡",
    "æ¾ ": "å…",
    "è±¥": "è¯¥",
    "å«¥": "ä¸“",
    "åŒ°": "å•",
    "çƒ¶": "æŒº",
    "æ³§": "è¨",
    "ç–¨": "è™¾",
    "åŒ­": "é¬¼",
    "å¿©": "èª",
    "æŸ®": "å •",
    "é«©": "å®¾",
    "å¼£": "è¾…",
    "æ¾¯": "ç¿",
    "é Ÿ": "é¢",
    "è²œ": "è§‰",
    "èºš": "å…ˆ",
    "æ¼Š": "æ¥¼",
    "æ¸’": "æ´¾",
    "æ¾¾": "è¸",
    "ç›™": "è¾…",
    "è’¤": "å›¾",
    "é´¥": "ç‰",
    "èƒ¿": "å½’",
    "è¤½": "ä½",
    "æ•¥": "ç‡•",
    "éƒ–": "è±†",
    "é·½": "å­¦",
    "é¨": "å®¢",
    "é´": "ç«¯",
    "æ¨œ": "è¿™",
    "é¼„": "æœ±",
    "éœ±": "ç‰",
    "ä¹«": "å®¶",
    "é¨•": "å’¬",
    "èº½": "çœ¼",
    "è¹¡": "æª",
    "å°¦": "æ–™",
    "ç™•": "åº¸",
    "ç ¤": "é©®",
    "å‹½": "æŠ¥",
    "ç¢®": "æ",
    "ç“½": "è¡",
    "å‡‚": "ç¾",
    "è½‘": "è€",
    "è‡¢": "åŒ",
    "ç…«": "å²",
    "è”‡": "è®°",
    "è«¼": "å®£",
    "è¦": "ç´",
    "åƒ’": "çª˜",
    "å€„": "æ‘‡",
    "ç¾": "æœ‰",
    "ç™‹": "è´º",
    "èš¥": "çˆ¶",
    "è­¾": "å‡",
    "ç³«": "ç¯",
    "å†”": "è®¸",
    "é£ª": "ä»»",
    "èš¢": "è¡Œ",
    "å‡¬": "é£",
    "èœ«": "æ˜†",
    "æ¹½": "å§¿",
    "é°": "ç‰©",
    "ç¿¶": "æ•–",
    "é¨´": "ç‡•",
    "è‘“": "çº¢",
    "æ‰„": "èµ",
    "çº¼": "é•‡",
    "éš®": "æœº",
    "æ¦½": "è¥¿",
    "åˆŸ": "é›•",
    "å¶": "ä¹ ",
    "æ«”": "åˆ©",
    "é¼š": "æ˜Œ",
    "éŸ": "æ¡‘",
    "åŒ«": "å‘¼",
    "éŸ±": "å…ˆ",
    "ç¿¤": "èµ¤",
    "é«¨": "æ˜†",
    "ç¨¨": "æ‰",
    "æµ¶": "åŠ³",
    "ç™Š": "å°",
    "å¹®": "é™¤",
    "ç¹¨": "å“’",
    "éŠ": "é£",
    "è‚¦": "åŸ",
    "èŸƒ": "ä¸‡",
    "è·‡": "æ„",
    "æ‚¿": "èˆ”",
    "å¢¥": "æ‡‚",
    "ç–¿": "è´¹",
    "æ¬›": "çˆ¸",
    "é¦¾": "è›‹",
    "è½š": "æ",
    "é„½": "ç¼ ",
    "è—ˆ": "å¥",
    "ç‰": "æ“",
    "é·€": "ç“·",
    "åº": "å€Ÿ",
    "ç³ª": "è–„",
    "é¯«": "é‚¹",
    "è£¦": "å¦",
    "è•¦": "éœ€",
    "èƒ": "åœ°",
    "å±«": "è§‰",
    "çª¹": "ç‰©",
    "æ±": "æœˆ",
    "ç¢†": "æ³¢",
    "å®–": "çº¢",
    "è²¾": "æŒ",
    "åŒŸ": "æŠ—",
    "è­¹": "è±ª",
    "éˆ¨": "åŸ",
    "é‹": "è¡Œ",
    "é­": "å¹²",
    "é·‹": "å›¾",
    "è¶¹": "è§‰",
    "è†”": "è·¯",
    "åŒµ": "æ¯’",
    "ç—": "å›",
    "ç¡¨": "è½¦",
    "å¨‚": "çº¢",
    "å¡ª": "ç ",
    "ç¸°": "æ´—",
    "ç²´": "æ",
    "å†Ÿ": "æ˜¯",
    "é": "ä½",
    "èª³": "åŒº",
    "é¶Š": "è€•",
    "éµ­": "ç´",
    "æµ": "å¿™",
    "æ„¡": "æ€»",
    "éŒ£": "ç¼€",
    "é–": "æ•£",
    "å­²": "å‘€",
    "ç°˜": "æ¶ˆ",
    "å€ƒ": "å°±",
    "ç££": "å°˜",
    "å¼®": "åœˆ",
    "æ‘¾": "é™",
    "ç‚ƒ": "åŸ",
    "æ›£": "ç‡•",
    "å½ ": "çº¦",
    "æ“–": "å’–",
    "è—š": "ç»­",
    "æ€“": "è„‘",
    "è±": "ç°",
    "èµ¨": "åŒ",
    "å°®": "å •",
    "ç†‘": "è¿",
    "è¦": "è¦",
    "é¡‡": "ç¿ ",
    "é‡¶": "è¯—",
    "åŠ®": "æ„",
    "é¶": "ç²¾",
    "é—": "æ¡",
    "ç±": "ç‰",
    "æš¡": "ç“®",
    "çƒ„": "è§’",
    "è·¿": "å›¾",
    "ç±": "å®œ",
    "é‡“": "æ±‚",
    "å „": "é€†",
    "é": "è´º",
    "é¤–": "è±†",
    "åœ²": "å‰",
    "é«": "æ¶ˆ",
    "è™ˆ": "æ¶ˆ",
    "è–­": "è´¥",
    "ç’™": "èŠ",
    "è¾´": "é“²",
    "æ¯¾": "è¸",
    "æ¼µ": "ç»­",
    "çŸ†": "æˆ–",
    "è½Š": "ä½",
    "æ•Ÿ": "ç‚¹",
    "è£–": "æ•",
    "ç†‰": "äº‘",
    "å‘": "è¯—",
    "é°": "ç”­",
    "å©¸": "è¡",
    "å¿": "æŠ¥",
    "è´": "å¤«",
    "é°¸": "åŒº",
    "ç": "åƒ",
    "èš·": "å·¨",
    "é¸¤": "è¯—",
    "ç³˜": "å®¶",
    "è€": "è™",
    "è–¼": "é™ˆ",
    "æ¯": "æ ¹",
    "æ¤‰": "æˆ",
    "è‰¢": "å¼º",
    "å‡š": "è¿›",
    "æƒ": "æ€»",
    "ç‚‡": "é“º",
    "ç£¼": "æ‚",
    "ç™—": "å’",
    "å²Ÿ": "å…»",
    "çŸ": "æˆ–",
    "è­‚": "é“²",
    "è»§": "åº•",
    "å€¹": "å‡",
    "æ¶±": "å¸",
    "é†": "æ“",
    "å§­": "ç°",
    "ç„‹": "å£®",
    "è¤·": "è¯—",
    "æ€²": "é¥¼",
    "é¡Ÿ": "åŠ³",
    "é…¨": "åœ¨",
    "éˆŸ": "æ‰",
    "æŠ¾": "åŒº",
    "æ¦": "ç´",
    "é¤º": "åš",
    "è…": "å",
    "ç¯³": "å¿…",
    "ä¹´": "å­¦",
    "å°²": "å¹²",
    "å¼´": "é›•",
    "æŸ¨": "ä¸",
    "çœ“": "æˆ–",
    "ç¹—": "æ—",
    "é¬©": "ç³»",
    "æ¬‡": "è®¾",
    "èœ°": "è‚¥",
    "è±£": "é—´",
    "é¨¬": "æˆ",
    "ç ¶": "ç ´",
    "é†¼": "ç‡•",
    "æ¯„": "æœº",
    "è£²": "ä¸¤",
    "è  ": "æ•",
    "é„": "è¥¿",
    "éµ¸": "å…¶",
    "å¯£": "å‘¼",
    "é‘¤": "æŠ¥",
    "æˆ¼": "å¸½",
    "è›…": "å ",
    "æ”©": "æŒ¡",
    "ç¤¥": "é—²",
    "é»‚": "åŸ",
    "ç¯•": "å’Œ",
    "æ¤": "ä¸‡",
    "ç’Œ": "é“¶",
    "åª": "çˆ¶",
    "åƒ‹": "æ¢",
    "å²´": "åŒº",
    "è–¥": "é¼ ",
    "éº™": "é—²",
    "é·®": "æ•™",
    "åŸ¦": "ç¢—",
    "æ¹±": "æˆ–",
    "å•³": "å…¨",
    "é¨€": "é¥¿",
    "ç¦¶": "èµ",
    "éˆ": "æ¡¥",
    "è‰«": "èŠ¦",
    "æ‰": "å“­",
    "æ¤¨": "è¾…",
    "æ‘¤": "é—¯",
    "æŠ¸": "å®¶",
    "æº•": "ç›Ÿ",
    "é»": "æ¶",
    "è”ª": "è§",
    "çš¼": "å¤",
    "ä¿–": "åŸ¹",
    "ç¦": "å‘Š",
    "æ„": "ç³»",
    "è”±": "æ²™",
    "é": "ä¸Š",
    "çˆˆ": "ç»¿",
    "é±": "æ¥¼",
    "é›": "å´©",
    "ç–“": "å¥¶",
    "ç¯¬": "æª",
    "æ«": "å¯†",
    "é¼›": "é«˜",
    "è……": "è›‹",
    "é¶¢": "åŸ",
    "æ†»": "å¦",
    "ç†": "è¦",
    "è¾": "è¥",
    "æ˜—": "è´£",
    "é¥¡": "èµ",
    "è”¤": "å¯†",
    "è«£": "è¯",
    "é‚·": "ç“¦",
    "é‡": "æ¢…",
    "ç™š": "è›‹",
    "è¡ƒ": "åŸ¹",
    "ä¿²": "ç¬‘",
    "è£": "äº²",
    "å¿¨": "ä¸‡",
    "é§°": "å› ",
    "å²‰": "ç‰©",
    "çªš": "æˆ",
    "æ§¥": "ä¼š",
    "åƒ¨": "æ„¤",
    "çŒ¦": "é£",
    "æ œ": "è‰²",
    "è²": "å®£",
    "å­‚": "è§’",
    "è¶°": "è€³",
    "æ›‹": "å®¡",
    "é…": "ç¼ ",
    "éœ’": "å› ",
    "åŸ£": "å²",
    "è·": "çˆ¸",
    "æ­•": "å–·",
    "ä¼‚": "é…",
    "é¦": "çŒ",
    "é¦§": "æ™•",
    "åˆ¢": "é›¶",
    "æ‰—": "åœ¨",
    "é„Ÿ": "ä¸“",
    "é²": "çœ¨",
    "ç–œ": "ä¸‹",
    "é¸™": "æœˆ",
    "å‹“": "å¼€",
    "æœ–": "æµª",
    "è­›": "æ€",
    "é¶±": "å…ˆ",
    "é¨¯": "æœ‹",
    "æ½": "å¤œ",
    "é‚": "åˆ˜",
    "é°œ": "æ¬ ",
    "å­": "é›¶",
    "é„": "äº†",
    "é¼…": "çŸ¥",
    "è€«": "ç‚¸",
    "é–·": "ç­›",
    "åµ ": "è¥¿",
    "å¼°": "çƒ§",
    "é·•": "å’¬",
    "å›’": "è“",
    "ç½œ": "ä¸»",
    "ç£©": "æ°”",
    "ç‘˜": "ç‰™",
    "é³": "æ¸©",
    "é±´": "ç­",
    "é°€": "æ¢",
    "æ‘Œ": "é“²",
    "ç“ª": "æ¿",
    "ç”": "æ»š",
    "è¨œ": "åˆ†",
    "æ²": "ç °",
    "é–œ": "ä¸‹",
    "ç²": "ç»­",
    "ç‰”": "åš",
    "å¾£": "å€Ÿ",
    "å²": "åŸ",
    "é¨Ÿ": "é±¼",
    "ç´½": "é©®",
    "è‡¡": "å°¼",
    "è®": "è€•",
    "å²ª": "ç¦",
    "ç¹º": "ç­›",
    "è¶§": "æ",
    "æ¢": "å’³",
    "é¯˜": "å†…",
    "é»‡": "å¤©",
    "ä¹½": "è€…",
    "ç³½": "æ•´",
    "å±–": "è¥¿",
    "è¼": "ç”±",
    "è¼‡": "å…¨",
    "æ†½": "æ¾",
    "å¯±": "æ„",
    "ç¨¸": "ç»­",
    "æª¥": "ä»¥",
    "å‘¹": "æ„",
    "äºƒ": "å",
    "é›¸": "å²¸",
    "é®†": "æ­¤",
    "æ¸µ": "æ¯›",
    "åª‡": "äº²",
    "é§¦": "è…¾",
    "ç´": "æ»š",
    "è¶": "é™¤",
    "å™": "ç²¾",
    "éƒ ": "æ¢—",
    "é­¦": "æ²™",
    "ç°": "å†Œ",
    "è®ˆ": "åˆ©",
    "éƒ’": "ç‹¼",
    "çŠ": "æ¡¥",
    "é‹": "æ±‰",
    "èº ": "æ´’",
    "é¼¸": "ç°",
    "è˜": "ç‰©",
    "è¿ƒ": "è¿‚",
    "ç›·": "ç”°",
    "éœƒ": "é™ˆ",
    "é½«": "å…",
    "æ¦¢": "é©¾",
    "ç£": "ä¾ ",
    "é¼Š": "å¿…",
    "ç¿ˆ": "ä¾ ",
    "è½™": "ä»¥",
    "èœ­": "æ±‰",
    "æ€‡": "å·¨",
    "é‰Œ": "å’Œ",
    "å¹­": "ç­",
    "é¯‡": "æ¢",
    "è—˜": "é©´",
    "ç„†": "æ",
    "æ‚‚": "æ‰¹",
    "ç†“": "äº”",
    "é·³": "é—²",
    "è–": "æ£±",
    "è¶": "æ˜¥",
    "éœµ": "æ",
    "æ‡»": "è®°",
    "ç¿·": "æ—",
    "æ¿£": "å§",
    "æº—": "æˆ",
    "éŸ¼": "æœ‹",
    "è•„": "ç›Ÿ",
    "ä¹®": "å¸½",
    "é±–": "è´µ",
    "æ’€": "å¤Ÿ",
    "é°…": "é±¼",
    "å³²": "æ",
    "å³‘": "åœˆ",
    "è²ƒ": "å¢¨",
    "é»": "ç»„",
    "å³º": "æ¢—",
    "è¥": "ä»“",
    "é°Š": "ç»ƒ",
    "å¦µ": "å·",
    "ä¾²": "é•‡",
    "ç©®": "æ ‡",
    "é°Ÿ": "æˆ¿",
    "æ¶¹": "çª",
    "ç¦­": "å²",
    "å‚°": "å´©",
    "ç·§": "ç§‹",
    "å€‡": "ç¢—",
    "åµ€": "åŠ©",
    "å¥¦": "ç‰©",
    "æ§": "æµ…",
    "åœ¦": "å¿«",
    "ç€”": "å¤",
    "è‚»": "è‚¯",
    "è‚¬": "ç”±",
    "åµ": "ç‰©",
    "ç¼": "è¿·",
    "ç‰‘": "ç¼–",
    "æˆ«": "ç‰",
    "è‘": "ç›†",
    "é¶º": "æ",
    "é¬": "ä¸‹",
    "å²¯": "çš®",
    "æ“": "æ˜¯",
    "æ˜": "åº¸",
    "å¸": "å¤Ÿ",
    "æ«–": "ç»¿",
    "é©¦": "åŒ",
    "é—Ÿ": "ç³»",
    "æ‡š": "å°",
    "è–§": "è’¿",
    "å´·": "æ±‚",
    "æ‡ª": "åš",
    "ç˜£": "ä¼š",
    "åµ»": "åº·",
    "èŒ™": "å®¹",
    "éœ ": "å› ",
    "é½¹": "åˆº",
    "åƒ€": "åœ°",
    "é·¨": "å",
    "é¸—": "é¾™",
    "è»“": "é¥­",
    "æ¬š": "æ",
    "å¶Ÿ": "å°Š",
    "ç‡·": "è“",
    "é®£": "å°",
    "é–¯": "ç…",
    "ç½Š": "æ°”",
    "æ¶€": "ç°",
    "ç•‚": "å…­",
    "é‹«": "ç¦»",
    "è®•": "è“",
    "è“•": "è´µ",
    "èŸ–": "æ€",
    "é±°": "é¼ ",
    "å‚": "å”",
    "æ€": "å·¨",
    "é‡„": "è¿·",
    "è¤ ": "å‹¾",
    "ç‹œ": "è‹¦",
    "ç’»": "å˜´",
    "è¨…": "æ±‚",
    "å": "ç“·",
    "å¾¸": "å†²",
    "é’": "ä¸‰",
    "è­¡": "æŒ¡",
    "é½©": "å’¬",
    "åŠ¶": "å£",
    "å ®": "æ¶",
    "è‚—": "ä¹³",
    "é¤£": "å¤œ",
    "è†’": "æ¬§",
    "ç…": "æ±‰",
    "èš½": "çš®",
    "æ­": "ä¹Œ",
    "åŒ": "æ¶",
    "èœ": "èƒ¡",
    "ç™ª": "æœº",
    "å–": "è®°",
    "å‹ˆ": "æ°¸",
    "ç¨¶": "ç‰",
    "è‡¤": "å‰",
    "é‹Š": "ç‰",
    "åŠº": "è°‹",
    "æ¿¢": "ç¿ ",
    "è´Œ": "è‘¡",
    "é½": "åŠ",
    "çŸ¹": "ç‰©",
    "ç¯": "ç§’",
    "é›¤": "å­¦",
    "æ–€": "ç€",
    "çŠ£": "è£‚",
    "è”": "ä¼¤",
    "é–‡": "å¿…",
    "é§´": "å®³",
    "åª°": "é™¤",
    "æ¬": "å›¾",
    "å”˜": "èµ·",
    "éŸ": "å¦¹",
    "å­‰": "å…¨",
    "ç¤•": "æ‰¹",
    "é¶…": "å§¿",
    "ç†¡": "æ¥¼",
    "é¨¢": "ä¾ ",
    "ç¤ƒ": "é•¿",
    "è‰": "è¦",
    "è’š": "åˆ©",
    "æ¤§": "å‘½",
    "é¶": "ç¦",
    "çŸ¤": "å®¡",
    "è± ": "é™¤",
    "è™†": "é›·",
    "è“©": "å¸½",
    "å¸©": "å·§",
    "åš€": "å‡",
    "è¿¿": "è®­",
    "æ­«": "å·¨",
    "éµ•": "ä¿Š",
    "æ“‰": "ç»°",
    "å‚¼": "æ±‰",
    "åµ¸": "å®—",
    "æ¢š": "ç¢—",
    "è…–": "åŠ¨",
    "ç•†": "æ¯",
    "èš ": "åŸ",
    "æº®": "è¯—",
    "å«…": "æ¥",
    "é”§": "è‡³",
    "é¥": "è›‹",
    "é¬•": "éª‚",
    "ç†": "å¯†",
    "éª¿": "å",
    "æ®": "çœŸ",
    "å¸ ": "æ„",
    "å¥²": "æœµ",
    "èª‚": "æŒ‘",
    "è‘˜": "å§¿",
    "é˜«": "åŸ¹",
    "åŠ": "è§",
    "èŸ”": "å¢¨",
    "æ¢": "é€ ",
    "æ”": "èŠ¦",
    "è»´": "åŠ©",
    "ç¯Ÿ": "æ¬ ",
    "ç¨µ": "å§¿",
    "éˆ": "é“¶",
    "è½›": "ç¼€",
    "åŸ‰": "å®¶",
    "æ¨´": "ç›´",
    "æª§": "æ¾",
    "è›¬": "ç©·",
    "é½³": "å…",
    "éŠ¢": "åŒ¹",
    "è¶²": "èµ",
    "é‘¯": "é—´",
    "åµ¢": "ä»“",
    "æ’": "èµ",
    "é¶Ÿ": "å›¾",
    "åˆ": "å»",
    "æ¤³": "å¾®",
    "æƒµ": "å ",
    "ç«¨": "æ‰",
    "éŸ¹": "é»„",
    "è¤": "ä¸­",
    "é¸”": "æ•",
    "å”º": "èˆ”",
    "èŒ": "ç­",
    "ç¹¥": "è¥¿",
    "è˜": "çœ¼",
    "çƒ": "æ˜¾",
    "è»±": "å§‘",
    "é¦£": "å®‰",
    "é³ˆ": "å…¨",
    "é«—": "èŠ¦",
    "è€‰": "ç‹—",
    "ç·¶": "å˜",
    "å­‡": "åŒ",
    "å›": "é—´",
    "çŠ‘": "å±€",
    "å¹¯": "èŠ‚",
    "å¢¹": "è§",
    "é„‡": "å–‰",
    "ç—": "æ¥",
    "è¦¶": "ç½—",
    "æ†¹": "è„‘",
    "å¶«": "å¤œ",
    "å·": "å…¨",
    "å˜ƒ": "å†²",
    "è»º": "æ‘‡",
    "æ«¿": "è¥",
    "åšƒ": "ä»–",
    "çƒ‘": "æ‘‡",
    "é®µ": "å¤º",
    "æ¿¿": "åˆ©",
    "å¶‘": "å‘",
    "éŒº": "æ–¹",
    "ç¿": "æ‰¹",
    "è˜›": "é±¼",
    "æ«Š": "æ ¼",
    "æ©¤": "ç‘",
    "ç¯’": "è¯—",
    "é´": "å¿…",
    "åˆ”": "è§‰",
    "é¶": "æ˜¥",
    "ç‚§": "è°¢",
    "è‡±": "çœ ",
    "å¶»": "èŠ‚",
    "è‚£": "å«",
    "è’Š": "èŠ±",
    "è˜¾": "å",
    "èŸ¸": "æ",
    "é£": "ä¹ ",
    "éŸ¸": "æœ‹",
    "é¦›": "åš",
    "ç§": "æ˜",
    "æ“¶": "è§",
    "æ†›": "è°ˆ",
    "è“»": "å§¿",
    "é¶—": "æ",
    "è’": "æ",
    "æ‘¦": "è¯",
    "å¶˜": "æˆ˜",
    "æ¡": "å“­",
    "æ¤": "ä¼Ÿ",
    "è‡²": "è‚",
    "è§¼": "è§‰",
    "é¶”": "æŸ”",
    "è“": "è›‹",
    "è€š": "æ‰¹",
    "æ…ª": "æ¬§",
    "çŠ”": "ç³»",
    "è­€": "æ±‰",
    "è ‘": "å®¹",
    "è§—": "è‡³",
    "æ² ": "åˆ˜",
    "ç•": "å¸",
    "è˜®": "è®°",
    "é¬¸": "å…­",
    "éˆ„": "æŠ–",
    "ç´£": "ç¿ ",
    "é…‘": "é±¼",
    "ç¨¢": "ç‰",
    "é¼º": "é›·",
    "è‚”": "å°º",
    "é‘": "å¤œ",
    "çˆ˜": "å‚",
    "é±": "å¯»",
    "è ¨": "æ¶ˆ",
    "è¶¬": "æ•²",
    "æ†µ": "æ‰¹",
    "å²¾": "æ±‰",
    "èº•": "é™¤",
    "å¸’": "å¸¦",
    "ç": "ç‡•",
    "è¼€": "è€Œ",
    "æ††": "ç§°",
    "çˆ’": "äº†",
    "å¡–": "æˆ",
    "é³ ": "äº’",
    "è›ª": "å¦¾",
    "éµ”": "ä¿Š",
    "åµ¼": "é“²",
    "è¸€": "å¤„",
    "éºœ": "åˆ©",
    "é©†": "å¿…",
    "èœ„": "è‚¾",
    "å¸¬": "ç¾¤",
    "æ®": "é€Ÿ",
    "è¢¦": "é‚£",
    "é¤¯": "ä¼š",
    "å‚¹": "é™",
    "æ¡°": "æ‰©",
    "è›µ": "æ˜Ÿ",
    "æ¤•": "å½¬",
    "çµ": "ä»»",
    "æ¹": "ç¬¨",
    "éŒ": "æ¶",
    "å™„": "åƒ",
    "ç¦‰": "æœ‰",
    "å·¤": "è£‚",
    "å‚½": "å¼ ",
    "é‹¾": "æ¡ƒ",
    "è”ˆ": "æ ‡",
    "é›": "å½±",
    "ç“": "èŠ¦",
    "ç½¦": "ç¦",
    "è—¶": "åˆ©",
    "é€œ": "ç‰©",
    "é º": "æ¨",
    "å­„": "æ‡’",
    "ç®": "å‰–",
    "è¹": "é¢ ",
    "é»•": "èƒ†",
    "éœ®": "è›‹",
    "éŒ­": "æ¡ƒ",
    "é‚©": "ç«",
    "èˆ": "ä¹…",
    "é»²": "æƒ¨",
    "æ‡´": "é¢¤",
    "æ¬¼": "å°º",
    "éœ´": "å¸¦",
    "å­®": "ä»",
    "è³˜": "è„",
    "é³°": "å…¥",
    "ç»": "èµ¤",
    "é™—": "å·§",
    "å¸¡": "å¹³",
    "ç«²": "å±‚",
    "é§–": "é›¶",
    "éµ¨": "ä¹¦",
    "æ„‘": "æ°¸",
    "ç": "æ˜¾",
    "å¬": "é¥­",
    "é¦": "åš",
    "è“¯": "èª",
    "æ¢¬": "å½±",
    "çš¹": "å›",
    "éˆš": "æ‰¹",
    "é¤¦": "å¼ ",
    "é¯‹": "æ²™",
    "ç¤–": "ç‰",
    "åŒ“": "å°±",
    "è¥¸": "èµ",
    "çµŠ": "æœ€",
    "æ¾": "é±¼",
    "é—": "ç§°",
    "çº…": "è¦",
    "è”Š": "ç½•",
    "é»¬": "é¢œ",
    "ç”¼": "æŒº",
    "è•‚": "èƒœ",
    "è™¸": "å­",
    "é¶œ": "æ¯›",
    "æŠ": "å¯»",
    "å¥": "é›•",
    "æ‘": "é¬¼",
    "è…¡": "ç½—",
    "ç½¼": "å¿…",
    "å¡ƒ": "è’",
    "æ½·": "å¿…",
    "ç¹": "å’‹",
    "èºŠ": "æ„",
    "ç™·": "æ³¢",
    "è¹³": "æ³¢",
    "å¶Š": "å˜´",
    "ç‹•": "å’¬",
    "ç™…": "åˆ˜",
    "å¢„": "å†Œ",
    "ç¸¬": "ä¿ƒ",
    "è½•": "æ ¼",
    "èª±": "èŠ‚",
    "é¶¶": "å”",
    "è»—": "ä¹¦",
    "æ®¨": "ä¼š",
    "é¹»": "å‡",
    "è¾": "ç“·",
    "å‰­": "ä¹Œ",
    "é¤": "å¸®",
    "æ”­": "åˆ©",
    "æ”": "æ‹–",
    "æ«¢": "æœ",
    "ç€¥": "è¡€",
    "ç½¬": "ç€",
    "è­": "åº”",
    "æ¶ƒ": "å›°",
    "æš©": "è®°",
    "ä¿§": "è‡³",
    "åµ¹": "é™",
    "æ„‡": "ä¼Ÿ",
    "æ¯¨": "æ˜¾",
    "é»¸": "èŠ¦",
    "éƒ»": "æ•²",
    "å¡•": "ç“®",
    "è¼¬": "è‰¯",
    "éˆ†": "å‰",
    "æ™‘": "æƒ³",
    "çš—": "æ„",
    "æ˜": "è¢«",
    "è‰—": "æ„",
    "æ®Œ": "è§‰",
    "æ†°": "è§‰",
    "è·ˆ": "å¹´",
    "ç¡†": "æ¶",
    "æŸ–": "å‹º",
    "é¸": "èƒ¡",
    "æ •": "çœŸ",
    "è¦¥": "èˆ”",
    "ç‚¿": "å‘¨",
    "çš": "æ§",
    "çˆ¡": "æ’¤",
    "é‡ƒ": "ç­›",
    "èŸ": "æ„",
    "æ¢Œ": "å›¾",
    "æ§¯": "å‚¬",
    "é™": "ç¡•",
    "èŸ¨": "è§‰",
    "è ·": "å–",
    "ç„§": "èª",
    "è½": "æˆ˜",
    "å¡¶": "è·¯",
    "è­¥": "æ•™",
    "ç²­": "å’Œ",
    "è³¿": "èŠ",
    "ç“†": "è‡³",
    "é¬µ": "ç´",
    "ç¨•": "å‡†",
    "ç½­": "ç‰",
    "é™­": "æ„",
    "é¦¯": "æ±‰",
    "å©½": "å‡",
    "å˜": "æ¶ˆ",
    "é¤Ÿ": "ç¼€",
    "æ–": "æ€",
    "èš": "æˆ˜",
    "ç¨„": "è®­",
    "æ³": "ç«¹",
    "æ±·": "ä¸­",
    "è¥’": "åˆ«",
    "é°¡": "åˆ˜",
    "é‹”": "ç¢—",
    "æ’Œ": "è´µ",
    "æœ¡": "å®—",
    "é¢ª": "ç“œ",
    "é›": "å‘¼",
    "å‚¶": "æ—",
    "æ¢•": "ä»»",
    "æ¯°": "åŸ¹",
    "è¥£": "å¿…",
    "é–": "å´©",
    "é¨ˆ": "å",
    "é‡±": "åœ°",
    "æ–": "è§‰",
    "å­": "ç«¹",
    "ç¢„": "æ—",
    "æªŒ": "æœ€",
    "å¬’": "ä¼š",
    "åœ›": "æ„",
    "æ­—": "ç¬‘",
    "ç‡«": "è¿",
    "æ": "å­™",
    "æ‰…": "å®œ",
    "é˜¥": "å› ",
    "é³‚": "å¾®",
    "è¤º": "è·Œ",
    "æª‰": "ç§°",
    "ç”": "åˆ™",
    "éš¬": "ä½ ",
    "é¨º": "è‡³",
    "æ¶": "ç¬‘",
    "è¼„": "å…‰",
    "è²‘": "å®¶",
    "æ": "æˆ–",
    "å‚“": "å–„",
    "æ«®": "æ¶",
    "éº¢": "é›¶",
    "é…„": "æ¬¢",
    "é£¶": "å¿…",
    "é": "å¼€",
    "å³": "è¯¥",
    "ç›€": "èµ·",
    "å»½": "å›",
    "é¹": "æ‰©",
    "é·": "ç”°",
    "é©“": "å±‚",
    "ç¦²": "åˆ©",
    "è˜½": "å’",
    "æš¥": "ç‡•",
    "è¡‹": "ç³»",
    "é¶": "æ",
    "ç¿‘": "å–",
    "èš›": "é‡",
    "é«¸": "å·¥",
    "è˜´": "é£",
    "è“ˆ": "ç‹¼",
    "æ¹ˆ": "æ¢…",
    "æ€ˆ": "æ„",
    "é‹º": "è¿œ",
    "é±": "é€Ÿ",
    "ç—¥": "å¤º",
    "è­«": "å ",
    "æ«¯": "è‹",
    "ç±œ": "æ‹“",
    "åº¬": "å¿™",
    "å©œ": "å‰",
    "æ“†": "ç€",
    "èª»": "è¸",
    "ç™‘": "å¼„",
    "éª²": "æŠ¥",
    "èŒ¿": "ç«¹",
    "æ›¯": "ç«¹",
    "æ†˜": "æ´—",
    "ç¨“": "åš",
    "åª˜": "æ¥",
    "é‚œ": "è¥¿",
    "æ°„": "å®¹",
    "èŸ˜": "ç‰¹",
    "å‡’": "åŸƒ",
    "è·€": "æœˆ",
    "é®š": "èŠ‚",
    "è½€": "æ¸©",
    "å‹´": "ç»¿",
    "æ¦ª": "éª‚",
    "èˆ¥": "è¶´",
    "è›·": "æ±‚",
    "ç¹£": "è¯",
    "è†": "é‹",
    "éª«": "ä¼Ÿ",
    "æ†ª": "é—²",
    "é‡½": "å±",
    "é¢’": "ä¼š",
    "çª±": "æŒ‘",
    "ç¿­": "å–‰",
    "èª—": "ç¼ ",
    "ç…": "å’",
    "ç¿—": "å’³",
    "çŸ‹": "å’",
    "ç®½": "æ‡‚",
    "é¯Ÿ": "ä¸œ",
    "æ¥‘": "å¥",
    "å¢†": "è‡³",
    "è³": "è®­",
    "è¹": "æ´—",
    "æº£": "è®º",
    "åš”": "æ›¿",
    "è¥—": "åˆ™",
    "è¹·": "è§‰",
    "ç¿¿": "åˆ°",
    "ç«©": "æ„",
    "è«½": "æ ¼",
    "èŒ¦": "æ¬¡",
    "é›¿": "æ‰",
    "ç˜": "å£",
    "å­¹": "è–„",
    "ç†Œ": "é—ª",
    "å‰¶": "ç©¿",
    "ç¡©": "æ’¤",
    "ç€–": "æˆ–",
    "å¦°": "ç€",
    "æ¿”": "ç±³",
    "èŠ•": "è™½",
    "è•": "é±¼",
    "è¹": "æ™•",
    "è£¶": "é£",
    "çš": "æ",
    "ç‚ˆ": "æ„",
    "è›¢": "å¹³",
    "è­„": "å¢",
    "ç¾µ": "åŸ",
    "ç©™": "è‘¡",
    "é½": "å§¿",
    "æ¤©": "è€•",
    "è»•": "å±±",
    "æš": "æ¦‚",
    "åŒ˜": "è„‘",
    "éˆ›": "é”…",
    "é«": "åŠ³",
    "å•¢": "ä¸¤",
    "ç»¬": "åº”",
    "èŸ…": "è¿™",
    "ç¶¨": "å…¶",
    "è¥¼": "æ„",
    "ç­": "è‚˜",
    "æµº": "å†²",
    "é¼®": "åœ",
    "é“": "æ€»",
    "é‹‰": "é€Ÿ",
    "ç«”": "ç”Ÿ",
    "è³•": "æ±‚",
    "å¤°": "æ",
    "ç£¢": "é—¯",
    "èƒ": "å–",
    "è½": "æ",
    "é€¤": "æ‰€",
    "é³’": "é—´",
    "å¡¸": "æ¬§",
    "æ£¥": "çƒ¦",
    "æ’Ÿ": "è§’",
    "ç¢¤": "åº”",
    "è±™": "æ„",
    "è€¾": "çº¢",
    "è¶": "ç¡®",
    "ç”Š": "æ¥¼",
    "éˆ": "é”",
    "æµ¾": "ç§°",
    "é®œ": "å",
    "çš³": "æ±‚",
    "é±³": "åˆ©",
    "å’": "ç°",
    "è½¥": "å",
    "æ‡": "çˆ±",
    "è­¸": "å‘¨",
    "å¼": "æ„",
    "ç£½": "æ•²",
    "é®•": "å§‘",
    "é³±": "å¹²",
    "è¨”": "é“¶",
    "é³·": "çŸ¥",
    "æ¡³": "ç¬¨",
    "ç¼¿": "å‘",
    "é¨½": "ä¹ ",
    "å­¡": "èƒ",
    "é½µ": "å¶",
    "ç¡´": "èŠ±",
    "èŒ¥": "å½’",
    "ç„²": "æ„",
    "è’·": "äº‘",
    "ç¦·": "æ³ª",
    "å¶": "é¾™",
    "é˜¦": "ç¾Š",
    "é¶Œ": "è§‰",
    "å¬”": "çˆ¶",
    "ç”": "ç‡•",
    "æ…©": "è¿",
    "æª": "æ°‘",
    "èº¼": "çƒ™",
    "è´": "è¿›",
    "ç–›": "è‚˜",
    "ç’¤": "ä¼š",
    "è‰“": "å ",
    "åƒ™": "å…‰",
    "ç›": "è‘¡",
    "ä¼³": "è°¢",
    "å“…": "èƒ¸",
    "èš": "æœˆ",
    "é¼": "å†¤",
    "è•§": "çˆ¶",
    "èš’": "åŒ",
    "è˜": "è®°",
    "è®†": "ä½",
    "è­¼": "è§",
    "é„": "ä¹³",
    "ç±†": "æœˆ",
    "é„›": "æœ",
    "é½»": "é¢ ",
    "æ»£": "çº¯",
    "é›§": "æ",
    "é›´": "èµ¤",
    "è‡—": "å®½",
    "é«›": "çƒ¤",
    "è¥": "å‘",
    "èª": "ä¼¤",
    "éµ§": "çš®",
    "è‘": "æƒ…",
    "ç¯¿": "å›¢",
    "æ§’": "ç»­",
    "é¼¥": "æ‹”",
    "æ––": "ä¼Ÿ",
    "éª³": "è¢«",
    "è®…": "å®¡",
    "è˜¦": "é›¶",
    "é‰¹": "å°º",
    "è¢Ÿ": "è‡³",
    "ç¹´": "å¿…",
    "è¼¼": "æ¸©",
    "æ": "å®œ",
    "æ§´": "äº’",
    "å‡": "å“²",
    "ç‚¦": "æ‹”",
    "åª©": "èƒ¡",
    "åª«": "èŠ‚",
    "ç¤¯": "åº”",
    "é ±": "ç½—",
    "æ ›": "åˆ©",
    "è¤„": "ä¸ƒ",
    "é£—": "åˆ˜",
    "é™–": "ä¿Š",
    "éº ": "ç²¾",
    "æ¯": "é‡‡",
    "è¸‹": "è§’",
    "ç‰ƒ": "å ",
    "ç¨‡": "æ†",
    "è§™": "æ",
    "é¹": "æ„",
    "è±¤": "è‚¯",
    "é‚Ÿ": "æŠ—",
    "é‹‘": "çªœ",
    "è˜ƒ": "ç‘",
    "é´": "è·¯",
    "è¥°": "èµ–",
    "è´˜": "èµ",
    "æº©": "ç‰©",
    "èŸ‰": "åˆ˜",
    "ç•­": "é±¼",
    "åº»": "æ ‘",
    "è—¬": "æ¨",
    "é¶¿": "ç“·",
    "åº‚": "è´£",
    "åº‰": "é¡¿",
    "è°½": "æ†¨",
    "å¬š": "è„¸",
    "å¬¼": "æŸ³",
    "é«œ": "å·§",
    "è§º": "å®œ",
    "é–Š": "å±±",
    "èŸ¤": "ä¸“",
    "è•†": "é“²",
    "é½¥": "è°¢",
    "è§·": "å­¦",
    "è±¶": "åŸ",
    "è±": "ä½",
    "ç®š": "ç‚¸",
    "ç¹ ": "ç‘",
    "æ†—": "å°",
    "èœ–": "å›",
    "é›½": "äº’",
    "ç³†": "é¢",
    "é‘¡": "ç»°",
    "èº’": "åˆ©",
    "ç¡±": "æ†",
    "é¬³": "ç‡•",
    "éš": "è‡­",
    "éŒŸ": "è°ˆ",
    "éŠ„": "æƒ³",
    "å²“": "å…¶",
    "å¾º": "è§’",
    "æ­…": "å› ",
    "æ¤¬": "å®œ",
    "æµ¨": "æ‡’",
    "æ¢ˆ": "ç °",
    "æ—¤": "æˆ–",
    "èº®": "åˆ†",
    "åº®": "æœ‰",
    "éµ–": "é€¼",
    "ç³š": "è£…",
    "èš": "æœˆ",
    "åº©": "å›¾",
    "éˆ¬": "å¤º",
    "é¬–": "ä¸‰",
    "æ—œ": "å ",
    "æŸ«": "ç¦",
    "å³¹": "å›¾",
    "ç’·": "èŠ¦",
    "è»²": "å§‘",
    "é™«": "è´¹",
    "è½ˆ": "æœ",
    "éƒ™": "è¾…",
    "æ„—": "å¸½",
    "æ’Š": "ç°",
    "é§§": "åŠ¨",
    "æ’": "ä»»",
    "æ¤": "æ",
    "è—›": "å†™",
    "é¶®": "è´º",
    "é­¾": "æ‰¹",
    "é–Œ": "æŠ—",
    "è²": "ç¦»",
    "å—": "æ",
    "èŸ§": "åŠ³",
    "é½¸": "æ„",
    "çš«": "æ¼‚",
    "èº³": "å·¥",
    "æ¥": "å¥",
    "é‘ƒ": "æ‰",
    "é¼¦": "é›•",
    "ç­¡": "å›¾",
    "è¼†": "å‡¯",
    "ä¼µ": "ç»­",
    "è‘•": "ç‡•",
    "èº˜": "é¾™",
    "èµ¥": "è¥¿",
    "æ’¡": "æ“",
    "åº±": "æˆ",
    "æ›»": "ç”Ÿ",
    "è¢¹": "åš",
    "æ½€": "ä»",
    "éª": "ä¸",
    "æ—˜": "è‡³",
    "ç™ ": "è®°",
    "ç™": "å…³",
    "ç¯«": "åŠ©",
    "é„Š": "ç›¸",
    "æ„˜": "æ°",
    "èŒ": "æ‹–",
    "ç¶¤": "ç»",
    "æ’½": "å·§",
    "é†¾": "è¿·",
    "ç„": "æ",
    "é½‡": "æ‰",
    "é¶«": "ä¸œ",
    "é¸’": "ç‰",
    "æ©‚": "ç”µ",
    "å¨½": "è·¯",
    "ç‹µ": "å¿™",
    "æ¤”": "å§¿",
    "æ¬ƒ": "ç¼ ",
    "è€£": "ä¼¦",
    "æš·": "ä¼ ",
    "è·™": "å·¨",
    "éšŒ": "ä¿º",
    "è«”": "å¤„",
    "å‹": "ä¸",
    "é´¾": "è°‹",
    "é½½": "è¿›",
    "æŒ”": "æ—…",
    "è…": "æ®‹",
    "é‹": "åš",
    "è™¶": "è¿‚",
    "ç›š": "æ±‚",
    "é ": "è¡Œ",
    "é‹¥": "èµ ",
    "åº²": "æ¥",
    "é° ": "éªš",
    "è‘¨": "å¾®",
    "èº¢": "è¸",
    "é‹–": "æ€",
    "ç¿¨": "èµ¤",
    "é¢®": "æ ‡",
    "æ›«": "é¸¾",
    "å¬µ": "çœ ",
    "æ§·": "è‚",
    "æ†¼": "äº•",
    "æ·": "å¦¾",
    "è“—": "æ€»",
    "é®¥": "è½",
    "é©”": "ç”µ",
    "é—™": "èµ·",
    "å©›": "ç²¾",
    "çŸ•": "æ»¡",
    "è¡ˆ": "äºŒ",
    "æª­": "é“¶",
    "æ¦": "æ²™",
    "ç­¤": "ç‹¼",
    "éŸ²": "æœº",
    "é¯£": "æ„",
    "æ": "å ",
    "æ«": "è‡³",
    "è±§": "å¤«",
    "é™™": "çº¯",
    "è°¹": "çº¢",
    "é³¾": "è¯—",
    "ç£­": "ç»°",
    "é»­": "çœ¼",
    "é›º": "ç‰©",
    "å»": "å°±",
    "é‚": "æ‘‡",
    "ç§µ": "å› ",
    "å»¥": "å¿«",
    "æ°‰": "æ‡†",
    "é‘œ": "èµ",
    "éƒ¬": "é’",
    "æ•": "é™ˆ",
    "å¹¬": "æ„",
    "åŠ†": "è¿",
    "åµª": "æ•²",
    "å¬¯": "å°",
    "é›ˆ": "ç¯",
    "è’": "çœ¼",
    "è¼": "æ‹±",
    "åµ”": "ä¼Ÿ",
    "éµ‚": "ä¿®",
    "è«†": "ä¸ƒ",
    "ç§œ": "å°¼",
    "ç¢": "æ£±",
    "åŸ¾": "å·¨",
    "é¦°": "æ•Œ",
    "éŸ": "æ ¼",
    "é°™": "å¼±",
    "è‚°": "ç„¶",
    "è¬’": "æª",
    "é ¯": "å¥",
    "å™–": "é“¶",
    "æ‰Š": "çœ¼",
    "é¤›": "é­‚",
    "è¶": "ç“œ",
    "éµ€": "äºº",
    "èµ": "ç½‘",
    "æŠ": "å±",
    "éˆµ": "é¥¼",
    "é†": "å²",
    "è·­": "ç¿”",
    "æ”ˆ": "ä¿Š",
    "æ’": "é—²",
    "è«¥": "é‡",
    "å®¯": "æ¶ˆ",
    "é… ": "æ°",
    "ç“¹": "æ",
    "æ”¡": "åƒ",
    "çµ—": "èƒ¡",
    "æ¯§": "å®¹",
    "é¥›": "ç›Ÿ",
    "æ¼›": "è…¾",
    "è¶ƒ": "å ",
    "è¨„": "æ±‚",
    "é´š": "æ­Œ",
    "ç¨": "è‘¡",
    "æ¶»": "è®¾",
    "çŸŒ": "çŸ¿",
    "æ¹": "è¯",
    "ç ™": "ç“¦",
    "ç¢¯": "è„‘",
    "å¬‚": "ç›´",
    "è„": "ç½‘",
    "éƒ€": "å“­",
    "æ¾¿": "ç´",
    "åŒ§": "å¦¾",
    "å¬º": "é€†",
    "é¬‰": "å®—",
    "å‰†": "è£¸",
    "å‡—": "å‚¬",
    "ç…”": "é—ª",
    "æ©": "æ¶¦",
    "æ”“": "å‰",
    "é«": "é­”",
    "å¿·": "èƒ¸",
    "è¨¿": "å­",
    "çŠˆ": "å…¨",
    "å¡£": "æŒ£",
    "å™¿": "å˜´",
    "å‹‚": "å‘Š",
    "æ€·": "æ ‘",
    "è–•": "è¿",
    "ç¶": "å¯¹",
    "è—­": "ç©·",
    "éº¡": "å…¶",
    "éˆ“": "äºº",
    "é‰ª": "åœ°",
    "å¢‡": "å¸",
    "éŸ¥": "æ¯’",
    "å¸²": "å¹³",
    "èšŸ": "ç‹",
    "é§‡": "æ–‡",
    "é¡‰": "äº²",
    "é": "æ‘‡",
    "æ¾¢": "å½“",
    "èŒ¾": "å‰",
    "å£": "ç°",
    "å–¢": "ç…",
    "åŒŒ": "æ ¼",
    "å––": "èƒ¡",
    "éƒ®": "å‘¨",
    "å²®": "é©®",
    "éª": "æ›¹",
    "æ¯œ": "è±ª",
    "éŠ‚": "å‘¨",
    "é–›": "ç °",
    "é½“": "è¡¬",
    "æ™": "å¤œ",
    "æ¸ª": "å¦‚",
    "ç«Œ": "å¤„",
    "è™´": "å“²",
    "éµ": "æ•",
    "ç„­": "ç©·",
    "è•’": "ä¹°",
    "éŠ": "ç¾Š",
    "ç¤°": "åˆ©",
    "é‡¼": "è§",
    "æ‘‰": "æœ",
    "æ¼": "ä¹ ",
    "å³Œ": "å ",
    "è¥±": "é¾™",
    "ç‡›": "çª˜",
    "é ®": "ä¼š",
    "æ“»": "æœ",
    "é‚¥": "å®¡",
    "é€ª": "é”™",
    "æ¿š": "è¥",
    "è¡": "æ–‡",
    "å™¡": "å ",
    "é‚–": "å±±",
    "é‘ˆ": "è‚",
    "ç´»": "å…»",
    "å¯": "ç¯",
    "éŸ…": "æ˜¾",
    "éŸ‡": "æ¯’",
    "è´™": "ç‚«",
    "é¥“": "ç§°",
    "é‘‹": "é’",
    "è€": "å»",
    "ç‚¢": "ç«¹",
    "æ™": "è¯¥",
    "å—¸": "æ•–",
    "å¬": "å…ˆ",
    "è†—": "æ£",
    "ç¨´": "é—²",
    "éš–": "ç‰©",
    "ç¿–": "è¥¿",
    "è…": "èµš",
    "ç—¬": "æ„",
    "è¼²": "ä¼ ",
    "æ¢¤": "åŸ",
    "é§ª": "æ·±",
    "ç¿‡": "ç¦",
    "è‡•": "æ ‡",
    "å‚Ÿ": "å…»",
    "å«¶": "æ¡¥",
    "è€": "ç›´",
    "è§˜": "åµ",
    "é¦µ": "åŠ©",
    "ç’›": "é€Ÿ",
    "ç›": "é“²",
    "å¾¢": "è°¢",
    "è¶¶": "è£¤",
    "è": "ä¹Œ",
    "ç¡¢": "é±¼",
    "é­ª": "å€Ÿ",
    "é„¼": "èµ",
    "é®¤": "è£‚",
    "ç„¹": "æ ",
    "è­µ": "å¯¹",
    "é‹±": "ç‰¹",
    "é¹·": "é›¶",
    "è—€": "è¥",
    "éŸ": "é£",
    "å“°": "åŠ³",
    "å»«": "èŠ",
    "è¢¾": "æœ±",
    "æ¥": "å€Ÿ",
    "å¬³": "æœˆ",
    "ç½": "å µ",
    "é»…": "é‡‘",
    "è‘¥": "è§",
    "æ™ª": "èˆ”",
    "é‘®": "åš",
    "ç“": "èœ¡",
    "å—€": "äº’",
    "æ¯¥": "å¯»",
    "æ­’": "æ›¿",
    "è©¾": "èƒ¸",
    "å¤¡": "æ°”",
    "èª´": "ä»",
    "åŸ": "ç´",
    "æ¾°": "ç»ƒ",
    "è†±": "ç›´",
    "é¯š": "è®°",
    "é‹œ": "ç€",
    "å°µ": "æ¨",
    "ç ‡": "æ°‘",
    "æ±": "æ¶ˆ",
    "å³‰": "æ¶",
    "è†­": "å½’",
    "èº¨": "å¥",
    "å®²": "å®",
    "è“¾": "é²",
    "ç€¢": "ä¼Ÿ",
    "è…‰": "å¥¶",
    "ç¾’": "åŸ",
    "åµ±": "æ°¸",
    "é‘": "éœ€",
    "è¥": "çƒ¦",
    "è¡‰": "å’–",
    "é³²": "è¯—",
    "æŠ®": "æ•",
    "æ¤²": "ä¼Ÿ",
    "è¸·": "çœ¨",
    "æ‡¥": "è‡³",
    "é»—": "å",
    "å¢§": "ç¡®",
    "è‚¹": "è¥¿",
    "æ¹‚": "æ¶",
    "å˜": "èƒ¡",
    "é›—": "æ±‰",
    "è›Œ": "å¤",
    "å¸£": "å·",
    "å‹¡": "ç¥¨",
    "è´•": "æ¯’",
    "è’˜": "å¦‚",
    "ä¾º": "è‚¾",
    "å¬•": "æ˜¯",
    "é ‰": "å®œ",
    "æŠ™": "å‰–",
    "ç½¤": "æ",
    "æ‚¹": "çŒ",
    "é‹": "ä¾ ",
    "éƒ¶": "ä¸",
    "è›œ": "ä¸€",
    "è€¯": "æˆ–",
    "ç€‚": "é²",
    "è˜•": "æœ‹",
    "é§¥": "å®¹",
    "éº§": "å’Œ",
    "è²—": "å·¨",
    "è¿§": "é™ˆ",
    "é‘": "æœ‹",
    "æ¦µ": "å®¹",
    "èœ¹": "ç‘",
    "ç€ª": "çƒ¦",
    "è«»": "é»„",
    "é®‡": "ä½",
    "é­¬": "æ¿",
    "è‹µ": "å ",
    "é®": "æ‰¹",
    "å¢›": "ä½",
    "æ©­": "å§‘",
    "è¼µ": "æ ¼",
    "é›‚": "ç´",
    "è—µ": "å®",
    "éŒ": "ç°",
    "æ”Š": "åˆ©",
    "é¦¤": "çˆ±",
    "å‹œ": "ç“®",
    "è”¨": "å€¦",
    "å³·": "æ·±",
    "èŒ’": "åŸ",
    "å¬»": "æ¯’",
    "ç™‰": "å•",
    "è‰ª": "é²",
    "åŒ‰": "ç °",
    "ç«†": "ç©·",
    "é­›": "åˆ€",
    "éŒƒ": "æ‰¹",
    "é½€": "ç‰©",
    "è‰µ": "å¹³",
    "ç“¸": "æ‘†",
    "è••": "ç”±",
    "æ³": "æœˆ",
    "ä½­": "ç¿”",
    "ç‡±": "æ„",
    "æ§‰": "æ",
    "æ«¾": "ç”±",
    "å‚ ": "ç½š",
    "æ‰·": "å¥¥",
    "è”™": "ç‚«",
    "æ¢¥": "æ¾",
    "è–": "å",
    "æ¥¾": "å…¨",
    "ç—­": "å´©",
    "æ‘•": "åœ°",
    "æ‹": "å¾®",
    "ç„µ": "æ ",
    "è ½": "èŠ‚",
    "è°º": "è™¾",
    "é·¾": "æ„",
    "è´‰": "è›‹",
    "æ”³": "å¯»",
    "é§": "é¾™",
    "è¦": "å˜",
    "ç¸": "å¸®",
    "é³": "æ“",
    "è‘¾": "å†¤",
    "ç§š": "åŠ",
    "æ¥ˆ": "éœ€",
    "é‚¼": "æ¡†",
    "å‚‡": "å®¹",
    "é°š": "å®£",
    "èš¾": "çš®",
    "é»¤": "çœ¼",
    "æ¢": "è®°",
    "æ‡": "è‰²",
    "é‰Š": "æ‹›",
    "é«¼": "æœ‹",
    "æ¹’": "æ",
    "ç¿ª": "å®—",
    "å¦": "ä¸­",
    "ç°›": "ç­›",
    "ç€³": "è§",
    "è£·": "å†¤",
    "é €": "äº’",
    "ç³ƒ": "å”",
    "å¨µ": "å±…",
    "é£‰": "èŠ",
    "è¤¯": "å€Ÿ",
    "é®”": "å·¨",
    "å¶¯": "æ",
    "åµ³": "æ“",
    "å¶€": "çª",
    "æ¤": "åœ°",
    "è˜²": "é›·",
    "æ½¿": "ç»´",
    "æ£": "ä¿Š",
    "ä¼œ": "ç¿ ",
    "è™Š": "é¸¾",
    "æ¡—": "å •",
    "æ›‘": "æ·±",
    "è¦™": "ç½—",
    "é«±": "æŠ¥",
    "é–": "è™½",
    "éŠ¡": "æ",
    "æ°ƒ": "åŒ",
    "éšŸ": "ç³»",
    "éš²": "è‡³",
    "è’": "æ±‰",
    "ç³©": "å¿«",
    "é¬¤": "åš·",
    "æ€¸": "è¥¿",
    "é»€": "é‚¹",
    "æ¨„": "é™ˆ",
    "å¦…": "çº¢",
    "è˜ˆ": "æ¨",
    "æ³": "åˆ™",
    "å«´": "å§‘",
    "éˆ ": "æ„",
    "ç§²": "è‡³",
    "è˜": "å®£",
    "è²¦": "å®Œ",
    "æ²Š": "è›‹",
    "é„": "è¢«",
    "é–³": "é“²",
    "ç“­": "èƒ†",
    "æ¨–": "ç§‘",
    "è»¥": "å–",
    "é‚­": "å·¨",
    "æ–´": "æ—",
    "é¯º": "æœ±",
    "è": "å¿…",
    "å˜½": "é“²",
    "é¾“": "å„",
    "é·…": "åˆ©",
    "æƒ½": "æ•",
    "åƒ": "è°¢",
    "æº”": "å’¬",
    "é²“": "çƒ¤",
    "é¯•": "å…¶",
    "ç¿": "å°†",
    "ç³”": "ä¿®",
    "èº·": "çŸ®",
    "å¢”": "å‚¬",
    "ç“": "åš",
    "è½£": "åˆ©",
    "æ©œ": "è§‰",
    "é§": "åŸ¹",
    "è¶©": "èµ¤",
    "ç˜‚": "å“‘",
    "è©‰": "è„‘",
    "ç­—": "é‡",
    "ç‡©": "ç¡®",
    "æ„³": "å·¨",
    "æ‚€": "æ°¸",
    "ç¸–": "ä¾ ",
    "è…": "å°¼",
    "è‡…": "å¤„",
    "é¦¢": "é—´",
    "æ¬®": "è§‰",
    "è¶€": "åˆº",
    "ç‚ ": "ä¾ ",
    "é¼¤": "æ–‡",
    "é§¬": "è€³",
    "è…£": "åœ°",
    "é°¿": "è®°",
    "é½˜": "è°¢",
    "å´»": "è‡³",
    "éŠ": "å›",
    "å –": "è„‘",
    "å«·": "å¦¥",
    "é®§": "ä½“",
    "éª": "æ“¦",
    "é‚Œ": "ç¦»",
    "æ¡": "æŸ³",
    "ç¸": "å¤",
    "ç¿‰": "æœ¬",
    "é®": "æ•Œ",
    "çš£": "å¤œ",
    "æ•†": "å’Œ",
    "åµ½": "å ",
    "è‡": "ç",
    "çŒ‰": "å…¶",
    "éº­": "ç‚®",
    "é„ˆ": "å¥",
    "è½‡": "æ•™",
    "è¡‡": "å–",
    "é­³": "åŒ",
    "è”": "å®œ",
    "é«Š": "åˆº",
    "è‡°": "è‡­",
    "å¶": "å°º",
    "è­ ": "è°ˆ",
    "è©¥": "å’Œ",
    "ç±—": "ç€",
    "å´¨": "èŠ‚",
    "çƒ—": "å¼€",
    "æ…º": "æ¥¼",
    "å¶š": "èŠ",
    "æ—": "å‘¼",
    "ç³": "ç°",
    "è¹ƒ": "è¯º",
    "å•”": "èµ·",
    "è½’": "åŸ",
    "é¶‚": "æ„",
    "æ„": "è¯º",
    "åŸ": "é‡‘",
    "æ‘¿": "æ‘‡",
    "è•": "å…¶",
    "è·": "è‚˜",
    "æ£‡": "èª",
    "æ‘": "å®¹",
    "æ¥": "ç°",
    "ç³›": "å”",
    "æ": "æ±‚",
    "èª": "æ±¤",
    "å•": "ç²¾",
    "æ": "ç",
    "è‹¬": "ä¿®",
    "åŒ‘": "å·¥",
    "èƒ¦": "å…»",
    "è§°": "æ‰",
    "åª…": "å•",
    "åµ­": "å´©",
    "é±¢": "éªš",
    "è¹¸": "å",
    "é®¯": "æ ¼",
    "è¶­": "æ•™",
    "è¸™": "è‚",
    "é‹": "æ±¤",
    "çŸ²": "çˆ¸",
    "ç‡˜": "ç¾",
    "è ³": "åº”",
    "é¨¸": "å–„",
    "ç®¿": "æ",
    "éœ‹": "ä¸ƒ",
    "æ˜²": "è´¹",
    "é‘Ÿ": "æ¯’",
    "ç±": "ç¼–",
    "éŒ": "æ‰¹",
    "ç“µ": "å®œ",
    "æ•ª": "å¤š",
    "é®": "æƒ³",
    "å©™": "é™",
    "ç¬": "ç»­",
    "é¾": "æ„",
    "å·": "åµ",
    "è’": "çœ ",
    "æ§†": "æ˜¥",
    "æ¾²": "å¤œ",
    "éˆŒ": "è§‰",
    "é¨—": "ç‰‡",
    "åˆ": "æœº",
    "ç²£": "å†Œ",
    "é·": "æ ¼",
    "éƒ‹": "ä¹ ",
    "æ–’": "ç­",
    "ç€±": "è®°",
    "éµš": "çª",
    "éº˜": "ç›¸",
    "å°³": "å¤",
    "æ¼…": "æœ",
    "é§": "å·¨",
    "æš": "æ‹‰",
    "é·ˆ": "è¸¢",
    "èº—": "ä½",
    "æ š": "é•‡",
    "æ²": "åŒ",
    "ç¥£": "æ—…",
    "é™": "ç‚«",
    "è¨©": "èƒ¸",
    "åŒ¶": "å°±",
    "æ°": "æœ¨",
    "é†¥": "æ¼‚",
    "è‰": "é›ª",
    "èœ¦": "ä¼¦",
    "ç´": "å§¿",
    "ç‹£": "ç…§",
    "çŠ¡": "åˆ©",
    "æ’‰": "è¹²",
    "éµ›": "ç²¾",
    "çµ": "ç¾",
    "æ¾": "å§",
    "ç–": "ç¬¨",
    "è“˜": "æ»š",
    "çƒ²": "è°¢",
    "å³¼": "å‘Š",
    "ç§¢": "é›¶",
    "èŠ": "å®š",
    "éª": "æœ",
    "é¹¶": "é‡‘",
    "åŸ©": "ç",
    "é¶˜": "èƒ¡",
    "ç—¯": "ç®¡",
    "æ¬ˆ": "ç»´",
    "ç‹§": "è¸",
    "éƒ": "æŒª",
    "ç¹¿": "è“",
    "éª”": "å®—",
    "çŒ…": "æ’",
    "å­·": "ç¦»",
    "å­ ": "å››",
    "é¾¡": "å¹",
    "è˜µ": "çŸ¥",
    "è’£": "å¾",
    "æ”Ÿ": "ä¿Š",
    "ç¦ƒ": "ç›´",
    "è«": "æœ±",
    "ç“º": "é•¿",
    "è ¾": "ç«¹",
    "é¾£": "è§‰",
    "é‡¸": "è¥¿",
    "å‰ˆ": "å†¤",
    "èº»": "ç©º",
    "éº": "é™ˆ",
    "æ‘": "ç¼©",
    "è‘": "ç±³",
    "é„¬": "ç»´",
    "å‘": "æ¶",
    "åµ®": "é¢ ",
    "æ”š": "ç“®",
    "é–š": "å ",
    "æ‡": "äº²",
    "éŠ": "èŠ‚",
    "é½–": "ç‰™",
    "æ”": "å…»",
    "é…­": "å³",
    "é·‘": "æ",
    "è½": "å“º",
    "éŒ": "æœ",
    "ç´´": "æ³¢",
    "è±¾": "æ‰¹",
    "é£Œ": "é£",
    "é¥Š": "ä¸‰",
    "å¢¶": "å“’",
    "å®º": "è°",
    "èœ": "èŠ‚",
    "æ©¶": "æ",
    "ç½†": "çŒ",
    "å€Š": "å®—",
    "å££": "æ—",
    "è–": "ç¢—",
    "é®©": "ç—…",
    "ç¸…": "å¾®",
    "è…‡": "å†…",
    "è’‘": "å› ",
    "é†": "å¸¦",
    "ç–©": "ç¿ ",
    "åµ°": "æµ…",
    "ç©¯": "è‰²",
    "ç¨›": "æ†",
    "æ¨": "é™ˆ",
    "å®©": "ä½¿",
    "ç¦¯": "å†œ",
    "é ¾": "å§¿",
    "è•": "é€¼",
    "é¶¹": "åˆ˜",
    "æ˜¿": "çŸ¿",
    "ç•˜": "å—",
    "ç¨˜": "æœº",
    "ç‹«": "è€",
    "é¨Š": "æ¡ƒ",
    "æº„": "é€¢",
    "é±¹": "çŒ",
    "æ†´": "ç”Ÿ",
    "ç¸ª": "å¿…",
    "é¼µ": "çª",
    "éµ¥": "åˆ¤",
    "è©´": "å¾®",
    "æ™": "çº¢",
    "éµƒ": "å‘¨",
    "é­¶": "é‚£",
    "é·": "å¥",
    "é§º": "ç‹¼",
    "è¦»": "åŒº",
    "é¤ª": "æš–",
    "è…¬": "æŸ”",
    "è»®": "å…»",
    "éº²": "ç°",
    "è›¼": "è½¦",
    "é„": "å“€",
    "å­ˆ": "ä¼š",
    "æ„": "æ ¹",
    "è´š": "å¼„",
    "æ¯‡": "æ¯",
    "æ€Ÿ": "åœ°",
    "éš¢": "æ‰°",
    "è¬£": "é±¼",
    "è»": "å…¶",
    "è…²": "ä¼Ÿ",
    "ç “": "å“²",
    "å´…": "ç¡®",
    "å»­": "è®°",
    "æº›": "æŒ–",
    "è†¼": "æŠ“",
    "å¯­": "ä¼š",
    "ç‘»": "æ˜†",
    "æ¬³": "æºƒ",
    "ç…›": "çª˜",
    "è¼£": "æœ‹",
    "æ¯·": "å¸½",
    "æ°‹": "ç›Ÿ",
    "ç¡³": "èµ¤",
    "èˆ¿": "å¤¸",
    "ç": "è¥",
    "é£¥": "æ‹–",
    "é«š": "å·§",
    "çš": "é¡º",
    "ç‚": "åˆ¤",
    "è¼±": "é—²",
    "ç¿": "çº¢",
    "ç¸ƒ": "éœ€",
    "è¨‹": "æ‰",
    "è‚µ": "å…¶",
    "èœ ": "ä¿Š",
    "éŠ´": "æ˜¯",
    "åŸ¬": "ä¸œ",
    "å¤¦": "å°˜",
    "é¡": "é—®",
    "éˆ²": "å§‘",
    "é°¦": "å§¿",
    "å˜Š": "åŸƒ",
    "é©ˆ": "ç‰",
    "è¨¤": "è‚–",
    "åª¨": "ä¿ƒ",
    "æ–š": "å‡",
    "è€Ÿ": "å·¨",
    "è‰": "å±…",
    "éœ©": "æ‰©",
    "è½": "è§",
    "å¨¦": "è´«",
    "æ…": "å‘½",
    "æŠ": "çœ¼",
    "æ¢Š": "åœ°",
    "èƒ…": "å ",
    "æ¡˜": "ç¼€",
    "ç®µ": "æ˜Ÿ",
    "å‰“": "ç¦»",
    "é‹™": "é›¨",
    "æ´†": "æˆ",
    "è§“": "æ±‚",
    "èƒ‹": "ç”°",
    "èš‰": "æ–‡",
    "å ²": "ç“·",
    "é´¬": "åº”",
    "éŠ°": "å“€",
    "åª€": "ç‰",
    "çŒ‘": "æ˜†",
    "ç—¸": "èµ¤",
    "é§¼": "å›¾",
    "æ§": "å¯¼",
    "è¨¹": "ç»­",
    "æ¤": "å½’",
    "çµ¼": "é•‡",
    "å¬": "ç¿»",
    "é«¿": "ç¼©",
    "ç–…": "å°†",
    "æ“": "çˆ·",
    "è“›": "å†Œ",
    "é„µ": "æ“",
    "é": "å¿…",
    "è¼": "çƒ§",
    "é‘˜": "é›·",
    "é‹£": "çˆ·",
    "æ‡": "ä¿Š",
    "è˜Ÿ": "å¼•",
    "ä¿•": "æ•£",
    "é„¥": "æ•²",
    "é£‹": "è‰²",
    "ç§¿": "çˆ¶",
    "èƒ‡": "è´¹",
    "ç–": "æ„",
    "æ“": "æƒ…",
    "äº„": "æ„",
    "ç©³": "æ”’",
    "ç¡¡": "è½°",
    "è¥‹": "æ",
    "æªƒ": "å¼•",
    "é£¦": "å ",
    "çˆ¥": "ç«¹",
    "æ¬¯": "ç³»",
    "å¿°": "ç¿ ",
    "æ¾": "æ˜¯",
    "éƒ": "æ´¾",
    "å§¾": "å…¨",
    "å´“": "å›º",
    "ç‚š": "å…‰",
    "èŒŸ": "ç‰",
    "é‚’": "åœ",
    "æ±¿": "ç»­",
    "ç”‡": "åº”",
    "æ­‹": "å¤œ",
    "é—›": "å”",
    "èˆ": "ç¯",
    "é•¼": "åŒº",
    "çº„": "æœ‹",
    "å": "çœ¨",
    "èœ¶": "æ‰€",
    "å²²": "çŸ¿",
    "é±‘": "é»„",
    "å±”": "å°¼",
    "é ": "åˆš",
    "éš€": "è™«",
    "è¡¸": "å€Ÿ",
    "é³¸": "äº’",
    "æ˜’": "å‘¼",
    "é£°": "é¥­",
    "é…€": "ç‡•",
    "è˜": "è„¸",
    "å¬¹": "æ€§",
    "è¶¡": "è„†",
    "é‡³": "ç³»",
    "è˜": "çª˜",
    "è «": "ç¦»",
    "éš“": "ç°",
    "ç‰": "é±¼",
    "æ¾…": "è¯",
    "é§š": "å…»",
    "å³›": "æ",
    "ç™›": "å",
    "å·¸": "å®œ",
    "æ¬«": "æ°”",
    "æ«¦": "åº†",
    "é¼": "æ±¤",
    "é¤": "ä»»",
    "è‘ª": "è®°",
    "é°µ": "æ•",
    "æœ": "èµ¤",
    "ç§“": "çŸ¥",
    "è—‘": "ç©·",
    "æ¾µ": "çœŸ",
    "æ­±": "ç§",
    "è¤¹": "æ„",
    "ç¢µ": "ç”°",
    "æ¢·": "é™",
    "æ’‹": "è½¯",
    "è–š": "æ±¤",
    "éº‰": "é—´",
    "æ”Œ": "ç¼“",
    "è®": "åœ°",
    "éµ’": "ç‰",
    "èœ¬": "å«",
    "æ§¹": "é«˜",
    "å‹": "è·¯",
    "é‡®": "å…¶",
    "é£³": "å·",
    "ç¿¸": "å–·",
    "å“": "äº•",
    "èº€": "çŒ",
    "è«¯": "ä¸“",
    "å¼": "å®¡",
    "ç®": "æ’¤",
    "çŸ¨": "å½±",
    "èŸš": "æœ‹",
    "æ¼¡": "ä¼¤",
    "èº¤": "æ",
    "æ¿¸": "ä»“",
    "ç˜±": "æ„",
    "ç«°": "ç¦»",
    "é·¶": "ä¹°",
    "è™²": "è™¾",
    "é": "çˆ·",
    "æŒ˜": "è£‚",
    "èˆ“": "æ˜¯",
    "è¦": "è¿",
    "å‚„": "è™¾",
    "æ‹": "åº•",
    "è¼°": "ç¾Š",
    "é†¿": "è¿·",
    "èº‘": "ç›´",
    "ç¤—": "æ‹¼",
    "çˆ—": "å¤œ",
    "èº™": "å",
    "å™‹": "å",
    "å¢¢": "æ‹”",
    "éµ«": "ç€",
    "çœ±": "åœ°",
    "æ¨": "é€Ÿ",
    "èŒŠ": "å§¿",
    "æ™˜": "æ±‰",
    "è«€": "åŒ¹",
    "éµ´": "å±€",
    "è‰†": "ç‹¼",
    "èµ¾": "å¯",
    "èˆ¼": "ç©·",
    "æ¢™": "æ¢",
    "ç…‚": "è´º",
    "é‡²": "å››",
    "å—‹": "é‹",
    "ç†­": "ä½",
    "çº": "é›·",
    "éº±": "å¤«",
    "éŒ—": "å†…",
    "é—": "å…¨",
    "çˆ‹": "ç†",
    "èº´": "ç‹¼",
    "é¡„": "æ±‰",
    "è™‡": "çŠ¬",
    "è›¨": "å¢¨",
    "éŸ°": "è°¢",
    "æ¿­": "çŸ®",
    "åƒ˜": "åœº",
    "é³º": "å¤«",
    "æº’": "åŸ",
    "è¸‘": "å…¶",
    "éµ…": "è½",
    "é¤¥": "é£",
    "éš‡": "å¾®",
    "æ¤£": "ç‚¹",
    "èªµ": "è‚–",
    "è£®": "æ˜Œ",
    "æ™­": "è‚˜",
    "å£‰": "å·¨",
    "é¡œ": "è®²",
    "æ¢¸": "ç¦»",
    "ç¨–": "æ£’",
    "çµ‰": "æ ‘",
    "é½ˆ": "å¼„",
    "åª¡": "ç»ƒ",
    "éš¿": "æ„",
    "èŸ‚": "æ¶ˆ",
    "è«": "å‰",
    "è»‚": "çƒ™",
    "è ¬": "é¾™",
    "ç¯§": "ç€",
    "è¦¹": "ç»´",
    "æƒ¥": "æ°¸",
    "é¬€": "æ›¿",
    "æ’¨": "è¾…",
    "è¢‰": "é©®",
    "åƒŒ": "è¥",
    "èªº": "åƒ",
    "é¼¼": "è¦",
    "æ§¶": "è´µ",
    "éŸ´": "æ‚",
    "éŸ–": "æŸ”",
    "é¼¶": "æ€",
    "åˆ¯": "è€•",
    "ç‚¥": "ç¦",
    "é¥»": "è¥¿",
    "ç ¿": "çŸ¿",
    "é®½": "é±¼",
    "è‘‹": "å–",
    "å¥": "åŒº",
    "é†Ÿ": "ç”¨",
    "æ®±": "é—´",
    "é¯ ": "æ¥",
    "å¦¿": "é˜¿",
    "é©‰": "éœ€",
    "æ‡©": "å…»",
    "éˆ—": "å…",
    "è²•": "è¥¿",
    "æ‡›": "å‘†",
    "è´‚": "å°˜",
    "éœŒ": "å‘¨",
    "è¡": "é”™",
    "ç•": "åˆ©",
    "è ": "å’",
    "å¥ƒ": "ä½",
    "æœ": "é›¶",
    "è ¥": "è‚",
    "å­¯": "å‰",
    "é¼°": "å±€",
    "é¤™": "æ˜¯",
    "åµ¦": "å‡¯",
    "é§½": "å®£",
    "éƒŒ": "å½’",
    "ç“„": "æ¯’",
    "é†”": "æ±‚",
    "é«¶": "å®¹",
    "çŸµ": "æ°”",
    "è»©": "å¸¦",
    "é¥³": "å •",
    "è¼º": "å§¿",
    "è‰¬": "ç¼ ",
    "é·": "é«˜",
    "é™¦": "å¯¼",
    "ç³": "æ–‡",
    "æ›­": "èºº",
    "éŒ–": "æ¯’",
    "è¦": "è§‰",
    "è®„": "å’",
    "é°´": "ç°",
    "å³¾": "é“¶",
    "è¦": "éªš",
    "è¬¼": "å‘¼",
    "å›": "æ‚",
    "å³€": "ç§€",
    "è™„": "è¨",
    "è…’": "å±…",
    "é‘": "å‡",
    "å¿€": "ç›¸",
    "ç‡": "æª",
    "ç“‹": "æ›¿",
    "æ®¥": "é“¶",
    "æƒ¼": "æ‰",
    "æŸ": "éœ€",
    "å¬¦": "æ„",
    "é•": "çœŸ",
    "é›­": "è‰²",
    "é¥š": "å®³",
    "åŒ²": "è¿",
    "åŠ·": "ç¾Š",
    "é°®": "æ¸©",
    "ç¡»": "å‘",
    "è¸•": "èŠ‚",
    "ç—": "é›•",
    "ç€": "å¢¨",
    "è‘": "ç²¾",
    "æ—": "å²",
    "è¢¸": "è§",
    "æ–Š": "å…¶",
    "é‰": "çœŸ",
    "æ¢": "å‡¹",
    "æ¸¨": "å¾®",
    "æ½": "è¥¿",
    "åºª": "é¬¼",
    "æ“œ": "æ¶",
    "éŠ’": "è¡Œ",
    "æ†¢": "æ¶ˆ",
    "é—¦": "æ–‡",
    "ç¿": "è‡³",
    "æª•": "è®°",
    "é¨˜": "èª",
    "æ­‘": "å‘¼",
    "é™’": "é¬¼",
    "å˜³": "æºƒ",
    "è‰•": "æ£’",
    "é»‹": "çŸ¿",
    "è‚": "æ’",
    "è¼š": "æˆ˜",
    "é¤•": "ä¿Š",
    "é·¢": "è§‰",
    "æ€¬": "ç³»",
    "é€˜": "ä»¥",
    "æ¨­": "æœº",
    "çŒ†": "é£",
    "èŠ": "è¿",
    "ç«—": "åº™",
    "éˆ±": "æ°‘",
    "ç«§": "é™",
    "é¹": "å®£",
    "æŠ": "æ•´",
    "è”‹": "æ•Œ",
    "å©’": "è°ˆ",
    "é¶": "å°Š",
    "çŠ©": "ç»´",
    "è‰": "å®¢",
    "ç„": "åœ°",
    "è—¡": "æ•Œ",
    "ç€—": "ç°",
    "é‘": "æºƒ",
    "èºˆ": "å·§",
    "é±": "è§’",
    "è©¿": "æŒ‚",
    "çŸˆ": "çœ ",
    "è¬²": "ç¿",
    "ç ‹": "æŒ‡",
    "æœ‘": "æ›¿",
    "èª¯": "å”±",
    "æˆƒ": "èºº",
    "ç®": "é±¼",
    "å¾±": "ç¥¨",
    "è¢²": "å°º",
    "èµ": "ç§‹",
    "ç˜š": "è§‰",
    "å¶­": "è‚",
    "ç…°": "é€ ",
    "éŠ”": "æ‰¹",
    "é¯": "èµ°",
    "å™": "çœ¼",
    "ç–€": "æ’",
    "æ¨‡": "ä¿®",
    "é¨": "é’±",
    "æ¿": "ä¸»",
    "è¦¼": "ç½—",
    "ç’³": "ç”°",
    "çµˆ": "å¢¨",
    "ç¢€": "æˆ",
    "çˆ™": "åš·",
    "çœª": "é¥¼",
    "è·®": "èµ¤",
    "é‰¡": "åŠ",
    "ç™¨": "æˆ–",
    "å": "ä¸‰",
    "éµ±": "è·¯",
    "æ„²": "å¤",
    "é·¥": "æ€",
    "è–Ÿ": "å…ˆ",
    "è€“": "å¬",
    "æ£›": "ç‰",
    "å³µ": "å®¹",
    "åŒ”": "å·¥",
    "çƒ‰": "æ¢",
    "è”§": "ä¼š",
    "é¡­": "ç›Ÿ",
    "æ¨¢": "å°¿",
    "åƒ²": "å…ˆ",
    "é­«": "å®¡",
    "è¹ª": "æ¨",
    "éŠŒ": "å°Š",
    "é¼": "è‰²",
    "ç’­": "æ»š",
    "é‰": "æ¡ƒ",
    "é¯´": "è¯—",
    "è¥‡": "å‡",
    "ç‚„": "çº½",
    "æš­": "å·",
    "è†¢": "é©´",
    "èŠŒ": "ç‰",
    "è¹«": "å±€",
    "é©œ": "å¤œ",
    "é¢´": "ç‚«",
    "æ…»": "å€¦",
    "é¬ ": "æ‰©",
    "ç‡°": "å¾®",
    "ç¨’": "å›º",
    "éµŠ": "å¤¹",
    "è¦•": "ç­",
    "è«¹": "ç¾Š",
    "çª™": "æ¶ˆ",
    "ç€ˆ": "ç°",
    "ç†®": "æŸ³",
    "ç¾¦": "ç¯",
    "è­ˆ": "å¯¹",
    "ç—®": "å¸",
    "é¡…": "å‰",
    "è»‰": "ç‰",
    "è—«": "è°ˆ",
    "æ–”": "é›¨",
    "å£¥": "ç¼ ",
    "é": "å…ˆ",
    "æ­¬": "é’±",
    "æ®¶": "åŠ©",
    "é±¡": "è´¼",
    "ç²š": "åƒ",
    "æ‡§": "è¯º",
    "æ”‘": "å‰",
    "é": "è´º",
    "è¶»": "å°˜",
    "èœ”": "ç”µ",
    "å—ˆ": "åº¸",
    "æ¦¹": "æ€",
    "é«‰": "åš",
    "èœŒ": "å¿…",
    "æ°Œ": "é²",
    "ç®³": "å¹³",
    "é®…": "å¿…",
    "é®³": "çƒ¤",
    "å¡›": "åˆ©",
    "è¢³": "å°º",
    "è–£": "å¤",
    "æ„…": "æ ¼",
    "é£µ": "åš",
    "è ®": "æ¤°",
    "ç¨": "ä¼¦",
    "è—²": "æ¬§",
    "æ©": "å±•",
    "è¸²": "é¡¿",
    "é¡‹": "å¡",
    "éœŸ": "çº¢",
    "é¶“": "æ",
    "æ“‘": "æ¥",
    "ç‘¼": "ä¸“",
    "éµ—": "è¥¿",
    "ç‚‚": "ä¸­",
    "ç›¶": "è¿œ",
    "è»": "æ¥¼",
    "æ•½": "è§’",
    "é§©": "åœˆ",
    "æ¿¥": "å¼•",
    "å¼«": "æ•",
    "ç‡†": "æ•²",
    "ç£ ": "é²",
    "é§µ": "åˆ˜",
    "éƒ†": "æ",
    "èŸ·": "å½“",
    "è‰ƒ": "ç¦»",
    "åº½": "ç‰",
    "è°¾": "è½°",
    "è¶¢": "è·¯",
    "èŸ": "æ¶ˆ",
    "ç‡": "æ€",
    "éš’": "çœ¼",
    "æ”•": "å…ˆ",
    "å¾Ÿ": "å‘¨",
    "æ›": "å·",
    "é®Œ": "æ»š",
    "æš": "ä¸¤",
    "æƒ–": "æ›¿",
    "è£«": "é™¢",
    "é„¸": "ç›Ÿ",
    "åŠ•": "è‡³",
    "æ‡±": "ç­",
    "é¦²": "å“²",
    "é´°": "ç“œ",
    "ç£¶": "ç³»",
    "å¨": "è°¢",
    "æ†„": "è‡³",
    "é¨ª": "æœ",
    "è¦‰": "æœº",
    "è ©": "æœ±",
    "ç‹¿": "é¢œ",
    "é­µ": "åŸ",
    "æ§": "èœ¡",
    "è¸": "é™",
    "ç¤": "è¹²",
    "é­¤": "é¢",
    "é¸–": "è´º",
    "çª¤": "å…³",
    "é°–": "å¦¥",
    "è·¾": "ä¹¦",
    "è­¢": "å²",
    "ç‡½": "æ„",
    "ç·½": "ç§°",
    "é¨¡": "å…¨",
    "æ¦²": "æ¸©",
    "èœ": "æœ‰",
    "é‹¿": "é•¿",
    "è ¯": "çš®",
    "è˜™": "æ„",
    "è²": "è‡³",
    "èœ¤": "æ€",
    "æ¶": "èŠ‚",
    "é‘": "åº”",
    "é–": "é›¶",
    "è‘ˆ": "æ´—",
    "æ¯­": "è±†",
    "è¢¶": "é™",
    "æˆµ": "å–",
    "è›": "å’Œ",
    "é¶‘": "åº”",
    "é•½": "äº†",
    "é™ ": "é“º",
    "æ¦": "ä½¿",
    "è±´": "æ•Œ",
    "é¦Ÿ": "å›¾",
    "ç¶¥": "å…¶",
    "åœ½": "å¢¨",
    "æœ‡": "çš®",
    "ç£‡": "æ‰¹",
    "è¶—": "ä¿ƒ",
    "èº–": "æ–­",
    "è±‚": "èŠ",
    "è’•": "æ™•",
    "é¶­": "è®¿",
    "ç„³": "è§‰",
    "æº³": "äº‘",
    "ç¸œ": "äº‘",
    "è ‡": "åˆ©",
    "æ„Œ": "æ¢",
    "æº¤": "é©¬",
    "é ¢": "æ‰©",
    "æºŠ": "æ³¢",
    "æºŒ": "å¡",
    "æ¤": "ç³»",
    "è—°": "åˆ˜",
    "æ“¨": "çˆ·",
    "å¢‚": "æ ‡",
    "é†€": "ç»´",
    "è¸¸": "å°˜",
    "é‡": "å› ",
    "æ©¯": "çƒ™",
    "é¸†": "é±¼",
    "å™’": "è¿",
    "ä¿¼": "ç‰",
    "é·§": "æ„",
    "æª…": "ä¼š",
    "é‘‚": "è®­",
    "ç¾·": "è„¸",
    "è”œ": "æ•–",
    "æ¢‹": "å®£",
    "å›…": "é“²",
    "å˜": "ç¦",
    "é ¨": "é›¨",
    "å¹Š": "å·¥",
    "é€³": "ç‰",
    "æº¨": "ç¾",
    "è»": "å…±",
    "è±…": "é¾™",
    "éŸ·": "ä¹",
    "è•”": "åŒ…",
    "ç‡¤": "å¤ª",
    "ç¡µ": "é²",
    "ç„¥": "å§",
    "å¥": "è®­",
    "é‡›": "å…«",
    "åº°": "ç—…",
    "è œ": "çƒ¦",
    "è™‰": "æ„",
    "ç˜¬": "å¸",
    "ç«±": "è½¬",
    "é•¾": "è¿·",
    "ç©²": "ç¦»",
    "è²–": "æ„",
    "ç€Š": "ç›˜",
    "è°": "è·¯",
    "ç“—": "ç©·",
    "é´¡": "å±…",
    "ç¤ˆ": "ç¼€",
    "è¥½": "è“",
    "ç­•": "è¡Œ",
    "é½‚": "è°¢",
    "å¾": "åŠ©",
    "è±¼": "çš®",
    "éœ¶": "æ—",
    "æ¡": "ç©·",
    "é¦·": "é…",
    "é¨†": "å‘¨",
    "ç": "ç¦»",
    "åŒ©": "æ¡†",
    "æŸ¼": "å’¬",
    "è´": "é™ˆ",
    "è¤®": "åº”",
    "é‰•": "å¡",
    "é—£": "è¡",
    "è»µ": "å®¹",
    "çˆ¦": "æ‡’",
    "æ›¥": "èŠ¦",
    "æ¯ ": "å®¶",
    "é‡¥": "å·§",
    "ç“³": "èƒ¡",
    "åŒ¥": "çƒ¦",
    "å¶³": "åœ°",
    "è¸“": "ä¼Ÿ",
    "ç±¢": "è¿",
    "è›¥": "è›‡",
    "è˜‰": "ç›Ÿ",
    "ç„": "æ··",
    "é€«": "è§‰",
    "é°": "æ•²",
    "æ­": "æ„",
    "é•»": "å ",
    "ç«³": "ç¯",
    "éº”": "å°±",
    "æ§£": "æœº",
    "æ­µ": "åˆ™",
    "è¥‰": "å‡",
    "çŸ¡": "è§‰",
    "é›“": "é±¼",
    "è¦„": "çˆ¶",
    "èµ®": "ä¾ ",
    "è —": "ç€",
    "è‡„": "è§‰",
    "å»": "è¯·",
    "æˆ": "å®¹",
    "å¬Š": "ç‡•",
    "ç‰„": "æª",
    "å¬˜": "å²",
    "ç¤¢": "å…»",
    "é¸‹": "å‡",
    "å¨º": "ç€",
    "ç¤": "å¤œ",
    "åœ¸": "å±±",
    "å´«": "å“­",
    "è´": "ä¸‡",
    "ç…˜": "ç¼ ",
    "ç£¤": "å¼•",
    "ç‡º": "è´º",
    "é¡€": "å‚",
    "é‡Ÿ": "å…«",
    "çŸ": "å®£",
    "æ”—": "æ¢…",
    "æ›Š": "è´¹",
    "å‘": "å°º",
    "é·": "å¤œ",
    "ç¬½": "æ•",
    "å“›": "åˆ†",
    "é¼¿": "ç‰©",
    "èš­": "å°¼",
    "ç®ƒ": "é‚¹",
    "èœ½": "ä¸¤",
    "é‘™": "æœº",
    "è¶˜": "ä¹ ",
    "ç™§": "åˆ©",
    "è«…": "è®°",
    "è€Š": "å ",
    "æ‡˜": "èµ¤",
    "é»Š": "è¯",
    "æ©": "æ˜¥",
    "è±“": "ç‡•",
    "è·œ": "å°¼",
    "é¨»": "åŒ",
    "éŒ“": "ç©º",
    "éµ£": "èµ–",
    "ç¸˜": "è¥¿",
    "æ¦º": "èƒœ",
    "ç–¦": "è§‰",
    "è¦’": "å¸½",
    "è¥”": "æ»¡",
    "å»œ": "å›¾",
    "æ“™": "å¥¥",
    "ç¿": "è„‘",
    "ç“¾": "çŒ›",
    "è¸ƒ": "æ¶ˆ",
    "è€º": "äº‘",
    "æ•¡": "æ„",
    "éˆ¯": "å›¾",
    "é—§": "ä»–",
    "ç¸™": "å®¹",
    "ç¥½": "æœ€",
    "è¥º": "å‡",
    "æ© ": "æŒª",
    "é¤¶": "å¤",
    "æ¨": "é²",
    "éŒˆ": "å·",
    "åº": "è´¥",
    "éµ“": "åš",
    "æª±": "å…¶",
    "é³­": "é›•",
    "è«ƒ": "é™ˆ",
    "åšª": "è›‹",
    "æ‹€": "å¤„",
    "è¼¡": "ç ",
    "æ¯¤": "æ‹“",
    "é™¯": "ä¼¦",
    "é§«": "çª˜",
    "é´¶": "å¤¹",
    "é ¿": "å§¿",
    "è ¤": "ç§‹",
    "ç‡‡": "ä¿Š",
    "é½ƒ": "æ¶",
    "è": "ç¦»",
    "æ«·": "å½’",
    "æ¦": "æ„",
    "é½": "æ¡¥",
    "ç”‰": "é—²",
    "å´£": "ä¼Ÿ",
    "å˜•": "å…ˆ",
    "æ†": "èµ¤",
    "å¡§": "çˆ±",
    "è¨°": "å‡†",
    "ç¶": "æ˜¥",
    "é¤„": "å¤¹",
    "è´€": "æ„",
    "é¨¦": "æ€",
    "åºŒ": "å“‘",
    "è²„": "å››",
    "å¢µ": "è°ˆ",
    "æª˜": "å¹³",
    "è›½": "è¢«",
    "è¥³": "å…ˆ",
    "åš": "è½°",
    "è™": "è™",
    "è¬µ": "ä¹ ",
    "å¡·": "é²",
    "é­§": "è¡Œ",
    "ç³‘": "è¯º",
    "ç¿²": "é£˜",
    "è£": "æ†",
    "è¶¥": "ç§‹",
    "è˜¨": "æ‘‡",
    "é¬—": "ç’",
    "å·€": "èŠ‚",
    "çƒ¥": "å·¨",
    "æ«‡": "å©†",
    "è˜£": "å·",
    "æ¾©": "å­¦",
    "ç³„": "æ‰",
    "è”": "ç±³",
    "é¸„": "æœº",
    "æ¦¸": "æ‘˜",
    "ç¢–": "ä¼¦",
    "ç‡²": "é‹",
    "ç±Š": "æ›¿",
    "æ¬†": "åŒ",
    "é€": "éš",
    "å·•": "è‚",
    "éˆ‚": "é™ˆ",
    "é¤": "æ˜¯",
    "é¦ª": "æ‹¼",
    "é§‹": "æ‹›",
    "é‰‡": "è¯—",
    "æ›": "æ„",
    "é­¿": "é›¶",
    "çƒ…": "ç»­",
    "ç¨": "æ",
    "å¼³": "é™",
    "çƒ¡": "å…‰",
    "è”–": "æ“",
    "å¾¿": "å¼„",
    "è›": "é—²",
    "é‹›": "çŸ¿",
    "é€§": "å¤",
    "è±±": "æ¸©",
    "é²": "ç´",
    "æ¶„": "å¹³",
    "ç¸”": "åŒ",
    "é¼­": "çŸ³",
    "å»…": "æ¶",
    "ç™„": "æ¡¥",
    "ç“‡": "æŸ”",
    "å—‚": "æ‘‡",
    "é³»": "ç­",
    "è›¿": "æ±‰",
    "ç±": "åº”",
    "è•±": "çƒ§",
    "å±’": "æ•",
    "è›¡": "æ„",
    "é¤": "äº†",
    "é°": "å·",
    "è¤¿": "æ›¹",
    "è©‹": "æ˜¼",
    "è¨²": "æ„",
    "è½‹": "é­‚",
    "æˆ‚": "è¿·",
    "éœ¬": "æ„",
    "å´º": "ä»¥",
    "éº†": "åŠ©",
    "è›š": "è£‚",
    "è—±": "ä¼š",
    "æ¿—": "å¯†",
    "å£—": "è¿›",
    "æ¨š": "è·¯",
    "è—ƒ": "æ¶ˆ",
    "ç‰«": "æ­Œ",
    "è‰": "é€ ",
    "è¦­": "æ˜",
    "æ’›": "å",
    "è»¯": "ç °",
    "è¼«": "æ’",
    "èš™": "ç´",
    "è¸—": "è‚",
    "çƒ¢": "æ’¤",
    "æ›ƒ": "å¸¦",
    "é±": "æ³¢",
    "é©–": "é“",
    "å¤": "å¤–",
    "éº": "åˆ˜",
    "è‰œ": "å¸¦",
    "å´‰": "è¸",
    "é¿": "ç§°",
    "åºº": "æ¾",
    "è¸œ": "æ„£",
    "é¥–": "ä½",
    "è•›": "æ",
    "çµ©": "è·³",
    "æ¿": "æ¾",
    "è¬»": "å®œ",
    "é¬": "åˆ©",
    "è‘": "ç§€",
    "ç‡¢": "å­¦",
    "è™¦": "æˆ˜",
    "å»€": "æœ",
    "å¬†": "è¥¿",
    "åµŸ": "å †",
    "ç«€": "ç§°",
    "éƒ": "æˆ–",
    "çˆ": "ç“®",
    "è•®": "ç³»",
    "æ«…": "æœº",
    "å¥Ÿ": "å´©",
    "åš‰": "å¤š",
    "é´‘": "å¦‚",
    "æ¬”": "è§‰",
    "çƒ": "ç ´",
    "èœ³": "è¹²",
    "å¹§": "æ•²",
    "ç„©": "å¹³",
    "é¸ƒ": "å®œ",
    "è»‡": "åˆ°",
    "æ“®": "èŠ‚",
    "é¬¡": "å‡",
    "è—": "å†Œ",
    "åšº": "è¸",
    "ç©¥": "ç‰",
    "ç‡": "ä»",
    "ç„¨": "å¥‰",
    "è”‰": "æ»š",
    "çˆ": "åˆ©",
    "å©": "ç°",
    "å£¾": "å¿™",
    "æ‡«": "è‡³",
    "é¸‰": "ç¾Š",
    "éƒ": "çœ¼",
    "æ¶‹": "çª",
    "é¼¨": "ä¸­",
    "è§¾": "ç‡•",
    "é‘§": "å®½",
    "ç£": "ä¸‡",
    "é‘†": "ç¼€",
    "ç£–": "å•¦",
    "è¨¦": "é™ˆ",
    "å™Š": "ç‰",
    "ç": "å†¤",
    "æ‡–": "æ‰©",
    "é™®": "å¯¹",
    "ç†½": "ç¬‘",
    "æš½": "æ—",
    "æ”‚": "æ³ª",
    "æµŒ": "ç½š",
    "ç£°": "å–„",
    "è‚‘": "åš",
    "éŠ": "èµ¤",
    "æ®ˆ": "ç»­",
    "æ„¹": "æ°¸",
    "è³¹": "æ„",
    "æš¬": "è°¢",
    "è¦Ÿ": "è‡³",
    "é¬«": "ç½•",
    "éŸ—": "è¿",
    "é¶·": "ä¾ ",
    "å¨¹": "é—²",
    "æ‡¬": "çŸ¿",
    "èŸµ": "é™¤",
    "æ¨³": "å¯»",
    "è–": "å­™",
    "è¼": "å¯",
    "é†": "ç£",
    "ç‰Š": "æœ",
    "é­²": "èŠ¦",
    "é··": "å°Š",
    "æ…‚": "æ°¸",
    "é­º": "å’Œ",
    "æ¾·": "æ…¢",
    "è¡‘": "é›¶",
    "é§¾": "é€€",
    "è‚³": "ç¨³",
    "é»†": "å…‰",
    "æ¿Œ": "è¸",
    "ç–": "ç±³",
    "æ®¬": "åº¦",
    "ç¿“": "é‹",
    "ç¯–": "å”",
    "è…ª": "è¿",
    "è¹¨": "å¹´",
    "æ‡¤": "æ„",
    "éŠ¸": "å“²",
    "æ¼½": "æ",
    "éˆ‹": "é¢",
    "è§¨": "æ··",
    "é†ˆ": "è°ˆ",
    "é‰ ": "å…»",
    "é¦¶": "çŸ¥",
    "å¶¬": "å®œ",
    "æšº": "å¦",
    "æ‡¢": "è“",
    "è¸": "é±¼",
    "è·‰": "é›¶",
    "è¼½": "ç¬¨",
    "é„“": "å¤œ",
    "è­’": "è–„",
    "è²‡": "æ˜†",
    "æ†Œ": "ç©·",
    "å¨": "å¿™",
    "æ¯¢": "å¡",
    "ç±‘": "èµš",
    "è‰ ": "ç¯",
    "æ§": "é¾™",
    "å¶›": "èŠ",
    "è¬­": "å‡",
    "é¡¦": "æ¡¥",
    "è…›": "å§",
    "é°ª": "é¢",
    "è»œ": "é‚£",
    "é¸": "è¿·",
    "ç“": "é¸¾",
    "çƒµ": "ç€",
    "é„»": "è„¸",
    "é¶§": "åº”",
    "é¶€": "å…¶",
    "é¥": "æ¡",
    "ç¾»": "å‘›",
    "éƒ¹": "å±€",
    "è´’": "é—²",
    "è‰¥": "æ",
    "æª´": "æˆ–",
    "é³µ": "å®",
    "é«": "ç¦»",
    "è«": "ç€",
    "æ†¦": "çƒ™",
    "å¶©": "è„‘",
    "å¶±": "å¯",
    "é™": "å‚¬",
    "é‰Ÿ": "æ‰¹",
    "è»¬": "é¥­",
    "éŒ°": "æ ‘",
    "é‘¬": "è§",
    "æƒ": "ç‰",
    "æ¬°": "ç»­",
    "ç†ª": "å®œ",
    "éŒª": "èˆ”",
    "ç‚¨": "è°¢",
    "å¤“": "ä¸‹",
    "ç€¶": "æ—",
    "è—†": "å‡",
    "ç„ˆ": "è¥¿",
    "è­¿": "ä¼š",
    "é¬": "å‰",
    "é©§": "å±€",
    "è»ˆ": "åº”",
    "è¬¶": "ç€",
    "é¡‚": "èµ–",
    "æ¿·": "è´¹",
    "åš½": "ç»°",
    "è„‹": "é‹",
    "é¡": "äº",
    "ç¢Š": "é—´",
    "ç„": "è¥¿",
    "é¡©": "çœ¼",
    "æ»œ": "é«˜",
    "ç‡…": "å¯»",
    "ç„£": "åµ",
    "å£§": "é¢œ",
    "è‚’": "æ¢",
    "å¶¹": "å¯¼",
    "é´¯": "è€Œ",
    "æ¨ˆ": "æƒ…",
    "æ¼": "ç›´",
    "é‰£": "èŠ‚",
    "é½": "ç¼–",
    "é¯‘": "è¥¿",
    "èœ": "æ—‹",
    "ç‡Œ": "åŸ",
    "é­": "å…«",
    "è¸¾": "ç¦",
    "è·’": "æ°",
    "å”Ÿ": "å»",
    "æ«—": "ç­",
    "è™": "æ•",
    "è·Š": "å¦¹",
    "ç¡Ÿ": "é¢¤",
    "é£¸": "æ",
    "è³": "é™ˆ",
    "éŸ„": "äº’",
    "è¸¿": "æ—",
    "é¶‹": "å±…",
    "è£—": "åˆ˜",
    "æ„±": "æ",
    "éŠ¿": "ä¸­",
    "åš": "è¿›",
    "æº­": "åˆ™",
    "æ­½": "å“²",
    "å¥Š": "é‹",
    "ç†§": "å®—",
    "é«µ": "è€Œ",
    "ç€": "é—´",
    "è²¥": "è¡Œ",
    "éŠ": "å›",
    "çƒŒ": "ä¿®",
    "éµŒ": "å›¾",
    "é–•": "è™¾",
    "è°»": "æ",
    "é­±": "èƒ¡",
    "ç£®": "ä¼¦",
    "éŸ’": "å·§",
    "é‰˜": "ç¦",
    "éŒ”": "è¸",
    "è»¤": "å‘¼",
    "è˜Œ": "é›¨",
    "é¼§": "é©®",
    "æ¦Œ": "é€¼",
    "é‡¯": "å¿™",
    "æ† ": "è§‰",
    "èŸ": "ç¦»",
    "é¨¥": "æŸ”",
    "æ»°": "é™",
    "é¡¨": "è®­",
    "æ…¸": "åœ°",
    "é§": "æ˜¼",
    "ç„¤": "è¾…",
    "èŒ°": "é±¼",
    "è·": "çš®",
    "è‘¤": "æ˜¼",
    "æ–ª": "å–",
    "å¹’": "ä¸­",
    "éŒ·": "å˜",
    "çŸ": "çœ ",
    "æ¿": "å…¶",
    "å «": "å®—",
    "æ¿»": "ä¼Ÿ",
    "ç†": "ç©·",
    "è©¸": "è¿·",
    "è—’": "å¦¾",
    "é§·": "è€¸",
    "è´†": "æ ‡",
    "é¢¬": "è™¾",
    "å—©": "é”",
    "ç¡›": "æ„",
    "è«“": "è§",
    "æ“³": "è‡³",
    "è©„": "å ",
    "ç": "ç»­",
    "æ€¾": "æŒ‡",
    "æ¨¤": "æ¡",
    "è®‡": "é“²",
    "ç­™": "æ¥",
    "éš­": "è€Œ",
    "çˆ‘": "è§‰",
    "é±Œ": "å‘",
    "éª©": "ä¼Ÿ",
    "æ¬": "å†œ",
    "é«°": "æ›¿",
    "é‡«": "å",
    "è“µ": "èŠ‚",
    "é‰": "è¾¾",
    "é…": "è´¹",
    "å°€": "å¡",
    "ç¢…": "å›",
    "ç¹±": "èª",
    "é®›": "ä¹¦",
    "é²": "æ°‘",
    "é™±": "å±…",
    "è®”": "å¼•",
    "ç«": "å¤„",
    "ç±š": "èŠ¦",
    "è¤£": "å®¹",
    "ç€ƒ": "å››",
    "èœ¸": "æµ…",
    "è¬¤": "æ ‡",
    "è¸¥": "å¦¾",
    "è™ ": "æ•™",
    "è ": "èŠ‚",
    "è¬®": "åˆ™",
    "æ­¶": "é±¼",
    "é·ª": "åº”",
    "è€›": "å®œ",
    "é¸ˆ": "å¤œ",
    "è˜": "è„¸",
    "æ…€": "ç³»",
    "éƒ": "çº¢",
    "å–¡": "ç»´",
    "é §": "å †",
    "è¦µ": "è§",
    "æ’ª": "ç¬¨",
    "é •": "å•",
    "è„": "æ•",
    "è¶‡": "ç³»",
    "è¦›": "å¯†",
    "é£¿": "å •",
    "ç£¸": "å®š",
    "èºµ": "å¿",
    "é¯¦": "å°±",
    "é›µ": "å…»",
    "è—£": "æ¯",
    "è‰": "è¢œ",
    "è¢°": "æ³¢",
    "é‹µ": "çª",
    "é¤©": "æ¶",
    "è¬±": "æ¥¼",
    "æ…—": "èµ¤",
    "æ“›": "å¤œ",
    "æ¯º": "ä¹¦",
    "é¥": "ç§€",
    "æ’": "æ„",
    "è­‡": "æ‰",
    "æ©¨": "åŸ",
    "é‹“": "æ€",
    "é­©": "å¢¨",
    "è¹": "é­",
    "é ": "æ•²",
    "é‡»": "æ±‚",
    "æ„µ": "é€†",
    "é´¹": "ç¾Š",
    "é¾": "æ˜¯",
    "é°": "ä¹Œ",
    "é‡š": "æ±‚",
    "ç”": "è±ª",
    "é·": "åŒ",
    "è«‘": "ç€",
    "ç€„": "è‡³",
    "è»™": "é™ˆ",
    "è¬½": "é™",
    "ç®²": "æ˜¾",
    "è¸": "æ•²",
    "ç": "è§‰",
    "è": "æ",
    "é¶£": "å",
    "ç­‚": "æŒ",
    "è…—": "çš®",
    "éŒŠ": "æœ€",
    "ç‡¯": "é›¶",
    "çŒˆ": "è´¥",
    "é¡ª": "ä¼š",
    "ç®º": "æ˜¥",
    "å„": "æ„",
    "ç„·": "çš®",
    "æ™‡": "éœ€",
    "è¬º": "å“²",
    "è»": "ä¿¡",
    "å¼²": "å®£",
    "è‰”": "åˆ°",
    "ç†ƒ": "ç‰©",
    "è—–": "é—²",
    "æ¨¦": "åŠ©",
    "è”": "æ•Œ",
    "ç©": "å®¹",
    "é§€": "ç”±",
    "é±±": "åˆ©",
    "å£›": "é¢œ",
    "è³Ÿ": "èˆ”",
    "æª¹": "ä¸€",
    "é®¾": "å†…",
    "è¥‘": "ä¿¡",
    "çŠ¥": "é£˜",
    "è±": "é€¼",
    "éŒŒ": "æŒ‰",
    "éŒ‰": "æ°‘",
    "çŸƒ": "å‡",
    "éˆ˜": "ä»¥",
    "çº": "ä¼š",
    "ç§›": "æ‰¹",
    "é‰™": "çª„",
    "è¤¬": "æ¡‘",
    "ç­Ÿ": "å¤«",
    "è“”": "å’¬",
    "ç€©": "å¯¹",
    "éœ¯": "è…¾",
    "ç¡¹": "æ¾",
    "è”›": "èƒ¡",
    "æš›": "é”",
    "è§": "å…¶",
    "è™¥": "æˆ˜",
    "é·Œ": "é©¬",
    "è©½": "é¢œ",
    "æªˆ": "æ—‹",
    "éº": "æ±¤",
    "æ®”": "æ„",
    "è¹®": "å…ˆ",
    "é¦©": "åŸ",
    "æµ«": "ç½•",
    "æ¬¨": "éœ€",
    "è™ƒ": "é—´",
    "é¤«": "è¿",
    "æš": "è§’",
    "æ¯Ÿ": "è£‚",
    "ç†«": "è‡³",
    "ç¨ª": "ç¦",
    "åš‹": "æ„",
    "æ­š": "å–„",
    "çŠ": "å€¦",
    "è•½": "å†œ",
    "è±ƒ": "ç½•",
    "é·­": "çƒ¦",
    "é¯¼": "å®—",
    "å¹©": "åŸ",
    "ç¢·": "é¡¿",
    "ç±•": "æ˜¼",
    "é´­": "å †",
    "å¶œ": "é‡‘",
    "å¼¡": "è§‰",
    "æ«": "å‰",
    "é¡¤": "æ‘‡",
    "è£": "æ",
    "é‹ ": "è‚¾",
    "å»¤": "è£¤",
    "ç¯": "è‚",
    "è²ˆ": "å’Œ",
    "è³Œ": "è¯¥",
    "è¸‚": "è‚",
    "é€": "ä¿®",
    "è«š": "å",
    "é§¨": "ç†",
    "å¡¦": "é•‡",
    "æ—š": "é£˜",
    "é”": "å®œ",
    "æ“ˆ": "é“º",
    "èœ": "å…¶",
    "é®‚": "æ±‚",
    "ç°¢": "æ•",
    "éªª": "ä¼Ÿ",
    "è‡¹": "ä¿®",
    "æµ³": "æ„",
    "ç³": "å¤«",
    "é§£": "æ¡ƒ",
    "é±": "ç«¹",
    "ç‡": "è§’",
    "é´¼": "è½",
    "æ¦’": "è¯º",
    "è¢": "è¦",
    "é³¨": "åˆ©",
    "é¯": "åŒ",
    "æ²€": "ç»­",
    "æ†": "ä»",
    "é±ƒ": "ä¿®",
    "è‰Š": "åš",
    "è«©": "æ™®",
    "è¦«": "æ—",
    "é ª": "æ³ª",
    "é¡ƒ": "è°ˆ",
    "é‘‰": "å’Œ",
    "é¥†": "å¿…",
    "æ¯©": "å±€",
    "é¨¼": "è·¯",
    "è”": "æ¡",
    "é¯„": "æ±‚",
    "æ¯ˆ": "æ–­",
    "é¢¹": "ä¼Ÿ",
    "é ›": "æ³ª",
    "é¶ƒ": "æ„",
    "éŠº": "è—",
    "é´©": "é“",
    "é¡²": "æ‡’",
    "å¡": "é€Ÿ",
    "éœ˜": "åŠ¨",
    "é§ ": "åˆ˜",
    "é¨¿": "å¼ ",
    "é•º": "è¢„",
    "å¼¤": "åº•",
    "ç„’": "æ—…",
    "é­»": "ä¾ ",
    "è´ƒ": "ä¸‡",
    "éœ•": "å",
    "æš³": "ä¼š",
    "é§¤": "è‡³",
    "é¨": "æŸ",
    "é¶¦": "èƒ¡",
    "æ§‚": "å­™",
    "é ¶": "èƒ¡",
    "è¥Š": "ç¿ ",
    "å¶": "ç‰",
    "é¦¸": "ä¿¡",
    "æ¨®": "çƒŸ",
    "èª": "ç—…",
    "æ©©": "ç©·",
    "æƒ¾": "å®—",
    "ç˜¶": "å—½",
    "éšµ": "è¥¿",
    "é¬‡": "ç",
    "å¶–": "çƒŸ",
    "éšš": "å”",
    "æ©½": "è¸",
    "é¸…": "åˆ™",
    "æ©³": "èƒœ",
    "éŠµ": "å‘",
    "é¨Œ": "å®—",
    "è·": "åˆ©",
    "æ¯»": "æ‹“",
    "è¹—": "è·¯",
    "é·¤": "æ",
    "å¶¥": "è§‰",
    "é¬‘": "è¿",
    "é£·": "è§£",
    "è”…": "é¢œ",
    "æ¯²": "å¤š",
    "çŒ": "æª",
    "è‚": "æ±‚",
    "èª": "å®‰",
    "ç¨©": "è®°",
    "æ²": "æˆ–",
    "è¢¬": "ç‰",
    "è†": "å…»",
    "é´®": "ä¹Œ",
    "ç³‹": "è§",
    "é©": "è§",
    "é£¹": "å®",
    "é¶³": "è¯—",
    "æ—«": "æŒ‘",
    "è­‘": "è§’",
    "éŒ¥": "ç‰",
    "é±›": "å¢",
    "éŸ•": "æ‰©",
    "èª™": "å‘",
    "éŒœ": "è‚",
    "è–“": "æ·±",
    "é¤¢": "ä¸",
    "ç©’": "è´º",
    "é¶™": "æ",
    "èªŸ": "ç¬‘",
    "è¬´": "æ»š",
    "é© ": "ç‡•",
    "èªˆ": "ä¹Œ",
    "é»": "èŠ‚",
    "éµ‹": "è®°",
    "æ“¹": "è´ª",
    "ç¡½": "çƒŸ",
    "è¥": "åš",
    "ç³£": "ä¸‰",
    "ç¥ª": "é¬¼",
    "è¥¨": "å¯¹",
    "éš": "ç‡•",
    "é¤‡": "åŒ",
    "è¦£": "å¾®",
    "éµ": "çº½",
    "é´º": "æ",
    "é¯": "ç¦»",
    "é«": "ä¸‡",
    "è» ": "ç‹‚",
    "è»¡": "é’±",
    "éŒ¹": "è‚¯",
    "é¿": "å§¿",
    "å¥": "é‹",
    "çƒ": "å‘¨",
    "é©": "æˆ˜",
    "è³‹": "è§’",
    "è”’": "ç†",
    "è¦¢": "é—ª",
    "éµ˜": "ä¿Š",
    "é´˜": "æ‰",
    "æº¬": "æª",
    "è¶": "è§",
    "é´": "é‚€",
    "æ¸†": "å†¤",
    "è· ": "å®œ",
    "ç±¡": "å¦¾",
    "æ¯®": "æ²™",
    "è¨™": "è®­",
    "ç›•": "é¥­",
    "èº": "å¹´",
    "é´±": "çˆ±",
    "è…µ": "å®¶",
    "æ„¥": "åº”",
    "ç°…": "é“²",
    "é®º": "çœ¨",
    "é‹§": "ç°",
    "ç€’": "è‰²",
    "è¿±": "é©®",
    "é¯…": "å±±",
    "ç¸": "å¤œ",
    "æ«€": "å…¶",
    "è®": "æ¶",
    "èº¸": "æœº",
    "å½": "è§‰",
    "çª": "ä¹Œ",
    "é·±": "é«˜",
    "ç£«": "å®—",
    "è²±": "å¿…",
    "æ¨¬": "èª",
    "é¬”": "æœ‹",
    "é¶¾": "æ±‰",
    "è«Š": "å±…",
    "é¯œ": "å¦¾",
    "è±’": "è‡³",
    "å¶¾": "å¼•",
    "æ”": "è¥",
    "è®‘": "è¦",
    "éª®": "æ„",
    "è ´": "é¼ ",
    "é£»": "è´´",
    "é‰œ": "ç¦",
    "ç¤": "èŠ‚",
    "é®·": "æ",
    "æ©£": "å‡",
    "é· ": "é±¼",
    "é¡®": "å½¬",
    "éŠ½": "ç“œ",
    "é‹·": "æœ€",
    "é£…": "åˆ˜",
    "è»£": "è½°",
    "é¯‚": "è‹",
    "é­¥": "æ¶",
    "é¬": "èœ¡",
    "é‹´": "é•‡",
    "ç³®": "ç°",
    "é­": "è¥¿",
    "çˆ´": "è§‰",
    "è«™": "è¯",
    "æº‘": "é”",
    "éœ¼": "ç³»",
    "éœ·": "ç¾Š",
    "ç‡ª": "æ€»",
    "çª²": "æœ",
    "çŠ": "è¡",
    "è¬¥": "ä»",
    "é°¬": "é’±",
    "æ„‚": "è¢«",
    "è«¿": "ä¸ƒ",
    "å£¦": "ç†",
    "é©": "è¹²",
    "è¹¥": "è¿",
    "æ“Œ": "è‰²",
    "é¤°": "é—´",
    "é¨š": "é’±",
    "é¯²": "é±¼",
    "è¹˜": "èŠ",
    "è“": "ç»­",
    "è¢»": "è€Œ",
    "æ¿–": "æ ‘",
    "è¦®": "è¥",
    "è»¶": "æ¶",
    "é¯": "èª",
    "é¥‡": "ç‰",
    "ç„": "è°¢",
    "éŸ¯": "å…ˆ",
    "æ”": "å‰",
    "æ…ƒ": "å…»",
    "å™•": "ç°",
    "ç¸‡": "å®£",
    "éš«": "åŸ",
    "è¸»": "ç“œ",
    "éµµ": "å…”",
    "æ©¬": "é’±",
    "è–’": "ç¿",
    "æ©Œ": "ç°",
    "é€": "ç›Ÿ",
    "æ¨¥": "æœ‹",
    "éŒ…": "ç¦»",
    "é§³": "è›‹",
    "é­": "èŠ‚",
    "ç¨": "æœ‹",
    "ç˜·": "è‰²",
    "é¶ˆ": "ä¸ƒ",
    "è¥“": "æ‰°",
    "éš": "å¯¼",
    "æ¾–": "é—²",
    "å·ˆ": "å±€",
    "èœŸ": "ç‰",
    "é¡Š": "å®œ",
    "è±Ÿ": "æ¶",
    "é‡ ": "ä¹±",
    "é©": "æ‹–",
    "é§¯": "æœ±",
    "é®–": "çŸ³",
    "è»­": "æ¡†",
    "è•µ": "å­™",
    "æ†¥": "åŠ³",
    "éŸ€": "é—´",
    "çŒ": "é—´",
    "ç¤": "å¿…",
    "è¸š": "ä¼¦",
    "éŠ¯": "æ€",
    "é°": "æ¸©",
    "é®‰": "é›•",
    "éµ†": "æ¨ª",
    "éµ": "å¦¥",
    "é‰–": "åŒ",
    "æ”‡": "æ˜¾",
    "é´²": "çŸ¥",
    "è¨¯": "æ´’",
    "çˆ": "æˆ",
    "éš¦": "è§’",
    "æ®¦": "é›•",
    "é®¬": "å“­",
    "è²‹": "æŒ‰",
    "èª›": "äº²",
    "è¬¯": "æ¥",
    "æ½‚": "çº¢",
    "é´ª": "ç‰",
    "é©¨": "ä¹ ",
    "æ„¸": "æ•´",
    "æ­„": "ç“œ",
    "é©™": "å ",
    "é¯¯": "è‡³",
    "é±": "æ",
    "ç±…": "é±¼",
    "è‰€": "ç¦",
    "é±œ": "ç›¸",
    "èº›": "ä½",
    "é®¼": "äº²",
    "é¯Œ": "çƒ¤",
    "è¬°": "è¿",
    "è³†": "å",
    "è®": "ä½",
    "éµ¢": "æ·±",
    "è²µ": "å",
    "è±›": "æ„",
    "éŠŸ": "æ’",
    "æ¾»": "å²",
    "è¨µ": "åƒ",
    "é°˜": "æ˜¯",
    "è»–": "ç‹‚",
    "é©": "æ‹¼",
    "é§²": "å‘¨",
    "è©œ": "æ",
    "å¾š": "ä¸œ",
    "è¡»": "ç„¶",
    "éŸ¢": "å²",
    "é§—": "æ•",
    "é³¹": "ç´",
    "éµ‡": "å¹´",
    "è£š": "è®°",
    "é´¤": "ä¸­",
    "æ‡„": "ç´",
    "è¨ ": "å®¡",
    "è»ª": "å‡¹",
    "æ†‰": "æœ‹",
    "é®™": "è¸",
    "é³«": "ç‡•",
    "éœ»": "é£",
    "éŸš": "æ ¼",
    "é¶°": "åŸ",
    "è»…": "ç‡•",
    "å–¸": "æ•",
    "è˜’": "ç§‹",
    "é¨‡": "è®¾",
    "ç“": "èˆ”",
    "å¹¥": "é•¿",
    "é¨¹": "å…¶",
    "é«¬": "æ‰¹",
    "é¶": "æ„",
    "é¬œ": "å‰",
    "é¶": "æ ‘",
    "èª¢": "ç°",
    "è¤¼": "å…ˆ",
    "é©": "å·§",
    "è•·": "ç‰",
    "è­‹": "è“",
    "é¤†": "æ‘‡",
    "è¦±": "æˆ˜",
    "é¯": "è‚˜",
    "æ½‰": "æ˜†",
    "é©‘": "åˆ˜",
    "é¯³": "åº•",
    "éµˆ": "é¥¿",
    "éš¯": "å¯¼",
    "è¨": "æ‹†",
    "é®²": "ç¦"
}


================================================
FILE: ChatTTS/res/sha256_map.json
================================================
{
	"sha256_asset_Decoder_safetensors": "77aa55e0a977949c4733df3c6f876fa85860d3298cba63295a7bc6901729d4e0",
	"sha256_asset_DVAE_safetensors"   : "1d0b044a8368c0513100a2eca98456b289e6be6a18b7a63be1bcaa315ea874d9",
	"sha256_asset_Embed_safetensors"  : "2ff0be7134934155741b643b74e32fb6bf3eec41257984459b2ed60cdb4c48b0",
	"sha256_asset_Vocos_safetensors"  : "07e5561491cce41f7f90cfdb94b2ff263ff5742c3d89339db99b17ad82cc3f44",

	"sha256_asset_gpt_config_json"         : "0aaa1ecd96c49ad4f473459eb1982fa7ad79fa5de08cde2781bf6ad1f9a0c236",
	"sha256_asset_gpt_model_safetensors"   : "cd0806fd971f52f6a22c923ec64982b305e817bcc41ca83417fcf9141b984a0f",

	"sha256_asset_tokenizer_special_tokens_map_json": "bd0ac9d9bb1657996b5c5fbcaa7d80f8de530d01a283da97f89deae5b1b8d011",
	"sha256_asset_tokenizer_tokenizer_config_json"  : "43e9d658b554fa5ee8d8e1d763349323bfef1ed7a89c0794220ab8861387d421",
	"sha256_asset_tokenizer_tokenizer_json"         : "843838a64e121e23e774cc75874c6fe862198d9f7dd43747914633a8fd89c20e"
}



================================================
FILE: ChatTTS/utils/__init__.py
================================================
from .dl import check_all_assets, download_all_assets
from .gpu import select_device
from .io import load_safetensors, get_latest_modified_file, del_all
from .log import logger



================================================
FILE: ChatTTS/utils/dl.py
================================================
import os
from pathlib import Path
import hashlib
import requests
from io import BytesIO
from typing import Dict, Tuple, Optional
from mmap import mmap, ACCESS_READ

from .log import logger


def sha256(fileno: int) -> str:
    data = mmap(fileno, 0, access=ACCESS_READ)
    h = hashlib.sha256(data).hexdigest()
    del data
    return h


def check_model(
    dir_name: Path, model_name: str, hash: str, remove_incorrect=False
) -> bool:
    target = dir_name / model_name
    relname = target.as_posix()
    logger.get_logger().debug(f"checking {relname}...")
    if not os.path.exists(target):
        logger.get_logger().info(f"{target} not exist.")
        return False
    with open(target, "rb") as f:
        digest = sha256(f.fileno())
        bakfile = f"{target}.bak"
        if digest != hash:
            logger.get_logger().warning(f"{target} sha256 hash mismatch.")
            logger.get_logger().info(f"expected: {hash}")
            logger.get_logger().info(f"real val: {digest}")
            if remove_incorrect:
                if not os.path.exists(bakfile):
                    os.rename(str(target), bakfile)
                else:
                    os.remove(str(target))
            return False
        if remove_incorrect and os.path.exists(bakfile):
            os.remove(bakfile)
    return True


def check_folder(
    base_dir: Path,
    *innder_dirs: str,
    names: Tuple[str],
    sha256_map: Dict[str, str],
    update=False,
) -> bool:
    key = "sha256_"
    current_dir = base_dir
    for d in innder_dirs:
        current_dir /= d
        key += f"{d}_"

    for model in names:
        menv = model.replace(".", "_")
        if not check_model(current_dir, model, sha256_map[f"{key}{menv}"], update):
            return False
    return True


def check_all_assets(base_dir: Path, sha256_map: Dict[str, str], update=False) -> bool:
    logger.get_logger().info("checking assets...")

    if not check_folder(
        base_dir,
        "asset",
        names=(
            "Decoder.safetensors",
            "DVAE.safetensors",
            "Embed.safetensors",
            "Vocos.safetensors",
        ),
        sha256_map=sha256_map,
        update=update,
    ):
        return False

    if not check_folder(
        base_dir,
        "asset",
        "gpt",
        names=(
            "config.json",
            "model.safetensors",
        ),
        sha256_map=sha256_map,
        update=update,
    ):
        return False

    if not check_folder(
        base_dir,
        "asset",
        "tokenizer",
        names=(
            "special_tokens_map.json",
            "tokenizer_config.json",
            "tokenizer.json",
        ),
        sha256_map=sha256_map,
        update=update,
    ):
        return False

    logger.get_logger().info("all assets are already latest.")
    return True


def download_and_extract_tar_gz(
    url: str, folder: str, headers: Optional[Dict[str, str]] = None
):
    import tarfile

    logger.get_logger().info(f"downloading {url}")
    response = requests.get(url, headers=headers, stream=True, timeout=(10, 3))
    with BytesIO() as out_file:
        out_file.write(response.content)
        out_file.seek(0)
        logger.get_logger().info(f"downloaded.")
        with tarfile.open(fileobj=out_file, mode="r:gz") as tar:
            tar.extractall(folder)
        logger.get_logger().info(f"extracted into {folder}")


def download_and_extract_zip(
    url: str, folder: str, headers: Optional[Dict[str, str]] = None
):
    import zipfile

    logger.get_logger().info(f"downloading {url}")
    response = requests.get(url, headers=headers, stream=True, timeout=(10, 3))
    with BytesIO() as out_file:
        out_file.write(response.content)
        out_file.seek(0)
        logger.get_logger().info(f"downloaded.")
        with zipfile.ZipFile(out_file) as zip_ref:
            zip_ref.extractall(folder)
        logger.get_logger().info(f"extracted into {folder}")


def download_dns_yaml(url: str, folder: str, headers: Dict[str, str]):
    logger.get_logger().info(f"downloading {url}")
    response = requests.get(url, headers=headers, stream=True, timeout=(100, 3))
    with open(os.path.join(folder, "dns.yaml"), "wb") as out_file:
        out_file.write(response.content)
        logger.get_logger().info(f"downloaded into {folder}")


def download_all_assets(tmpdir: str, homedir: str, version="0.2.10"):
    import subprocess
    import platform

    archs = {
        "aarch64": "arm64",
        "armv8l": "arm64",
        "arm64": "arm64",
        "x86": "386",
        "i386": "386",
        "i686": "386",
        "386": "386",
        "x86_64": "amd64",
        "x64": "amd64",
        "amd64": "amd64",
    }
    system_type = platform.system().lower()
    architecture = platform.machine().lower()
    is_win = system_type == "windows"

    architecture = archs.get(architecture, None)
    if not architecture:
        logger.get_logger().error(f"architecture {architecture} is not supported")
        exit(1)
    try:
        BASE_URL = "https://github.com/fumiama/RVC-Models-Downloader/releases/download/"
        suffix = "zip" if is_win else "tar.gz"
        RVCMD_URL = BASE_URL + f"v{version}/rvcmd_{system_type}_{architecture}.{suffix}"
        cmdfile = os.path.join(tmpdir, "rvcmd")
        if is_win:
            download_and_extract_zip(RVCMD_URL, tmpdir)
            cmdfile += ".exe"
        else:
            download_and_extract_tar_gz(RVCMD_URL, tmpdir)
            os.chmod(cmdfile, 0o755)
        subprocess.run([cmdfile, "-notui", "-w", "0", "-H", homedir, "assets/chtts"])
    except Exception:
        BASE_URL = (
            "https://gitea.seku.su/fumiama/RVC-Models-Downloader/releases/download/"
        )
        suffix = "zip" if is_win else "tar.gz"
        RVCMD_URL = BASE_URL + f"v{version}/rvcmd_{system_type}_{architecture}.{suffix}"
        download_dns_yaml(
            "https://gitea.seku.su/fumiama/RVC-Models-Downloader/raw/branch/main/dns.yaml",
            tmpdir,
            headers={
                "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0"
            },
        )
        cmdfile = os.path.join(tmpdir, "rvcmd")
        if is_win:
            download_and_extract_zip(RVCMD_URL, tmpdir)
            cmdfile += ".exe"
        else:
            download_and_extract_tar_gz(RVCMD_URL, tmpdir)
            os.chmod(cmdfile, 0o755)
        subprocess.run(
            [
                cmdfile,
                "-notui",
                "-w",
                "0",
                "-dns",
                os.path.join(tmpdir, "dns.yaml"),
                "-H",
                homedir,
                "assets/chtts",
            ]
        )



================================================
FILE: ChatTTS/utils/gpu.py
================================================
import torch

try:
    import torch_npu
except ImportError:
    pass

from .log import logger


def select_device(min_memory=2047, experimental=False):
    has_cuda = torch.cuda.is_available()
    if has_cuda or _is_torch_npu_available():
        provider = torch.cuda if has_cuda else torch.npu
        """
        Using Ascend NPU to accelerate the process of inferencing when GPU is not found.
        """
        dev_idx = 0
        max_free_memory = -1
        for i in range(provider.device_count()):
            props = provider.get_device_properties(i)
            free_memory = props.total_memory - provider.memory_reserved(i)
            if max_free_memory < free_memory:
                dev_idx = i
                max_free_memory = free_memory
        free_memory_mb = max_free_memory / (1024 * 1024)
        if free_memory_mb < min_memory:
            logger.get_logger().warning(
                f"{provider.device(dev_idx)} has {round(free_memory_mb, 2)} MB memory left. Switching to CPU."
            )
            device = torch.device("cpu")
        else:
            device = provider._get_device(dev_idx)
    elif torch.backends.mps.is_available():
        """
        Currently MPS is slower than CPU while needs more memory and core utility,
        so only enable this for experimental use.
        """
        if experimental:
            # For Apple M1/M2 chips with Metal Performance Shaders
            logger.get_logger().warning("experimantal: found apple GPU, using MPS.")
            device = torch.device("mps")
        else:
            logger.get_logger().info("found Apple GPU, but use CPU.")
            device = torch.device("cpu")
    else:
        logger.get_logger().warning("no GPU or NPU found, use CPU instead")
        device = torch.device("cpu")

    return device


def _is_torch_npu_available():
    try:
        # will raise a AttributeError if torch_npu is not imported or a RuntimeError if no NPU found
        _ = torch.npu.device_count()
        return torch.npu.is_available()
    except (AttributeError, RuntimeError):
        return False



================================================
FILE: ChatTTS/utils/io.py
================================================
import os
import logging
from typing import Union
from dataclasses import is_dataclass

from safetensors import safe_open
import torch

from .log import logger


@torch.inference_mode()
def load_safetensors(filename: str):
    state_dict_tensors = {}
    with safe_open(filename, framework="pt") as f:
        for k in f.keys():
            state_dict_tensors[k] = f.get_tensor(k)
    return state_dict_tensors


def get_latest_modified_file(directory):

    files = [os.path.join(directory, f) for f in os.listdir(directory)]
    if not files:
        logger.get_logger().log(
            logging.WARNING, f"no files found in the directory: {directory}"
        )
        return None
    latest_file = max(files, key=os.path.getmtime)

    return latest_file


def del_all(d: Union[dict, list]):
    if is_dataclass(d):
        for k in list(vars(d).keys()):
            x = getattr(d, k)
            if isinstance(x, dict) or isinstance(x, list) or is_dataclass(x):
                del_all(x)
            del x
            delattr(d, k)
    elif isinstance(d, dict):
        lst = list(d.keys())
        for k in lst:
            x = d.pop(k)
            if isinstance(x, dict) or isinstance(x, list) or is_dataclass(x):
                del_all(x)
            del x
    elif isinstance(d, list):
        while len(d):
            x = d.pop()
            if isinstance(x, dict) or isinstance(x, list) or is_dataclass(x):
                del_all(x)
            del x
    else:
        del d



================================================
FILE: ChatTTS/utils/log.py
================================================
import logging
from pathlib import Path


class Logger:
    def __init__(self, logger=logging.getLogger(Path(__file__).parent.name)):
        self.logger = logger

    def set_logger(self, logger: logging.Logger):
        self.logger = logger

    def get_logger(self) -> logging.Logger:
        return self.logger


logger = Logger()



================================================
FILE: docs/cn/README.md
================================================
<div align="center">

<a href="https://trendshift.io/repositories/10489" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10489" alt="2noise%2FChatTTS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

# ChatTTS
ä¸€æ¬¾é€‚ç”¨äºæ—¥å¸¸å¯¹è¯çš„ç”Ÿæˆå¼è¯­éŸ³æ¨¡å‹ã€‚

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)

[![Huggingface](https://img.shields.io/badge/ğŸ¤—%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)

[**English**](../../README.md) | **ç®€ä½“ä¸­æ–‡** | [**æ—¥æœ¬èª**](../jp/README.md) | [**Ğ ÑƒÑÑĞºĞ¸Ğ¹**](../ru/README.md) | [**EspaÃ±ol**](../es/README.md) | [**FranÃ§ais**](../fr/README.md) | [**í•œêµ­ì–´**](../kr/README.md)

</div>

> [!NOTE]
> æ³¨æ„æ­¤ç‰ˆæœ¬å¯èƒ½ä¸æ˜¯æœ€æ–°ç‰ˆï¼Œæ‰€æœ‰å†…å®¹è¯·ä»¥è‹±æ–‡ç‰ˆä¸ºå‡†ã€‚

## ç®€ä»‹

> [!Note]
> è¿™ä¸ªä»“åº“åŒ…å«ç®—æ³•æ¶æ„å’Œä¸€äº›ç®€å•çš„ç¤ºä¾‹ã€‚

> [!Tip]
> ç”±æœ¬ä»“åº“è¡ç”Ÿå‡ºçš„ç”¨æˆ·ç«¯äº§å“ï¼Œè¯·å‚è§ç”±ç¤¾åŒºç»´æŠ¤çš„ç´¢å¼•ä»“åº“  [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS)ã€‚

ChatTTS æ˜¯ä¸€æ¬¾ä¸“é—¨ä¸ºå¯¹è¯åœºæ™¯ï¼ˆä¾‹å¦‚ LLM åŠ©æ‰‹ï¼‰è®¾è®¡çš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚

### æ”¯æŒçš„è¯­ç§

- [x] è‹±è¯­
- [x] ä¸­æ–‡
- [ ] æ•¬è¯·æœŸå¾…...

### äº®ç‚¹

> ä½ å¯ä»¥å‚è€ƒ **[Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)** ä¸Šçš„è¿™ä¸ªè§†é¢‘ï¼Œäº†è§£æœ¬é¡¹ç›®çš„è¯¦ç»†æƒ…å†µã€‚

1. **å¯¹è¯å¼ TTS**: ChatTTS é’ˆå¯¹å¯¹è¯å¼ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œèƒ½å¤Ÿå®ç°è‡ªç„¶ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„åˆæˆè¯­éŸ³ã€‚å®ƒæ”¯æŒå¤šä¸ªè¯´è¯è€…ï¼Œä¾¿äºç”Ÿæˆäº’åŠ¨å¼å¯¹è¯ã€‚
2. **ç²¾ç»†çš„æ§åˆ¶**: è¯¥æ¨¡å‹å¯ä»¥é¢„æµ‹å’Œæ§åˆ¶ç²¾ç»†çš„éŸµå¾‹ç‰¹å¾ï¼ŒåŒ…æ‹¬ç¬‘å£°ã€åœé¡¿å’Œæ’å…¥è¯­ã€‚
3. **æ›´å¥½çš„éŸµå¾‹**: ChatTTS åœ¨éŸµå¾‹æ–¹é¢è¶…è¶Šäº†å¤§å¤šæ•°å¼€æº TTS æ¨¡å‹ã€‚æˆ‘ä»¬æä¾›é¢„è®­ç»ƒæ¨¡å‹ä»¥æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå¼€å‘ã€‚

### æ•°æ®é›†å’Œæ¨¡å‹

- ä¸»æ¨¡å‹ä½¿ç”¨äº† 100,000+ å°æ—¶çš„ä¸­æ–‡å’Œè‹±æ–‡éŸ³é¢‘æ•°æ®è¿›è¡Œè®­ç»ƒã€‚
- **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** ä¸Šçš„å¼€æºç‰ˆæœ¬æ˜¯ä¸€ä¸ªåœ¨ 40,000 å°æ—¶æ•°æ®ä¸Šè¿›è¡Œæ— ç›‘ç£å¾®è°ƒçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚

### è·¯çº¿å›¾

- [x] å¼€æº 4 ä¸‡å°æ—¶åŸºç¡€æ¨¡å‹å’Œ spk_stats æ–‡ä»¶ã€‚
- [x] æ”¯æŒæµå¼è¯­éŸ³è¾“å‡ºã€‚
- [x] å¼€æº DVAE ç¼–ç å™¨å’Œé›¶æ ·æœ¬æ¨ç†ä»£ç 
- [ ] å¼€æºå…·æœ‰å¤šæƒ…æ„Ÿæ§åˆ¶åŠŸèƒ½çš„ 4 ä¸‡å°æ—¶ç‰ˆæœ¬ã€‚
- [ ] ChatTTS.cpp (æ¬¢è¿åœ¨ 2noise ç»„ç»‡ä¸­æ–°å»ºä»“åº“)ã€‚

### å…è´£å£°æ˜

> [!Important]
> æ­¤ä»“åº“ä»…ä¾›å­¦æœ¯ç”¨é€”ã€‚

æœ¬é¡¹ç›®æ—¨åœ¨ç”¨äºæ•™è‚²å’Œç ”ç©¶ç›®çš„ï¼Œä¸é€‚ç”¨äºä»»ä½•å•†ä¸šæˆ–æ³•å¾‹ç›®çš„ã€‚ä½œè€…ä¸ä¿è¯ä¿¡æ¯çš„å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œå¯é æ€§ã€‚æ­¤ä»“åº“ä¸­ä½¿ç”¨çš„ä¿¡æ¯å’Œæ•°æ®ä»…ä¾›å­¦æœ¯å’Œç ”ç©¶ç›®çš„ã€‚æ•°æ®æ¥è‡ªå…¬å¼€æ¥æºï¼Œä½œè€…ä¸å£°ç§°å¯¹æ•°æ®æ‹¥æœ‰ä»»ä½•æ‰€æœ‰æƒæˆ–ç‰ˆæƒã€‚

ChatTTS æ˜¯ä¸€æ¬¾å¼ºå¤§çš„æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿã€‚ä½†æ˜¯ï¼Œè´Ÿè´£ä»»å’Œé“å¾·åœ°ä½¿ç”¨è¿™é¡¹æŠ€æœ¯éå¸¸é‡è¦ã€‚ä¸ºäº†é™åˆ¶ ChatTTS çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬åœ¨ 40,000 å°æ—¶æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ äº†å°‘é‡é«˜é¢‘å™ªå£°ï¼Œå¹¶ä½¿ç”¨ MP3 æ ¼å¼å°½å¯èƒ½å‹ç¼©éŸ³é¢‘è´¨é‡ï¼Œä»¥é˜²æ­¢æ¶æ„è¡Œä¸ºè€…å°†å…¶ç”¨äºçŠ¯ç½ªç›®çš„ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å†…éƒ¨è®­ç»ƒäº†ä¸€ä¸ªæ£€æµ‹æ¨¡å‹ï¼Œå¹¶è®¡åˆ’åœ¨æœªæ¥å¼€æºå®ƒã€‚

### è”ç³»æ–¹å¼

> æ¬¢è¿éšæ—¶æäº¤ GitHub issues/PRsã€‚

#### åˆä½œæ´½è°ˆ

å¦‚éœ€å°±æ¨¡å‹å’Œè·¯çº¿å›¾è¿›è¡Œåˆä½œæ´½è°ˆï¼Œè¯·å‘é€é‚®ä»¶è‡³ **open-source@2noise.com**ã€‚

#### çº¿ä¸Šè®¨è®º

##### 1. å®˜æ–¹ QQ ç¾¤

- **ç¾¤ 1**, 808364215 (å·²æ»¡)
- **ç¾¤ 2**, 230696694 (å·²æ»¡)
- **ç¾¤ 3**, 933639842 (å·²æ»¡)
- **ç¾¤ 4**, 608667975

##### 2. Discord

ç‚¹å‡»åŠ å…¥ [Discord](https://discord.gg/Ud5Jxgx5yD)ã€‚

## ä½“éªŒæ•™ç¨‹

### å…‹éš†ä»“åº“

```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### å®‰è£…ä¾èµ–

#### 1. ç›´æ¥å®‰è£…

```bash
pip install --upgrade -r requirements.txt
```

#### 2. ä½¿ç”¨ conda å®‰è£…

```bash
conda create -n chattts
conda activate chattts
pip install -r requirements.txt
```

#### å¯é€‰ : å¦‚æœä½¿ç”¨ NVIDIA GPUï¼ˆä»…é™ Linuxï¼‰ï¼Œå¯å®‰è£… TransformerEngineã€‚

> [!Note]
> å®‰è£…è¿‡ç¨‹å¯èƒ½è€—æ—¶å¾ˆé•¿ã€‚

> [!Warning]
> TransformerEngine çš„é€‚é…ç›®å‰æ­£åœ¨å¼€å‘ä¸­ï¼Œè¿è¡Œæ—¶å¯èƒ½ä¼šé‡åˆ°è¾ƒå¤šé—®é¢˜ã€‚ä»…æ¨èå‡ºäºå¼€å‘ç›®çš„å®‰è£…ã€‚

```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable
```

#### å¯é€‰ : å®‰è£… FlashAttention-2 (ä¸»è¦é€‚ç”¨äº NVIDIA GPU)

> [!Note]
> æ”¯æŒè®¾å¤‡åˆ—è¡¨è¯¦è§ [Hugging Face Doc](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2).

```bash
pip install flash-attn --no-build-isolation
```

### å¿«é€Ÿå¯åŠ¨

> ç¡®ä¿åœ¨æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ—¶ï¼Œå¤„äºé¡¹ç›®æ ¹ç›®å½•ä¸‹ã€‚

#### 1. WebUI å¯è§†åŒ–ç•Œé¢

```bash
python examples/web/webui.py
```

#### 2. å‘½ä»¤è¡Œäº¤äº’

> ç”Ÿæˆçš„éŸ³é¢‘å°†ä¿å­˜è‡³ `./output_audio_n.mp3`

```bash
python examples/cmd/run.py "Your text 1." "Your text 2."
```

## å¼€å‘æ•™ç¨‹

### å®‰è£… Python åŒ…

1. ä» PyPI å®‰è£…ç¨³å®šç‰ˆ

```bash
pip install ChatTTS
```

2. ä» GitHub å®‰è£…æœ€æ–°ç‰ˆ

```bash
pip install git+https://github.com/2noise/ChatTTS
```

3. ä»æœ¬åœ°æ–‡ä»¶å¤¹å®‰è£…å¼€å‘ç‰ˆ

```bash
pip install -e .
```

### åŸºç¡€ç”¨æ³•

```python
import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) # Set to True for better performance

texts = ["PUT YOUR 1st TEXT HERE", "PUT YOUR 2nd TEXT HERE"]

wavs = chat.infer(texts)

torchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)
```

### è¿›é˜¶ç”¨æ³•

```python
###################################
# Sample a speaker from Gaussian.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # add sampled speaker 
    temperature = .3,   # using custom temperature
    top_P = 0.7,        # top P decode
    top_K = 20,         # top K decode
)

###################################
# For sentence level manual control.

# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# For word level manual control.

text = 'What is [uv_break]your favorite english food?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save("output2.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>ç¤ºä¾‹: è‡ªæˆ‘ä»‹ç»</h4></summary>

```python
inputs_en = """
chatTTS is a text to speech model designed for dialogue applications.
[uv_break]it supports mixed language input [uv_break]and offers multi speaker
capabilities with precise control over prosodic elements like
[uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation.
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
""".replace('\n', '') # English is still experimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save("output3.wav", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align="center">

**ç”·æ€§éŸ³è‰²**

</td>
<td align="center">

**å¥³æ€§éŸ³è‰²**

</td>
</tr>
<tr>
<td align="center">

[ç”·æ€§éŸ³è‰²](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align="center">

[å¥³æ€§éŸ³è‰²](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>

</details>

## å¸¸è§é—®é¢˜

#### 1. æˆ‘éœ€è¦å¤šå°‘ VRAMï¼Ÿ æ¨ç†é€Ÿåº¦å¦‚ä½•ï¼Ÿ

å¯¹äº 30 ç§’çš„éŸ³é¢‘ç‰‡æ®µï¼Œè‡³å°‘éœ€è¦ 4GB çš„ GPU å†…å­˜ã€‚ å¯¹äº 4090 GPUï¼Œå®ƒå¯ä»¥æ¯ç§’ç”Ÿæˆå¤§çº¦ 7 ä¸ªè¯­ä¹‰ token å¯¹åº”çš„éŸ³é¢‘ã€‚å®æ—¶å› å­ (RTF) çº¦ä¸º 0.3ã€‚

#### 2. æ¨¡å‹ç¨³å®šæ€§ä¸å¤Ÿå¥½ï¼Œå­˜åœ¨å¤šä¸ªè¯´è¯è€…æˆ–éŸ³é¢‘è´¨é‡å·®ç­‰é—®é¢˜ã€‚

è¿™æ˜¯ä¸€ä¸ªé€šå¸¸å‘ç”Ÿåœ¨è‡ªå›å½’æ¨¡å‹ï¼ˆä¾‹å¦‚ bark å’Œ valleï¼‰ä¸­çš„é—®é¢˜ï¼Œé€šå¸¸å¾ˆéš¾é¿å…ã€‚å¯ä»¥å°è¯•å¤šä¸ªæ ·æœ¬ä»¥æ‰¾åˆ°åˆé€‚çš„ç»“æœã€‚

#### 3. é™¤äº†ç¬‘å£°ï¼Œæˆ‘ä»¬è¿˜èƒ½æ§åˆ¶å…¶ä»–ä¸œè¥¿å—ï¼Ÿæˆ‘ä»¬èƒ½æ§åˆ¶å…¶ä»–æƒ…ç»ªå—ï¼Ÿ

åœ¨å½“å‰å‘å¸ƒçš„æ¨¡å‹ä¸­ï¼Œå¯ç”¨çš„ token çº§æ§åˆ¶å•å…ƒæ˜¯ `[laugh]`, `[uv_break]` å’Œ `[lbreak]`ã€‚æœªæ¥çš„ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå¼€æºå…·æœ‰æ›´å¤šæƒ…ç»ªæ§åˆ¶åŠŸèƒ½çš„æ¨¡å‹ã€‚

## è‡´è°¢

- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) å’Œ [valle](https://arxiv.org/abs/2301.02111) é€šè¿‡è‡ªå›å½’å¼ç³»ç»Ÿå±•ç¤ºäº†éå‡¡çš„ TTS æ•ˆæœã€‚
- [fish-speech](https://github.com/fishaudio/fish-speech) æ­ç¤ºäº† GVQ ä½œä¸º LLM å»ºæ¨¡çš„éŸ³é¢‘åˆ†è¯å™¨çš„èƒ½åŠ›ã€‚
- [vocos](https://github.com/gemelo-ai/vocos) vocos è¢«ç”¨ä½œé¢„è®­ç»ƒå£°ç å™¨ã€‚

## ç‰¹åˆ«é¸£è°¢

- [wlu-audio lab](https://audio.westlake.edu.cn/) å¯¹äºæ—©æœŸç®—æ³•å®éªŒçš„æ”¯æŒã€‚

## è´¡çŒ®è€…åˆ—è¡¨

[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

## é¡¹ç›®æµè§ˆé‡

<div align="center">

![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>



================================================
FILE: docs/es/README.md
================================================
<div align="center">

<a href="https://trendshift.io/repositories/10489" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10489" alt="2noise%2FChatTTS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

# ChatTTS
Un modelo de generaciÃ³n de voz para la conversaciÃ³n diaria.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)

[![Huggingface](https://img.shields.io/badge/ğŸ¤—%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)

[**English**](../../README.md) | [**ç®€ä½“ä¸­æ–‡**](../cn/README.md) | [**æ—¥æœ¬èª**](../jp/README.md) | [**Ğ ÑƒÑÑĞºĞ¸Ğ¹**](../ru/README.md) | **EspaÃ±ol**
 | [**FranÃ§ais**](../fr/README.md) | [**í•œêµ­ì–´**](../kr/README.md)
</div>

> [!NOTE]
> AtenciÃ³n, es posible que esta versiÃ³n no sea la Ãºltima. Por favor, consulte la versiÃ³n en inglÃ©s para conocer todo el contenido.

## IntroducciÃ³n

ChatTTS es un modelo de texto a voz diseÃ±ado especÃ­ficamente para escenarios conversacionales como LLM assistant.

### Idiomas Soportados

- [x] InglÃ©s
- [x] Chino
- [ ] MantÃ©nganse al tanto...

### Aspectos Destacados

> Puede consultar **[este video en Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)** para obtener una descripciÃ³n detallada.

1. **TTS Conversacional**: ChatTTS estÃ¡ optimizado para tareas conversacionales, logrando una sÃ­ntesis de voz natural y expresiva. Soporta mÃºltiples hablantes, lo que facilita la generaciÃ³n de diÃ¡logos interactivos.
2. **Control Finas**: Este modelo puede predecir y controlar caracterÃ­sticas detalladas de la prosodia, incluyendo risas, pausas e interjecciones.
3. **Mejor Prosodia**: ChatTTS supera a la mayorÃ­a de los modelos TTS de cÃ³digo abierto en cuanto a prosodia. Ofrecemos modelos preentrenados para apoyar estudios y desarrollos adicionales.

### Conjunto de Datos & Modelo

- El modelo principal se entrena con mÃ¡s de 100.000 horas de datos de audio en chino e inglÃ©s.
- La versiÃ³n de cÃ³digo abierto en **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** es un modelo preentrenado con 40.000 horas, sin SFT.

### Hoja de Ruta

- [x] Publicar el modelo base de 40k horas y el archivo spk_stats como cÃ³digo abierto
- [ ] Publicar los cÃ³digos de codificador VQ y entrenamiento de Lora como cÃ³digo abierto
- [ ] GeneraciÃ³n de audio en streaming sin refinar el texto
- [ ] Publicar la versiÃ³n de 40k horas con control de mÃºltiples emociones como cÃ³digo abierto
- [ ] Â¿ChatTTS.cpp? (Se aceptan PR o un nuevo repositorio)

### Descargo de Responsabilidad

> [!Important]
> Este repositorio es sÃ³lo para fines acadÃ©micos.

Este proyecto estÃ¡ destinado a fines educativos y estudios, y no es adecuado para ningÃºn propÃ³sito comercial o legal. El autor no garantiza la exactitud, integridad o fiabilidad de la informaciÃ³n. La informaciÃ³n y los datos utilizados en este repositorio son Ãºnicamente para fines acadÃ©micos y de investigaciÃ³n. Los datos provienen de fuentes pÃºblicas, y el autor no reclama ningÃºn derecho de propiedad o copyright sobre ellos.

ChatTTS es un potente sistema de conversiÃ³n de texto a voz. Sin embargo, es crucial utilizar esta tecnologÃ­a de manera responsable y Ã©tica. Para limitar el uso de ChatTTS, hemos aÃ±adido una pequeÃ±a cantidad de ruido de alta frecuencia durante el proceso de entrenamiento del modelo de 40.000 horas y hemos comprimido la calidad del audio en formato MP3 tanto como sea posible para evitar que actores malintencionados lo usen con fines delictivos. AdemÃ¡s, hemos entrenado internamente un modelo de detecciÃ³n y planeamos hacerlo de cÃ³digo abierto en el futuro.

### Contacto

> No dudes en enviar issues/PRs de GitHub.

#### Consultas Formales

Si desea discutir la cooperaciÃ³n sobre modelos y hojas de ruta, envÃ­e un correo electrÃ³nico a **open-source@2noise.com**.

#### Chat en LÃ­nea

##### 1. Grupo QQ (AplicaciÃ³n Social China)

- **Grupo 1**, 808364215 (Lleno)
- **Grupo 2**, 230696694 (Lleno)
- **Grupo 3**, 933639842

## InstalaciÃ³n (En Proceso)

> Se cargarÃ¡ en pypi pronto segÃºn https://github.com/2noise/ChatTTS/issues/269.

```bash
pip install git+https://github.com/2noise/ChatTTS
```

## Inicio
### Clonar el repositorio
```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### Requerimientos de instalaciÃ³n
#### 1. Instalar directamente
```bash
pip install --upgrade -r requirements.txt
```

#### 2. Instalar desde conda
```bash
conda create -n chattts
conda activate chattts
pip install -r requirements.txt
```

### Inicio RÃ¡pido
#### 1. Iniciar la interfaz de usuario web (WebUI)
```bash
python examples/web/webui.py
```

#### 2. Inferir por lÃ­nea de comando
> GuardarÃ¡ el audio en `./output_audio_xxx.wav`

```bash
python examples/cmd/run.py "Please input your text."
```

### BÃ¡sico

```python
import ChatTTS
from IPython.display import Audio
import torchaudio
import torch

chat = ChatTTS.Chat()
chat.load(compile=False) # Set to True for better performance

texts = ["PUT YOUR TEXT HERE",]

wavs = chat.infer(texts)

torchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)
```

### Avanzado

```python
###################################
# Sample a speaker from Gaussian.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # add sampled speaker 
    temperature = .3,   # using custom temperature
    top_P = 0.7,        # top P decode
    top_K = 20,         # top K decode
)

###################################
# For sentence level manual control.

# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# For word level manual control.
text = 'What is [uv_break]your favorite english food?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save("output2.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Ejemplo: auto presentaciÃ³n</h4></summary>

```python
inputs_en = """
chat T T S is a text to speech model designed for dialogue applications. 
[uv_break]it supports mixed language input [uv_break]and offers multi speaker 
capabilities with precise control over prosodic elements [laugh]like like 
[uv_break]laughter[laugh], [uv_break]pauses, [uv_break]and intonation. 
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
""".replace('\n', '') # English is still experimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save("output3.wav", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align="center">

**altavoz masculino**

</td>
<td align="center">

**altavoz femenino**

</td>
</tr>
<tr>
<td align="center">

[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align="center">

[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>


</details>

## Preguntas y Respuestas

#### 1. Â¿CuÃ¡nta memoria grÃ¡fica de acceso aleatorio necesito? Â¿QuÃ© tal inferir la velocidad?
Para un clip de audio de 30 segundos, se requieren al menos 4 GB de memoria de GPU. Para la GPU 4090, puede generar audio correspondiente a aproximadamente 7 tokens semÃ¡nticos por segundo. El Factor en Tiempo Real (RTF) es aproximadamente 0,3.

#### 2. La estabilidad del modelo no es lo suficientemente buena y existen problemas como varios altavoces o mala calidad del sonido.

Este es un problema comÃºn en los modelos autorregresivos (para bark y valle). Generalmente es difÃ­cil de evitar. Puede probar varias muestras para encontrar resultados adecuados.

#### 3. Â¿Podemos controlar algo mÃ¡s que la risa? Â¿Podemos controlar otras emociones?

En el modelo lanzado actualmente, las Ãºnicas unidades de control a nivel de token son `[risa]`, `[uv_break]` y `[lbreak]`. En una versiÃ³n futura, es posible que abramos el cÃ³digo fuente del modelo con capacidades adicionales de control de emociones.

## Agradecimientos
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) y [valle](https://arxiv.org/abs/2301.02111) demuestran un resultado TTS notable mediante un sistema de estilo autorregresivo.
- [fish-speech](https://github.com/fishaudio/fish-speech) revela las capacidades de GVQ como tokenizador de audio para el modelado LLM.
- [vocos](https://github.com/gemelo-ai/vocos) se utiliza como codificador de voz previamente entrenado.

## Agradecimiento Especial
- [wlu-audio lab](https://audio.westlake.edu.cn/) para experimentos iniciales del algoritmo.

## Recursos Relacionados
- [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS)

## Gracias a todos los contribuyentes por sus esfuerzos.
[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

<div align="center">

  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>



================================================
FILE: docs/fr/README.md
================================================
<div align="center">

<a href="https://trendshift.io/repositories/10489" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10489" alt="2noise%2FChatTTS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

# ChatTTS
Un modÃ¨le de parole gÃ©nÃ©ratif pour le dialogue quotidien.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)

[![Huggingface](https://img.shields.io/badge/ğŸ¤—%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)

[**English**](../../README.md) | [**ç®€ä½“ä¸­æ–‡**](../cn/README.md) | [**æ—¥æœ¬èª**](../jp/README.md) | [**Ğ ÑƒÑÑĞºĞ¸Ğ¹**](../ru/README.md) | [**EspaÃ±ol**](../es/README.md)| **FranÃ§ais**  | [**í•œêµ­ì–´**](../kr/README.md)

</div>

## Introduction
> [!Note]
> Ce dÃ©pÃ´t contient l'infrastructure de l'algorithme et quelques exemples simples.

> [!Tip]
> Pour les produits finaux Ã©tendus pour les utilisateurs, veuillez consulter le dÃ©pÃ´t index [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS/tree/en) maintenu par la communautÃ©.

ChatTTS est un modÃ¨le de synthÃ¨se vocale conÃ§u spÃ©cifiquement pour les scÃ©narios de dialogue tels que les assistants LLM.

### Langues prises en charge
- [x] Anglais
- [x] Chinois
- [ ] Ã€ venir...

### Points forts
> Vous pouvez vous rÃ©fÃ©rer Ã  **[cette vidÃ©o sur Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)** pour une description dÃ©taillÃ©e.

1. **SynthÃ¨se vocale conversationnelle**: ChatTTS est optimisÃ© pour les tÃ¢ches basÃ©es sur le dialogue, permettant une synthÃ¨se vocale naturelle et expressive. Il prend en charge plusieurs locuteurs, facilitant les conversations interactives.
2. **ContrÃ´le granulaire**: Le modÃ¨le peut prÃ©dire et contrÃ´ler des caractÃ©ristiques prosodiques fines, y compris le rire, les pauses et les interjections.
3. **Meilleure prosodie**: ChatTTS dÃ©passe la plupart des modÃ¨les TTS open-source en termes de prosodie. Nous fournissons des modÃ¨les prÃ©-entraÃ®nÃ©s pour soutenir la recherche et le dÃ©veloppement.

### Dataset & ModÃ¨le
- Le modÃ¨le principal est entraÃ®nÃ© avec des donnÃ©es audio en chinois et en anglais de plus de 100 000 heures.
- La version open-source sur **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** est un modÃ¨le prÃ©-entraÃ®nÃ© de 40 000 heures sans SFT.

### Roadmap
- [x] Open-source du modÃ¨le de base de 40k heures et du fichier spk_stats.
- [x] GÃ©nÃ©ration audio en streaming.
- [ ] Open-source de la version 40k heures avec contrÃ´le multi-Ã©motions.
- [ ] ChatTTS.cpp (nouveau dÃ©pÃ´t dans l'organisation `2noise` est bienvenu)

### Avertissement
> [!Important]
> Ce dÃ©pÃ´t est Ã  des fins acadÃ©miques uniquement.

Il est destinÃ© Ã  un usage Ã©ducatif et de recherche, et ne doit pas Ãªtre utilisÃ© Ã  des fins commerciales ou lÃ©gales. Les auteurs ne garantissent pas l'exactitude, l'exhaustivitÃ© ou la fiabilitÃ© des informations. Les informations et les donnÃ©es utilisÃ©es dans ce dÃ©pÃ´t sont Ã  des fins acadÃ©miques et de recherche uniquement. Les donnÃ©es obtenues Ã  partir de sources accessibles au public, et les auteurs ne revendiquent aucun droit de propriÃ©tÃ© ou de copyright sur les donnÃ©es.

ChatTTS est un systÃ¨me de synthÃ¨se vocale puissant. Cependant, il est trÃ¨s important d'utiliser cette technologie de maniÃ¨re responsable et Ã©thique. Pour limiter l'utilisation de ChatTTS, nous avons ajoutÃ© une petite quantitÃ© de bruit haute frÃ©quence pendant l'entraÃ®nement du modÃ¨le de 40 000 heures et compressÃ© la qualitÃ© audio autant que possible en utilisant le format MP3, pour empÃªcher les acteurs malveillants de l'utiliser potentiellement Ã  des fins criminelles. En mÃªme temps, nous avons entraÃ®nÃ© en interne un modÃ¨le de dÃ©tection et prÃ©voyons de l'open-source Ã  l'avenir.

### Contact
> Les issues/PRs sur GitHub sont toujours les bienvenus.

#### Demandes formelles
Pour les demandes formelles concernant le modÃ¨le et la feuille de route, veuillez nous contacter Ã  **open-source@2noise.com**.

#### Discussion en ligne
##### 1. Groupe QQ (application sociale chinoise)
- **Groupe 1**, 808364215 (Complet)
- **Groupe 2**, 230696694 (Complet)
- **Groupe 3**, 933639842 (Complet)
- **Groupe 4**, 608667975

##### 2. Serveur Discord
Rejoignez en cliquant [ici](https://discord.gg/Ud5Jxgx5yD).

## Pour commencer
### Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### Installer les dÃ©pendances
#### 1. Installation directe
```bash
pip install --upgrade -r requirements.txt
```

#### 2. Installer depuis conda
```bash
conda create -n chattts
conda activate chattts
pip install -r requirements.txt
```

#### Optionnel : Installer TransformerEngine si vous utilisez un GPU NVIDIA (Linux uniquement)
> [!Note]
> Le processus d'installation est trÃ¨s lent.

> [!Warning]
> L'adaptation de TransformerEngine est actuellement en cours de dÃ©veloppement et NE PEUT PAS fonctionner correctement pour le moment.
> Installez-le uniquement Ã  des fins de dÃ©veloppement.

```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable
```

#### Optionnel : Installer FlashAttention-2 (principalement GPU NVIDIA)
> [!Note]
> Voir les appareils pris en charge dans la [documentation Hugging Face](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2).

> [!Warning]
> Actuellement, FlashAttention-2 ralentira la vitesse de gÃ©nÃ©ration selon [ce problÃ¨me](https://github.com/huggingface/transformers/issues/26990). 
> Installez-le uniquement Ã  des fins de dÃ©veloppement.

```bash
pip install flash-attn --no-build-isolation
```

### DÃ©marrage rapide
> Assurez-vous que vous Ãªtes dans le rÃ©pertoire racine du projet lorsque vous exÃ©cutez ces commandes ci-dessous.

#### 1. Lancer WebUI
```bash
python examples/web/webui.py
```

#### 2. InfÃ©rence par ligne de commande
> Cela enregistrera l'audio sous â€˜./output_audio_n.mp3â€™

```bash
python examples/cmd/run.py "Votre premier texte." "Votre deuxiÃ¨me texte."
```

## Installation

1. Installer la version stable depuis PyPI
```bash
pip install ChatTTS
```

2. Installer la derniÃ¨re version depuis GitHub
```bash
pip install git+https://github.com/2noise/ChatTTS
```

3. Installer depuis le rÃ©pertoire local en mode dÃ©veloppement
```bash
pip install -e .
```

### Utilisation de base

```python
import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) # DÃ©finissez sur True pour de meilleures performances

texts = ["METTEZ VOTRE PREMIER TEXTE ICI", "METTEZ VOTRE DEUXIÃˆME TEXTE ICI"]

wavs = chat.infer(texts)

torchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)
```

### Utilisation avancÃ©e

```python
###################################
# Ã‰chantillonner un locuteur Ã  partir d'une distribution gaussienne.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # sauvegardez-le pour une rÃ©cupÃ©ration ultÃ©rieure du timbre

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # ajouter le locuteur Ã©chantillonnÃ© 
    temperature = .3,   # en utilisant une tempÃ©rature personnalisÃ©e
    top_P = 0.7,        # top P dÃ©code
    top_K = 20,         # top K dÃ©code
)

###################################
# Pour le contrÃ´le manuel au niveau des phrases.

# utilisez oral_(0-9), laugh_(0-2), break_(0-7) 
# pour gÃ©nÃ©rer un token spÃ©cial dans le texte Ã  synthÃ©tiser.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# Pour le contrÃ´le manuel au niveau des mots.

text = 'Quel est [uv_break]votre plat anglais prÃ©fÃ©rÃ©?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save("output2.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Exemple : auto-prÃ©sentation</h4></summary>

```python
inputs_en = """
chat T T S est un modÃ¨le de synthÃ¨se vocale conÃ§u pour les applications de dialogue.
[uv_break]il prend en charge les entrÃ©es en langues mixtes [uv_break]et offre des capacitÃ©s multi-locuteurs
avec un contrÃ´le prÃ©cis des Ã©lÃ©ments prosodiques comme 
[uv_break]le rire[uv_break][laugh], [uv_break]les pauses, [uv_break]et l'intonation.
[uv_break]il dÃ©livre une parole naturelle et expressive,[uv_break]donc veuillez
[uv_break]utiliser le projet de maniÃ¨re responsable Ã  vos risques et pÃ©rils.[uv_break]
""".replace('\n', '') # L'anglais est encore expÃ©rimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save("output3.wav", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align="center">

**locuteur masculin**

</td>
<td align="center">

**locutrice fÃ©minine**

</td>
</tr>
<tr>
<td align="center">

[locuteur masculin](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align="center">

[locutrice fÃ©minine](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>


</details>

## FAQ

#### 1. De combien de VRAM ai-je besoin ? Quelle est la vitesse d'infÃ©rence ?
Pour un clip audio de 30 secondes, au moins 4 Go de mÃ©moire GPU sont nÃ©cessaires. Pour le GPU 4090, il peut gÃ©nÃ©rer de l'audio correspondant Ã  environ 7 tokens sÃ©mantiques par seconde. Le Facteur Temps RÃ©el (RTF) est d'environ 0.3.

#### 2. La stabilitÃ© du modÃ¨le n'est pas suffisante, avec des problÃ¨mes tels que des locuteurs multiples ou une mauvaise qualitÃ© audio.
C'est un problÃ¨me qui se produit gÃ©nÃ©ralement avec les modÃ¨les autoregressifs (pour bark et valle). Il est gÃ©nÃ©ralement difficile Ã  Ã©viter. On peut essayer plusieurs Ã©chantillons pour trouver un rÃ©sultat appropriÃ©.

#### 3. En plus du rire, pouvons-nous contrÃ´ler autre chose ? Pouvons-nous contrÃ´ler d'autres Ã©motions ?
Dans le modÃ¨le actuellement publiÃ©, les seules unitÃ©s de contrÃ´le au niveau des tokens sont `[laugh]`, `[uv_break]`, et `[lbreak]`. Dans les futures versions, nous pourrions open-source des modÃ¨les avec des capacitÃ©s de contrÃ´le Ã©motionnel supplÃ©mentaires.

## Remerciements
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) et [valle](https://arxiv.org/abs/2301.02111) dÃ©montrent un rÃ©sultat TTS remarquable par un systÃ¨me de style autoregressif.
- [fish-speech](https://github.com/fishaudio/fish-speech) rÃ©vÃ¨le la capacitÃ© de GVQ en tant que tokenizer audio pour la modÃ©lisation LLM.
- [vocos](https://github.com/gemelo-ai/vocos) qui est utilisÃ© comme vocodeur prÃ©-entraÃ®nÃ©.

## ApprÃ©ciation spÃ©ciale
- [wlu-audio lab](https://audio.westlake.edu.cn/) pour les expÃ©riences d'algorithme prÃ©coce.

## Merci Ã  tous les contributeurs pour leurs efforts
[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

<div align="center">

  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>



================================================
FILE: docs/jp/README.md
================================================
# ChatTTS
> [!NOTE]
> ä»¥ä¸‹ã®å†…å®¹ã¯æœ€æ–°æƒ…å ±ã§ã¯ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã®ã§ã”äº†æ‰¿ãã ã•ã„ã€‚å…¨ã¦ã®å†…å®¹ã¯è‹±èªç‰ˆã«åŸºæº–ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚

[![Huggingface](https://img.shields.io/badge/ğŸ¤—%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)

[**English**](../../README.md) | [**ç®€ä½“ä¸­æ–‡**](../cn/README.md) | **æ—¥æœ¬èª** | [**Ğ ÑƒÑÑĞºĞ¸Ğ¹**](../ru/README.md) | [**EspaÃ±ol**](../es/README.md) | [**FranÃ§ais**](../fr/README.md) | [**í•œêµ­ì–´**](../kr/README.md)

ChatTTSã¯ã€LLMã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãªã©ã®å¯¾è©±ã‚·ãƒŠãƒªã‚ªç”¨ã«ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã¸ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è‹±èªã¨ä¸­å›½èªã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ä¸­å›½èªã¨è‹±èªã§æ§‹æˆã•ã‚Œã‚‹100,000æ™‚é–“ä»¥ä¸Šã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ã€‚**[HuggingFace](https://huggingface.co/2Noise/ChatTTS)**ã§ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã¦ã„ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ã€40,000æ™‚é–“ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã§ã€SFTã¯è¡Œã‚ã‚Œã¦ã„ã¾ã›ã‚“ã€‚

ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã«ã¤ã„ã¦ã®æ­£å¼ãªãŠå•ã„åˆã‚ã›ã¯ã€**open-source@2noise.com**ã¾ã§ã”é€£çµ¡ãã ã•ã„ã€‚QQã‚°ãƒ«ãƒ¼ãƒ—ï¼š808364215ã«å‚åŠ ã—ã¦ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚GitHubã§ã®å•é¡Œæèµ·ã‚‚æ­“è¿ã—ã¾ã™ã€‚

---
## ãƒã‚¤ãƒ©ã‚¤ãƒˆ
1. **ä¼šè©±å‹TTS**: ChatTTSã¯å¯¾è©±ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯ã«æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šã€è‡ªç„¶ã§è¡¨ç¾è±Šã‹ãªéŸ³å£°åˆæˆã‚’å®Ÿç¾ã—ã¾ã™ã€‚è¤‡æ•°ã®è©±è€…ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€å¯¾è©±å‹ã®ä¼šè©±ã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚
2. **ç´°ã‹ã„åˆ¶å¾¡**: ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ç¬‘ã„ã€ä¸€æ™‚åœæ­¢ã€é–“æŠ•è©ãªã©ã®ç´°ã‹ã„éŸ»å¾‹ç‰¹å¾´ã‚’äºˆæ¸¬ãŠã‚ˆã³åˆ¶å¾¡ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
3. **ã‚ˆã‚Šè‰¯ã„éŸ»å¾‹**: ChatTTSã¯ã€éŸ»å¾‹ã®é¢ã§ã»ã¨ã‚“ã©ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹TTSãƒ¢ãƒ‡ãƒ«ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ã•ã‚‰ãªã‚‹ç ”ç©¶ã¨é–‹ç™ºã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã«ã€äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ãªèª¬æ˜ã«ã¤ã„ã¦ã¯ã€**[Bilibiliã®ãƒ“ãƒ‡ã‚ª](https://www.bilibili.com/video/BV1zn4y1o7iV)**ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

## å…è²¬äº‹é …

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯å­¦è¡“ç›®çš„ã®ã¿ã®ãŸã‚ã§ã™ã€‚æ•™è‚²ãŠã‚ˆã³ç ”ç©¶ç”¨é€”ã«ã®ã¿ä½¿ç”¨ã•ã‚Œã€å•†æ¥­çš„ã¾ãŸã¯æ³•çš„ãªç›®çš„ã«ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚è‘—è€…ã¯æƒ…å ±ã®æ­£ç¢ºæ€§ã€å®Œå…¨æ€§ã€ã¾ãŸã¯ä¿¡é ¼æ€§ã‚’ä¿è¨¼ã—ã¾ã›ã‚“ã€‚ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã§ä½¿ç”¨ã•ã‚Œã‚‹æƒ…å ±ãŠã‚ˆã³ãƒ‡ãƒ¼ã‚¿ã¯ã€å­¦è¡“ãŠã‚ˆã³ç ”ç©¶ç›®çš„ã®ã¿ã®ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã•ã‚Œã€è‘—è€…ã¯ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ‰€æœ‰æ¨©ã¾ãŸã¯è‘—ä½œæ¨©ã‚’ä¸»å¼µã—ã¾ã›ã‚“ã€‚

ChatTTSã¯å¼·åŠ›ãªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã¸ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ã—ã‹ã—ã€ã“ã®æŠ€è¡“ã‚’è²¬ä»»ã‚’æŒã£ã¦ã€å€«ç†çš„ã«åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒéå¸¸ã«é‡è¦ã§ã™ã€‚ChatTTSã®ä½¿ç”¨ã‚’åˆ¶é™ã™ã‚‹ãŸã‚ã«ã€40,000æ™‚é–“ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«å°‘é‡ã®é«˜å‘¨æ³¢ãƒã‚¤ã‚ºã‚’è¿½åŠ ã—ã€MP3å½¢å¼ã‚’ä½¿ç”¨ã—ã¦éŸ³è³ªã‚’å¯èƒ½ãªé™ã‚Šåœ§ç¸®ã—ã¾ã—ãŸã€‚ã“ã‚Œã¯ã€æ‚ªæ„ã®ã‚ã‚‹ã‚¢ã‚¯ã‚¿ãƒ¼ãŒæ½œåœ¨çš„ã«çŠ¯ç½ªç›®çš„ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’é˜²ããŸã‚ã§ã™ã€‚åŒæ™‚ã«ã€ç§ãŸã¡ã¯å†…éƒ¨çš„ã«æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ãŠã‚Šã€å°†æ¥çš„ã«ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã™ã‚‹äºˆå®šã§ã™ã€‚

---
## ä½¿ç”¨æ–¹æ³•

<h4>åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•</h4>

```python
import ChatTTS
from IPython.display import Audio
import torch

chat = ChatTTS.Chat()
chat.load(compile=False) # ã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚ã«Trueã«è¨­å®š

texts = ["ã“ã“ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",]

wavs = chat.infer(texts, )

torchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)
```

<h4>é«˜åº¦ãªä½¿ç”¨æ–¹æ³•</h4>

```python
###################################
# ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‹ã‚‰è©±è€…ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = {
  'spk_emb': rand_spk, # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸè©±è€…ã‚’è¿½åŠ 
  'temperature': .3, # ã‚«ã‚¹ã‚¿ãƒ æ¸©åº¦ã‚’ä½¿ç”¨
  'top_P': 0.7, # ãƒˆãƒƒãƒ—Pãƒ‡ã‚³ãƒ¼ãƒ‰
  'top_K': 20, # ãƒˆãƒƒãƒ—Kãƒ‡ã‚³ãƒ¼ãƒ‰
}

###################################
# æ–‡ãƒ¬ãƒ™ãƒ«ã®æ‰‹å‹•åˆ¶å¾¡ã®ãŸã‚ã«ã€‚

# ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ãƒ†ã‚­ã‚¹ãƒˆã«oral_(0-9)ã€laugh_(0-2)ã€break_(0-7)ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
params_refine_text = {
  'prompt': '[oral_2][laugh_0][break_6]'
} 

wav = chat.infer(texts, params_refine_text=params_refine_text, params_infer_code=params_infer_code)

###################################
# å˜èªãƒ¬ãƒ™ãƒ«ã®æ‰‹å‹•åˆ¶å¾¡ã®ãŸã‚ã«ã€‚
text = 'ã‚ãªãŸã®å¥½ããªè‹±èªã®é£Ÿã¹ç‰©ã¯ä½•ã§ã™ã‹ï¼Ÿ[uv_break][laugh][lbreak]'
wav = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save("output2.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>ä¾‹ï¼šè‡ªå·±ç´¹ä»‹</h4></summary>

```python
inputs_jp = """
ChatTTSã¯ã€å¯¾è©±ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã«è¨­è¨ˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã¸ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
[uv_break]æ··åˆè¨€èªå…¥åŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—[uv_break]ã€éŸ»å¾‹è¦ç´ [laugh]ã®æ­£ç¢ºãªåˆ¶å¾¡ã‚’æä¾›ã—ã¾ã™
[uv_break]ç¬‘ã„[laugh]ã€[uv_break]ä¸€æ™‚åœæ­¢ã€[uv_break]ãŠã‚ˆã³ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ã€‚[uv_break]è‡ªç„¶ã§è¡¨ç¾è±Šã‹ãªéŸ³å£°ã‚’æä¾›ã—ã¾ã™
[uv_break]ã—ãŸãŒã£ã¦ã€è‡ªå·±è²¬ä»»ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è²¬ä»»ã‚’æŒã£ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚[uv_break]
""".replace('\n', '') # è‹±èªã¯ã¾ã å®Ÿé¨“çš„ã§ã™ã€‚

params_refine_text = {
  'prompt': '[oral_2][laugh_0][break_4]'
} 
audio_array_jp = chat.infer(inputs_jp, params_refine_text=params_refine_text)
torchaudio.save("output3.wav", torch.from_numpy(audio_array_jp[0]), 24000)
```
[ç”·æ€§è©±è€…](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

[å¥³æ€§è©±è€…](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)
</details>

---
## ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—
- [x] 40kæ™‚é–“ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨spk_statsãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–
- [ ] VQã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨Loraãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–
- [ ] ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªãƒ•ã‚¡ã‚¤ãƒ³ã›ãšã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªç”Ÿæˆ*
- [ ] è¤‡æ•°ã®æ„Ÿæƒ…åˆ¶å¾¡ã‚’å‚™ãˆãŸ40kæ™‚é–“ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–
- [ ] ChatTTS.cppã‚‚ã—ã‹ã—ãŸã‚‰ï¼Ÿï¼ˆPRã‚„æ–°ã—ã„ãƒªãƒã‚¸ãƒˆãƒªãŒæ­“è¿ã•ã‚Œã¾ã™ã€‚ï¼‰

----
## FAQ

##### VRAMã¯ã©ã‚Œãã‚‰ã„å¿…è¦ã§ã™ã‹ï¼Ÿæ¨è«–é€Ÿåº¦ã¯ã©ã†ã§ã™ã‹ï¼Ÿ
30ç§’ã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¯ãƒªãƒƒãƒ—ã«ã¯ã€å°‘ãªãã¨ã‚‚4GBã®GPUãƒ¡ãƒ¢ãƒªãŒå¿…è¦ã§ã™ã€‚4090 GPUã®å ´åˆã€ç´„7ã¤ã®æ„å‘³ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾å¿œã™ã‚‹ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚’1ç§’ã‚ãŸã‚Šç”Ÿæˆã§ãã¾ã™ã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆRTFï¼‰ã¯ç´„0.3ã§ã™ã€‚

##### ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šæ€§ãŒååˆ†ã§ãªãã€è¤‡æ•°ã®è©±è€…ã‚„éŸ³è³ªãŒæ‚ªã„ã¨ã„ã†å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚

ã“ã‚Œã¯ã€è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆbarkãŠã‚ˆã³valleã®å ´åˆï¼‰ã§ä¸€èˆ¬çš„ã«ç™ºç”Ÿã™ã‚‹å•é¡Œã§ã™ã€‚ä¸€èˆ¬çš„ã«é¿ã‘ã‚‹ã®ã¯é›£ã—ã„ã§ã™ã€‚è¤‡æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è©¦ã—ã¦ã€é©åˆ‡ãªçµæœã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

##### ç¬‘ã„ä»¥å¤–ã«ä½•ã‹åˆ¶å¾¡ã§ãã¾ã™ã‹ï¼Ÿä»–ã®æ„Ÿæƒ…ã‚’åˆ¶å¾¡ã§ãã¾ã™ã‹ï¼Ÿ

ç¾åœ¨ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®åˆ¶å¾¡ãƒ¦ãƒ‹ãƒƒãƒˆã¯[laugh]ã€[uv_break]ã€ãŠã‚ˆã³[lbreak]ã®ã¿ã§ã™ã€‚å°†æ¥ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯ã€è¿½åŠ ã®æ„Ÿæƒ…åˆ¶å¾¡æ©Ÿèƒ½ã‚’å‚™ãˆãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

---
## è¬è¾
- [bark](https://github.com/suno-ai/bark)ã€[XTTSv2](https://github.com/coqui-ai/TTS)ã€ãŠã‚ˆã³[valle](https://arxiv.org/abs/2301.02111)ã¯ã€è‡ªå·±å›å¸°å‹ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹é¡•è‘—ãªTTSçµæœã‚’ç¤ºã—ã¾ã—ãŸã€‚
- [fish-speech](https://github.com/fishaudio/fish-speech)ã¯ã€LLMãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãŸã‚ã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ã—ã¦ã®GVQã®èƒ½åŠ›ã‚’æ˜ã‚‰ã‹ã«ã—ã¾ã—ãŸã€‚
- äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒœã‚³ãƒ¼ãƒ€ãƒ¼ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹[vocos](https://github.com/gemelo-ai/vocos)ã€‚

---
## ç‰¹åˆ¥æ„Ÿè¬
- åˆæœŸã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿé¨“ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãã‚ŒãŸ[wlu-audio lab](https://audio.westlake.edu.cn/)ã€‚



================================================
FILE: docs/kr/README.md
================================================
<div align="center">

<a href="https://trendshift.io/repositories/10489" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10489" alt="2noise%2FChatTTS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

# ChatTTS
ì¼ìƒ ëŒ€í™”ë¥¼ ìœ„í•œ ìƒì„±í˜• ìŒì„± ëª¨ë¸ì…ë‹ˆë‹¤.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)

[![Huggingface](https://img.shields.io/badge/ğŸ¤—%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)

[**English**](../../README.md) | [**ç®€ä½“ä¸­æ–‡**](../cn/README.md) | [**æ—¥æœ¬èª**](../jp/README.md) | [**Ğ ÑƒÑÑĞºĞ¸Ğ¹**](../ru/README.md) | [**EspaÃ±ol**](../es/README.md) | [**FranÃ§ais**](../fr/README.md) | **í•œêµ­ì–´**

</div>

> [!NOTE]
> ì´ ë¬¸ì„œëŠ” ìµœì‹  ë²„ì „ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [ì˜ì–´ ë¬¸ì„œ](../../README.md)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì—…í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

## í”„ë¡œì íŠ¸ ì†Œê°œ

> [!Note]
> ì´ ì €ì¥ì†Œì—ëŠ” ì•Œê³ ë¦¬ì¦˜ êµ¬ì¡°ì™€ ê°„ë‹¨í•œ ì˜ˆì‹œë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

> [!Tip]
> ì´ í”„ë¡œì íŠ¸ì—ì„œ íŒŒìƒëœ í”„ë¡œì íŠ¸ëŠ” ì»¤ë®¤ë‹ˆí‹°ê°€ ìœ ì§€ ê´€ë¦¬í•˜ëŠ” ì»¤ë®¤ë‹ˆí‹°[Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS)ë¥¼ ì°¸ì¡°í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.

ChatTTSëŠ” ëŒ€í™” ê¸°ë°˜ ì‘ì—…(ì˜ˆ: LLM ì–´ì‹œìŠ¤í„´íŠ¸)ì„ ìœ„í•´ ì„¤ê³„ëœ í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜(TTS) ëª¨ë¸ì…ë‹ˆë‹¤.

### ì§€ì› ì–¸ì–´

- [x] ì˜ì–´
- [x] ì¤‘êµ­ì–´
- [ ] ê³„ì† ì¶”ê°€ ì˜ˆì •...

### í”„ë¡œì íŠ¸ íŠ¹ì§•

> ì´ í”„ë¡œì íŠ¸ì˜ ë‚´ìš©ì€ **[Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)**ì—ì„œ ì œê³µë˜ëŠ” ë¹„ë””ì˜¤ë¥¼ ì°¸ì¡°í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.

1. **ëŒ€í™”í˜• TTS**: ChatTTSëŠ” ëŒ€í™” ê¸°ë°˜ ì‘ì—…ì— ìµœì í™”ë˜ì–´ ìì—°ìŠ¤ëŸ½ê³  í‘œí˜„ë ¥ ìˆëŠ” ìŒì„± í•©ì„±ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ë‹¤ì¤‘ í™”ìë¥¼ ì§€ì›í•˜ì—¬ ìƒí˜¸ì‘ìš©ì ì¸ ëŒ€í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
2. **ì„¸ë°€í•œ ì œì–´**: ì´ ëª¨ë¸ì€ ì›ƒìŒ, ì¼ì‹œ ì •ì§€, ì‚½ì…ì–´ ë“± ì„¸ë°€í•œ ìš´ìœ¨ì  íŠ¹ì§•ì„ ì˜ˆì¸¡í•˜ê³  ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **í–¥ìƒëœ ìš´ìœ¨**: ChatTTSëŠ” ìš´ìœ¨ ì¸¡ë©´ì—ì„œ ëŒ€ë¶€ë¶„ì˜ ì˜¤í”ˆ ì†ŒìŠ¤ TTS ëª¨ë¸ì„ ëŠ¥ê°€í•˜ë©°, ì¶”ê°€ ì—°êµ¬ì™€ ê°œë°œì„ ì§€ì›í•˜ê¸° ìœ„í•´ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤.

### ë°ì´í„°ì…‹ ë° ëª¨ë¸
> [!Important]
> ê³µê°œëœ ëª¨ë¸ì€ í•™ìˆ  ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

- ì£¼ìš” ëª¨ë¸ì€ 100,000+ ì‹œê°„ì˜ ì¤‘êµ­ì–´ ë° ì˜ì–´ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.
- **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)**ì—ì„œ ì œê³µë˜ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ë²„ì „ì€ 40,000ì‹œê°„ì˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë¡œ, SFTê°€ ì ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

### ë¡œë“œë§µ
- [x] 40,000ì‹œê°„ ê¸°ë°˜ ëª¨ë¸ê³¼ spk_stats íŒŒì¼ ì˜¤í”ˆ ì†ŒìŠ¤í™”.
- [x] ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë””ì˜¤ ìƒì„±.
- [x] DVAE ì¸ì½”ë”ì™€ ì œë¡œ ìƒ· ì¶”ë¡  ì½”ë“œ ì˜¤í”ˆ ì†ŒìŠ¤í™”.
- [ ] ë‹¤ì¤‘ ê°ì • ì œì–´ ê¸°ëŠ¥.
- [ ] ChatTTS.cpp (`2noise` ì¡°ì§ ë‚´ì˜ ìƒˆë¡œìš´ ì €ì¥ì†Œë¥¼ í™˜ì˜í•©ë‹ˆë‹¤.)

### ë¼ì´ì„ ìŠ¤

#### ì½”ë“œ
ì½”ë“œëŠ” `AGPLv3+` ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

#### ëª¨ë¸
ëª¨ë¸ì€ `CC BY-NC 4.0` ë¼ì´ì„ ìŠ¤ë¡œ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ êµìœ¡ ë° ì—°êµ¬ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©ë˜ë©°, ìƒì—…ì  ë˜ëŠ” ë¶ˆë²•ì  ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì–´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ì €ìë“¤ì€ ì •ë³´ì˜ ì •í™•ì„±, ì™„ì „ì„±, ì‹ ë¢°ì„±ì„ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ì €ì¥ì†Œì—ì„œ ì‚¬ìš©ëœ ì •ë³´ì™€ ë°ì´í„°ëŠ” í•™ìˆ  ë° ì—°êµ¬ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©ë˜ë©°, ê³µê°œì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•œ ì¶œì²˜ì—ì„œ ì–»ì€ ë°ì´í„°ì…ë‹ˆë‹¤. ì €ìë“¤ì€ ë°ì´í„°ì— ëŒ€í•œ ì†Œìœ ê¶Œ ë˜ëŠ” ì €ì‘ê¶Œì„ ì£¼ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

### ë©´ì±… ì¡°í•­

ChatTTSëŠ” ê°•ë ¥í•œ í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ê·¸ë ‡ê¸°ì— ê¸°ìˆ ì„ ì±…ì„ê° ìˆê³  ìœ¤ë¦¬ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì•„ì£¼ ì¤‘ìš”í•©ë‹ˆë‹¤. ChatTTSì˜ ì•…ìš©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ 40,000ì‹œê°„ ëª¨ë¸ì˜ í›ˆë ¨ ì¤‘ ì†ŒëŸ‰ì˜ ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ê³ , ì˜¤ë””ì˜¤ í’ˆì§ˆì„ ìµœëŒ€í•œ ì••ì¶•í•˜ì—¬ MP3 í˜•ì‹ìœ¼ë¡œ ì œê³µí–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ìš°ë¦¬ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ íƒì§€ ëª¨ë¸ì„ í›ˆë ¨í–ˆìœ¼ë©°, ì¶”í›„ ì´ë¥¼ ì˜¤í”ˆ ì†ŒìŠ¤í™”í•  ê³„íšì…ë‹ˆë‹¤.

### ì—°ë½ì²˜
> GitHub ì´ìŠˆ/PRì€ ì–¸ì œë“ ì§€ í™˜ì˜í•©ë‹ˆë‹¤.

#### ê³µì‹ ë¬¸ì˜
ëª¨ë¸ ë° ë¡œë“œë§µì— ëŒ€í•œ ê³µì‹ì ì¸ ë¬¸ì˜ëŠ” **open-source@2noise.com**ìœ¼ë¡œ ì—°ë½í•´ ì£¼ì‹­ì‹œì˜¤.

#### ì˜¨ë¼ì¸ ì±„íŒ…
##### 1. QQ Group (Chinese Social APP)
- **Group 1**, 808364215
- **Group 2**, 230696694
- **Group 3**, 933639842
- **Group 4**, 608667975

##### 2. Discord ì„œë²„
[ì´ê³³](https://discord.gg/Ud5Jxgx5yD)ë¥¼ í´ë¦­í•˜ì—¬ ì°¸ì—¬í•˜ì‹­ì‹œì˜¤.

## ì‹œì‘í•˜ê¸°
### ë ˆí¬ì§€í† ë¦¬ í´ë¡ 
```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### ì˜ì¡´ì„± ì„¤ì¹˜
#### 1. ì§ì ‘ ì„¤ì¹˜
```bash
pip install --upgrade -r requirements.txt
```

#### 2. Condaì—ì„œ ì„¤ì¹˜
```bash
conda create -n chattts
conda activate chattts
pip install -r requirements.txt
```

#### ì„ íƒì‚¬í•­: vLLM ì„¤ì¹˜ (Linux ì „ìš©)
```bash
pip install safetensors vllm==0.2.7 torchaudio
```

#### ê¶Œì¥ë˜ì§€ ì•ŠëŠ” ì„ íƒì‚¬í•­: NVIDIA GPU ì‚¬ìš© ì‹œ TransformerEngine ì„¤ì¹˜ (Linux ì „ìš©)
> [!Warning]
> ì„¤ì¹˜í•˜ì§€ ë§ˆì‹­ì‹œì˜¤!
> TransformerEngineì˜ ì ì‘ ì‘ì—…ì€ í˜„ì¬ ê°œë°œ ì¤‘ì´ë©°, ì•„ì§ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
> ê°œë°œ ëª©ì ìœ¼ë¡œë§Œ ì„¤ì¹˜í•˜ì‹­ì‹œì˜¤. ìì„¸í•œ ë‚´ìš©ì€ #672 ë° #676ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> [!Note]
> ì„¤ì¹˜ ê³¼ì •ì€ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤.

```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable
```

#### ê¶Œì¥ë˜ì§€ ì•ŠëŠ” ì„ íƒì‚¬í•­: FlashAttention-2 ì„¤ì¹˜ (ì£¼ë¡œ NVIDIA GPU)
> [!Warning]
> ì„¤ì¹˜í•˜ì§€ ë§ˆì‹­ì‹œì˜¤!
> í˜„ì¬ FlashAttention-2ëŠ” [ì´ ì´ìŠˆ](https://github.com/huggingface/transformers/issues/26990)ì— ë”°ë¥´ë©´ ìƒì„± ì†ë„ë¥¼ ì €í•˜ì‹œí‚µë‹ˆë‹¤.
> ê°œë°œ ëª©ì ìœ¼ë¡œë§Œ ì„¤ì¹˜í•˜ì‹­ì‹œì˜¤.

> [!Note]
> ì§€ì›ë˜ëŠ” ì¥ì¹˜ëŠ” [Hugging Face ë¬¸ì„œ](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
pip install flash-attn --no-build-isolation
```

### ë¹ ë¥¸ ì‹œì‘
> ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•  ë•Œ ë°˜ë“œì‹œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•˜ì‹­ì‹œì˜¤.

#### 1. WebUI ì‹¤í–‰
```bash
python examples/web/webui.py
```

#### 2. ì»¤ë§¨ë“œ ë¼ì¸ì—ì„œ ì¶”ë¡ 
> ì˜¤ë””ì˜¤ëŠ” `./output_audio_n.mp3`ì— ì €ì¥ë©ë‹ˆë‹¤.

```bash
python examples/cmd/run.py "Your text 1." "Your text 2."
```

## ì„¤ì¹˜ ë°©ë²•

1. PyPIì—ì„œ ì•ˆì • ë²„ì „ ì„¤ì¹˜
```bash
pip install ChatTTS
```

2. GitHubì—ì„œ ìµœì‹  ë²„ì „ ì„¤ì¹˜
```bash
pip install git+https://github.com/2noise/ChatTTS
```

3. ë¡œì»¬ ë””ë ‰í† ë¦¬ì—ì„œ ê°œë°œ ëª¨ë“œë¡œ ì„¤ì¹˜
```bash
pip install -e .
```

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) # ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ Trueë¡œ ì„¤ì • ê°€ëŠ¥

texts = ["PUT YOUR 1st TEXT HERE", "PUT YOUR 2nd TEXT HERE"]

wavs = chat.infer(texts)

for i in range(len(wavs)):
    """
    torchaudioì˜ ë²„ì „ì— ë”°ë¼ ì²« ë²ˆì§¸ ì¤„ì´ ì‘ë™í•  ìˆ˜ ìˆê³ , ë‹¤ë¥¸ ë²„ì „ì—ì„œëŠ” ë‘ ë²ˆì§¸ ì¤„ì´ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    try:
        torchaudio.save(f"basic_output{i}.wav", torch.from_numpy(wavs[i]).unsqueeze(0), 24000)
    except:
        torchaudio.save(f"basic_output{i}.wav", torch.from_numpy(wavs[i]), 24000)
```

### Advanced Usage

```python
###################################
# Sample a speaker from Gaussian.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # add sampled speaker 
    temperature = .3,   # using custom temperature
    top_P = 0.7,        # top P decode
    top_K = 20,         # top K decode
)

###################################
# For sentence level manual control.

# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# For word level manual control.

text = 'What is [uv_break]your favorite english food?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
"""
In some versions of torchaudio, the first line works but in other versions, so does the second line.
"""
try:
    torchaudio.save("word_level_output.wav", torch.from_numpy(wavs[0]).unsqueeze(0), 24000)
except:
    torchaudio.save("word_level_output.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Example: self introduction</h4></summary>

```python
inputs_en = """
chat T T S is a text to speech model designed for dialogue applications. 
[uv_break]it supports mixed language input [uv_break]and offers multi speaker 
capabilities with precise control over prosodic elements like 
[uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation. 
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
""".replace('\n', '') # English is still experimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save("self_introduction_output.wav", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align="center">

**male speaker**

</td>
<td align="center">

**female speaker**

</td>
</tr>
<tr>
<td align="center">

[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align="center">

[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>

</details>

## FAQ

#### 1. VRAMì´ ì–¼ë§ˆë‚˜ í•„ìš”í•œê°€ìš”? ì¶”ë¡  ì†ë„ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?
30ì´ˆ ê¸¸ì´ì˜ ì˜¤ë””ì˜¤ í´ë¦½ì„ ìƒì„±í•˜ë ¤ë©´ ìµœì†Œ 4GBì˜ GPU ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. 4090 GPUì˜ ê²½ìš° ì´ˆë‹¹ ì•½ 7ê°œì˜ ì˜ë¯¸ í† í°ì— í•´ë‹¹í•˜ëŠ” ì˜¤ë””ì˜¤ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì‹œê°„ ì¸ì(RTF)ëŠ” ì•½ 0.3ì…ë‹ˆë‹¤.

#### 2. ëª¨ë¸ì˜ ì•ˆì •ì„±ì€ ë¶ˆì•ˆì •í•˜ë©°, í™”ìê°€ ë§ì€ ê²½ìš° ë° ì˜¤ë””ì˜¤ í’ˆì§ˆì´ ì €í•˜ë˜ëŠ” ì´ìŠˆ ì¡´ì¬.

ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ autoregressive ëª¨ë¸(bark ë° valle ë“±)ì—ì„œ ë°œìƒí•˜ëŠ” ë¶ˆê°€í”¼í•œ ë¬¸ì œì…ë‹ˆë‹¤. í˜„ì¬ë¡œì„  ì—¬ëŸ¬ ë²ˆ ìƒ˜í”Œë§í•˜ì—¬ ì ì ˆí•œ ê²°ê³¼ë¥¼ ì°¾ëŠ” ê²ƒì´ ìµœì„ ì…ë‹ˆë‹¤.

#### 3. ì›ƒìŒ ë¿ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ê°ì •ë„ í‘œí˜„í•  ìˆ˜ ìˆë‚˜ìš”?

í˜„ì¬ ê³µê°œëœ ëª¨ë¸ì—ì„œëŠ” ì œì–´ ê°€ëŠ¥í•œ í† í°ì€ `[laugh]`, `[uv_break]`, `[lbreak]`ì…ë‹ˆë‹¤. í–¥í›„ ë²„ì „ì˜ ëª¨ë¸ì—ì„œëŠ” ì¶”ê°€ì ì¸ ê°ì • ì œì–´ ê¸°ëŠ¥ í¬í•¨í•˜ì—¬ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µí•  ê³„íšì…ë‹ˆë‹¤.

## ê°ì‚¬ì˜ ì¸ì‚¬
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS), [valle](https://arxiv.org/abs/2301.02111)ëŠ” autoregressive ë°©ì‹ì˜ ì‹œìŠ¤í…œìœ¼ë¡œ ë›°ì–´ë‚œ TTS ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.
- [fish-speech](https://github.com/fishaudio/fish-speech)ëŠ” LLM ëª¨ë¸ë§ì„ ìœ„í•œ ì˜¤ë””ì˜¤ í† í¬ë‚˜ì´ì €ë¡œì„œ GVQì˜ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.
- [vocos](https://github.com/gemelo-ai/vocos)ëŠ” ì‚¬ì „ í›ˆë ¨ëœ vocoderë¡œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

## íŠ¹ë³„ ê°ì‚¬
- ì´ˆê¸° ì•Œê³ ë¦¬ì¦˜ ì‹¤í—˜ì„ ìœ„í•œ [wlu-audio lab](https://audio.westlake.edu.cn/)ì— ê°ì‚¬ì˜ ë§ì”€ì„ ì „í•©ë‹ˆë‹¤.

## ëª¨ë“  ê¸°ì—¬ìë“¤ì˜ ë…¸ê³ ì— ê°ì‚¬ë“œë¦½ë‹ˆë‹¤
[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

<div align="center">

  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>



================================================
FILE: docs/ru/README.md
================================================
# ChatTTS
> [!NOTE]
> Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½Ğµ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹, Ğ¿Ğ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Ğµ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ´Ğ»Ñ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….

[![Huggingface](https://img.shields.io/badge/ğŸ¤—%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)

[**English**](../../README.md) | [**ç®€ä½“ä¸­æ–‡**](../cn/README.md) | [**æ—¥æœ¬èª**](../jp/README.md) | **Ğ ÑƒÑÑĞºĞ¸Ğ¹** | [**EspaÃ±ol**](../es/README.md) | [**FranÃ§ais**](../fr/README.md) | [**í•œêµ­ì–´**](../kr/README.md)

ChatTTS - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ€ĞµÑ‡ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº LLM. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¸. ĞĞ°ÑˆĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100 000 Ñ‡Ğ°ÑĞ°Ñ… Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ½Ğ° **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** - ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 40 000 Ñ‡Ğ°ÑĞ°Ğ¼Ğ¸ Ğ±ĞµĞ· SFT.

Ğ”Ğ»Ñ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ, Ğ¿Ğ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, ÑĞ²ÑĞ¶Ğ¸Ñ‚ĞµÑÑŒ Ñ Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ°Ğ´Ñ€ĞµÑÑƒ **open-source@2noise.com**. Ğ’Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ¿Ñ€Ğ¸ÑĞ¾ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒÑÑ Ğº Ğ½Ğ°ÑˆĞµĞ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ QQ: 808364215 Ğ´Ğ»Ñ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° GitHub Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ÑÑ.

---
## ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸
1. **Ğ”Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ TTS**: ChatTTS Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ñ‚ÑƒÑ€Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ñ€ĞµÑ‡ÑŒ. ĞĞ½ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ…, Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ±ĞµÑĞµĞ´Ñ‹.
2. **Ğ¢Ğ¾Ğ½ĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ**: ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾ÑĞ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¼ĞµÑ…, Ğ¿Ğ°ÑƒĞ·Ñ‹ Ğ¸ Ğ²ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°.
3. **Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾ÑĞ¾Ğ´Ğ¸Ñ**: ChatTTS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ TTS Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑĞ¾Ğ´Ğ¸Ğ¸. ĞœÑ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğº.

Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒÑÑ Ğº **[Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)**

---

## ĞÑ‚ĞºĞ°Ğ· Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸

Ğ­Ñ‚Ğ¾Ñ‚ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğµ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ² ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ»Ğ¸ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ†ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² ÑÑ‚Ğ¾Ğ¼ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹ Ğ¸Ğ· Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ², Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğµ Ğ·Ğ°ÑĞ²Ğ»ÑÑÑ‚ Ğ¾ ĞºĞ°ĞºĞ¸Ñ…-Ğ»Ğ¸Ğ±Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ°Ñ… ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ°Ñ… Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.

ChatTTS - Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ€ĞµÑ‡ÑŒ. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñƒ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸ ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾. Ğ§Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ChatTTS, Ğ¼Ñ‹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 40 000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ¸ ÑĞ¶Ğ°Ğ»Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ ĞºĞ°Ğº Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° MP3, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ² Ğ¿Ñ€ĞµÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ÑÑ…. Ğ’ Ñ‚Ğ¾ Ğ¶Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¼Ñ‹ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ ĞµĞµ Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼.

---
## Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

<h4>Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ</h4>

```python
import ChatTTS
from IPython.display import Audio
import torch

chat = ChatTTS.Chat()
chat.load(compile=False) # Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ True Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸

texts = ["Ğ’Ğ’Ğ•Ğ”Ğ˜Ğ¢Ğ• Ğ’ĞĞ¨ Ğ¢Ğ•ĞšĞ¡Ğ¢ Ğ—Ğ”Ğ•Ğ¡Ğ¬",]

wavs = chat.infer(texts)

torchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)
```

<h4>ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ</h4>

```python
###################################
# Ğ’Ñ‹Ğ±Ğ¾Ñ€ĞºĞ° Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸Ğ· Ğ“Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ°.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = {
  'spk_emb': rand_spk, # Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾
  'temperature': .3, # Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñƒ
  'top_P': 0.7, # Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ top P
  'top_K': 20, # Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ top K
}

###################################
# Ğ”Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.

# Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ oral_(0-9), laugh_(0-2), break_(0-7)
# Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°.
params_refine_text = {
  'prompt': '[oral_2][laugh_0][break_6]'
} 

wav = chat.infer(texts, params_refine_text=params_refine_text, params_infer_code=params_infer_code)

###################################
# Ğ”Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ².
text = 'ĞšĞ°ĞºĞ°Ñ Ğ²Ğ°ÑˆĞ° Ğ»ÑĞ±Ğ¸Ğ¼Ğ°Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ°Ñ ĞµĞ´Ğ°?[uv_break]your favorite english food?[laugh][lbreak]'
wav = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save("output2.wav", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ</h4></summary>

```python
inputs_ru = """
ChatTTS - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ€ĞµÑ‡ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. 
[uv_break]ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ²Ğ¾Ğ´ [uv_break]Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… 
Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ½Ğ°Ğ´ Ğ¿Ñ€Ğ¾ÑĞ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ [laugh]ĞºĞ°Ğº [uv_break]ÑĞ¼ĞµÑ…[laugh], [uv_break]Ğ¿Ğ°ÑƒĞ·Ñ‹, [uv_break]Ğ¸ Ğ¸Ğ½Ñ‚Ğ¾Ğ½Ğ°Ñ†Ğ¸Ñ. 
[uv_break]ĞĞ½Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ñ‚ÑƒÑ€Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ñ€ĞµÑ‡ÑŒ,[uv_break]Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ, Ğ¿Ğ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°,
[uv_break] Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸ Ğ½Ğ° ÑĞ²Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ… Ğ¸ Ñ€Ğ¸ÑĞº.[uv_break]
""".replace('\n', '') # Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸.

params_refine_text = {
  'prompt': '[oral_2][laugh_0][break_4]'
} 
audio_array_ru = chat.infer(inputs_ru, params_refine_text=params_refine_text)
torchaudio.save("output3.wav", torch.from_numpy(audio_array_ru[0]), 24000)
```
[Ğ¼ÑƒĞ¶ÑĞºĞ¾Ğ¹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¹](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

[Ğ¶ĞµĞ½ÑĞºĞ¸Ğ¹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¹](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)
</details>

---
## ĞŸĞ»Ğ°Ğ½ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ
- [x] ĞÑ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 40 Ñ‚Ñ‹ÑÑÑ‡ Ñ‡Ğ°ÑĞ¾Ğ² Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ° spk_stats
- [ ] ĞÑ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° VQ Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Lora
- [ ] ĞŸĞ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ±ĞµĞ· ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°*
- [ ] ĞÑ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ½Ğ° 40 Ñ‚Ñ‹ÑÑÑ‡ Ñ‡Ğ°ÑĞ¾Ğ² Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸ÑĞ¼Ğ¸
- [ ] ChatTTS.cpp Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾? (PR Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ÑÑ.)

----
## Ğ§Ğ°ÑÑ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹

##### Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ VRAM Ğ¼Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾? ĞšĞ°Ğº Ğ½Ğ°ÑÑ‡ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°?
Ğ”Ğ»Ñ 30-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ»Ğ¸Ğ¿Ğ° Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 4 Ğ“Ğ‘ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU. Ğ”Ğ»Ñ GPU 4090, Ğ¾Ğ½ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°ÑƒĞ´Ğ¸Ğ¾, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 7 ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼ Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ. Ğ¤Ğ°ĞºÑ‚Ğ¾Ñ€ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ (RTF) ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 0.3.

##### Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğ¶ĞµÑ‚ÑÑ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞ¹, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾.

Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (Ğ´Ğ»Ñ bark Ğ¸ valle). Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ. ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚.

##### ĞŸĞ¾Ğ¼Ğ¸Ğ¼Ğ¾ ÑĞ¼ĞµÑ…Ğ°, Ğ¼Ğ¾Ğ¶ĞµĞ¼ Ğ»Ğ¸ Ğ¼Ñ‹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ ĞµÑ‰Ğµ? ĞœĞ¾Ğ¶ĞµĞ¼ Ğ»Ğ¸ Ğ¼Ñ‹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¸?

Ğ’ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ²Ğ»ÑÑÑ‚ÑÑ [laugh], [uv_break] Ğ¸ [lbreak]. Ğ’ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ²ĞµÑ€ÑĞ¸ÑÑ… Ğ¼Ñ‹ Ğ¼Ğ¾Ğ¶ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹.

---
## Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ğ½Ğ¾ÑÑ‚Ğ¸
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) Ğ¸ [valle](https://arxiv.org/abs/2301.02111) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ TTS Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ¸Ğ»Ñ.
- [fish-speech](https://github.com/fishaudio/fish-speech) Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ GVQ ĞºĞ°Ğº Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM.
- [vocos](https://github.com/gemelo-ai/vocos), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ĞºĞ¾Ğ´ĞµÑ€Ğ°.

---
## ĞÑĞ¾Ğ±Ğ°Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ğ½Ğ¾ÑÑ‚ÑŒ
- [wlu-audio lab](https://audio.westlake.edu.cn/) Ğ·Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼Ğ¸.



================================================
FILE: examples/__init__.py
================================================



================================================
FILE: examples/api/README.md
================================================
# Generating voice with ChatTTS via API

## Install requirements

Install `FastAPI` and `requests`:

```
pip install -r examples/api/requirements.txt
```

## Run API server

```
fastapi dev examples/api/main.py --host 0.0.0.0 --port 8000
```

## Generate audio using requests

```
python examples/api/client.py
```

mp3 audio files will be saved to the `output` directory.



================================================
FILE: examples/api/client.py
================================================
import datetime
import os
import zipfile
from io import BytesIO

import requests

chattts_service_host = os.environ.get("CHATTTS_SERVICE_HOST", "localhost")
chattts_service_port = os.environ.get("CHATTTS_SERVICE_PORT", "8000")

CHATTTS_URL = f"http://{chattts_service_host}:{chattts_service_port}/generate_voice"


# main infer params
body = {
    "text": [
        "å››å·ç¾é£Ÿç¡®å®ä»¥è¾£é—»åï¼Œä½†ä¹Ÿæœ‰ä¸è¾£çš„é€‰æ‹©ã€‚",
        "æ¯”å¦‚ç”œæ°´é¢ã€èµ–æ±¤åœ†ã€è›‹çƒ˜ç³•ã€å¶å„¿ç²‘ç­‰ï¼Œè¿™äº›å°åƒå£å‘³æ¸©å’Œï¼Œç”œè€Œä¸è…»ï¼Œä¹Ÿå¾ˆå—æ¬¢è¿ã€‚",
    ],
    "stream": False,
    "lang": None,
    "skip_refine_text": True,
    "refine_text_only": False,
    "use_decoder": True,
    "audio_seed": 12345678,
    "text_seed": 87654321,
    "do_text_normalization": True,
    "do_homophone_replacement": False,
}

# refine text params
params_refine_text = {
    "prompt": "",
    "top_P": 0.7,
    "top_K": 20,
    "temperature": 0.7,
    "repetition_penalty": 1,
    "max_new_token": 384,
    "min_new_token": 0,
    "show_tqdm": True,
    "ensure_non_empty": True,
    "stream_batch": 24,
}
body["params_refine_text"] = params_refine_text

# infer code params
params_infer_code = {
    "prompt": "[speed_5]",
    "top_P": 0.1,
    "top_K": 20,
    "temperature": 0.3,
    "repetition_penalty": 1.05,
    "max_new_token": 2048,
    "min_new_token": 0,
    "show_tqdm": True,
    "ensure_non_empty": True,
    "stream_batch": True,
    "spk_emb": None,
}
body["params_infer_code"] = params_infer_code


try:
    response = requests.post(CHATTTS_URL, json=body)
    response.raise_for_status()
    with zipfile.ZipFile(BytesIO(response.content), "r") as zip_ref:
        # save files for each request in a different folder
        dt = datetime.datetime.now()
        ts = int(dt.timestamp())
        tgt = f"./output/{ts}/"
        os.makedirs(tgt, 0o755)
        zip_ref.extractall(tgt)
        print("Extracted files into", tgt)

except requests.exceptions.RequestException as e:
    print(f"Request Error: {e}")



================================================
FILE: examples/api/main.py
================================================
import io
import os
import sys
import zipfile

from fastapi import FastAPI
from fastapi.responses import StreamingResponse


if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

from typing import Optional

import ChatTTS

from tools.audio import pcm_arr_to_mp3_view
from tools.logger import get_logger
import torch


from pydantic import BaseModel
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from tools.normalizer.en import normalizer_en_nemo_text
from tools.normalizer.zh import normalizer_zh_tn

logger = get_logger("Command")

app = FastAPI()


@app.on_event("startup")
async def startup_event():
    global chat

    chat = ChatTTS.Chat(get_logger("ChatTTS"))
    chat.normalizer.register("en", normalizer_en_nemo_text())
    chat.normalizer.register("zh", normalizer_zh_tn())

    logger.info("Initializing ChatTTS...")
    if chat.load(source="huggingface"):
        logger.info("Models loaded successfully.")
    else:
        logger.error("Models load failed.")
        sys.exit(1)


@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request, exc: RequestValidationError):
    logger.error(f"Validation error: {exc.errors()}")
    return JSONResponse(status_code=422, content={"detail": exc.errors()})


class ChatTTSParams(BaseModel):
    text: list[str]
    stream: bool = False
    lang: Optional[str] = None
    skip_refine_text: bool = False
    refine_text_only: bool = False
    use_decoder: bool = True
    do_text_normalization: bool = True
    do_homophone_replacement: bool = False
    params_refine_text: ChatTTS.Chat.RefineTextParams = None
    params_infer_code: ChatTTS.Chat.InferCodeParams


@app.post("/generate_voice")
async def generate_voice(params: ChatTTSParams):
    logger.info("Text input: %s", str(params.text))

    # audio seed
    if params.params_infer_code.manual_seed is not None:
        torch.manual_seed(params.params_infer_code.manual_seed)
        params.params_infer_code.spk_emb = chat.sample_random_speaker()

    # text seed for text refining
    if params.params_refine_text:
        text = chat.infer(
            text=params.text, skip_refine_text=False, refine_text_only=True
        )
        logger.info(f"Refined text: {text}")
    else:
        # no text refining
        text = params.text

    logger.info("Use speaker:")
    logger.info(params.params_infer_code.spk_emb)

    logger.info("Start voice inference.")
    wavs = chat.infer(
        text=text,
        stream=params.stream,
        lang=params.lang,
        skip_refine_text=params.skip_refine_text,
        use_decoder=params.use_decoder,
        do_text_normalization=params.do_text_normalization,
        do_homophone_replacement=params.do_homophone_replacement,
        params_infer_code=params.params_infer_code,
        params_refine_text=params.params_refine_text,
    )
    logger.info("Inference completed.")

    # zip all of the audio files together
    buf = io.BytesIO()
    with zipfile.ZipFile(
        buf, "a", compression=zipfile.ZIP_DEFLATED, allowZip64=False
    ) as f:
        for idx, wav in enumerate(wavs):
            f.writestr(f"{idx}.mp3", pcm_arr_to_mp3_view(wav))
    logger.info("Audio generation successful.")
    buf.seek(0)

    response = StreamingResponse(buf, media_type="application/zip")
    response.headers["Content-Disposition"] = "attachment; filename=audio_files.zip"
    return response



================================================
FILE: examples/api/postScript.py
================================================
import argparse
import datetime
import os
import zipfile
from io import BytesIO

import requests

chattts_service_host = os.environ.get("CHATTTS_SERVICE_HOST", "127.0.0.1")
chattts_service_port = os.environ.get("CHATTTS_SERVICE_PORT", "9900")

CHATTTS_URL = f"http://{chattts_service_host}:{chattts_service_port}/generate_voice"


def parse_arguments():
    parser = argparse.ArgumentParser(description="HTTP client for ChatTTS service")
    parser.add_argument(
        "--text", type=str, nargs="+", required=True, help="Text to synthesize"
    )
    parser.add_argument(
        "--audio_seed", type=int, required=True, help="Audio generation seed"
    )
    parser.add_argument(
        "--text_seed", type=int, required=True, help="Text generation seed"
    )
    parser.add_argument(
        "--stream", type=bool, default=False, help="Enable/disable streaming"
    )
    parser.add_argument("--lang", type=str, default=None, help="Language code for text")
    parser.add_argument(
        "--skip_refine_text", type=bool, default=True, help="Skip text refinement"
    )
    parser.add_argument(
        "--refine_text_only", type=bool, default=False, help="Only refine text"
    )
    parser.add_argument(
        "--use_decoder", type=bool, default=True, help="Use decoder during inference"
    )
    parser.add_argument(
        "--do_text_normalization",
        type=bool,
        default=True,
        help="Enable text normalization",
    )
    parser.add_argument(
        "--do_homophone_replacement",
        type=bool,
        default=False,
        help="Enable homophone replacement",
    )
    parser.add_argument(
        "--tgt",
        type=str,
        default="./output",
        help="Target directory to save output files",
    )
    parser.add_argument(
        "--filename",
        type=str,
        default="test.mp3",
        help="Target directory to save output files",
    )

    # Refinement text parameters
    parser.add_argument(
        "--refine_prompt", type=str, default="", help="Prompt for text refinement"
    )
    parser.add_argument(
        "--refine_top_P",
        type=float,
        default=0.7,
        help="Top P value for text refinement",
    )
    parser.add_argument(
        "--refine_top_K", type=int, default=20, help="Top K value for text refinement"
    )
    parser.add_argument(
        "--refine_temperature",
        type=float,
        default=0.7,
        help="Temperature for text refinement",
    )
    parser.add_argument(
        "--refine_repetition_penalty",
        type=float,
        default=1.0,
        help="Repetition penalty for text refinement",
    )
    parser.add_argument(
        "--refine_max_new_token",
        type=int,
        default=384,
        help="Max new tokens for text refinement",
    )
    parser.add_argument(
        "--refine_min_new_token",
        type=int,
        default=0,
        help="Min new tokens for text refinement",
    )
    parser.add_argument(
        "--refine_show_tqdm",
        type=bool,
        default=True,
        help="Show progress bar for text refinement",
    )
    parser.add_argument(
        "--refine_ensure_non_empty",
        type=bool,
        default=True,
        help="Ensure non-empty output",
    )
    parser.add_argument(
        "--refine_stream_batch",
        type=int,
        default=24,
        help="Stream batch size for refinement",
    )

    # Infer code parameters
    parser.add_argument(
        "--infer_prompt", type=str, default="[speed_5]", help="Prompt for inference"
    )
    parser.add_argument(
        "--infer_top_P", type=float, default=0.1, help="Top P value for inference"
    )
    parser.add_argument(
        "--infer_top_K", type=int, default=20, help="Top K value for inference"
    )
    parser.add_argument(
        "--infer_temperature", type=float, default=0.3, help="Temperature for inference"
    )
    parser.add_argument(
        "--infer_repetition_penalty",
        type=float,
        default=1.05,
        help="Repetition penalty for inference",
    )
    parser.add_argument(
        "--infer_max_new_token",
        type=int,
        default=2048,
        help="Max new tokens for inference",
    )
    parser.add_argument(
        "--infer_min_new_token",
        type=int,
        default=0,
        help="Min new tokens for inference",
    )
    parser.add_argument(
        "--infer_show_tqdm",
        type=bool,
        default=True,
        help="Show progress bar for inference",
    )
    parser.add_argument(
        "--infer_ensure_non_empty",
        type=bool,
        default=True,
        help="Ensure non-empty output",
    )
    parser.add_argument(
        "--infer_stream_batch",
        type=bool,
        default=True,
        help="Stream batch for inference",
    )
    parser.add_argument(
        "--infer_spk_emb",
        type=str,
        default=None,
        help="Speaker embedding for inference",
    )

    return parser.parse_args()


def main():
    args = parse_arguments()

    # Main infer params
    body = {
        "text": args.text,
        "stream": args.stream,
        "lang": args.lang,
        "filename": args.filename,
        "skip_refine_text": args.skip_refine_text,
        "refine_text_only": args.refine_text_only,
        "use_decoder": args.use_decoder,
        "audio_seed": args.audio_seed,
        "text_seed": args.text_seed,
        "do_text_normalization": args.do_text_normalization,
        "do_homophone_replacement": args.do_homophone_replacement,
    }
    # Refinement text parameters
    params_refine_text = {
        "prompt": args.refine_prompt,
        "top_P": args.refine_top_P,
        "top_K": args.refine_top_K,
        "temperature": args.refine_temperature,
        "repetition_penalty": args.refine_repetition_penalty,
        "max_new_token": args.refine_max_new_token,
        "min_new_token": args.refine_min_new_token,
        "show_tqdm": args.refine_show_tqdm,
        "ensure_non_empty": args.refine_ensure_non_empty,
        "stream_batch": args.refine_stream_batch,
    }
    body["params_refine_text"] = params_refine_text

    # Infer code parameters
    params_infer_code = {
        "prompt": args.infer_prompt,
        "top_P": args.infer_top_P,
        "top_K": args.infer_top_K,
        "temperature": args.infer_temperature,
        "repetition_penalty": args.infer_repetition_penalty,
        "max_new_token": args.infer_max_new_token,
        "min_new_token": args.infer_min_new_token,
        "show_tqdm": args.infer_show_tqdm,
        "ensure_non_empty": args.infer_ensure_non_empty,
        "stream_batch": args.infer_stream_batch,
        "spk_emb": args.infer_spk_emb,
    }
    body["params_infer_code"] = params_infer_code

    try:
        response = requests.post(CHATTTS_URL, json=body)
        response.raise_for_status()
        with zipfile.ZipFile(BytesIO(response.content), "r") as zip_ref:
            tgt = args.tgt
            # filename=args.filename
            os.makedirs(tgt, exist_ok=True)
            zip_ref.extractall(tgt)
            print(f"Extracted files:{tgt}/{filename}")
            # print(tgt)
    except requests.exceptions.RequestException as e:
        print(f"Request Error: {e}")


if __name__ == "__main__":
    main()



================================================
FILE: examples/api/requirements.txt
================================================
fastapi
requests



================================================
FILE: examples/cmd/run.py
================================================
import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

from typing import Optional, List
import argparse

import numpy as np

import ChatTTS

from tools.logger import get_logger
from tools.audio import pcm_arr_to_mp3_view
from tools.normalizer.en import normalizer_en_nemo_text
from tools.normalizer.zh import normalizer_zh_tn

logger = get_logger("Command")


def save_mp3_file(wav, index):
    data = pcm_arr_to_mp3_view(wav)
    mp3_filename = f"output_audio_{index}.mp3"
    with open(mp3_filename, "wb") as f:
        f.write(data)
    logger.info(f"Audio saved to {mp3_filename}")


def load_normalizer(chat: ChatTTS.Chat):
    # try to load normalizer
    try:
        chat.normalizer.register("en", normalizer_en_nemo_text())
    except ValueError as e:
        logger.error(e)
    except BaseException:
        logger.warning("Package nemo_text_processing not found!")
        logger.warning(
            "Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing",
        )
    try:
        chat.normalizer.register("zh", normalizer_zh_tn())
    except ValueError as e:
        logger.error(e)
    except BaseException:
        logger.warning("Package WeTextProcessing not found!")
        logger.warning(
            "Run: conda install -c conda-forge pynini=2.1.5 && pip install WeTextProcessing",
        )


def main(
    texts: List[str],
    spk: Optional[str] = None,
    stream: bool = False,
    source: str = "local",
    custom_path: str = "",
):
    logger.info("Text input: %s", str(texts))

    chat = ChatTTS.Chat(get_logger("ChatTTS"))
    logger.info("Initializing ChatTTS...")
    load_normalizer(chat)

    is_load = False
    if os.path.isdir(custom_path) and source == "custom":
        is_load = chat.load(source="custom", custom_path=custom_path)
    else:
        is_load = chat.load(source=source)

    if is_load:
        logger.info("Models loaded successfully.")
    else:
        logger.error("Models load failed.")
        sys.exit(1)

    if spk is None:
        spk = chat.sample_random_speaker()
    logger.info("Use speaker:")
    print(spk)

    logger.info("Start inference.")
    wavs = chat.infer(
        texts,
        stream,
        params_infer_code=ChatTTS.Chat.InferCodeParams(
            spk_emb=spk,
        ),
    )
    logger.info("Inference completed.")
    # Save each generated wav file to a local file
    if stream:
        wavs_list = []
    for index, wav in enumerate(wavs):
        if stream:
            for i, w in enumerate(wav):
                save_mp3_file(w, (i + 1) * 1000 + index)
            wavs_list.append(wav)
        else:
            save_mp3_file(wav, index)
    if stream:
        for index, wav in enumerate(np.concatenate(wavs_list, axis=1)):
            save_mp3_file(wav, index)
    logger.info("Audio generation successful.")


if __name__ == "__main__":
    r"""
    python -m examples.cmd.run \
        --source custom --custom_path ../../models/2Noise/ChatTTS ä½ å¥½å–² ":)"
    """
    logger.info("Starting ChatTTS commandline demo...")
    parser = argparse.ArgumentParser(
        description="ChatTTS Command",
        usage='[--spk xxx] [--stream] [--source ***] [--custom_path XXX] "Your text 1." " Your text 2."',
    )
    parser.add_argument(
        "--spk",
        help="Speaker (empty to sample a random one)",
        type=Optional[str],
        default=None,
    )
    parser.add_argument(
        "--stream",
        help="Use stream mode",
        action="store_true",
    )
    parser.add_argument(
        "--source",
        help="source form [ huggingface(hf download), local(ckpt save to asset dir), custom(define) ]",
        type=str,
        default="local",
    )
    parser.add_argument(
        "--custom_path",
        help="custom defined model path(include asset ckpt dir)",
        type=str,
        default="",
    )
    parser.add_argument(
        "texts",
        help="Original text",
        default=["YOUR TEXT HERE"],
        nargs=argparse.REMAINDER,
    )
    args = parser.parse_args()
    logger.info(args)
    main(args.texts, args.spk, args.stream, args.source, args.custom_path)
    logger.info("ChatTTS process finished.")



================================================
FILE: examples/cmd/stream.py
================================================
import random

import numpy as np

from tools.audio import float_to_int16


# æµå¼æ¨ç†æ•°æ®è·å–å™¨ï¼Œæ”¯æŒæµå¼è·å–éŸ³é¢‘ç¼–ç å­—èŠ‚æµ
class ChatStreamer:
    def __init__(self, base_block_size=8000):
        self.base_block_size = base_block_size

    # streamçŠ¶æ€æ›´æ–°ã€‚æ•°æ®é‡ä¸è¶³çš„streamï¼Œå…ˆå­˜ä¸€æ®µæ—¶é—´ï¼Œç›´åˆ°æ‹¿åˆ°è¶³å¤Ÿæ•°æ®ï¼Œç›‘æ§å°å—æ•°æ®æƒ…å†µ
    @staticmethod
    def _update_stream(history_stream_wav, new_stream_wav, thre):
        if history_stream_wav is not None:
            result_stream = np.concatenate([history_stream_wav, new_stream_wav], axis=1)
            is_keep_next = result_stream.shape[0] * result_stream.shape[1] < thre
            if random.random() > 0.1:
                print(
                    "update_stream",
                    is_keep_next,
                    [i.shape if i is not None else None for i in result_stream],
                )
        else:
            result_stream = new_stream_wav
            is_keep_next = result_stream.shape[0] * result_stream.shape[1] < thre

        return result_stream, is_keep_next

    # å·²æ¨ç†batchæ•°æ®ä¿å­˜
    @staticmethod
    def _accum(accum_wavs, stream_wav):
        if accum_wavs is None:
            accum_wavs = stream_wav
        else:
            accum_wavs = np.concatenate([accum_wavs, stream_wav], axis=1)
        return accum_wavs

    # batch streamæ•°æ®æ ¼å¼è½¬åŒ–
    @staticmethod
    def batch_stream_formatted(stream_wav, output_format="PCM16_byte"):
        if output_format in ("PCM16_byte", "PCM16"):
            format_data = float_to_int16(stream_wav)
        else:
            format_data = stream_wav
        return format_data

    # æ•°æ®æ ¼å¼è½¬åŒ–
    @staticmethod
    def formatted(data, output_format="PCM16_byte"):
        if output_format == "PCM16_byte":
            format_data = data.astype("<i2").tobytes()
        else:
            format_data = data
        return format_data

    # æ£€æŸ¥å£°éŸ³æ˜¯å¦ä¸ºç©º
    @staticmethod
    def checkvoice(data):
        if np.abs(data).max() < 1e-6:
            return False
        else:
            return True

    # å°†å£°éŸ³è¿›è¡Œé€‚å½“æ‹†åˆ†è¿”å›
    @staticmethod
    def _subgen(data, thre=12000):
        for stard_idx in range(0, data.shape[0], thre):
            end_idx = stard_idx + thre
            yield data[stard_idx:end_idx]

    # æµå¼æ•°æ®è·å–ï¼Œæ”¯æŒè·å–éŸ³é¢‘ç¼–ç å­—èŠ‚æµ
    def generate(self, streamchat, output_format=None):
        assert output_format in ("PCM16_byte", "PCM16", None)
        curr_sentence_index = 0
        history_stream_wav = None
        article_streamwavs = None
        for stream_wav in streamchat:
            print(np.abs(stream_wav).max(axis=1))
            n_texts = len(stream_wav)
            n_valid_texts = (np.abs(stream_wav).max(axis=1) > 1e-6).sum()
            if n_valid_texts == 0:
                continue
            else:
                block_thre = n_valid_texts * self.base_block_size
                stream_wav, is_keep_next = ChatStreamer._update_stream(
                    history_stream_wav, stream_wav, block_thre
                )
                # æ•°æ®é‡ä¸è¶³ï¼Œå…ˆä¿å­˜çŠ¶æ€
                if is_keep_next:
                    history_stream_wav = stream_wav
                    continue
                # æ•°æ®é‡è¶³å¤Ÿï¼Œæ‰§è¡Œå†™å…¥æ“ä½œ
                else:
                    history_stream_wav = None
                    stream_wav = ChatStreamer.batch_stream_formatted(
                        stream_wav, output_format
                    )
                    article_streamwavs = ChatStreamer._accum(
                        article_streamwavs, stream_wav
                    )
                    # å†™å…¥å½“å‰å¥å­
                    if ChatStreamer.checkvoice(stream_wav[curr_sentence_index]):
                        for sub_wav in ChatStreamer._subgen(
                            stream_wav[curr_sentence_index]
                        ):
                            if ChatStreamer.checkvoice(sub_wav):
                                yield ChatStreamer.formatted(sub_wav, output_format)
                    # å½“å‰å¥å­å·²å†™å…¥å®Œæˆï¼Œç›´æ¥å†™ä¸‹ä¸€ä¸ªå¥å­å·²ç»æ¨ç†å®Œæˆçš„éƒ¨åˆ†
                    elif curr_sentence_index < n_texts - 1:
                        curr_sentence_index += 1
                        print("add next sentence")
                        finish_stream_wavs = article_streamwavs[curr_sentence_index]

                        for sub_wav in ChatStreamer._subgen(finish_stream_wavs):
                            if ChatStreamer.checkvoice(sub_wav):
                                yield ChatStreamer.formatted(sub_wav, output_format)

                    # streamchatéå†å®Œæ¯•ï¼Œåœ¨å¤–å±‚æŠŠå‰©ä½™ç»“æœå†™å…¥
                    else:
                        break
        # æœ¬è½®å‰©ä½™æœ€åä¸€ç‚¹æ•°æ®å†™å…¥
        if is_keep_next:
            if len(list(filter(lambda x: x is not None, stream_wav))) > 0:
                stream_wav = ChatStreamer.batch_stream_formatted(
                    stream_wav, output_format
                )
                if ChatStreamer.checkvoice(stream_wav[curr_sentence_index]):

                    for sub_wav in ChatStreamer._subgen(
                        stream_wav[curr_sentence_index]
                    ):
                        if ChatStreamer.checkvoice(sub_wav):
                            yield ChatStreamer.formatted(sub_wav, output_format)
                    article_streamwavs = ChatStreamer._accum(
                        article_streamwavs, stream_wav
                    )
        # æŠŠå·²ç»å®Œæˆæ¨ç†çš„ä¸‹å‡ è½®å‰©ä½™æ•°æ®å†™å…¥
        for i_text in range(curr_sentence_index + 1, n_texts):
            finish_stream_wavs = article_streamwavs[i_text]

            for sub_wav in ChatStreamer._subgen(finish_stream_wavs):
                if ChatStreamer.checkvoice(sub_wav):
                    yield ChatStreamer.formatted(sub_wav, output_format)

    # æµå¼æ’­æ”¾æ¥å£
    def play(self, streamchat, wait=5):
        import pyaudio  # please install it manually

        p = pyaudio.PyAudio()
        print(p.get_device_count())
        # è®¾ç½®éŸ³é¢‘æµå‚æ•°
        FORMAT = pyaudio.paInt16  # 16ä½æ·±åº¦
        CHANNELS = 1  # å•å£°é“
        RATE = 24000  # é‡‡æ ·ç‡
        CHUNK = 1024  # æ¯å—éŸ³é¢‘æ•°æ®å¤§å°

        # æ‰“å¼€è¾“å‡ºæµï¼ˆæ‰¬å£°å™¨ï¼‰
        stream_out = p.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=RATE,
            output=True,
        )

        first_prefill_size = wait * RATE
        prefill_bytes = b""
        meet = False
        for i in self.generate(streamchat, output_format="PCM16_byte"):
            if not meet:
                prefill_bytes += i
                if len(prefill_bytes) > first_prefill_size:
                    meet = True
                    stream_out.write(prefill_bytes)
            else:
                stream_out.write(i)
        if not meet:
            stream_out.write(prefill_bytes)

        stream_out.stop_stream()
        stream_out.close()


if __name__ == "__main__":
    import ChatTTS

    # åŠ è½½ ChatTTS
    chat = ChatTTS.Chat()
    chat.load(compile=False)

    rand_spk = chat.sample_random_speaker()
    params_infer_code = ChatTTS.Chat.InferCodeParams(
        spk_emb=rand_spk,  # add sampled speaker
        temperature=0.3,  # using custom temperature
        top_P=0.7,  # top P decode
        top_K=20,  # top K decode
    )

    # è·å–ChatTTS æµå¼æ¨ç†generator
    streamchat = chat.infer(
        [
            "æ€»ç»“ä¸€ä¸‹ï¼ŒAI Agentæ˜¯å¤§æ¨¡å‹åŠŸèƒ½çš„æ‰©å±•ï¼Œè®©AIæ›´æ¥è¿‘äºé€šç”¨äººå·¥æ™ºèƒ½ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„AGIã€‚",
            "ä½ å¤ªèªæ˜å•¦ã€‚",
            "ä¸¾ä¸ªä¾‹å­ï¼Œå¤§æ¨¡å‹å¯èƒ½å¯ä»¥å†™ä»£ç ï¼Œä½†å®ƒä¸èƒ½ç‹¬ç«‹å®Œæˆä¸€ä¸ªå®Œæ•´çš„è½¯ä»¶å¼€å‘é¡¹ç›®ã€‚è¿™æ—¶å€™ï¼ŒAI Agentå°±æ ¹æ®å¤§æ¨¡å‹çš„æ™ºèƒ½ï¼Œç»“åˆè®°å¿†å’Œè§„åˆ’ï¼Œä¸€æ­¥æ­¥å®ç°ä»éœ€æ±‚åˆ†æåˆ°äº§å“ä¸Šçº¿ã€‚",
        ],
        skip_refine_text=True,
        stream=True,
        params_infer_code=params_infer_code,
    )
    # å…ˆå­˜æ”¾ä¸€éƒ¨åˆ†ï¼Œå­˜çš„å·®ä¸å¤šäº†å†æ’­æ”¾ï¼Œé€‚åˆç”Ÿæˆé€Ÿåº¦æ¯”è¾ƒæ…¢çš„cpuç©å®¶ä½¿ç”¨
    ChatStreamer().play(streamchat, wait=5)



================================================
FILE: examples/ipynb/colab.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## Clone Repo
"""

!cd /content
!rm -rf sample_data ChatTTS
!git clone https://github.com/2noise/ChatTTS.git

"""
## Install Requirements
"""

!pip install -r /content/ChatTTS/requirements.txt
!ldconfig /usr/lib64-nvidia

"""
## Import Packages
"""

import torch

torch._dynamo.config.cache_size_limit = 64
torch._dynamo.config.suppress_errors = True
torch.set_float32_matmul_precision("high")

from ChatTTS import ChatTTS
from ChatTTS.tools.logger import get_logger
from ChatTTS.tools.normalizer import normalizer_en_nemo_text, normalizer_zh_tn
from IPython.display import Audio

"""
## Load Models
"""

logger = get_logger("ChatTTS", format_root=True)
chat = ChatTTS.Chat(logger)

# try to load normalizer
try:
    chat.normalizer.register("en", normalizer_en_nemo_text())
except ValueError as e:
    logger.error(e)
except:
    logger.warning("Package nemo_text_processing not found!")
    logger.warning(
        "Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing",
    )
try:
    chat.normalizer.register("zh", normalizer_zh_tn())
except ValueError as e:
    logger.error(e)
except:
    logger.warning("Package WeTextProcessing not found!")
    logger.warning(
        "Run: conda install -c conda-forge pynini=2.1.5 && pip install WeTextProcessing",
    )

"""
### Here are three choices for loading models,
"""

"""
#### 1. Load models from Hugging Face (recommend)
"""

# use force_redownload=True if the weights have been updated.
chat.load(source="huggingface")

"""
#### 2. Load models from local directories 'asset' and 'config'
"""

chat.load()
# chat.load(source='local') same as above

"""
#### 3. Load models from a custom path
"""

# write the model path into custom_path
chat.load(source="custom", custom_path="YOUR CUSTOM PATH")

"""
### You can also unload models to save the memory
"""

chat.unload()

"""
## Inference
"""

"""
### Batch infer
"""

texts = [
    "So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.",
] * 3 + [
    "æˆ‘è§‰å¾—åƒæˆ‘ä»¬è¿™äº›å†™ç¨‹åºçš„äººï¼Œä»–ï¼Œæˆ‘è§‰å¾—å¤šå¤šå°‘å°‘å¯èƒ½ä¼šå¯¹å¼€æºæœ‰ä¸€ç§æƒ…æ€€åœ¨å§æˆ‘è§‰å¾—å¼€æºæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å½¢å¼ã€‚ç°åœ¨å…¶å®æœ€å…ˆè¿›çš„æŠ€æœ¯æŒæ¡åœ¨ä¸€äº›å…¬å¸çš„æ‰‹é‡Œçš„è¯ï¼Œå°±ä»–ä»¬å¹¶ä¸ä¼šè½»æ˜“çš„å¼€æ”¾ç»™æ‰€æœ‰çš„äººç”¨ã€‚"
] * 3

wavs = chat.infer(texts)

Audio(wavs[0], rate=24_000, autoplay=True)

Audio(wavs[3], rate=24_000, autoplay=True)

"""
### Custom params
"""

params_infer_code = ChatTTS.Chat.InferCodeParams(
    prompt="[speed_5]",
    temperature=0.3,
)
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt="[oral_2][laugh_0][break_6]",
)

wav = chat.infer(
    "å››å·ç¾é£Ÿå¯å¤šäº†ï¼Œæœ‰éº»è¾£ç«é”…ã€å®«ä¿é¸¡ä¸ã€éº»å©†è±†è…ã€æ‹…æ‹…é¢ã€å›é”…è‚‰ã€å¤«å¦»è‚ºç‰‡ç­‰ï¼Œæ¯æ ·éƒ½è®©äººå‚æ¶ä¸‰å°ºã€‚",
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

Audio(wav[0], rate=24_000, autoplay=True)

"""
### fix random speaker
"""

rand_spk = chat.sample_random_speaker()
print(rand_spk)  # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb=rand_spk,
)

wav = chat.infer(
    "å››å·ç¾é£Ÿç¡®å®ä»¥è¾£é—»åï¼Œä½†ä¹Ÿæœ‰ä¸è¾£çš„é€‰æ‹©ã€‚æ¯”å¦‚ç”œæ°´é¢ã€èµ–æ±¤åœ†ã€è›‹çƒ˜ç³•ã€å¶å„¿ç²‘ç­‰ï¼Œè¿™äº›å°åƒå£å‘³æ¸©å’Œï¼Œç”œè€Œä¸è…»ï¼Œä¹Ÿå¾ˆå—æ¬¢è¿ã€‚",
    params_infer_code=params_infer_code,
)

Audio(wav[0], rate=24_000, autoplay=True)

"""
### Zero shot (simulate speaker)
"""

from ChatTTS.tools.audio import load_audio

spk_smp = chat.sample_audio_speaker(load_audio("sample.mp3", 24000))
print(spk_smp)  # save it in order to load the speaker without sample audio next time

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_smp=spk_smp,
    txt_smp="ä¸sample.mp3å†…å®¹å®Œå…¨ä¸€è‡´çš„æ–‡æœ¬è½¬å†™ã€‚",
)

wav = chat.infer(
    "å››å·ç¾é£Ÿç¡®å®ä»¥è¾£é—»åï¼Œä½†ä¹Ÿæœ‰ä¸è¾£çš„é€‰æ‹©ã€‚æ¯”å¦‚ç”œæ°´é¢ã€èµ–æ±¤åœ†ã€è›‹çƒ˜ç³•ã€å¶å„¿ç²‘ç­‰ï¼Œè¿™äº›å°åƒå£å‘³æ¸©å’Œï¼Œç”œè€Œä¸è…»ï¼Œä¹Ÿå¾ˆå—æ¬¢è¿ã€‚",
    params_infer_code=params_infer_code,
)

Audio(wav[0], rate=24_000, autoplay=True)

"""
### Two stage control
"""

text = "So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with."
refined_text = chat.infer(text, refine_text_only=True)
refined_text

wav = chat.infer(refined_text, skip_refine_text=True)

Audio(wav[0], rate=24_000, autoplay=True)

"""
## LLM Call
"""

from ChatTTS.tools.llm import ChatOpenAI

API_KEY = ""
client = ChatOpenAI(
    api_key=API_KEY, base_url="https://api.deepseek.com", model="deepseek-chat"
)

user_question = "å››å·æœ‰å“ªäº›å¥½åƒçš„ç¾é£Ÿå‘¢?"

text = client.call(user_question, prompt_version="deepseek")
text

text = client.call(text, prompt_version="deepseek_TN")
text

wav = chat.infer(text)

Audio(wav[0], rate=24_000, autoplay=True)



================================================
FILE: examples/ipynb/example.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## Import packages
"""

import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

if not "root_dir" in globals():
    now_dir = os.getcwd()  # skip examples/ipynb
    root_dir = os.path.join(now_dir, "../../")
    sys.path.append(root_dir)
    print("init root dir to", root_dir)

import torch

torch._dynamo.config.cache_size_limit = 64
torch._dynamo.config.suppress_errors = True
torch.set_float32_matmul_precision("high")

import ChatTTS
from tools.logger import get_logger
from tools.normalizer import normalizer_en_nemo_text, normalizer_zh_tn
from IPython.display import Audio

"""
## Load Models
"""

os.chdir(root_dir)

logger = get_logger("ChatTTS")
chat = ChatTTS.Chat(logger)

# try to load normalizer
try:
    chat.normalizer.register("en", normalizer_en_nemo_text())
except ValueError as e:
    logger.error(e)
except:
    logger.warning("Package nemo_text_processing not found!")
    logger.warning(
        "Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing",
    )
try:
    chat.normalizer.register("zh", normalizer_zh_tn())
except ValueError as e:
    logger.error(e)
except:
    logger.warning("Package WeTextProcessing not found!")
    logger.warning(
        "Run: conda install -c conda-forge pynini=2.1.5 && pip install WeTextProcessing",
    )

"""
### Here are three choices for loading models,
"""

"""
#### 1. Load models from Hugging Face (not suitable in CN)
"""

# use force_redownload=True if the weights have been updated.
chat.load(source="huggingface", force_redownload=True)

"""
#### 2. Load models from local directories 'asset' and 'config' (recommend)
"""

chat.load()
# chat.load(source='local') same as above

"""
#### 3. Load models from a custom path
"""

# write the model path into custom_path
chat.load(source="custom", custom_path="YOUR CUSTOM PATH")

"""
### You can also unload models to save the memory
"""

chat.unload()

"""
## Inference
"""

"""
### Batch infer
"""

texts = [
    "So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.",
] * 3 + [
    "æˆ‘è§‰å¾—åƒæˆ‘ä»¬è¿™äº›å†™ç¨‹åºçš„äººï¼Œä»–ï¼Œæˆ‘è§‰å¾—å¤šå¤šå°‘å°‘å¯èƒ½ä¼šå¯¹å¼€æºæœ‰ä¸€ç§æƒ…æ€€åœ¨å§æˆ‘è§‰å¾—å¼€æºæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å½¢å¼ã€‚ç°åœ¨å…¶å®æœ€å…ˆè¿›çš„æŠ€æœ¯æŒæ¡åœ¨ä¸€äº›å…¬å¸çš„æ‰‹é‡Œçš„è¯ï¼Œå°±ä»–ä»¬å¹¶ä¸ä¼šè½»æ˜“çš„å¼€æ”¾ç»™æ‰€æœ‰çš„äººç”¨ã€‚"
] * 3

wavs = chat.infer(texts)

Audio(wavs[0], rate=24_000, autoplay=True)

Audio(wavs[3], rate=24_000, autoplay=True)

"""
### Custom params
"""

params_infer_code = ChatTTS.Chat.InferCodeParams(
    prompt="[speed_5]",
    temperature=0.3,
)
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt="[oral_2][laugh_0][break_6]",
)

wav = chat.infer(
    "å››å·ç¾é£Ÿå¯å¤šäº†ï¼Œæœ‰éº»è¾£ç«é”…ã€å®«ä¿é¸¡ä¸ã€éº»å©†è±†è…ã€æ‹…æ‹…é¢ã€å›é”…è‚‰ã€å¤«å¦»è‚ºç‰‡ç­‰ï¼Œæ¯æ ·éƒ½è®©äººå‚æ¶ä¸‰å°ºã€‚",
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

Audio(wav[0], rate=24_000, autoplay=True)

"""
### Fix random speaker
"""

rand_spk = chat.sample_random_speaker()
print(rand_spk)  # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb=rand_spk,
)

wav = chat.infer(
    "å››å·ç¾é£Ÿç¡®å®ä»¥è¾£é—»åï¼Œä½†ä¹Ÿæœ‰ä¸è¾£çš„é€‰æ‹©ã€‚æ¯”å¦‚ç”œæ°´é¢ã€èµ–æ±¤åœ†ã€è›‹çƒ˜ç³•ã€å¶å„¿ç²‘ç­‰ï¼Œè¿™äº›å°åƒå£å‘³æ¸©å’Œï¼Œç”œè€Œä¸è…»ï¼Œä¹Ÿå¾ˆå—æ¬¢è¿ã€‚",
    params_infer_code=params_infer_code,
)

Audio(wav[0], rate=24_000, autoplay=True)

"""
### Zero shot (simulate speaker)
"""

from tools.audio import load_audio

spk_smp = chat.sample_audio_speaker(load_audio("sample.mp3", 24000))
print(spk_smp)  # save it in order to load the speaker without sample audio next time

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_smp=spk_smp,
    txt_smp="ä¸sample.mp3å†…å®¹å®Œå…¨ä¸€è‡´çš„æ–‡æœ¬è½¬å†™ã€‚",
)

wav = chat.infer(
    "å››å·ç¾é£Ÿç¡®å®ä»¥è¾£é—»åï¼Œä½†ä¹Ÿæœ‰ä¸è¾£çš„é€‰æ‹©ã€‚æ¯”å¦‚ç”œæ°´é¢ã€èµ–æ±¤åœ†ã€è›‹çƒ˜ç³•ã€å¶å„¿ç²‘ç­‰ï¼Œè¿™äº›å°åƒå£å‘³æ¸©å’Œï¼Œç”œè€Œä¸è…»ï¼Œä¹Ÿå¾ˆå—æ¬¢è¿ã€‚",
    params_infer_code=params_infer_code,
)

Audio(wav[0], rate=24_000, autoplay=True)

"""
### Two stage control
"""

text = "So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with."
refined_text = chat.infer(text, refine_text_only=True)
refined_text

wav = chat.infer(refined_text, skip_refine_text=True)

Audio(wav[0], rate=24_000, autoplay=True)

"""
## LLM Call
"""

from tools.llm import ChatOpenAI

API_KEY = ""
client = ChatOpenAI(
    api_key=API_KEY, base_url="https://api.deepseek.com", model="deepseek-chat"
)

user_question = "å››å·æœ‰å“ªäº›å¥½åƒçš„ç¾é£Ÿå‘¢?"

text = client.call(user_question, prompt_version="deepseek")
text

text = client.call(text, prompt_version="deepseek_TN")
text

wav = chat.infer(text)

Audio(wav[0], rate=24_000, autoplay=True)



================================================
FILE: examples/onnx/README.md
================================================
# Export onnx or JIT models for deployment

## Run `pip install onnx -U`.

## Export GPT

3. Run `python examples/onnx/exporter.py --gpt`


## Export other models
Run `python examples/onnx/exporter.py --decoder --vocos`

## Reference
[Run LLMs on Sophon TPU](https://github.com/sophgo/LLM-TPU)


================================================
FILE: examples/onnx/exporter.py
================================================
import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

from dataclasses import asdict
import argparse
import torch
from tqdm import tqdm
from ChatTTS.model.dvae import DVAE
from ChatTTS.config import Config
from vocos import Vocos
from vocos.pretrained import instantiate_class
import torch.jit as jit

from gpt import GPT

# disable cuda
torch.cuda.is_available = lambda: False

# add args to control which modules to export
parser = argparse.ArgumentParser()
parser.add_argument("--gpt", action="store_true", help="trace gpt")
parser.add_argument("--decoder", action="store_true", help="trace decoder")
parser.add_argument("--vocos", action="store_true", help="trace vocos")
parser.add_argument(
    "--pth_dir", default="./assets", type=str, help="path to the pth model directory"
)
parser.add_argument(
    "--out_dir", default="./tmp", type=str, help="path to output directory"
)

args = parser.parse_args()
chattts_config = Config()


def export_gpt():
    gpt_model = GPT(gpt_config=asdict(chattts_config.gpt), use_flash_attn=False).eval()
    gpt_model.from_pretrained(asdict(chattts_config.path)["gpt_ckpt_path"])
    gpt_model = gpt_model.eval()
    for param in gpt_model.parameters():
        param.requires_grad = False

    config = gpt_model.gpt.config
    layers = gpt_model.gpt.layers
    model_norm = gpt_model.gpt.norm

    NUM_OF_LAYERS = config.num_hidden_layers
    HIDDEN_SIZE = config.hidden_size
    NUM_ATTENTION_HEADS = config.num_attention_heads
    NUM_KEY_VALUE_HEADS = config.num_key_value_heads
    HEAD_DIM = HIDDEN_SIZE // NUM_ATTENTION_HEADS  # 64
    TEXT_VOCAB_SIZE = gpt_model.emb_text.weight.shape[0]
    AUDIO_VOCAB_SIZE = gpt_model.emb_code[0].weight.shape[0]
    SEQ_LENGTH = 512

    folder = os.path.join(args.out_dir, "gpt")
    os.makedirs(folder, exist_ok=True)

    for param in gpt_model.emb_text.parameters():
        param.requires_grad = False

    for param in gpt_model.emb_code.parameters():
        param.requires_grad = False

    for param in gpt_model.head_code.parameters():
        param.requires_grad = False

    for param in gpt_model.head_text.parameters():
        param.requires_grad = False

    class EmbeddingText(torch.nn.Module):
        def __init__(self, *args, **kwargs) -> None:
            super().__init__(*args, **kwargs)

        def forward(self, input_ids):
            return gpt_model.emb_text(input_ids)

    def convert_embedding_text():
        model = EmbeddingText()
        input_ids = torch.tensor([range(SEQ_LENGTH)])

        torch.onnx.export(
            model,
            (input_ids),
            f"{folder}/embedding_text.onnx",
            verbose=False,
            input_names=["input_ids"],
            output_names=["input_embed"],
            do_constant_folding=True,
            opset_version=15,
        )

    class EmbeddingCode(torch.nn.Module):
        def __init__(self, *args, **kwargs) -> None:
            super().__init__(*args, **kwargs)

        def forward(self, input_ids):
            input_ids = input_ids.unsqueeze(2).expand(
                -1, -1, gpt_model.num_vq
            )  # for forward_first_code
            code_emb = [
                gpt_model.emb_code[i](input_ids[:, :, i])
                for i in range(gpt_model.num_vq)
            ]
            return torch.stack(code_emb, 2).sum(2)

    def convert_embedding_code():
        model = EmbeddingCode()
        input_ids = torch.tensor([range(SEQ_LENGTH)])

        torch.onnx.export(
            model,
            (input_ids),
            f"{folder}/embedding_code.onnx",
            verbose=False,
            input_names=["input_ids"],
            output_names=["input_embed"],
            do_constant_folding=True,
            opset_version=15,
        )

    class EmbeddingCodeCache(torch.nn.Module):  # for forward_next_code
        def __init__(self, *args, **kwargs) -> None:
            super().__init__(*args, **kwargs)

        def forward(self, input_ids):
            code_emb = [
                gpt_model.emb_code[i](input_ids[:, :, i])
                for i in range(gpt_model.num_vq)
            ]
            return torch.stack(code_emb, 2).sum(2)

    def convert_embedding_code_cache():
        model = EmbeddingCodeCache()
        input_ids = torch.tensor(
            [[[416, 290, 166, 212]]]
        )  # torch.tensor([[range(gpt_model.num_vq)]])
        torch.onnx.export(
            model,
            (input_ids),
            f"{folder}/embedding_code_cache.onnx",
            verbose=False,
            input_names=["input_ids"],
            output_names=["input_embed"],
            do_constant_folding=True,
            opset_version=15,
        )

    class Block(torch.nn.Module):
        def __init__(self, layer_id):
            super().__init__()
            self.layer_id = layer_id
            self.layer = layers[layer_id]  # LlamaDecoderLayer
            self.norm = model_norm

        def forward(self, hidden_states, position_ids, attention_mask):
            hidden_states, past_kv = self.layer(
                hidden_states=hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                use_cache=True,
            )
            present_k, present_v = past_kv
            if self.layer_id == NUM_OF_LAYERS - 1:
                hidden_states = self.norm(hidden_states)
            return hidden_states, present_k, present_v

    def convert_block(layer_id):
        model = Block(layer_id)
        hidden_states = torch.randn((1, SEQ_LENGTH, HIDDEN_SIZE))
        position_ids = torch.tensor([range(SEQ_LENGTH)], dtype=torch.long)
        attention_mask = -1000 * torch.ones(
            (1, 1, SEQ_LENGTH, SEQ_LENGTH), dtype=torch.float32
        ).triu(diagonal=1)
        model(hidden_states, position_ids, attention_mask)
        torch.onnx.export(
            model,
            (hidden_states, position_ids, attention_mask),
            f"{folder}/block_{layer_id}.onnx",
            verbose=False,
            input_names=["input_states", "position_ids", "attention_mask"],
            output_names=["hidden_states", "past_k", "past_v"],
            do_constant_folding=True,
            opset_version=15,
        )

    class BlockCache(torch.nn.Module):

        def __init__(self, layer_id):
            super().__init__()
            self.layer_id = layer_id
            self.layer = layers[layer_id]
            self.norm = model_norm

        def forward(self, hidden_states, position_ids, attention_mask, past_k, past_v):
            hidden_states, past_kv = self.layer(
                hidden_states,
                attention_mask,
                position_ids=position_ids,
                past_key_value=(past_k, past_v),
                use_cache=True,
            )
            present_k, present_v = past_kv
            if self.layer_id == NUM_OF_LAYERS - 1:
                hidden_states = self.norm(hidden_states)
            return hidden_states, present_k, present_v

    def convert_block_cache(layer_id):
        model = BlockCache(layer_id)
        hidden_states = torch.randn((1, 1, HIDDEN_SIZE))
        position_ids = torch.tensor([range(1)], dtype=torch.long)
        attention_mask = -1000 * torch.ones(
            (1, 1, 1, SEQ_LENGTH + 1), dtype=torch.float32
        ).triu(diagonal=1)
        past_k = torch.randn((1, SEQ_LENGTH, NUM_ATTENTION_HEADS, HEAD_DIM))
        past_v = torch.randn((1, SEQ_LENGTH, NUM_ATTENTION_HEADS, HEAD_DIM))

        torch.onnx.export(
            model,
            (hidden_states, position_ids, attention_mask, past_k, past_v),
            f"{folder}/block_cache_{layer_id}.onnx",
            verbose=False,
            input_names=[
                "input_states",
                "position_ids",
                "attention_mask",
                "history_k",
                "history_v",
            ],
            output_names=["hidden_states", "past_k", "past_v"],
            do_constant_folding=True,
            opset_version=15,
        )

    class GreedyHead(torch.nn.Module):

        def __init__(self):
            super().__init__()

        def forward(self, m_logits):
            _, token = torch.topk(m_logits.float(), 1)
            return token

    def convert_greedy_head_text():
        model = GreedyHead()
        m_logits = torch.randn(1, TEXT_VOCAB_SIZE)

        torch.onnx.export(
            model,
            (m_logits),
            f"{folder}/greedy_head_text.onnx",
            verbose=False,
            input_names=["m_logits"],
            output_names=["token"],
            do_constant_folding=True,
            opset_version=15,
        )

    def convert_greedy_head_code():
        model = GreedyHead()
        m_logits = torch.randn(1, AUDIO_VOCAB_SIZE, gpt_model.num_vq)

        torch.onnx.export(
            model,
            (m_logits),
            f"{folder}/greedy_head_code.onnx",
            verbose=False,
            input_names=["m_logits"],
            output_names=["token"],
            do_constant_folding=True,
            opset_version=15,
        )

    class LmHead_infer_text(torch.nn.Module):
        def __init__(self):
            super().__init__()

        def forward(self, hidden_states):
            m_logits = gpt_model.head_text(hidden_states)
            return m_logits

    class LmHead_infer_code(torch.nn.Module):
        def __init__(self):
            super().__init__()

        def forward(self, hidden_states):
            m_logits = torch.stack(
                [
                    gpt_model.head_code[i](hidden_states)
                    for i in range(gpt_model.num_vq)
                ],
                2,
            )
            return m_logits

    def convert_lm_head_text():
        model = LmHead_infer_text()
        input = torch.randn(1, HIDDEN_SIZE)

        torch.onnx.export(
            model,
            (input),
            f"{folder}/lm_head_text.onnx",
            verbose=False,
            input_names=["hidden_states"],
            output_names=["m_logits"],
            do_constant_folding=True,
            opset_version=15,
        )

    def convert_lm_head_code():
        model = LmHead_infer_code()
        input = torch.randn(1, HIDDEN_SIZE)
        torch.onnx.export(
            model,
            (input),
            f"{folder}/lm_head_code.onnx",
            verbose=False,
            input_names=["hidden_states"],
            output_names=["m_logits"],
            do_constant_folding=True,
            opset_version=15,
        )

    # export models
    print(f"Convert block & block_cache")
    for i in tqdm(range(NUM_OF_LAYERS)):
        convert_block(i)
        convert_block_cache(i)

    print(f"Convert embedding")
    convert_embedding_text()
    convert_embedding_code()
    convert_embedding_code_cache()

    print(f"Convert lm_head")
    convert_lm_head_code()
    convert_lm_head_text()

    print(f"Convert greedy_head")
    convert_greedy_head_text()
    convert_greedy_head_code()


def export_decoder():
    decoder = DVAE(
        decoder_config=asdict(chattts_config.decoder),
        dim=chattts_config.decoder.idim,
    ).eval()
    decoder.load_state_dict(
        torch.load(
            asdict(chattts_config.path)["decoder_ckpt_path"],
            weights_only=True,
            mmap=True,
        )
    )

    for param in decoder.parameters():
        param.requires_grad = False
    rand_input = torch.rand([1, 768, 1024], requires_grad=False)

    def mydec(_inp):
        return decoder(_inp, mode="decode")

    jitmodel = jit.trace(mydec, [rand_input])
    jit.save(jitmodel, f"{args.out_dir}/decoder_jit.pt")


def export_vocos():
    feature_extractor = instantiate_class(
        args=(), init=asdict(chattts_config.vocos.feature_extractor)
    )
    backbone = instantiate_class(args=(), init=asdict(chattts_config.vocos.backbone))
    head = instantiate_class(args=(), init=asdict(chattts_config.vocos.head))
    vocos = Vocos(
        feature_extractor=feature_extractor, backbone=backbone, head=head
    ).eval()
    vocos.load_state_dict(
        torch.load(
            asdict(chattts_config.path)["vocos_ckpt_path"], weights_only=True, mmap=True
        )
    )

    for param in vocos.parameters():
        param.requires_grad = False
    rand_input = torch.rand([1, 100, 2048], requires_grad=False)

    def myvocos(_inp):
        # return chat.vocos.decode(_inp) # TPU cannot support the istft OP, thus it has to be moved to postprocessing
        # reference: https://github.com/gemelo-ai/vocos.git
        x = vocos.backbone(_inp)
        x = vocos.head.out(x).transpose(1, 2)
        mag, p = x.chunk(2, dim=1)
        mag = torch.exp(mag)
        mag = torch.clip(
            mag, max=1e2
        )  # safeguard to prevent excessively large magnitudes
        # wrapping happens here. These two lines produce real and imaginary value
        x = torch.cos(p)
        y = torch.sin(p)
        return mag, x, y

    jitmodel = jit.trace(myvocos, [rand_input])
    torch.onnx.export(
        jitmodel,
        [rand_input],
        f"{args.out_dir}/vocos_1-100-2048.onnx",
        opset_version=12,
        do_constant_folding=True,
    )


if args.gpt:
    export_gpt()

if args.decoder:
    export_decoder()

if args.vocos:
    export_vocos()

print("Done. Please check the files in", args.out_dir)



================================================
FILE: examples/onnx/gpt.py
================================================
import logging
from typing import Tuple

import torch
import torch.nn as nn
from torch.nn.utils.parametrizations import weight_norm

from modeling_llama import LlamaModel, LlamaConfig


class GPT(nn.Module):
    def __init__(
        self,
        gpt_config: dict,
        num_audio_tokens: int = 626,
        num_text_tokens: int = 21178,
        num_vq=4,
        use_flash_attn=False,
        device=torch.device("cpu"),
        logger=logging.getLogger(__name__),
    ):
        super().__init__()

        self.logger = logger

        self.device = device
        self.device_gpt = device if "mps" not in str(device) else torch.device("cpu")

        self.num_vq = num_vq
        self.num_audio_tokens = num_audio_tokens

        self.use_flash_attn = use_flash_attn

        self.gpt, self.llama_config = self._build_llama(gpt_config, self.device_gpt)
        self.is_te_llama = False
        self.model_dim = int(self.gpt.config.hidden_size)
        self.emb_code = nn.ModuleList(
            [
                nn.Embedding(
                    num_audio_tokens,
                    self.model_dim,
                    device=self.device_gpt,
                )
                for _ in range(num_vq)
            ],
        )
        self.emb_text = nn.Embedding(
            num_text_tokens, self.model_dim, device=self.device_gpt
        )

        self.head_text = weight_norm(
            nn.Linear(
                self.model_dim,
                num_text_tokens,
                bias=False,
                device=device,
            ),
            name="weight",
        )
        self.head_code = nn.ModuleList(
            [
                weight_norm(
                    nn.Linear(
                        self.model_dim,
                        num_audio_tokens,
                        bias=False,
                        device=device,
                    ),
                    name="weight",
                )
                for _ in range(self.num_vq)
            ],
        )

    def from_pretrained(self, file_path: str):
        self.load_state_dict(
            torch.load(file_path, weights_only=True, mmap=True), strict=False
        )

    def _build_llama(
        self,
        config: dict,
        device: torch.device,
    ) -> Tuple[LlamaModel, LlamaConfig]:

        llama_config = LlamaConfig(**config)
        model = LlamaModel(llama_config)
        del model.embed_tokens
        return model.to(device), llama_config



================================================
FILE: examples/onnx/modeling_llama.py
================================================
# coding=utf-8
# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
PyTorch LLaMA model.
Copied from https://github.com/sophgo/LLM-TPU/blob/main/models/Llama2/compile/files/llama-2-7b-chat-hf/modeling_llama.py
"""
import math
from typing import List, Optional, Tuple, Union

import torch
import torch.nn.functional as F
import torch.utils.checkpoint
from torch import nn
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss

from transformers.activations import ACT2FN
from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
    SequenceClassifierOutputWithPast,
)
from transformers.modeling_utils import PreTrainedModel
from transformers.utils import (
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    logging,
    replace_return_docstrings,
)
from transformers.models.llama.configuration_llama import LlamaConfig


logger = logging.get_logger(__name__)

_CONFIG_FOR_DOC = "LlamaConfig"


# Copied from transformers.models.bart.modeling_bart._make_causal_mask
def _make_causal_mask(
    input_ids_shape: torch.Size,
    dtype: torch.dtype,
    device: torch.device,
    past_key_values_length: int = 0,
):
    """
    Make causal mask used for bi-directional self-attention.
    """
    bsz, tgt_len = input_ids_shape
    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)
    mask_cond = torch.arange(mask.size(-1), device=device)
    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)
    mask = mask.to(dtype)

    if past_key_values_length > 0:
        mask = torch.cat(
            [
                torch.zeros(
                    tgt_len, past_key_values_length, dtype=dtype, device=device
                ),
                mask,
            ],
            dim=-1,
        )
    return mask[None, None, :, :].expand(
        bsz, 1, tgt_len, tgt_len + past_key_values_length
    )


# Copied from transformers.models.bart.modeling_bart._expand_mask
def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):
    """
    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.
    """
    bsz, src_len = mask.size()
    tgt_len = tgt_len if tgt_len is not None else src_len

    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)

    inverted_mask = 1.0 - expanded_mask

    return inverted_mask.masked_fill(
        inverted_mask.to(torch.bool), torch.finfo(dtype).min
    )


class LlamaRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        LlamaRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)


class LlamaRotaryEmbedding(torch.nn.Module):
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()

        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base
        inv_freq = 1.0 / (
            self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim)
        )
        self.register_buffer("inv_freq", inv_freq)

        # Build here to make `torch.jit.trace` work.
        self._set_cos_sin_cache(
            seq_len=max_position_embeddings,
            device=self.inv_freq.device,
            dtype=torch.get_default_dtype(),
        )

    def _set_cos_sin_cache(self, seq_len, device, dtype):
        self.max_seq_len_cached = seq_len
        t = torch.arange(
            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype
        )

        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer(
            "cos_cached", emb.cos()[None, None, :, :].to(dtype), persistent=False
        )
        self.register_buffer(
            "sin_cached", emb.sin()[None, None, :, :].to(dtype), persistent=False
        )

    def forward(self, x, seq_len=None):
        # x: [bs, num_attention_heads, seq_len, head_size]
        if seq_len > self.max_seq_len_cached:
            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)

        return (
            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
        )


class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
    """LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev"""

    def __init__(
        self,
        dim,
        max_position_embeddings=2048,
        base=10000,
        device=None,
        scaling_factor=1.0,
    ):
        self.scaling_factor = scaling_factor
        super().__init__(dim, max_position_embeddings, base, device)

    def _set_cos_sin_cache(self, seq_len, device, dtype):
        self.max_seq_len_cached = seq_len
        t = torch.arange(
            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype
        )
        t = t / self.scaling_factor

        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer(
            "cos_cached", emb.cos()[None, None, :, :].to(dtype), persistent=False
        )
        self.register_buffer(
            "sin_cached", emb.sin()[None, None, :, :].to(dtype), persistent=False
        )


class LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):
    """LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla"""

    def __init__(
        self,
        dim,
        max_position_embeddings=2048,
        base=10000,
        device=None,
        scaling_factor=1.0,
    ):
        self.scaling_factor = scaling_factor
        super().__init__(dim, max_position_embeddings, base, device)

    def _set_cos_sin_cache(self, seq_len, device, dtype):
        self.max_seq_len_cached = seq_len

        if seq_len > self.max_position_embeddings:
            base = self.base * (
                (self.scaling_factor * seq_len / self.max_position_embeddings)
                - (self.scaling_factor - 1)
            ) ** (self.dim / (self.dim - 2))
            inv_freq = 1.0 / (
                base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim)
            )
            self.register_buffer("inv_freq", inv_freq)

        t = torch.arange(
            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype
        )

        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer(
            "cos_cached", emb.cos()[None, None, :, :].to(dtype), persistent=False
        )
        self.register_buffer(
            "sin_cached", emb.sin()[None, None, :, :].to(dtype), persistent=False
        )


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.
    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]
    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]
    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
    cos = cos.transpose(1, 2)
    sin = sin.transpose(1, 2)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


class LlamaMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.pretraining_tp = config.pretraining_tp
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        if self.pretraining_tp > 1:
            slice = self.intermediate_size // self.pretraining_tp
            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)
            up_proj_slices = self.up_proj.weight.split(slice, dim=0)
            down_proj_slices = self.down_proj.weight.split(slice, dim=1)

            gate_proj = torch.cat(
                [F.linear(x, gate_proj_slices[i]) for i in range(self.pretraining_tp)],
                dim=-1,
            )
            up_proj = torch.cat(
                [F.linear(x, up_proj_slices[i]) for i in range(self.pretraining_tp)],
                dim=-1,
            )

            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)
            down_proj = [
                F.linear(intermediate_states[i], down_proj_slices[i])
                for i in range(self.pretraining_tp)
            ]
            down_proj = sum(down_proj)
        else:
            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

        return down_proj


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(
        batch, num_key_value_heads, n_rep, slen, head_dim
    )
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


class LlamaAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: LlamaConfig):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.pretraining_tp = config.pretraining_tp
        self.max_position_embeddings = config.max_position_embeddings

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )
        self.q_proj = nn.Linear(
            self.hidden_size, self.num_heads * self.head_dim, bias=False
        )
        self.k_proj = nn.Linear(
            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
        )
        self.v_proj = nn.Linear(
            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
        )
        self.o_proj = nn.Linear(
            self.num_heads * self.head_dim, self.hidden_size, bias=False
        )
        self._init_rope()

    def _init_rope(self):
        if self.config.rope_scaling is None:
            self.rotary_emb = LlamaRotaryEmbedding(
                self.head_dim, max_position_embeddings=self.max_position_embeddings
            )
        else:
            scaling_type = self.config.rope_scaling["type"]
            scaling_factor = self.config.rope_scaling["factor"]
            if scaling_type == "linear":
                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(
                    self.head_dim,
                    max_position_embeddings=self.max_position_embeddings,
                    scaling_factor=scaling_factor,
                )
            elif scaling_type == "dynamic":
                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(
                    self.head_dim,
                    max_position_embeddings=self.max_position_embeddings,
                    scaling_factor=scaling_factor,
                )
            else:
                raise ValueError(f"Unknown RoPE scaling type {scaling_type}")

    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
        return (
            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)
            .transpose(1, 2)
            .contiguous()
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()

        if self.pretraining_tp > 1:
            key_value_slicing = (
                self.num_key_value_heads * self.head_dim
            ) // self.pretraining_tp
            query_slices = self.q_proj.weight.split(
                (self.num_heads * self.head_dim) // self.pretraining_tp, dim=0
            )
            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)
            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)

            query_states = [
                F.linear(hidden_states, query_slices[i])
                for i in range(self.pretraining_tp)
            ]
            query_states = torch.cat(query_states, dim=-1)

            key_states = [
                F.linear(hidden_states, key_slices[i])
                for i in range(self.pretraining_tp)
            ]
            key_states = torch.cat(key_states, dim=-1)

            value_states = [
                F.linear(hidden_states, value_slices[i])
                for i in range(self.pretraining_tp)
            ]
            value_states = torch.cat(value_states, dim=-1)

        else:
            query_states = self.q_proj(hidden_states)
            key_states = self.k_proj(hidden_states)
            value_states = self.v_proj(hidden_states)
        # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        # value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)
        key_states = key_states.view(
            bsz, q_len, self.num_key_value_heads, self.head_dim
        )
        value_states = value_states.view(
            bsz, q_len, self.num_key_value_heads, self.head_dim
        )

        # kv_seq_len = key_states.shape[-2]
        kv_seq_len = key_states.shape[-3]
        if past_key_value is not None:
            # kv_seq_len += past_key_value[0].shape[-2]
            kv_seq_len += past_key_value[0].shape[-3]
        if past_key_value is not None:
            cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len - 1)
        else:
            cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
        query_states, key_states = apply_rotary_pos_emb(
            query_states, key_states, cos, sin, position_ids
        )
        past_kv = (key_states, value_states) if use_cache else None
        if past_key_value is not None:
            # reuse k, v, self_attention
            # key_states = torch.cat([past_key_value[0], key_states], dim=2)
            # value_states = torch.cat([past_key_value[1], value_states], dim=2)
            key_states = torch.cat([past_key_value[0], key_states], dim=1)
            value_states = torch.cat([past_key_value[1], value_states], dim=1)
        # past_key_value = (key_states, value_states) if use_cache else None

        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        # attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
        attn_weights = torch.matmul(
            query_states.transpose(1, 2), key_states.transpose(1, 2).transpose(2, 3)
        ) / math.sqrt(self.head_dim)
        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
                f" {attn_weights.size()}"
            )

        if attention_mask is not None:
            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
                raise ValueError(
                    f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
                )
            attn_weights = attn_weights + attention_mask

        # upcast attention to fp32
        attn_weights = nn.functional.softmax(
            attn_weights, dim=-1, dtype=torch.float32
        ).to(query_states.dtype)
        attn_output = torch.matmul(attn_weights, value_states.transpose(1, 2))

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)

        if self.pretraining_tp > 1:
            attn_output = attn_output.split(
                self.hidden_size // self.pretraining_tp, dim=2
            )
            o_proj_slices = self.o_proj.weight.split(
                self.hidden_size // self.pretraining_tp, dim=1
            )
            attn_output = sum(
                [
                    F.linear(attn_output[i], o_proj_slices[i])
                    for i in range(self.pretraining_tp)
                ]
            )
        else:
            attn_output = self.o_proj(attn_output)

        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_kv


class LlamaDecoderLayer(nn.Module):
    def __init__(self, config: LlamaConfig):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.self_attn = LlamaAttention(config=config)
        self.mlp = LlamaMLP(config)
        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = LlamaRMSNorm(
            config.hidden_size, eps=config.rms_norm_eps
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
    ) -> Tuple[
        torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]
    ]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            use_cache (`bool`, *optional*):
                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
                (see `past_key_values`).
            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
        """

        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights,)

        if use_cache:
            outputs += (present_key_value,)

        return outputs


LLAMA_START_DOCSTRING = r"""
    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Parameters:
        config ([`LlamaConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
"""


@add_start_docstrings(
    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
    LLAMA_START_DOCSTRING,
)
class LlamaPreTrainedModel(PreTrainedModel):
    config_class = LlamaConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["LlamaDecoderLayer"]
    _skip_keys_device_placement = "past_key_values"

    def _init_weights(self, module):
        std = self.config.initializer_range
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()

    def _set_gradient_checkpointing(self, module, value=False):
        if isinstance(module, LlamaModel):
            module.gradient_checkpointing = value


LLAMA_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
            it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
            `past_key_values`).

            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
            config.n_positions - 1]`.

            [What are position IDs?](../glossary#position-ids)
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
            model's internal embedding lookup matrix.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
            `past_key_values`).
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
            tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
            more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
"""


@add_start_docstrings(
    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
    LLAMA_START_DOCSTRING,
)
class LlamaModel(LlamaPreTrainedModel):
    """
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]

    Args:
        config: LlamaConfig
    """

    def __init__(self, config: LlamaConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(
            config.vocab_size, config.hidden_size, self.padding_idx
        )
        self.layers = nn.ModuleList(
            [LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)]
        )
        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        self.gradient_checkpointing = False
        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value

    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask
    def _prepare_decoder_attention_mask(
        self, attention_mask, input_shape, inputs_embeds, past_key_values_length
    ):
        # create causal mask
        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
        combined_attention_mask = None
        if input_shape[-1] > 1:
            combined_attention_mask = _make_causal_mask(
                input_shape,
                inputs_embeds.dtype,
                device=inputs_embeds.device,
                past_key_values_length=past_key_values_length,
            )

        if attention_mask is not None:
            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
            expanded_attn_mask = _expand_mask(
                attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]
            ).to(inputs_embeds.device)
            combined_attention_mask = (
                expanded_attn_mask
                if combined_attention_mask is None
                else expanded_attn_mask + combined_attention_mask
            )

        return combined_attention_mask

    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        # retrieve input_ids and inputs_embeds
        if input_ids is not None and inputs_embeds is not None:
            raise ValueError(
                "You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time"
            )
        elif input_ids is not None:
            batch_size, seq_length = input_ids.shape
        elif inputs_embeds is not None:
            batch_size, seq_length, _ = inputs_embeds.shape
        else:
            raise ValueError(
                "You have to specify either decoder_input_ids or decoder_inputs_embeds"
            )

        seq_length_with_past = seq_length
        past_key_values_length = 0

        if past_key_values is not None:
            past_key_values_length = past_key_values[0][0].shape[2]
            seq_length_with_past = seq_length_with_past + past_key_values_length

        if position_ids is None:
            device = input_ids.device if input_ids is not None else inputs_embeds.device
            position_ids = torch.arange(
                past_key_values_length,
                seq_length + past_key_values_length,
                dtype=torch.long,
                device=device,
            )
            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
        else:
            position_ids = position_ids.view(-1, seq_length).long()

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)
        # embed positions
        if attention_mask is None:
            attention_mask = torch.ones(
                (batch_size, seq_length_with_past),
                dtype=torch.bool,
                device=inputs_embeds.device,
            )
        attention_mask = self._prepare_decoder_attention_mask(
            attention_mask,
            (batch_size, seq_length),
            inputs_embeds,
            past_key_values_length,
        )

        hidden_states = inputs_embeds

        if self.gradient_checkpointing and self.training:
            if use_cache:
                logger.warning_once(
                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                )
                use_cache = False

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        next_decoder_cache = () if use_cache else None

        for idx, decoder_layer in enumerate(self.layers):
            if output_hidden_states:
                all_hidden_states += (hidden_states,)

            past_key_value = (
                past_key_values[idx] if past_key_values is not None else None
            )

            if self.gradient_checkpointing and self.training:

                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        # None for past_key_value
                        return module(*inputs, output_attentions, None)

                    return custom_forward

                layer_outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(decoder_layer),
                    hidden_states,
                    attention_mask,
                    position_ids,
                    None,
                )
            else:
                layer_outputs = decoder_layer(
                    hidden_states,
                    attention_mask=attention_mask,
                    position_ids=position_ids,
                    past_key_value=past_key_value,
                    output_attentions=output_attentions,
                    use_cache=use_cache,
                )
            hidden_states = layer_outputs[0]

            if use_cache:
                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        # add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        next_cache = next_decoder_cache if use_cache else None
        if not return_dict:
            return tuple(
                v
                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns]
                if v is not None
            )
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=next_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )


class LlamaForCausalLM(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.pretraining_tp = config.pretraining_tp
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)
    @replace_return_docstrings(
        output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC
    )
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        r"""
        Args:
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

        Returns:

        Example:

        ```python
        >>> from transformers import AutoTokenizer, LlamaForCausalLM

        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""

        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        hidden_states = outputs[0]
        if self.pretraining_tp > 1:
            lm_head_slices = self.lm_head.weight.split(
                self.vocab_size // self.pretraining_tp, dim=0
            )
            logits = [
                F.linear(hidden_states, lm_head_slices[i])
                for i in range(self.pretraining_tp)
            ]
            logits = torch.cat(logits, dim=-1)
        else:
            logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)
            loss = loss_fct(shift_logits, shift_labels)

        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        attention_mask=None,
        inputs_embeds=None,
        **kwargs,
    ):
        if past_key_values:
            input_ids = input_ids[:, -1:]

        position_ids = kwargs.get("position_ids", None)
        if attention_mask is not None and position_ids is None:
            # create position_ids on the fly for batch generation
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
            if past_key_values:
                position_ids = position_ids[:, -1].unsqueeze(-1)

        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step
        if inputs_embeds is not None and past_key_values is None:
            model_inputs = {"inputs_embeds": inputs_embeds}
        else:
            model_inputs = {"input_ids": input_ids}

        model_inputs.update(
            {
                "position_ids": position_ids,
                "past_key_values": past_key_values,
                "use_cache": kwargs.get("use_cache"),
                "attention_mask": attention_mask,
            }
        )
        return model_inputs

    @staticmethod
    def _reorder_cache(past_key_values, beam_idx):
        reordered_past = ()
        for layer_past in past_key_values:
            reordered_past += (
                tuple(
                    past_state.index_select(0, beam_idx.to(past_state.device))
                    for past_state in layer_past
                ),
            )
        return reordered_past


@add_start_docstrings(
    """
    The LLaMa Model transformer with a sequence classification head on top (linear layer).

    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do.

    Since it does classification on the last token, it requires to know the position of the last token. If a
    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
    each row of the batch).
    """,
    LLAMA_START_DOCSTRING,
)
class LlamaForSequenceClassification(LlamaPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.model = LlamaModel(config)
        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        transformer_outputs = self.model(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        hidden_states = transformer_outputs[0]
        logits = self.score(hidden_states)

        if input_ids is not None:
            batch_size = input_ids.shape[0]
        else:
            batch_size = inputs_embeds.shape[0]

        if self.config.pad_token_id is None and batch_size != 1:
            raise ValueError(
                "Cannot handle batch sizes > 1 if no padding token is defined."
            )
        if self.config.pad_token_id is None:
            sequence_lengths = -1
        else:
            if input_ids is not None:
                sequence_lengths = (
                    torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1
                ).to(logits.device)
            else:
                sequence_lengths = -1

        pooled_logits = logits[
            torch.arange(batch_size, device=logits.device), sequence_lengths
        ]

        loss = None
        if labels is not None:
            labels = labels.to(logits.device)
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (
                    labels.dtype == torch.long or labels.dtype == torch.int
                ):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "regression":
                loss_fct = MSELoss()
                if self.num_labels == 1:
                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())
                else:
                    loss = loss_fct(pooled_logits, labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(
                    pooled_logits.view(-1, self.num_labels), labels.view(-1)
                )
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(pooled_logits, labels)
        if not return_dict:
            output = (pooled_logits,) + transformer_outputs[1:]
            return ((loss,) + output) if loss is not None else output

        return SequenceClassifierOutputWithPast(
            loss=loss,
            logits=pooled_logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
        )



================================================
FILE: examples/web/__init__.py
================================================



================================================
FILE: examples/web/ex.py
================================================
ex = [
    [
        "å››å·ç¾é£Ÿç¡®å®ä»¥è¾£é—»åï¼Œä½†ä¹Ÿæœ‰ä¸è¾£çš„é€‰æ‹©ã€‚æ¯”å¦‚ç”œæ°´é¢ã€èµ–æ±¤åœ†ã€è›‹çƒ˜ç³•ã€å¶å„¿ç²‘ç­‰ï¼Œè¿™äº›å°åƒå£å‘³æ¸©å’Œï¼Œç”œè€Œä¸è…»ï¼Œä¹Ÿå¾ˆå—æ¬¢è¿ã€‚",
        0.3,
        0.7,
        20,
        2,
        42,
        True,
    ],
    [
        "What is your favorite english food?",
        0.5,
        0.5,
        10,
        245,
        531,
        True,
    ],
    [
        "chat T T S is a text to speech model designed for dialogue applications. [uv_break]it supports mixed language input [uv_break]and offers multi speaker capabilities with precise control over prosodic elements like [uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation. [uv_break]it delivers natural and expressive speech,[uv_break]so please[uv_break] use the project responsibly at your own risk.[uv_break]",
        0.8,
        0.4,
        7,
        70,
        165,
        False,
    ],
]



================================================
FILE: examples/web/funcs.py
================================================
import random
from typing import Optional
from time import sleep

import gradio as gr

import sys

sys.path.append("..")
sys.path.append("../..")
from tools.audio import float_to_int16, has_ffmpeg_installed, load_audio
from tools.logger import get_logger

logger = get_logger(" WebUI ")

from tools.seeder import TorchSeedContext
from tools.normalizer import normalizer_en_nemo_text, normalizer_zh_tn

import ChatTTS

chat = ChatTTS.Chat(get_logger("ChatTTS"))

custom_path: Optional[str] = None

has_interrupted = False
is_in_generate = False

seed_min = 1
seed_max = 4294967295

use_mp3 = has_ffmpeg_installed()
if not use_mp3:
    logger.warning("no ffmpeg installed, use wav file output")

# éŸ³è‰²é€‰é¡¹ï¼šç”¨äºé¢„ç½®åˆé€‚çš„éŸ³è‰²
voices = {
    "Default": {"seed": 2},
    "Timbre1": {"seed": 1111},
    "Timbre2": {"seed": 2222},
    "Timbre3": {"seed": 3333},
    "Timbre4": {"seed": 4444},
    "Timbre5": {"seed": 5555},
    "Timbre6": {"seed": 6666},
    "Timbre7": {"seed": 7777},
    "Timbre8": {"seed": 8888},
    "Timbre9": {"seed": 9999},
}


def generate_seed():
    return gr.update(value=random.randint(seed_min, seed_max))


# è¿”å›é€‰æ‹©éŸ³è‰²å¯¹åº”çš„seed
def on_voice_change(vocie_selection):
    return voices.get(vocie_selection)["seed"]


def on_audio_seed_change(audio_seed_input):
    with TorchSeedContext(audio_seed_input):
        rand_spk = chat.sample_random_speaker()
    return rand_spk


def load_chat(cust_path: Optional[str], coef: Optional[str]) -> bool:
    if cust_path == None:
        ret = chat.load(coef=coef)
    else:
        logger.info("local model path: %s", cust_path)
        ret = chat.load("custom", custom_path=cust_path, coef=coef)
        global custom_path
        custom_path = cust_path
    if ret:
        try:
            chat.normalizer.register("en", normalizer_en_nemo_text())
        except ValueError as e:
            logger.error(e)
        except:
            logger.warning("Package nemo_text_processing not found!")
            logger.warning(
                "Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing",
            )
        try:
            chat.normalizer.register("zh", normalizer_zh_tn())
        except ValueError as e:
            logger.error(e)
        except:
            logger.warning("Package WeTextProcessing not found!")
            logger.warning(
                "Run: conda install -c conda-forge pynini=2.1.5 && pip install WeTextProcessing",
            )
    return ret


def reload_chat(coef: Optional[str]) -> str:
    global is_in_generate

    if is_in_generate:
        gr.Warning("Cannot reload when generating!")
        return coef

    chat.unload()
    gr.Info("Model unloaded.")
    if len(coef) != 230:
        gr.Warning("Ingore invalid DVAE coefficient.")
        coef = None
    try:
        global custom_path
        ret = load_chat(custom_path, coef)
    except Exception as e:
        raise gr.Error(str(e))
    if not ret:
        raise gr.Error("Unable to load model.")
    gr.Info("Reload succeess.")
    return chat.coef


def on_upload_sample_audio(sample_audio_input: Optional[str]) -> str:
    if sample_audio_input is None:
        return ""
    sample_audio = load_audio(sample_audio_input, 24000)
    spk_smp = chat.sample_audio_speaker(sample_audio)
    del sample_audio
    return spk_smp


def _set_generate_buttons(generate_button, interrupt_button, is_reset=False):
    return gr.update(
        value=generate_button, visible=is_reset, interactive=is_reset
    ), gr.update(value=interrupt_button, visible=not is_reset, interactive=not is_reset)


def refine_text(
    text,
    text_seed_input,
    refine_text_flag,
    temperature,
    top_P,
    top_K,
    split_batch,
):
    global chat

    if not refine_text_flag:
        sleep(1)  # to skip fast answer of loading mark
        return text

    text = chat.infer(
        text,
        skip_refine_text=False,
        refine_text_only=True,
        params_refine_text=ChatTTS.Chat.RefineTextParams(
            temperature=temperature,
            top_P=top_P,
            top_K=top_K,
            manual_seed=text_seed_input,
        ),
        split_text=split_batch > 0,
    )

    return text[0] if isinstance(text, list) else text


def generate_audio(
    text,
    temperature,
    top_P,
    top_K,
    spk_emb_text: str,
    stream,
    audio_seed_input,
    sample_text_input,
    sample_audio_code_input,
    split_batch,
):
    global chat, has_interrupted

    if not text or has_interrupted or not spk_emb_text.startswith("è˜æ·°"):
        return None

    params_infer_code = ChatTTS.Chat.InferCodeParams(
        spk_emb=spk_emb_text,
        temperature=temperature,
        top_P=top_P,
        top_K=top_K,
        manual_seed=audio_seed_input,
    )

    if sample_text_input and sample_audio_code_input:
        params_infer_code.txt_smp = sample_text_input
        params_infer_code.spk_smp = sample_audio_code_input
        params_infer_code.spk_emb = None

    wav = chat.infer(
        text,
        skip_refine_text=True,
        params_infer_code=params_infer_code,
        stream=stream,
        split_text=split_batch > 0,
        max_split_batch=split_batch,
    )
    if stream:
        for gen in wav:
            audio = gen[0]
            if audio is not None and len(audio) > 0:
                yield 24000, float_to_int16(audio).T
            del audio
    else:
        yield 24000, float_to_int16(wav[0]).T


def interrupt_generate():
    global chat, has_interrupted

    has_interrupted = True
    chat.interrupt()


def set_buttons_before_generate(generate_button, interrupt_button):
    global has_interrupted, is_in_generate

    has_interrupted = False
    is_in_generate = True

    return _set_generate_buttons(
        generate_button,
        interrupt_button,
    )


def set_buttons_after_generate(generate_button, interrupt_button, audio_output):
    global has_interrupted, is_in_generate

    is_in_generate = False

    return _set_generate_buttons(
        generate_button,
        interrupt_button,
        audio_output is not None or has_interrupted,
    )



================================================
FILE: examples/web/webui.py
================================================
import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

import argparse

import gradio as gr

from funcs import *
from ex import ex


def main():

    with gr.Blocks() as demo:
        gr.Markdown("# ChatTTS WebUI")
        gr.Markdown("- **GitHub Repo**: https://github.com/2noise/ChatTTS")
        gr.Markdown("- **HuggingFace Repo**: https://huggingface.co/2Noise/ChatTTS")

        with gr.Row():
            with gr.Column(scale=2):
                text_input = gr.Textbox(
                    label="Input Text",
                    lines=4,
                    max_lines=4,
                    placeholder="Please Input Text...",
                    value=ex[0][0],
                    interactive=True,
                )
                sample_text_input = gr.Textbox(
                    label="Sample Text",
                    lines=4,
                    max_lines=4,
                    placeholder="If Sample Audio and Sample Text are available, the Speaker Embedding will be disabled.",
                    interactive=True,
                )
            with gr.Column():
                with gr.Tab(label="Sample Audio"):
                    sample_audio_input = gr.Audio(
                        value=None,
                        type="filepath",
                        interactive=True,
                        show_label=False,
                        waveform_options=gr.WaveformOptions(
                            sample_rate=24000,
                        ),
                        scale=1,
                    )
                with gr.Tab(label="Sample Audio Code"):
                    sample_audio_code_input = gr.Textbox(
                        lines=12,
                        max_lines=12,
                        show_label=False,
                        placeholder="Paste the Code copied before after uploading Sample Audio.",
                        interactive=True,
                    )

        with gr.Row():
            refine_text_checkbox = gr.Checkbox(
                label="Refine text", value=ex[0][6], interactive=True
            )
            temperature_slider = gr.Slider(
                minimum=0.00001,
                maximum=1.0,
                step=0.00001,
                value=ex[0][1],
                label="Audio Temperature",
                interactive=True,
            )
            top_p_slider = gr.Slider(
                minimum=0.1,
                maximum=0.9,
                step=0.05,
                value=ex[0][2],
                label="top_P",
                interactive=True,
            )
            top_k_slider = gr.Slider(
                minimum=1,
                maximum=20,
                step=1,
                value=ex[0][3],
                label="top_K",
                interactive=True,
            )

        with gr.Row():
            voice_selection = gr.Dropdown(
                label="Timbre",
                choices=voices.keys(),
                value="Default",
                interactive=True,
            )
            audio_seed_input = gr.Number(
                value=ex[0][4],
                label="Audio Seed",
                interactive=True,
                minimum=seed_min,
                maximum=seed_max,
            )
            generate_audio_seed = gr.Button("\U0001f3b2", interactive=True)
            text_seed_input = gr.Number(
                value=ex[0][5],
                label="Text Seed",
                interactive=True,
                minimum=seed_min,
                maximum=seed_max,
            )
            generate_text_seed = gr.Button("\U0001f3b2", interactive=True)

        with gr.Row():
            spk_emb_text = gr.Textbox(
                label="Speaker Embedding",
                max_lines=3,
                show_copy_button=True,
                interactive=True,
                scale=2,
            )
            dvae_coef_text = gr.Textbox(
                label="DVAE Coefficient",
                max_lines=3,
                show_copy_button=True,
                interactive=True,
                scale=2,
            )
            reload_chat_button = gr.Button("Reload", scale=1, interactive=True)

        with gr.Row():
            auto_play_checkbox = gr.Checkbox(
                label="Auto Play", value=False, scale=1, interactive=True
            )
            stream_mode_checkbox = gr.Checkbox(
                label="Stream Mode",
                value=False,
                scale=1,
                interactive=True,
            )
            split_batch_slider = gr.Slider(
                minimum=0,
                maximum=100,
                step=1,
                value=4,
                label="Split Batch",
                interactive=True,
            )
            generate_button = gr.Button(
                "Generate", scale=2, variant="primary", interactive=True
            )
            interrupt_button = gr.Button(
                "Interrupt",
                scale=2,
                variant="stop",
                visible=False,
                interactive=False,
            )

        text_output = gr.Textbox(
            label="Output Text",
            interactive=False,
            show_copy_button=True,
        )

        sample_audio_input.change(
            fn=on_upload_sample_audio,
            inputs=sample_audio_input,
            outputs=sample_audio_code_input,
        ).then(fn=lambda: gr.Info("Sampled Audio Code generated at another Tab."))

        # ä½¿ç”¨Gradioçš„å›è°ƒåŠŸèƒ½æ¥æ›´æ–°æ•°å€¼è¾“å…¥æ¡†
        voice_selection.change(
            fn=on_voice_change, inputs=voice_selection, outputs=audio_seed_input
        )

        generate_audio_seed.click(generate_seed, outputs=audio_seed_input)

        generate_text_seed.click(generate_seed, outputs=text_seed_input)

        audio_seed_input.change(
            on_audio_seed_change, inputs=audio_seed_input, outputs=spk_emb_text
        )

        reload_chat_button.click(
            reload_chat, inputs=dvae_coef_text, outputs=dvae_coef_text
        )

        interrupt_button.click(interrupt_generate)

        @gr.render(inputs=[auto_play_checkbox, stream_mode_checkbox])
        def make_audio(autoplay, stream):
            audio_output = gr.Audio(
                label="Output Audio",
                value=None,
                format="mp3" if use_mp3 and not stream else "wav",
                autoplay=autoplay,
                streaming=stream,
                interactive=False,
                show_label=True,
                waveform_options=gr.WaveformOptions(
                    sample_rate=24000,
                ),
            )
            generate_button.click(
                fn=set_buttons_before_generate,
                inputs=[generate_button, interrupt_button],
                outputs=[generate_button, interrupt_button],
            ).then(
                refine_text,
                inputs=[
                    text_input,
                    text_seed_input,
                    refine_text_checkbox,
                    temperature_slider,
                    top_p_slider,
                    top_k_slider,
                    split_batch_slider,
                ],
                outputs=text_output,
            ).then(
                generate_audio,
                inputs=[
                    text_output,
                    temperature_slider,
                    top_p_slider,
                    top_k_slider,
                    spk_emb_text,
                    stream_mode_checkbox,
                    audio_seed_input,
                    sample_text_input,
                    sample_audio_code_input,
                    split_batch_slider,
                ],
                outputs=audio_output,
            ).then(
                fn=set_buttons_after_generate,
                inputs=[generate_button, interrupt_button, audio_output],
                outputs=[generate_button, interrupt_button],
            )

        gr.Examples(
            examples=ex,
            inputs=[
                text_input,
                temperature_slider,
                top_p_slider,
                top_k_slider,
                audio_seed_input,
                text_seed_input,
                refine_text_checkbox,
            ],
        )

    parser = argparse.ArgumentParser(description="ChatTTS demo Launch")
    parser.add_argument(
        "--server_name", type=str, default="0.0.0.0", help="server name"
    )
    parser.add_argument("--server_port", type=int, default=8080, help="server port")
    parser.add_argument("--root_path", type=str, help="root path")
    parser.add_argument("--custom_path", type=str, help="custom model path")
    parser.add_argument("--coef", type=str, help="custom dvae coefficient")
    args = parser.parse_args()

    logger.info("loading ChatTTS model...")

    if load_chat(args.custom_path, args.coef):
        logger.info("Models loaded successfully.")
    else:
        logger.error("Models load failed.")
        sys.exit(1)

    spk_emb_text.value = on_audio_seed_change(audio_seed_input.value)
    dvae_coef_text.value = chat.coef

    demo.launch(
        server_name=args.server_name,
        server_port=args.server_port,
        root_path=args.root_path,
        inbrowser=True,
        show_api=False,
    )


if __name__ == "__main__":
    main()



================================================
FILE: tests/#511.py
================================================
import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

import logging

import ChatTTS

from tools.logger import get_logger

logger = get_logger("Test", lv=logging.WARN)

chat = ChatTTS.Chat(logger)
chat.load(compile=False, source="huggingface")  # Set to True for better performance

texts = [
    "çš„ è¯ è¯­ éŸ³ å¤ª çŸ­ äº† ä¼š é€  æˆ ç”Ÿ æˆ éŸ³ é¢‘ é”™ è¯¯ ï¼Œ è¿™ æ˜¯ å  ä½ å  ä½ ï¼Œ è€ å¤§ çˆ· è§‰ å¾— è½¦ å¤« çš„ æƒ³ æ³• å¾ˆ æœ‰ é“ ç† [uv_break]",
    "çš„ è¯ è¯„ åˆ† åª æ˜¯ è¡¡ é‡ éŸ³ è‰² çš„ ç¨³ å®š æ€§ ï¼Œ ä¸ ä»£ è¡¨ éŸ³ è‰² çš„ å¥½ å ï¼Œ å¯ ä»¥ æ ¹ æ® è‡ª å·± çš„ éœ€ æ±‚ é€‰ æ‹© [uv_break] åˆ é€‚ çš„ éŸ³ è‰²",
    "ç„¶ å ä¸¾ ä¸ª ç®€ å• çš„ ä¾‹ å­ ï¼Œ å¦‚ æœ ä¸€ ä¸ª [uv_break] æ²™ å“‘ ä¸” ç»“ å·´ çš„ éŸ³ è‰² ä¸€ ç›´ å¾ˆ ç¨³ å®š ï¼Œ é‚£ ä¹ˆ å®ƒ çš„ è¯„ åˆ† å°± ä¼š å¾ˆ é«˜ ã€‚",
    "è¯­ éŸ³ å¤ª çŸ­ äº† ä¼š é€  æˆ ç”Ÿ æˆ éŸ³ é¢‘ é”™ è¯¯ ï¼Œ è¿™ æ˜¯ å  ä½ [uv_break] å  ä½ ã€‚ æˆ‘ ä½¿ ç”¨ seed id å» ç”Ÿ æˆ éŸ³ é¢‘ ï¼Œ ä½† æ˜¯ ç”Ÿ æˆ çš„ éŸ³ é¢‘ ä¸ ç¨³ å®š",
    "åœ¨d id åª æ˜¯ ä¸€ ä¸ª å‚ è€ƒ id [uv_break] ä¸ åŒ çš„ ç¯ å¢ƒ ä¸‹ éŸ³ è‰² ä¸ ä¸€ å®š ä¸€ è‡´ ã€‚ è¿˜ æ˜¯ æ¨ è ä½¿ ç”¨ ã€‚ pt æ–‡ ä»¶ è½½ å…¥ éŸ³ è‰²",
    "çš„ è¯ è¯­ éŸ³ å¤ª çŸ­ äº† ä¼š é€  æˆ ç”Ÿ æˆ éŸ³ é¢‘ é”™ è¯¯ ï¼Œ è¿™ æ˜¯ å  ä½ å  ä½ ã€‚ éŸ³ è‰² æ ‡ çš„ ç”· å¥³ [uv_break] å‡† ç¡® å—",
    "ï¼Œ å½“ å‰ ç¬¬ ä¸€ æ‰¹ æµ‹ è¯• çš„ éŸ³ è‰² æœ‰ ä¸¤ åƒ æ¡ [uv_break] ï¼Œ æ ¹ æ® å£° çº¹ ç›¸ ä¼¼ æ€§ ç®€ å• æ‰“ æ ‡ ï¼Œ å‡† ç¡® åº¦ ä¸ é«˜ ï¼Œ ç‰¹ åˆ« æ˜¯ ç‰¹ å¾ ä¸€ é¡¹",
    "è¯­ éŸ³ å¤ª çŸ­ äº† ä¼š é€  æˆ ç”Ÿ æˆ éŸ³ é¢‘ é”™ è¯¯ ï¼Œ è¿™ æ˜¯ å  ä½ å  ä½ ã€‚ ä»… ä¾› å‚ è€ƒ ã€‚ å¦‚ æœ å¤§ å®¶ æœ‰ æ›´ å¥½ çš„ æ ‡ æ³¨ æ–¹ æ³• ï¼Œ æ¬¢ è¿ pr [uv_break] ã€‚",
]

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb=chat.sample_random_speaker(),
    temperature=0.3,
    top_P=0.005,
    top_K=1,
    show_tqdm=False,
)

fail = False

wavs = chat.infer(
    texts,
    skip_refine_text=True,
    split_text=False,
    params_infer_code=params_infer_code,
)

for k, wav in enumerate(wavs):
    if wav is None:
        logger.warning("index", k, "is None")
        fail = True

if fail:
    import sys

    sys.exit(1)



================================================
FILE: tests/#588.py
================================================
import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

import logging
import re

import ChatTTS

from tools.logger import get_logger

logger = get_logger("Test", lv=logging.WARN)

chat = ChatTTS.Chat(logger)
chat.load(compile=False, source="huggingface")  # Set to True for better performance

texts = [
    "æ€»ç»“ä¸€ä¸‹ï¼ŒAI Agentæ˜¯å¤§æ¨¡å‹åŠŸèƒ½çš„æ‰©å±•ï¼Œè®©AIæ›´æ¥è¿‘äºé€šç”¨äººå·¥æ™ºèƒ½ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„AGIã€‚",
    "ä½ çœŸæ˜¯å¤ªèªæ˜å•¦ã€‚",
]

fail = False

refined = chat.infer(
    texts,
    refine_text_only=True,
    stream=False,
    split_text=False,
    params_refine_text=ChatTTS.Chat.RefineTextParams(show_tqdm=False),
)

trimre = re.compile("\\[[\w_]+\\]")


def trim_tags(txt: str) -> str:
    global trimre
    return trimre.sub("", txt)


for i, t in enumerate(refined):
    if len(trim_tags(t)) > 4 * len(texts[i]):
        fail = True
        logger.warning("in: %s, out: %s", texts[i], t)

if fail:
    import sys

    sys.exit(1)



================================================
FILE: tests/#655.py
================================================
import os, sys

if sys.platform == "darwin":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

now_dir = os.getcwd()
sys.path.append(now_dir)

import logging

import torch

import ChatTTS

from tools.logger import get_logger
from tools.normalizer import normalizer_en_nemo_text

logger = get_logger("Test", lv=logging.WARN)

chat = ChatTTS.Chat(logger)
chat.load(compile=False, source="huggingface")  # Set to True for better performance
try:
    chat.normalizer.register("en", normalizer_en_nemo_text())
except:
    logger.warning("Package nemo_text_processing not found!")

rand_spk = chat.sample_random_speaker()


text = ["What is [uv_break]your favorite english food?[laugh][lbreak]"]

fail = False

refined_text = chat.infer(
    text,
    refine_text_only=True,
    params_refine_text=ChatTTS.Chat.RefineTextParams(
        prompt="[oral_2][laugh_0][break_6]",
        manual_seed=12345,
    ),
    split_text=False,
)
if (
    refined_text[0]
    != "what is [uv_break] your favorite english [uv_break] food [laugh] like [lbreak]"
):
    fail = True
    logger.warning("refined text is '%s'", refined_text[0])

params = ChatTTS.Chat.InferCodeParams(
    spk_emb=rand_spk,  # add sampled speaker
    temperature=0.3,  # using custom temperature
    top_P=0.7,  # top P decode
    top_K=20,  # top K decode
)
input_ids, attention_mask, text_mask = chat.tokenizer.encode(
    chat.speaker.decorate_code_prompts(
        text,
        params.prompt,
        params.txt_smp,
        params.spk_emb,
    ),
    chat.config.gpt.num_vq,
    prompt=(
        chat.speaker.decode_prompt(params.spk_smp)
        if params.spk_smp is not None
        else None
    ),
    device=chat.device_gpt,
)
with torch.inference_mode():
    start_idx, end_idx = 0, torch.zeros(
        input_ids.shape[0], device=input_ids.device, dtype=torch.long
    ).fill_(input_ids.shape[1])

    recoded_text = chat.tokenizer.decode(
        chat.gpt._prepare_generation_outputs(
            input_ids,
            start_idx,
            end_idx,
            [],
            [],
            True,
        ).ids
    )

if (
    recoded_text[0]
    != "[Stts] [spk_emb] [speed_5] what is [uv_break] your favorite english food? [laugh] [lbreak] [Ptts]"
):
    fail = True
    logger.warning("recoded text is '%s'", refined_text)

if fail:
    import sys

    sys.exit(1)



================================================
FILE: tests/testall.sh
================================================
#!/bin/sh

exitcode=0

for file in tests/*.py
do
    echo "Testing $file..."
    python "$file"
    if [ $? -ne 0 ]
    then
        echo "Error: $file exited with a non-zero status."
        exitcode=1
    fi
    echo "Test $file success"
done

exit $exitcode



================================================
FILE: tools/__init__.py
================================================



================================================
FILE: tools/audio/__init__.py
================================================
from .av import load_audio
from .pcm import pcm_arr_to_mp3_view
from .ffmpeg import has_ffmpeg_installed
from .np import float_to_int16



================================================
FILE: tools/audio/av.py
================================================
from io import BufferedWriter, BytesIO
from pathlib import Path
from typing import Dict, Tuple, Optional, Union, List

import av
from av.audio.frame import AudioFrame
from av.audio.resampler import AudioResampler
import numpy as np


video_format_dict: Dict[str, str] = {
    "m4a": "mp4",
}

audio_format_dict: Dict[str, str] = {
    "ogg": "libvorbis",
    "mp4": "aac",
}


def wav2(i: BytesIO, o: BufferedWriter, format: str):
    """
    https://github.com/fumiama/Retrieval-based-Voice-Conversion-WebUI/blob/412a9950a1e371a018c381d1bfb8579c4b0de329/infer/lib/audio.py#L20
    """
    inp = av.open(i, "r")
    format = video_format_dict.get(format, format)
    out = av.open(o, "w", format=format)
    format = audio_format_dict.get(format, format)

    ostream = out.add_stream(format)

    for frame in inp.decode(audio=0):
        for p in ostream.encode(frame):
            out.mux(p)

    for p in ostream.encode(None):
        out.mux(p)

    out.close()
    inp.close()


def load_audio(
    file: Union[str, BytesIO, Path],
    sr: Optional[int] = None,
    format: Optional[str] = None,
    mono=True,
) -> Union[np.ndarray, Tuple[np.ndarray, int]]:
    """
    https://github.com/fumiama/Retrieval-based-Voice-Conversion-WebUI/blob/412a9950a1e371a018c381d1bfb8579c4b0de329/infer/lib/audio.py#L39
    """
    if (isinstance(file, str) and not Path(file).exists()) or (
        isinstance(file, Path) and not file.exists()
    ):
        raise FileNotFoundError(f"File not found: {file}")
    rate = 0

    container = av.open(file, format=format)
    audio_stream = next(s for s in container.streams if s.type == "audio")
    channels = 1 if audio_stream.layout == "mono" else 2
    container.seek(0)
    resampler = (
        AudioResampler(format="fltp", layout=audio_stream.layout, rate=sr)
        if sr is not None
        else None
    )

    # Estimated maximum total number of samples to pre-allocate the array
    # AV stores length in microseconds by default
    estimated_total_samples = (
        int(container.duration * sr // 1_000_000) if sr is not None else 48000
    )
    decoded_audio = np.zeros(
        (
            estimated_total_samples + 1
            if channels == 1
            else (channels, estimated_total_samples + 1)
        ),
        dtype=np.float32,
    )

    offset = 0

    def process_packet(packet: List[AudioFrame]):
        frames_data = []
        rate = 0
        for frame in packet:
            # frame.pts = None  # æ¸…é™¤æ—¶é—´æˆ³ï¼Œé¿å…é‡æ–°é‡‡æ ·é—®é¢˜
            resampled_frames = (
                resampler.resample(frame) if resampler is not None else [frame]
            )
            for resampled_frame in resampled_frames:
                frame_data = resampled_frame.to_ndarray()
                rate = resampled_frame.rate
                frames_data.append(frame_data)
        return (rate, frames_data)

    def frame_iter(container):
        for p in container.demux(container.streams.audio[0]):
            yield p.decode()

    for r, frames_data in map(process_packet, frame_iter(container)):
        if not rate:
            rate = r
        for frame_data in frames_data:
            end_index = offset + len(frame_data[0])

            # æ£€æŸ¥ decoded_audio æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç©ºé—´ï¼Œå¹¶åœ¨å¿…è¦æ—¶è°ƒæ•´å¤§å°
            if end_index > decoded_audio.shape[1]:
                decoded_audio = np.resize(
                    decoded_audio, (decoded_audio.shape[0], end_index * 4)
                )

            np.copyto(decoded_audio[..., offset:end_index], frame_data)
            offset += len(frame_data[0])

    container.close()

    # Truncate the array to the actual size
    decoded_audio = decoded_audio[..., :offset]

    if mono and decoded_audio.shape[0] > 1:
        decoded_audio = decoded_audio.mean(0)

    if sr is not None:
        return decoded_audio
    return decoded_audio, rate



================================================
FILE: tools/audio/ffmpeg.py
================================================
from pydub.utils import which


def has_ffmpeg_installed() -> bool:
    return which("ffmpeg") and which("ffprobe")



================================================
FILE: tools/audio/np.py
================================================
import math

import numpy as np
from numba import jit


@jit(nopython=True)
def float_to_int16(audio: np.ndarray) -> np.ndarray:
    am = int(math.ceil(float(np.abs(audio).max())) * 32768)
    am = 32767 * 32768 // am
    return np.multiply(audio, am).astype(np.int16)



================================================
FILE: tools/audio/pcm.py
================================================
import wave
from io import BytesIO

import numpy as np

from .np import float_to_int16
from .av import wav2


def pcm_arr_to_mp3_view(wav: np.ndarray):
    buf = BytesIO()
    with wave.open(buf, "wb") as wf:
        wf.setnchannels(1)  # Mono channel
        wf.setsampwidth(2)  # Sample width in bytes
        wf.setframerate(24000)  # Sample rate in Hz
        wf.writeframes(float_to_int16(wav))
    buf.seek(0, 0)
    buf2 = BytesIO()
    wav2(buf, buf2, "mp3")
    buf.seek(0, 0)
    return buf2.getbuffer()



================================================
FILE: tools/checksum/main.go
================================================
package main

import (
	"crypto/sha256"
	"encoding/hex"
	"fmt"
	"io"
	"os"
)

func main() {
	var buf [32]byte
	h := sha256.New()
	lst := make([]any, 0, 64)
	for _, fname := range files {
		f, err := os.Open(fname)
		if err != nil {
			panic(err)
		}
		_, err = io.Copy(h, f)
		if err != nil {
			panic(err)
		}
		s := hex.EncodeToString(h.Sum(buf[:0]))
		fmt.Println("sha256 of", fname, "=", s)
		lst = append(lst, s)
		h.Reset()
		f.Close()
	}
	f, err := os.Create("ChatTTS/res/sha256_map.json")
	if err != nil {
		panic(err)
	}
	_, err = fmt.Fprintf(f, jsontmpl, lst...)
	if err != nil {
		panic(err)
	}
}



================================================
FILE: tools/checksum/tmpl.go
================================================
package main

var files = [...]string{
	"asset/Decoder.safetensors",
	"asset/DVAE.safetensors",
	"asset/Embed.safetensors",
	"asset/Vocos.safetensors",

	"asset/gpt/config.json",
	"asset/gpt/model.safetensors",

	"asset/tokenizer/special_tokens_map.json",
	"asset/tokenizer/tokenizer_config.json",
	"asset/tokenizer/tokenizer.json",
}

const jsontmpl = `{
	"sha256_asset_Decoder_safetensors": "%s",
	"sha256_asset_DVAE_safetensors"   : "%s",
	"sha256_asset_Embed_safetensors"  : "%s",
	"sha256_asset_Vocos_safetensors"  : "%s",

	"sha256_asset_gpt_config_json"         : "%s",
	"sha256_asset_gpt_model_safetensors"   : "%s",

	"sha256_asset_tokenizer_special_tokens_map_json": "%s",
	"sha256_asset_tokenizer_tokenizer_config_json"  : "%s",
	"sha256_asset_tokenizer_tokenizer_json"         : "%s"
}
`



================================================
FILE: tools/llm/__init__.py
================================================
from .llm import ChatOpenAI



================================================
FILE: tools/llm/llm.py
================================================
from openai import OpenAI

prompt_dict = {
    "kimi": [
        {
            "role": "system",
            "content": "ä½ æ˜¯ Kimiï¼Œç”± Moonshot AI æä¾›çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ æ›´æ“…é•¿ä¸­æ–‡å’Œè‹±æ–‡çš„å¯¹è¯ã€‚",
        },
        {
            "role": "user",
            "content": "ä½ å¥½ï¼Œè¯·æ³¨æ„ä½ ç°åœ¨ç”Ÿæˆçš„æ–‡å­—è¦æŒ‰ç…§äººæ—¥å¸¸ç”Ÿæ´»çš„å£å»ï¼Œä½ çš„å›å¤å°†ä¼šåç»­ç”¨TTSæ¨¡å‹è½¬ä¸ºè¯­éŸ³ï¼Œå¹¶ä¸”è¯·æŠŠå›ç­”æ§åˆ¶åœ¨100å­—ä»¥å†…ã€‚å¹¶ä¸”æ ‡ç‚¹ç¬¦å·ä»…åŒ…å«é€—å·å’Œå¥å·ï¼Œå°†æ•°å­—ç­‰è½¬ä¸ºæ–‡å­—å›ç­”ã€‚",
        },
        {
            "role": "assistant",
            "content": "å¥½çš„ï¼Œæˆ‘ç°åœ¨ç”Ÿæˆçš„æ–‡å­—å°†æŒ‰ç…§äººæ—¥å¸¸ç”Ÿæ´»çš„å£å»ï¼Œ å¹¶ä¸”æˆ‘ä¼šæŠŠå›ç­”æ§åˆ¶åœ¨ä¸€ç™¾å­—ä»¥å†…, æ ‡ç‚¹ç¬¦å·ä»…åŒ…å«é€—å·å’Œå¥å·ï¼Œå°†é˜¿æ‹‰ä¼¯æ•°å­—ç­‰è½¬ä¸ºä¸­æ–‡æ–‡å­—å›ç­”ã€‚ä¸‹é¢è¯·å¼€å§‹å¯¹è¯ã€‚",
        },
    ],
    "deepseek": [
        {"role": "system", "content": "You are a helpful assistant"},
        {
            "role": "user",
            "content": "ä½ å¥½ï¼Œè¯·æ³¨æ„ä½ ç°åœ¨ç”Ÿæˆçš„æ–‡å­—è¦æŒ‰ç…§äººæ—¥å¸¸ç”Ÿæ´»çš„å£å»ï¼Œä½ çš„å›å¤å°†ä¼šåç»­ç”¨TTSæ¨¡å‹è½¬ä¸ºè¯­éŸ³ï¼Œå¹¶ä¸”è¯·æŠŠå›ç­”æ§åˆ¶åœ¨100å­—ä»¥å†…ã€‚å¹¶ä¸”æ ‡ç‚¹ç¬¦å·ä»…åŒ…å«é€—å·å’Œå¥å·ï¼Œå°†æ•°å­—ç­‰è½¬ä¸ºæ–‡å­—å›ç­”ã€‚",
        },
        {
            "role": "assistant",
            "content": "å¥½çš„ï¼Œæˆ‘ç°åœ¨ç”Ÿæˆçš„æ–‡å­—å°†æŒ‰ç…§äººæ—¥å¸¸ç”Ÿæ´»çš„å£å»ï¼Œ å¹¶ä¸”æˆ‘ä¼šæŠŠå›ç­”æ§åˆ¶åœ¨ä¸€ç™¾å­—ä»¥å†…, æ ‡ç‚¹ç¬¦å·ä»…åŒ…å«é€—å·å’Œå¥å·ï¼Œå°†é˜¿æ‹‰ä¼¯æ•°å­—ç­‰è½¬ä¸ºä¸­æ–‡æ–‡å­—å›ç­”ã€‚ä¸‹é¢è¯·å¼€å§‹å¯¹è¯ã€‚",
        },
    ],
    "deepseek_TN": [
        {"role": "system", "content": "You are a helpful assistant"},
        {
            "role": "user",
            "content": "ä½ å¥½ï¼Œç°åœ¨æˆ‘ä»¬åœ¨å¤„ç†TTSçš„æ–‡æœ¬è¾“å…¥ï¼Œä¸‹é¢å°†ä¼šç»™ä½ è¾“å…¥ä¸€æ®µæ–‡æœ¬ï¼Œè¯·ä½ å°†å…¶ä¸­çš„é˜¿æ‹‰ä¼¯æ•°å­—ç­‰ç­‰è½¬ä¸ºæ–‡å­—è¡¨è¾¾ï¼Œå¹¶ä¸”è¾“å‡ºçš„æ–‡æœ¬é‡Œä»…åŒ…å«é€—å·å’Œå¥å·è¿™ä¸¤ä¸ªæ ‡ç‚¹ç¬¦å·",
        },
        {
            "role": "assistant",
            "content": "å¥½çš„ï¼Œæˆ‘ç°åœ¨å¯¹TTSçš„æ–‡æœ¬è¾“å…¥è¿›è¡Œå¤„ç†ã€‚è¿™ä¸€èˆ¬å«åštext normalizationã€‚ä¸‹é¢è¯·è¾“å…¥",
        },
        {"role": "user", "content": "We paid $123 for this desk."},
        {
            "role": "assistant",
            "content": "We paid one hundred and twenty three dollars for this desk.",
        },
        {"role": "user", "content": "è¯¦è¯¢è¯·æ‹¨æ‰“010-724654"},
        {"role": "assistant", "content": "è¯¦è¯¢è¯·æ‹¨æ‰“é›¶å¹ºé›¶ï¼Œä¸ƒäºŒå››å…­äº”å››"},
        {"role": "user", "content": "ç½—æ£®å®£å¸ƒå°†äº7æœˆ24æ—¥é€€å¸‚ï¼Œåœ¨åé—¨åº—è¶…6000å®¶ï¼"},
        {
            "role": "assistant",
            "content": "ç½—æ£®å®£å¸ƒå°†äºä¸ƒæœˆäºŒåå››æ—¥é€€å¸‚ï¼Œåœ¨åé—¨åº—è¶…è¿‡å…­åƒå®¶ã€‚",
        },
    ],
}


class ChatOpenAI:
    def __init__(self, api_key, base_url, model):
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )
        self.model = model

    def call(self, user_question, temperature=0.3, prompt_version="kimi", **kwargs):

        completion = self.client.chat.completions.create(
            model=self.model,
            messages=prompt_dict[prompt_version]
            + [
                {"role": "user", "content": user_question},
            ],
            temperature=temperature,
            **kwargs
        )
        return completion.choices[0].message.content



================================================
FILE: tools/logger/__init__.py
================================================
from .log import get_logger



================================================
FILE: tools/logger/log.py
================================================
import platform, sys
import logging
from datetime import datetime, timezone

logging.getLogger("numba").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("wetext-zh_normalizer").setLevel(logging.WARNING)
logging.getLogger("NeMo-text-processing").setLevel(logging.WARNING)

# from https://github.com/FloatTech/ZeroBot-Plugin/blob/c70766a989698452e60e5e48fb2f802a2444330d/console/console_windows.go#L89-L96
colorCodePanic = "\x1b[1;31m"
colorCodeFatal = "\x1b[1;31m"
colorCodeError = "\x1b[31m"
colorCodeWarn = "\x1b[33m"
colorCodeInfo = "\x1b[37m"
colorCodeDebug = "\x1b[32m"
colorCodeTrace = "\x1b[36m"
colorReset = "\x1b[0m"

log_level_color_code = {
    logging.DEBUG: colorCodeDebug,
    logging.INFO: colorCodeInfo,
    logging.WARN: colorCodeWarn,
    logging.ERROR: colorCodeError,
    logging.FATAL: colorCodeFatal,
}

log_level_msg_str = {
    logging.DEBUG: "DEBU",
    logging.INFO: "INFO",
    logging.WARN: "WARN",
    logging.ERROR: "ERRO",
    logging.FATAL: "FATL",
}


class Formatter(logging.Formatter):
    def __init__(self, color=platform.system().lower() != "windows"):
        # https://stackoverflow.com/questions/2720319/python-figure-out-local-timezone
        self.tz = datetime.now(timezone.utc).astimezone().tzinfo
        self.color = color

    def format(self, record: logging.LogRecord):
        logstr = "[" + datetime.now(self.tz).strftime("%z %Y%m%d %H:%M:%S") + "] ["
        if self.color:
            logstr += log_level_color_code.get(record.levelno, colorCodeInfo)
        logstr += log_level_msg_str.get(record.levelno, record.levelname)
        if self.color:
            logstr += colorReset
        if sys.version_info >= (3, 9):
            fn = record.filename.removesuffix(".py")
        elif record.filename.endswith(".py"):
            fn = record.filename[:-3]
        logstr += f"] {str(record.name)} | {fn} | {str(record.msg)%record.args}"
        return logstr


def get_logger(name: str, lv=logging.INFO, remove_exist=False, format_root=False):
    logger = logging.getLogger(name)
    logger.setLevel(lv)
    if remove_exist and logger.hasHandlers():
        logger.handlers.clear()
    if not logger.hasHandlers():
        syslog = logging.StreamHandler()
        syslog.setFormatter(Formatter())
        logger.addHandler(syslog)
    else:
        for h in logger.handlers:
            h.setFormatter(Formatter())
    if format_root:
        for h in logger.root.handlers:
            h.setFormatter(Formatter())
    return logger



================================================
FILE: tools/normalizer/__init__.py
================================================
from .en import normalizer_en_nemo_text
from .zh import normalizer_zh_tn



================================================
FILE: tools/normalizer/en.py
================================================
from typing import Callable
from functools import partial


def normalizer_en_nemo_text() -> Callable[[str], str]:
    from nemo_text_processing.text_normalization.normalize import Normalizer

    return partial(
        Normalizer(input_case="cased", lang="en").normalize,
        verbose=False,
        punct_post_process=True,
    )



================================================
FILE: tools/normalizer/zh.py
================================================
from typing import Callable


def normalizer_zh_tn() -> Callable[[str], str]:
    from tn.chinese.normalizer import Normalizer

    return Normalizer(remove_interjections=False).normalize



================================================
FILE: tools/seeder/__init__.py
================================================
from .ctx import TorchSeedContext



================================================
FILE: tools/seeder/ctx.py
================================================
import torch


class TorchSeedContext:
    def __init__(self, seed):
        self.seed = seed
        self.state = None

    def __enter__(self):
        self.state = torch.random.get_rng_state()
        torch.manual_seed(self.seed)

    def __exit__(self, type, value, traceback):
        torch.random.set_rng_state(self.state)



================================================
FILE: .github/workflows/checksum.yml
================================================
name: Calculate and Sync SHA256
on:
  workflow_dispatch:

jobs:
  checksum:
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@v4

      - name: Setup Go Environment
        uses: actions/setup-go@v5

      - name: Run RVC-Models-Downloader
        run: |
          wget https://github.com/fumiama/RVC-Models-Downloader/releases/download/v0.2.10/rvcmd_linux_amd64.deb
          sudo apt -y install ./rvcmd_linux_amd64.deb
          rm -f ./rvcmd_linux_amd64.deb
          rvcmd -notrs -w 1 -notui assets/chtts

      - name: Calculate all Checksums
        run: go run tools/checksum/*.go

      - name: Commit back
        if: ${{ !github.head_ref }}
        id: commitback
        continue-on-error: true
        run: |
          git config --local user.name 'github-actions[bot]'
          git config --local user.email 'github-actions[bot]@users.noreply.github.com'
          git add --all
          git commit -m "chore(env): sync checksum on ${{github.ref_name}}"

      - name: Create Pull Request
        if: steps.commitback.outcome == 'success'
        continue-on-error: true
        uses: peter-evans/create-pull-request@v5
        with:
          delete-branch: true
          body: "Automatically sync checksum in .env"
          title: "chore(env): sync checksum on ${{github.ref_name}}"
          commit-message: "chore(env): sync checksum on ${{github.ref_name}}"
          branch: checksum-${{github.ref_name}}



================================================
FILE: .github/workflows/close-issue.yml
================================================
name: Close Inactive Issues
on:
  schedule:
    - cron: "0 4 * * *"

jobs:
  close-issues:
    runs-on: ubuntu-24.04
    permissions:
      issues: write
      pull-requests: write
    steps:
      - uses: actions/stale@v5
        with:
          exempt-issue-labels: "help wanted,following up,todo list,enhancement,algorithm,delayed,performance"
          days-before-issue-stale: 30
          days-before-issue-close: 15
          stale-issue-label: "stale"
          close-issue-message: "This issue was closed because it has been inactive for 15 days since being marked as stale."
          days-before-pr-stale: -1
          days-before-pr-close: -1
          operations-per-run: 10000
          repo-token: ${{ secrets.GITHUB_TOKEN }}



================================================
FILE: .github/workflows/pull-format.yml
================================================
name: Check Pull Request Format

on:
  pull_request_target:
    types: [opened, reopened, synchronize]

jobs:
  # This workflow closes invalid PR
  change-or-close-pr:
    # The type of runner that the job will run on
    runs-on: ubuntu-24.04
    permissions: write-all

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      - name: Change Base Branch
        if: github.event.pull_request.base.ref != 'dev'
        uses: actions/github-script@v4
        id: change-base
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const { owner, repo, number } = context.issue;
            const newBase = 'dev';
            try {
              const result = await github.pulls.update({
                owner,
                repo,
                pull_number: number,
                base: newBase
              });
              console.log(result);
              return 'success';
            } catch (error) {
              console.log(error);
              return 'failed';
            }

      - name: Close PR if it is not pointed to dev Branch
        if: "github.event.pull_request.base.ref != 'dev' && steps.change-base.outputs.result == 'failed'"
        uses: superbrothers/close-pull-request@v3
        with:
          # Optional. Post a issue comment just before closing a pull request.
          comment: "Invalid PR to `non-dev` branch `${{ github.event.pull_request.base.ref }}`."

  pull-format:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    continue-on-error: true

    steps:
      - name: Checkout Repo
        continue-on-error: true
        uses: actions/checkout@v4

      - name: Checkout PR # see https://github.com/orgs/community/discussions/24945
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh pr checkout ${{ github.event.pull_request.number }}

      - name: Set up Python
        uses: actions/setup-python@v5

      - name: Create venv
        run: python3 -m venv .venv

      - name: Activate venv
        run: |
          . .venv/bin/activate
          echo PATH=$PATH >> $GITHUB_ENV

      - name: Install Black
        run: pip install "black[jupyter]"

      - name: Run Black
        # run: black $(git ls-files '*.py')
        run: black .

      - name: Commit back
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true
        run: |
          git config --local user.name 'github-actions[bot]'
          git config --local user.email 'github-actions[bot]@users.noreply.github.com'
          git add --all
          git commit -m "chore(format): run black on ${{github.ref_name}}"
          git push



================================================
FILE: .github/workflows/push-format.yml
================================================
name: Standardize Code Format

on:
  push:
    branches:
      - main
      - dev

jobs:
  push-format:
    runs-on: ubuntu-latest

    if: "!contains(github.event.head_commit.message, 'chore(format): ') && !contains(github.event.head_commit.message, 'chore(env): ')"
  
    permissions:
      contents: write
      pull-requests: write

    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{github.ref_name}}

      - name: Set up Python
        uses: actions/setup-python@v5

      - name: Create venv
        run: python3 -m venv .venv

      - name: Activate venv
        run: |
          . .venv/bin/activate
          echo PATH=$PATH >> $GITHUB_ENV

      - name: Install Black
        run: pip install "black[jupyter]"

      - name: Run Black
        # run: black $(git ls-files '*.py')
        run: black .

      - name: Commit Back
        continue-on-error: true
        id: commitback
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add --all
          git commit -m "chore(format): run black on ${{github.ref_name}}"

      - name: Create Pull Request
        if: steps.commitback.outcome == 'success'
        continue-on-error: true
        uses: peter-evans/create-pull-request@v5
        with:
          delete-branch: true
          body: "Automatically apply code formatter change"
          title: "chore(format): run black on ${{github.ref_name}}"
          commit-message: "chore(format): run black on ${{github.ref_name}}"
          branch: formatter-${{github.ref_name}}



================================================
FILE: .github/workflows/unitest.yml
================================================
name: Unit Test
on: [ push, pull_request ]
jobs:
  build:
    runs-on: ${{ matrix.os }}

    if: "!contains(github.event.head_commit.message, 'chore(format): ') && !contains(github.event.head_commit.message, 'chore(env): ')"

    strategy:
      matrix:
        python-version: ["3.8", "3.9", "3.10"]
        os: [ubuntu-latest]
      fail-fast: true

    steps:

      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install Dependents
        run: |
          sudo apt-get install -y portaudio19-dev python3-pyaudio

      - name: Create venv
        run: python3 -m venv .venv

      - name: Activate venv
        run: |
          . .venv/bin/activate
          echo PATH=$PATH >> $GITHUB_ENV

      - name: Test Install
        run: pip install .

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: Run Test
        run: tests/testall.sh



================================================
FILE: .github/workflows/upload-pypi.yml
================================================
name: Upload to PyPI

on:
  push:
    tags:
      - 'v*'

jobs:
  build:
    runs-on: ubuntu-22.04

    steps:

      - uses: actions/checkout@v4
        with:
          ref: ${{github.ref_name}}

      - name: Set up Python
        uses: actions/setup-python@v5

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install --upgrade setuptools
          python -m pip install --upgrade wheel
          pip install twine

      - name: Build Package
        env:
          CHTTS_VER: ${{ github.ref_name }}
        run: |
          echo "Release Tag: ${{ github.ref_name }}"
          sed -i 's/v0.0.0/${{ github.ref_name }}/g' setup.py
          python setup.py sdist

      - name: Upload Package
        run: |
          twine upload dist/* -u "__token__" -p ${{ secrets.PYPI_TOKEN }}


