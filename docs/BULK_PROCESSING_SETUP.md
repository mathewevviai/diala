# Bulk Processing Development Setup Guide

## Overview

This guide walks through setting up the complete audio-to-RAG bulk processing system for development. It covers all dependencies, services, and configuration needed to run the full pipeline locally.

## Prerequisites

### System Requirements
- **Python**: 3.9+ (3.11+ recommended)
- **Node.js**: 18+ 
- **Redis**: 6.0+ (for session management)
- **FFmpeg**: Latest version (for audio processing)
- **Git**: For version control

### Hardware Recommendations
- **RAM**: 16GB+ (audio processing and embeddings are memory-intensive)
- **Storage**: 50GB+ free space (for temporary audio files)
- **CPU**: Multi-core processor (concurrent processing benefits from multiple cores)
- **Network**: Stable internet connection (for API calls and model downloads)

## Installation Steps

### 1. Clone and Setup Repository

```bash
# Clone repository
git clone <repository-url>
cd diala

# Check current branch and recent commits
git status
git log --oneline -5
```

### 2. Backend Setup

#### Python Environment
```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install additional audio processing dependencies
pip install demucs==4.0.0
pip install pyannote.audio==3.1.1
pip install whisper==1.1.10
```

#### Environment Configuration
Create `backend/.env` file:

```bash
# API Keys
JINA_API_KEY=your_jina_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# Convex Configuration
CONVEX_URL=http://127.0.0.1:3210
NEXT_PUBLIC_CONVEX_URL=http://127.0.0.1:3210
CONVEX_HTTP_URL=http://localhost:3211

# Redis Configuration
REDIS_URL=redis://localhost:6379

# Security (generate your own for production)
JWT_SECRET_KEY=your_secret_key_here
API_KEY=your_api_key_here

# Audio Processing Configuration
WHISPER_MODEL_SIZE=base
HUGGINGFACE_TOKEN=your_huggingface_token_here

# Bulk Processing Configuration
MAX_CONCURRENT_ITEMS=3
ENABLE_VOICE_SEPARATION=true
BULK_EXPORT_PATH=/tmp/bulk_exports

# Environment Mode
ENVIRONMENT=development
DEBUG_SEARCH=false

# Chatterbox Configuration (if using local TTS)
CHATTERBOX_MODE=local
CHATTERBOX_API_URL=http://localhost:8001
CHATTERBOX_PRELOAD_MODEL=false
```

#### Install System Dependencies

**Ubuntu/Debian**:
```bash
# FFmpeg for audio processing
sudo apt update
sudo apt install ffmpeg

# Redis server
sudo apt install redis-server
sudo systemctl start redis-server

# Additional audio processing libraries
sudo apt install libsndfile1 portaudio19-dev
```

**macOS**:
```bash
# Using Homebrew
brew install ffmpeg redis

# Start Redis
brew services start redis
```

**Windows**:
```bash
# Using Chocolatey
choco install ffmpeg redis-64

# Or download from official sites and add to PATH
```

### 3. Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Create environment file
cp .env.example .env.local
```

Edit `frontend/.env.local`:
```bash
# Convex Configuration (auto-generated by Convex CLI)
CONVEX_DEPLOYMENT=your_deployment
NEXT_PUBLIC_CONVEX_URL=http://127.0.0.1:3210

# Backend API
NEXT_PUBLIC_API_URL=http://localhost:8000
NEXT_PUBLIC_WS_URL=ws://localhost:8000
```

### 4. Convex Database Setup

```bash
cd frontend

# Install Convex CLI
npm install -g convex

# Login to Convex (or create account)
npx convex login

# Initialize Convex project (if not already done)
npx convex dev

# This will:
# - Set up your Convex deployment
# - Generate .env.local with CONVEX_DEPLOYMENT and NEXT_PUBLIC_CONVEX_URL
# - Start the Convex dev server on localhost:3210
```

Keep this terminal running for the Convex dev server.

## Service Dependencies

### Required External Services

#### 1. Jina AI API
```bash
# Sign up at https://jina.ai/
# Get API key from dashboard
# Add to backend/.env as JINA_API_KEY
```

#### 2. OpenAI API (for Whisper)
```bash
# Sign up at https://platform.openai.com/
# Create API key
# Add to backend/.env as OPENAI_API_KEY
```

#### 3. HuggingFace Token (for Speaker Diarization)
```bash
# Sign up at https://huggingface.co/
# Go to Settings > Access Tokens
# Create token with "Read" access
# Add to backend/.env as HUGGINGFACE_TOKEN
```

### Optional Services

#### Vector Database APIs (for testing exports)
```bash
# Pinecone
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENVIRONMENT=us-west1-gcp

# For Weaviate cloud
WEAVIATE_URL=https://your-cluster.weaviate.network
WEAVIATE_API_KEY=your_weaviate_key
```

## Starting the Development Environment

### 1. Start Core Services

```bash
# Terminal 1: Redis (if not auto-started)
redis-server

# Terminal 2: Convex Dev Server
cd frontend
npx convex dev
```

### 2. Start Backend

```bash
# Terminal 3: Backend API
cd backend
source venv/bin/activate
python -m src.main

# Should start on http://localhost:8000
```

### 3. Start Frontend

```bash
# Terminal 4: Frontend Development Server
cd frontend
npm run dev

# Should start on http://localhost:3000
```

### 4. Verify Setup

Visit `http://localhost:3000` and navigate to the bulk processing section. You should see:
- ✅ Platform selection (TikTok, YouTube, etc.)
- ✅ Content selection interface
- ✅ Embedding model options (Jina V4, Gemini)
- ✅ Vector database selection (Pinecone, ChromaDB, Weaviate)

## Development Workflow

### Running the Complete Pipeline

1. **Start a Bulk Processing Job**:
```bash
# Use the frontend UI or API directly
curl -X POST http://localhost:8000/api/public/bulk/process \
  -H "Content-Type: application/json" \
  -d '{
    "job_id": "test-job-123",
    "platform": "tiktok", 
    "input_method": "urls",
    "selected_content": ["7516961325537332502"],
    "embedding_model": {
      "id": "jina-v4",
      "jina_v4_task": "retrieval.passage",
      "jina_v4_late_chunking": true
    },
    "vector_db": {"id": "pinecone"},
    "settings": {"chunkSize": 1024, "chunkOverlap": 100}
  }'
```

2. **Monitor Progress**:
```bash
# Check job status
curl http://localhost:8000/api/public/bulk/job/test-job-123/status

# Or use WebSocket for real-time updates
wscat -c ws://localhost:8000/api/public/bulk/ws/bulk-processing/test-job-123
```

3. **Export Results**:
```bash
# Start export
curl -X POST http://localhost:8000/api/public/bulk/export \
  -H "Content-Type: application/json" \
  -d '{
    "job_id": "test-job-123",
    "format": "vector"
  }'

# Download when ready
curl -O http://localhost:8000/api/public/bulk/download/export-123
```

### Testing Individual Components

#### Audio Processing
```bash
cd backend
python test_audio_preparation.py
```

#### Embedding Generation
```bash
cd backend  
python test_enhanced_transcription.py
```

#### Vector Database Export
```bash
cd backend
python test_vector_database_service.py
```

#### Complete Integration
```bash
cd backend
python test_bulk_integration.py
```

## Configuration Options

### Audio Processing
```python
# In backend/src/services/bulk_processing_service.py
audio_config = {
    "enable_audio_processing": True,
    "enable_transcription": True, 
    "whisper_model": "base",        # tiny, base, small, medium, large
    "segment_audio": True,
    "clean_silence": True,
    "separate_voices": True,        # Requires Demucs
    "max_segment_duration": 30,
    "audio_format": "wav"
}
```

### Embedding Configuration
```python
# Jina V4 settings
jina_config = {
    "task": "retrieval.passage",   # Optimal for transcripts
    "dimensions": 1024,            # 128-2048
    "late_chunking": True,         # Better for long content
    "optimize_for_rag": True,      # RAG-specific optimization
    "multi_vector": False,         # Single vector per chunk
    "truncate_at_max": True        # Safe truncation
}
```

### Bulk Processing
```python
# Performance settings
bulk_config = {
    "max_concurrent_items": 3,     # Adjust based on RAM
    "max_items_per_batch": 25,     # Items per job
    "retry_attempts": 3,           # Retry failed items
    "timeout_seconds": 300,        # Per-item timeout
    "enable_progress_tracking": True
}
```

## Debugging and Troubleshooting

### Common Issues

#### 1. "ModuleNotFoundError: No module named 'src'"
```bash
# Ensure you're in the backend directory
cd backend
# And running with python -m
python -m src.main
```

#### 2. "Redis connection failed"
```bash
# Check Redis is running
redis-cli ping
# Should return "PONG"

# If not running:
sudo systemctl start redis-server  # Linux
brew services start redis          # macOS
```

#### 3. "Convex connection failed"
```bash
# Ensure Convex dev server is running
cd frontend
npx convex dev

# Check URL in .env.local matches Convex output
```

#### 4. "FFmpeg not found"
```bash
# Check FFmpeg installation
ffmpeg -version

# If not installed, install using system package manager
```

#### 5. Audio processing fails
```bash
# Check HuggingFace token
python -c "
from transformers import pipeline
pipe = pipeline('automatic-speech-recognition')
print('HuggingFace access OK')
"
```

### Debug Logging

Enable detailed logging:

```python
# In backend/src/main.py
import logging
logging.basicConfig(level=logging.DEBUG)

# Or set environment variable
export LOG_LEVEL=DEBUG
```

### Memory Monitoring

```bash
# Monitor memory usage during processing
top -p $(pgrep -f "python -m src.main")

# Or use htop for better visualization
htop
```

### Network Debugging

```bash
# Test API connectivity
curl -v http://localhost:8000/health

# Test WebSocket connection
wscat -c ws://localhost:8000/api/public/bulk/ws/test

# Check Convex connectivity
curl http://localhost:3210/
```

## Performance Optimization

### Development Settings

For faster development cycles:

```bash
# Use smaller Whisper model
WHISPER_MODEL_SIZE=tiny

# Reduce concurrent processing
MAX_CONCURRENT_ITEMS=1

# Disable voice separation (slower)
ENABLE_VOICE_SEPARATION=false

# Use shorter test content
# Select videos under 30 seconds for testing
```

### Memory Management

```python
# Monitor memory usage in Python
import psutil
import os

def check_memory():
    process = psutil.Process(os.getpid())
    memory_mb = process.memory_info().rss / 1024 / 1024
    print(f"Memory usage: {memory_mb:.1f} MB")
```

### Batch Size Tuning

```python
# Start with smaller batches for development
bulk_config = {
    "max_concurrent_items": 1,     # Single item at a time
    "max_items_per_batch": 5,      # Small batches
    "chunk_size": 512,             # Smaller chunks
}

# Increase for production
bulk_config = {
    "max_concurrent_items": 3,     # Concurrent processing
    "max_items_per_batch": 25,     # Larger batches
    "chunk_size": 1024,            # Optimal chunks
}
```

## Testing

### Unit Tests
```bash
cd backend

# Test individual services
python -m pytest tests/test_audio_preparation.py
python -m pytest tests/test_embedding_service.py
python -m pytest tests/test_vector_connectors.py
```

### Integration Tests
```bash
# Test complete pipeline
python test_bulk_integration.py

# Test specific workflows
python test_content_chunking.py
python test_enhanced_transcription.py
```

### Load Testing
```bash
# Test with multiple concurrent jobs
python test_concurrent_processing.py

# Test memory usage with large files
python test_memory_usage.py
```

## Production Deployment Considerations

### Environment Variables for Production
```bash
# Use production values
ENVIRONMENT=production
DEBUG_SEARCH=false

# Use larger instance sizes
MAX_CONCURRENT_ITEMS=5
WHISPER_MODEL_SIZE=large

# Production Redis/Convex URLs
REDIS_URL=redis://production-redis:6379
CONVEX_URL=https://your-production-convex.convex.cloud
```

### Resource Limits
```bash
# Set memory limits
ulimit -v 16777216  # 16GB virtual memory limit

# Set file descriptor limits
ulimit -n 65536     # More file handles for concurrent processing
```

### Monitoring Setup
```bash
# Add production monitoring
pip install prometheus-client
pip install grafana-api

# Enable metrics collection
ENABLE_METRICS=true
METRICS_PORT=9090
```

This setup guide provides everything needed to develop, test, and deploy the comprehensive audio-to-RAG bulk processing system. The modular architecture allows for incremental development and testing of individual components while supporting the complete end-to-end pipeline.