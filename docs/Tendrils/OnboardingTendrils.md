The following sections provide detailed onboarding tendril blueprints for each major module in the Diala dashboard. Each onboarding flow is designed as a self-contained, product-like experience that introduces the module’s capabilities, guides the user through setup, and seamlessly ties into Diala’s backend infrastructure. The design follows Diala’s neobrutalist aesthetic – bold typography, high-contrast color blocks, heavy borders, and slightly askew card layouts – ensuring a consistent look and feel across the app. Each blueprint covers the module’s Purpose, step-by-step Flow, key Frontend Components, Backend Architecture (Convex functions and external API calls), references to the Design System, State Management approach, UX Interaction details, Technical Implementation, system Integration Points, and potential Future Enhancements.
Agents Module Onboarding Tendril
Purpose
The Agents onboarding flow introduces new users to creating and configuring an AI Voice Agent – a virtual caller that can engage customers. It serves as a lead funnel by demonstrating how easily a custom voice agent can be set up and deployed. The tendril’s goal is to have the user create their first agent profile (with a persona, voice, and language) and understand the agent’s role in Diala. This guided setup connects to Diala’s core systems by actually saving the agent configuration to the backend and preparing it for use in calls. By the end, users see a preview of their agent in action (e.g. a simulated call greeting), which both educates and entices them to continue using the platform.
Flow Steps
Welcome & Agent Naming: The user is greeted with a welcome card on a distinctive background (e.g. vibrant blue, consistent with the voice agent theme
file-fjpkexbjp2dfhprtk7mhwq
). They’re prompted to enter their name or company name, which personalizes the experience (“Welcome, [Name]! Let’s set up your first Voice Agent.”). This step hooks the user and transitions into agent creation.
Define Agent Profile: The user inputs basic details for the agent: Agent Name (e.g. “Diala-Tone” or a custom name) and Purpose (a high-level role like Sales, Support, etc.). This might be presented on a tilted card with input fields (using the design system’s <Input> component for text
file-fjpkexbjp2dfhprtk7mhwq
). For guidance, the UI can show an example agent profile snippet (name, purpose, short description) in a stylized way. A Continue button (disabled until required fields are filled) moves to the next step.
Select Voice & Language: The user chooses the voice characteristics for the agent. This includes selecting a Language (with accent) and a Voice Style. For example, they might pick English (US) vs English (UK), and a voice persona like Friendly or Professional. The UI could display a grid of voice cards: each card showing a language flag and voice name. When a card is selected, it gets a bold black border and maybe a subtle pulse animation. In parallel, the user selects the Voice Provider or TTS engine if applicable (e.g. a default like ElevenLabs or an open-source voice). Both selectedLanguage and selectedVoiceAgent state must be chosen before continuing (the Next button becomes enabled only when both are set
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). This step leverages the neobrutalist style by showing each option as a chunky card or button with high contrast (e.g. a button with a background color representing language, and an icon like UilLanguage for languages
file-fjpkexbjp2dfhprtk7mhwq
). Audio previews can be offered – e.g. a small play icon to hear a sample of the voice – using a <Button size="icon"> with the UilPlay icon.
Configure Agent Behavior: The user configures the agent’s behavior and knowledge. This includes setting a Personality/Pitch and connecting any knowledge base. For Personality, the UI might offer presets like “Customer Support – Empathetic” or “Sales – Persuasive”, which map to different system prompts and tonalities. (In code, this was indicated by selectedPitch such as “customer-support”
file-fjpkexbjp2dfhprtk7mhwq
.) The design can use another grid of cards for tone/pitch options, each with an icon (e.g. UilCommentDots for conversational tone). The user also sees an option to attach a Retrieval-Augmented Generation Knowledge Base to the agent – for example, a toggle or button to link to RAG content. If they have no custom data yet, this section will be informational: “Your agent will use general knowledge and example scripts. (Integrate custom knowledge via AutoRAG later.)” If a knowledge base exists, they can select it here (the UI might list available RAG workflows or documents).
Summary & Launch: A summary screen appears on a rotated card that recaps the agent’s settings – e.g. “Agent: Diala-Tone\nLanguage: English (US)\nVoice Style: Professional\nPurpose: Sales & Discovery\nTone: Friendly” – styled in bold text with colored badges or icons next to each attribute (language flag, voice icon, etc.).
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The user confirms everything looks good. On confirmation, the agent profile is saved via a Convex function (e.g. createAgent mutation on the backend). Right after saving, the onboarding triggers a quick demo: the agent “calls” the user. For instance, a modal or next screen shows a Call Simulation – the agent’s avatar and name, a phone icon, and a transcript bubble where the agent says a greeting (“Hello, this is Diala-Tone, your new AI sales agent. Ready to make some calls!”). If audio is available, it plays using the chosen voice. This delightful finish demonstrates the agent in action and closes the loop on creation.
Components (Frontend)
WelcomeCard: Reused from existing onboarding components (like the one used in Voice and RAG flows) to capture the user’s name
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. It’s a branded card with input and a bold welcome message.
Card & CardContent: The design heavily uses the <Card> component with <CardHeader> and <CardContent> from the design system
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Each step’s UI is typically contained in one or more cards that may be rotated slightly (using utility classes like rotate-1 or -rotate-1 for a playful effect). For example, the profile form appears in a card with a slight tilt and thick black border.
Input Fields and Buttons: The agent profile form uses <Input> for text (name, purpose) and <Textarea> if a longer description is needed. These inputs use the neobrutalist style – e.g. tall input boxes with a 4px black border and bold placeholder text
file-fjpkexbjp2dfhprtk7mhwq
. Buttons throughout are high-contrast and bold: primary actions in bright colors (e.g. yellow or Diala blue) with uppercase labels and black text for contrast
file-fjpkexbjp2dfhprtk7mhwq
. Icons from Unicons (like UilPlus for add, UilArrowRight for continue) are placed inside buttons or headings to add visual cues.
Agent Preview Card: A custom component (could be similar to an AgentCard used in the dashboard
file-fjpkexbjp2dfhprtk7mhwq
) shows a mini profile of the configured agent. It might display the agent’s name, an avatar (perhaps a robot icon like UilRobot in a colored circle
file-fjpkexbjp2dfhprtk7mhwq
), and tags for language, voice, etc. This appears in the summary step to review settings.
OnboardingNav (Step Indicator): A step navigation UI at the bottom or top shows progress (e.g. “Step 3 of 5: Voice & Language”). This can be implemented by the <OnboardingNav> component similar to what the voice onboarding used
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. It displays steps as numbered circles or tabs, with completed ones filled in (using background color and a checkmark overlay)
file-fjpkexbjp2dfhprtk7mhwq
, and the current step highlighted (scale up and drop-shadow effect in CSS
file-fjpkexbjp2dfhprtk7mhwq
). Users can see how many steps remain and click on previous completed steps to review or edit (the nav allows backward navigation by calling onStepChange for earlier steps
file-fjpkexbjp2dfhprtk7mhwq
).
Modal/Dialog Components: If the final demo is shown, a modal overlay might be used – e.g. a <CallSimulationModal> (similar in structure to the provided CallAnalyticsModal or LiveCallMonitorModal used elsewhere
file-fjpkexbjp2dfhprtk7mhwq
) that appears with the agent on a “call.” Alternatively, the simulation can be inline on the final screen with a phone UI card.
Backend Architecture (Convex & APIs)
Convex Functions: When the user completes the agent setup, a Convex mutation (e.g. api.agent.createAgent) is invoked to save the agent profile to the database. This would include fields like name, description, language, voice settings, system prompt (derived from purpose/tone), etc., as seen in the voiceAgents data structure
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The Convex backend ensures the agent is persisted and accessible across the app (e.g. it will show up in the main Agents dashboard list and be used in calls).
AI Voice Services: If the onboarding demo includes generating an audio greeting, backend calls will be made to external APIs. Specifically, the system would use the chosen TTS (Text-to-Speech) provider to synthesize the agent’s greeting. For example, if ElevenLabs is configured (noted by an API key in settings
file-fjpkexbjp2dfhprtk7mhwq
), the app would send the greeting text and voice parameters to ElevenLabs API and stream back audio. Similarly, if the demo included an AI-crafted greeting text, an OpenAI API call could produce it (using the agent’s system prompt as context). For simplicity, the greeting might be pre-scripted, but a more dynamic approach could involve an OpenAI Chat completion to produce a personalized welcome line.
Convex Real-time updates: Although not critical in onboarding, if any step needed to fetch options (say available languages or voices), a Convex query could supply those. In our case, language and voice lists are mostly static, so they can be hardcoded or fetched from a static endpoint. However, linking to RAG sources might involve a Convex query to list available knowledge bases (RAG workflows stored in Convex) and a mutation to attach one to the agent profile.
Data Model: The new agent’s data structure includes performance stats and lastActive timestamps as placeholders
file-fjpkexbjp2dfhprtk7mhwq
 – these will initially be empty or default. The creation function may initialize some of these (e.g. set status: 'active' and zero counts). This integration ensures that immediately after onboarding, the user can navigate to the Agents dashboard and see their new agent listed with default metrics.
No Calls in Onboarding: Importantly, the “simulated call” at the end is not a real phone call but a local demo. Thus, no actual telephony API (like Telnyx) is invoked during onboarding (avoiding costs or complexities). The real Telnyx integration (for actual calls) happens when the user later uses the Calls module or playground. The onboarding just hints at that by using a fake call interface.
Analytics Hook: Optionally, the onboarding could log an event via Convex or an analytics service indicating a new agent was created (useful for the app to track conversion from onboarding to actual usage).
Design System & Neobrutalist Elements
The Agents onboarding UI adheres to the same design principles found elsewhere in Diala’s app for consistency and brand identity:
Typography: All titles and labels use the Noyh Bold font (imported at app level
file-fjpkexbjp2dfhprtk7mhwq
) in uppercase with heavy weight
file-fjpkexbjp2dfhprtk7mhwq
. For example, step titles like “Voice & Language” or the final “Agent Ready” are rendered in large, black uppercase text for emphasis. Supporting text uses bold sans-serif in slightly smaller size (often gray for secondary text).
Color Blocks: Each step likely features a signature background color overlay. Since Voice Agent is core to Diala, a blue theme (RGB 0,82,255) was used in the main onboarding menu
file-fjpkexbjp2dfhprtk7mhwq
 and could carry into this flow. We’ll use a blue backdrop with a subtle grid pattern (achieved by CSS gradients as seen in other onboardings
file-fjpkexbjp2dfhprtk7mhwq
). Cards themselves use white or light-gray backgrounds, with splashes of color for icons or badges (e.g. yellow or purple badges for language/purpose). All colors are high-saturation and flat, aligning with neobrutalism.
Borders & Shadows: All interactive elements (cards, buttons, modals) have thick black borders (4px) and offset shadows. For instance, cards use border-4 border-black with a drop shadow that mimics a hand-drawn offset (e.g. shadow-[6px_6px_0_rgba(0,0,0,1)] on hover to deepen the effect
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). This gives a slightly 3D, layered look. Inactive states might use lighter shadows (4px offset) and hover states increase to 6px or 8px, as consistently done in the app
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
.
Playful Layout: Many cards are deliberately rotated a few degrees (transform rotate-1 or -rotate-1)
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 to convey a creative, less regimented vibe. In this onboarding, perhaps the welcome card is rotated one way, the next form card rotates opposite, etc., alternating for each step. This matches the style in the transcripts onboarding where multiple cards have alternating rotations
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Despite the skew, alignment of form elements inside is maintained for usability.
Icons and Badges: Unicons icons are used generously to label sections (e.g. UilRobot for Agent info, UilMicrophone or UilVoice for voice selection, UilSetting or UilSlider for settings step)
file-fjpkexbjp2dfhprtk7mhwq
. Icons appear in solid color blocks with black outlines to form “neo-brutalist badges.” For example, a small square with a black border containing a yellow background and a black icon can precede a heading like “Select Language” – reinforcing meaning with color (yellow often used for knowledge/RAG, blue for voice, etc.). Badges in text (like showing agent status or language) are styled via the design system’s <Badge> component with custom classes for border and background
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
.
Overall, the design ensures the agent onboarding feels like part of the same family as the Voice, RAG, and Transcripts flows – bold, engaging, and a bit quirky – while clearly communicating the function of each UI element.
State Management
The onboarding component uses React useState hooks to manage form data and step transitions. Key state variables include:
currentStep (number): Tracks which step of the flow the user is on (initially 1). The component conditionally renders different content based on this
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The state is advanced via setCurrentStep(n) when the user clicks “Continue” on a step after validation.
Form fields like agentName, agentPurpose, selectedLanguage, selectedVoice, selectedTone: Strings (or IDs) captured from user input. For example, when the user types the agent’s name, the agentName state updates onChange. Selecting a language sets selectedLanguage (toggling logic ensures clicking the same option twice can deselect if needed
file-fjpkexbjp2dfhprtk7mhwq
). These states enable/disable the Next button for that step (e.g., require non-empty name, or require both language and voice selected as seen with combined condition !selectedAudio || !selectedLanguage in the Voice flow
file-fjpkexbjp2dfhprtk7mhwq
 – similarly, Agents flow will ensure required inputs are present).
showDemoModal (boolean): Controls whether the final call simulation modal is shown. Initially false, it flips to true once the agent is saved and the user triggers the demo. The modal component likely uses its own internal state for things like playback progress or closing.
Possibly isSaving or isLoading: A flag while the agent profile is being saved to the backend. Upon clicking “Finish” on the summary, isSaving can be set true and a loading spinner or a disabled state shown on the button to indicate processing. Once the Convex function returns success, isSaving goes false and we proceed to demo.
We may also maintain an object like formData that aggregates all agent fields, but given the few inputs, individual useState hooks are sufficient. Alternatively, a single useReducer or form library could manage the multi-step form data, but that might be overkill here.
The state is lifted within the onboarding component itself since it’s a dedicated flow. If needed, some global state (context or store) could provide user-specific info like the user’s organization or list of existing knowledge bases for RAG selection, but such data can also be fetched on the fly via Convex queries at the moment of needing them.
UX Interaction & Animations
This onboarding emphasizes a smooth, instructive user experience with interactive feedback at each step:
Progressive Disclosure: Only one step’s inputs are shown at a time, reducing cognitive load. The transition from one step to the next can be enhanced with a brief animation – e.g. the current card slides out or fades, and the next card slides in. The neobrutalist style can be extended to animations by making them slightly bouncy or offset (to match the playful theme).
Button States & Wobble: To encourage user action, the “Continue” button on each step might use the wobble animation (keyframes) once the step’s requirements are met. In the voice onboarding code, for example, the next-step button gained a wobble effect when prerequisites were selected
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We can apply similar here: after the user fills agent name and purpose, the “Next” button could wobble to draw attention. Likewise, on the voice selection step, once a language and voice are picked, the continue button pulses or shakes lightly
file-fjpkexbjp2dfhprtk7mhwq
. These animations use CSS (@keyframes) and inline styles toggled by state conditions.
Visual Feedback on Selection: Selecting a voice or language card provides immediate feedback: the card’s background may turn a brighter shade, an icon or border indicates selection (for instance, a checkmark or the border color changes from black to a highlight color). The code’s isSelected prop for language options shows a possible pattern
file-fjpkexbjp2dfhprtk7mhwq
. We’ll implement it such that clicking a card toggles its isSelected state and triggers a re-render with the new style (and maybe a short scale-up animation to emphasize activation).
Modals & Overlays: The final call demo uses a modal overlay that darkens the background (e.g. semi-transparent black overlay) and pops a centered card mimicking a phone call UI. The user can interact by pressing a “Play” button to hear the greeting again, or “End Demo” to close. This modal respects the design system (close button is a bold “X” icon, and the modal card has the thick border and drop shadow). It also ensures focus trapping (so that keyboard users are kept within the modal until it’s closed).
Error Handling: If any backend call fails (e.g. the agent save or the TTS fetch), the UI will show a brief error message – likely in a modal or banner. For instance, if saving fails, we might keep the user on summary step and show a red-outlined alert card saying “Error: Could not save agent. Please try again.” with a retry button. Errors during the demo (like audio failing to load) might be caught and shown as a notification (“Demo unavailable right now”). These edge cases ensure the user isn’t left confused if something goes wrong.
Skip Option: Although not always necessary, a “Skip Demo” or “Finish Setup” option can be provided for users who don’t want to go through the entire flow. This would likely appear on the summary screen. If clicked, it would still save the agent and then navigate the user to the main dashboard (Agents list) without showing the call simulation. It’s a small UX consideration to respect user’s time.
Throughout, the interactions aim to educate (by guiding input choices and showing outcomes) and delight (with fun visuals and a gratifying final preview of their AI agent coming to life).
Technical Implementation Details
Under the hood, the Agents onboarding is implemented as a React functional component (e.g. AgentsOnboarding.tsx) within the Next.js app, likely under the /onboarding/agents route. Key technical aspects include:
It is a client-side component ('use client' at top) because it manages interactive state and uses hooks, similar to other onboarding pages
file-fjpkexbjp2dfhprtk7mhwq
. This means it’s part of a Next.js page that doesn’t use Server-Side Rendering – appropriate since the content is highly dynamic and user-specific.
Routing & Isolation: This onboarding flow is separate from the main dashboard. It can be launched from a welcome screen (like the “Choose Your Path” menu
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). That screen routes to /onboarding/voice, /onboarding/rag, etc., and we would add a tile for Agents if not already present. Because it’s self-contained, users can experience it without having any agents; it essentially acts as a wizard that will populate the Agents module. It could also be re-used as a “Add New Agent” wizard if needed (triggered from the Agents dashboard via a “Create Agent” button
file-fjpkexbjp2dfhprtk7mhwq
).
Reusing Components: We leverage some existing components from the codebase to avoid reinventing the wheel. For example, the <CreateAgentModal> logic (if it exists) could be repurposed within the flow. In the provided code, there is create-agent-modal.tsx (likely a form for adding an agent)
file-fjpkexbjp2dfhprtk7mhwq
. Instead of using it as a modal, we can extract its form fields and validation logic for our multi-step process. Similarly, the <AgentCard> component (used to display agent info on the dashboard
file-fjpkexbjp2dfhprtk7mhwq
) can be used to render the preview in the summary. This reduces duplication and ensures consistency – the agent created in onboarding will look the same as those in the main app.
Convex Integration: We use the Convex React client, which is already set up in the app’s context (<Providers> component wraps the app with ConvexProvider
file-fjpkexbjp2dfhprtk7mhwq
). To call the backend, we use the useAction or useMutation hook from Convex. For example: const saveAgent = useAction(api.agents.create);. On the final step, await saveAgent(agentData) is called. The Convex function agents.create would handle inserting into the database. If Convex returns the new agent’s ID or object, we could use it to update local state or navigate to the agent’s detail page post-onboarding (for now, probably not needed during onboarding, but available).
Audio Playback: To implement the voice demo, the component may use the Web Audio API or simply an HTML5 <audio> element. The audio source would be the URL or binary returned from the TTS API call. A simpler approach: call a backend endpoint (Convex or Next.js API route) that returns an audio file URL after generating it. Then set that URL as the src of an audio element in the modal and call audio.play(). The UI includes controls for play/pause (maybe using a custom <Button> with play/pause icons UilPlay/UilPause for styling
file-fjpkexbjp2dfhprtk7mhwq
, but under the hood controlling the audio element). We also handle the cleanup – when the modal closes, we stop the audio and release the object URL if one was created.
Testing & Edge Cases: We ensure the multi-step form has validation at each step (e.g., required fields not empty, selections made). We also ensure that going “Back” to a previous step (if we allow it via the step indicator or a Back button) repopulates the form with the earlier inputs (since state is still retained). The component’s state persists as long as the user is on the onboarding route; if they refresh, they’d start over (which is acceptable). If needed, we could persist partial progress in localStorage or Convex to resume, but given the short flow, that’s optional.
Completion Path: Upon finishing, beyond showing the demo, we’ll likely programmatically navigate the user to the main app (maybe to the Agents dashboard page) when they exit the onboarding. This can be done with Next.js useRouter() to push a new route (e.g., router.push('/dashboard/agents')). We saw similar usage on the main onboarding selection page to route to the chosen path
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Here, after agent creation, a call like router.push('/dashboard/agents?new=1') could be used to both navigate and perhaps trigger a highlight on the new agent.
By engineering the onboarding in this way, we ensure it’s not just a dummy tutorial – it actually creates data and uses the real services, making the experience authentic. Yet it’s contained enough that a user can go through it without prior setup and end up with a tangible result (their custom AI agent ready to work).
Integration Points
The Agents onboarding ties into Diala’s broader infrastructure at several points to make the experience meaningful:
Agents Dashboard: The primary integration is that the created agent becomes part of the main Agents module. After onboarding, the new agent appears in the “Active Voice Agents” list on the dashboard, complete with the stats (mostly zeros or default) and status active
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The agent’s configuration (prompt, voice, etc.) is stored such that if the user opens the Agent Detail Modal later (accessible via the Agents page
file-fjpkexbjp2dfhprtk7mhwq
), they will see what they set up during onboarding. In this way, the onboarding acts as an alternate entry to the same agent creation process that an advanced user could do manually.
Calls Module: Any agent created is immediately available for use in calls and campaigns. For example, if the user proceeds to the Calls dashboard and starts a new call or campaign, they can select this agent to handle calls. The agent’s convexEntryPoint or identifier might be used by the call system to invoke the right logic (we see something like convexEntryPoint: 'agents.salesAlpha' in the calls data
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, implying each agent may have an identifier used in call workflows). Our created agent would have its unique entry point.
RAG (AutoRAG) Integration: If the user attached a knowledge base or if Diala automatically uses YouTube transcripts to train agents, that connection happens via the agent’s profile. The example agent data has a ragSources array listing modules or workflows
file-fjpkexbjp2dfhprtk7mhwq
. In onboarding, we might not fill this unless the user explicitly chose a knowledge base. But the structure is there – if later the user runs an AutoRAG workflow to build a knowledge base, they could link it to the agent (likely through an agent settings modal). Conversely, if this agent had been created by an advanced user, they might have chosen some default content to train on. The onboarding could behind the scenes flag that this new agent should train on some basic YouTube transcript set for their industry – for instance, if they chose Sales purpose, the system might automatically kick off a training job using a preset YouTube playlist for sales calls. (This is a speculative integration: to truly feed into backend workflows, an event could be emitted to start an AutoRAG job for the agent).
External Telephony: While no real call is made in onboarding, the agent is configured with everything needed to perform on actual calls. This means the conversation engine (OpenAI GPT model) and TTS/STT services can already work with it. When a real call comes in or out with this agent, the call pipeline will load the agent’s profile from the database (including voice settings and prompt) and spin up the necessary services (e.g. allocate a Deepgram transcription stream and an ElevenLabs voice for it). In short, the output of onboarding is not just UI fluff – it directly feeds the runtime behavior of the voice agent.
Design System & Theming: On a more front-end integration note, the onboarding uses the same design system components (cards, buttons, etc.), which means any updates to global styles (like a change in the Button component styling) will automatically reflect in the onboarding. For example, if the team updates the <Button> to a new hover effect, the onboarding’s buttons (being instances of that component) get the update too. This tight integration ensures consistency and reduces maintenance.
Analytics & User Onboarding Funnel: From a product perspective, this module ties into the user onboarding funnel. Likely, completion of the Agents onboarding is a key milestone (it shows the user has created an AI agent). This event might be tracked via an analytics integration (Segment, Mixpanel, etc.). If the platform has a “Setup Progress” checklist, the event of agent creation could check off an item like “✅ Create your first Voice Agent.” This is not a direct user-facing integration, but worth noting as part of the holistic experience.
Future Agent Enhancements: The agent created here will also benefit from any future system improvements. For instance, if later the platform adds an Agent Training feature (where the agent improves from call transcripts), all agents including this one would be part of that. The onboarding doesn’t need to handle that now, but it means the agent’s ID and records are ready to accumulate call data, training data, performance metrics (success rate, avg call time updated over time
file-fjpkexbjp2dfhprtk7mhwq
), etc., as the user uses it in real calls.
Future Enhancements
In future iterations of the Agents onboarding, several improvements and extensions could further increase its power and appeal:
Deeper Customization Steps: Additional steps could be introduced for fine-tuning the agent. For example, a step to upload a custom script or greeting for the agent (if the company wants a specific intro line), or to choose a persona image or avatar that will represent the agent in dashboards and calls. Currently, agents likely use default icons
file-fjpkexbjp2dfhprtk7mhwq
, but custom avatars (even just an emoji or generated icon) could make them more relatable.
Interactive Voice Tuning: We could let users test the voice during onboarding. A mini step where after selecting voice and language, the user can type a sentence and hear it spoken in the chosen voice. This try-and-adjust loop would utilize the TTS and allow them to toggle voice settings (pitch, speed) if the system supports it. It would showcase the realistic voice capabilities (one of Diala’s selling points is “realistic background sounds” and presumably natural speech
file-fjpkexbjp2dfhprtk7mhwq
). If background noise effects are available (as hinted by “background sounds”
file-fjpkexbjp2dfhprtk7mhwq
), the onboarding could allow choosing a background ambience for calls (e.g. office noise vs. quiet studio) to simulate realism – a fun feature for power users.
Agent Skill Configuration: Beyond purpose, let users pick “skills” or scenarios the agent is optimized for. For instance, checkboxes for Handling Objections, Appointment Setting, Demo Scheduling. These could correspond to different preset prompts or connect to specific RAG knowledge packs. In code, we see ragSources referencing things like "Objection Mindset" or "Price Objections" modules
file-fjpkexbjp2dfhprtk7mhwq
. A future onboarding step could list such modules for the chosen purpose and let the user toggle them. The agent would then automatically have those knowledge packs linked. This makes the agent immediately more powerful and tailored.
Integration with Contact Data: As Diala evolves, the onboarding might import some user-specific context. For example, if the user’s company has a set of FAQs or a product catalog, an advanced onboarding could prompt: “Want to train your agent on your own data? Upload a PDF or enter a website.” This would effectively initiate an AutoRAG process during agent creation. It blurs the line between the Agents and AutoRAG modules, but if done in a guided way, it could be extremely sticky – the user sees their agent learn their material in real-time. This might be a branching path: a simple agent creation vs. an advanced one with custom data ingestion.
Multi-Agent Onboarding: Down the line, if a user needs a team of agents (for different roles), the onboarding could allow creating multiple agents in one flow. For example, after creating one agent, it asks “Do you also need a support agent?” and could quickly replicate the process with slightly different defaults. However, this might be beyond the initial scope – typically one agent is enough to show value, and the Swarms module covers grouping multiple agents.
Guided Training Mode: Provide an option at the end of onboarding to “Practice a call with your agent.” This would switch to a Playground or test call mode where the user can speak or type and the agent responds, without involving actual phone lines. It’s like a training sandbox so the user gains confidence in the agent. This could reuse the Playground infrastructure in a focused way. It’s a logical next step after hearing the greeting: let the user try a sample conversation and give a thumbs-up/down if the agent’s response was good. Those signals could in the future fine-tune the agent (via reinforcement learning or by adjusting settings).
Onboarding Hints in Dashboard: Once the agent is created, subtle hints or tooltips in the main dashboard could appear (just-in-time education). For example, on the Agents page, highlight the “Active Voice Agents” card
file-fjpkexbjp2dfhprtk7mhwq
 and say “Here’s your new agent. Click to view details or edit.” On the Calls page, highlight how to start a call with that agent. This isn’t part of the onboarding UI itself, but a continuation of the user journey. Implementing this might involve setting a flag that the user completed agent onboarding and then showing a one-time tooltip in relevant places.
By implementing these future enhancements, the Agents onboarding would not only create an agent but also ensure the user fully understands and utilizes it, increasing the likelihood of Diala’s adoption in their workflow.
Swarms Module Onboarding Tendril
Purpose
The Swarms onboarding flow guides users through creating and managing Agent Swarms, which are groups of AI agents working in concert. In Diala, swarms represent powerful teams of voice agents deployed for coordinated campaigns (e.g. a sales “swarm” making calls in parallel, or a support “swarm” handling a surge of inquiries). This onboarding acts as a self-contained feature to set up a swarm from scratch, showcasing how grouping agents can amplify results. It serves as a lead funnel by illustrating a high-impact use case: rather than a single agent, imagine a “battalion” of agents – this drives home the scale Diala enables. The flow’s purpose is to get the user to create their first swarm (naming it, choosing its mission, and selecting member agents), while seamlessly plugging into backend systems by actually provisioning that swarm (persisting in the database and ready to be used in campaigns). In doing so, it also educates the user on the benefits of teamwork among AI agents (coordination, aggregated analytics, etc.).
Flow Steps
Welcome & Concept Intro: The user is welcomed with a card on a vibrant purple backdrop (purple is used in the app to denote swarms or grouping
file-fjpkexbjp2dfhprtk7mhwq
). A bold title might say “Agent Swarms Setup” with an icon like UilLayerGroup (stack of layers) to signify a group
file-fjpkexbjp2dfhprtk7mhwq
. The welcome message acknowledges the user (“Welcome back, [Name]!”) and teases the benefit: “Let’s group your agents into a powerful Swarm for coordinated calling.” A brief description explains what swarms are (e.g. “Swarms allow multiple voice agents to work together on targeted campaigns – think of it as your AI call team.”). A Get Started button moves to the next step.
Swarm Details (Name & Purpose): The user enters key info for the swarm. This is similar to creating a team: Swarm Name (e.g. “Sales Battalion”, “Support Squad”) and Purpose/Mission (a one-word or short phrase like “Outbound Sales” or “Customer Support”). These inputs are captured with text fields. For inspiration, the UI might show examples or even allow choosing from templates (like a dropdown of common swarm types: Sales, Support, Appointment Setting). Each template could pre-fill the purpose and even suggest which agents to include, simplifying the process for new users. The interface uses two input fields with labels and possibly helper text (e.g. “Give your swarm a memorable name. This will be visible in analytics and reports.”). The design uses the neobrutalist card style – perhaps a rotated white card on the purple background, with black bordered inputs. A continue button becomes active once a name is entered (purpose might be optional or default to the template chosen).
Select Member Agents: Now the user selects which voice agents will be in this swarm. The onboarding lists the user’s available agents in a scrollable list or grid. Each agent is displayed similarly to the Agents dashboard’s AgentCard
file-fjpkexbjp2dfhprtk7mhwq
, showing name, role/purpose, and status (active/training) with a status dot (green for active, orange for training, etc. as per getStatusColor logic
file-fjpkexbjp2dfhprtk7mhwq
). The user can toggle inclusion by clicking an agent card – selected ones are highlighted (e.g. outlined in the swarm’s color or with a checkmark). If the user doesn’t have multiple agents yet (quite possible if they only created one in the prior onboarding), this step can encourage them: “You have 1 agent. Create more to build a larger swarm.” It might allow proceeding with just one agent (technically a swarm of one), but ideally the user would have at least two. The flow might integrate with the Agents onboarding: for instance, right here provide a shortcut “+ Create New Agent” (maybe a small button with UilPlus
file-fjpkexbjp2dfhprtk7mhwq
). If clicked, it could pop open the CreateAgentModal or even link to the Agents onboarding. This way, users not only create a swarm but potentially another agent in the process. After selecting agents, the user clicks Next. (If none selected, we’d show a warning “Please select at least one agent” and disable advance.)
Configure Swarm Settings: This step (optional/advanced) allows tweaking how the swarm operates. For example, if Diala supports call distribution modes (round-robin, simultaneous, etc.), the user could choose one. Or if the swarm is for a campaign, maybe set a campaign goal like number of calls per agent, or schedule (run swarm now vs later). In the initial implementation, we might keep it simple: default settings that all agents will call from a shared number pool and start immediately when tasked. We can present a summary: “This swarm will use all selected agents concurrently for higher throughput.” with an info icon UilInfoCircle that on hover explains details (e.g., “Agents will automatically coordinate to avoid calling the same contact twice,” etc.). If there are any parameters (like max calls per agent or time windows), they can be sliders or toggles here. Keeping it minimal ensures users aren’t overwhelmed – they can accept defaults and proceed.
Review & Create Swarm: A summary card is shown, combining all chosen details. For example, “Swarm: Support Squad\nPurpose: Customer Support\nAgents: 2 agents (Echo-Diala, Diala-Belle)”, along with the swarm color or icon. The UI might present a stylized “swarm card” similar to how swarms appear in the dashboard grid
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 – i.e., the card header has the swarm name with purpose badges, and a list of agent names underneath
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. This gives a realistic preview. The user confirms by clicking Create Swarm. On click, a Convex function is called to save the swarm. After a quick success confirmation (perhaps the card transforms visually or a checkmark animation shows), the onboarding transitions to a final screen.
Swarm Ready (Demo/Next Steps): The final screen celebrates the creation. It might say “‘Support Squad’ is ready!” with a swarm icon. While there isn’t an obvious “demo” for a swarm like there was for a single agent, we can demonstrate the concept: for instance, simulate a live swarm dashboard view. We could show a mock realtime status: “Agents online: 2/2. Calls today: 0. Ready to deploy.” Or animate two agent avatars calling in parallel. This is more of a visualization than an interactive demo. Another approach: prompt the user to launch a campaign with this swarm (which segues into the Calls module). For example, a big button “Start a Calling Campaign with this Swarm” could be displayed. Clicking it could end the onboarding and navigate to the Calls dashboard’s campaign creation section (prefilling the swarm selection). If we include that, it directly drives the user to use the swarm immediately. Otherwise, simply instruct: “You can now use ‘Support Squad’ in your calls and campaigns. Visit the Calls section to deploy your swarm.”. Finally, a Done or Go to Dashboard button will exit the onboarding, bringing the user to /dashboard/swarms where they can see their newly created swarm in the list.
Components (Frontend)
Cards with Headers: Each step’s UI is primarily contained in a Card. The Swarm Details card might have a header with the UilUsersAlt icon (group of people) next to “Create New Swarm” in bold
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Using CardHeader and CardContent, we ensure consistent padding and border usage. The header background could use a light purple or appropriate thematic color. We saw in the swarms dashboard, card headers are color-coded by purpose (Discovery = purple, Support = green, etc.)
file-fjpkexbjp2dfhprtk7mhwq
. For onboarding, since purpose isn’t decided until input, we might keep the header a neutral or the base purple for all.
Form Inputs: The Name and Purpose fields use the standard <Input> for text. We may reuse the UI from the swarms dashboard’s Create Swarm modal if it exists. There’s likely a CreateSwarmModal given the code references setShowCreateModal(true)
file-fjpkexbjp2dfhprtk7mhwq
 and presumably a corresponding component. If available, we can extract its inner form. If not, implementing two <Input> components with labels is straightforward. To maintain style, we give them a class for thick border and maybe slightly rounded corners (in code, modals and inputs often use classes like rounded-[3px] border-4 border-black
file-fjpkexbjp2dfhprtk7mhwq
).
Agent Selection List: This is a critical component. It could be a custom list with checkboxes or toggle buttons, but a more visual approach: show each agent in a mini-card format. We can use the existing <AgentCard> or create a simplified version for listing. Possibly, a two-column grid if there are several agents. Each agent card includes the agent’s name and an icon or avatar, plus perhaps a small badge for status (active/inactive) as seen in swarms dashboard listing
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The design system’s <Badge variant="outline"> with colored background is used in the swarms card to show agent status
file-fjpkexbjp2dfhprtk7mhwq
, we can mimic that here or simply use an icon + text. Selecting an agent card can internally toggle a boolean, and we visually indicate selection by changing its style (e.g., add bg-yellow-100 or some highlight, and a checkmark in a corner). We might incorporate a small <Checkbox> component on each card for clarity (with a custom styled checkbox that fits the brutalist theme: perhaps a black border square that fills with a black check when selected).
Swarm Settings controls: If we include any (like a distribution mode), we can use radio buttons or a <Select> dropdown from the design system for simplicity. For example, a label “Call Distribution:” with options Simultaneous vs Sequential. These can be radio inputs styled with black border and a dot, or buttons that toggle. The design system likely has a Toggle or Switch component (like <Switch> or using UilToggleOn/Off icons
file-fjpkexbjp2dfhprtk7mhwq
) to represent boolean settings. We could use those if relevant (e.g., a switch for “Start immediately” vs scheduled).
Summary Card: This card will look much like a Swarm card in the dashboard. In fact, we might directly use the layout from the Swarms grid: a card with a colored header containing the swarm name and purpose badges, and inside a list of agents. The difference is in onboarding it’s static info for confirmation. We’ll use <Badge> elements for purpose and agent count as done in the real card
file-fjpkexbjp2dfhprtk7mhwq
. Each agent could be listed simply as name (with or without type). We can borrow the snippet that lists agents in a swarm card
file-fjpkexbjp2dfhprtk7mhwq
 but simplify (maybe omit the status badges here for brevity, or include them if we want to confirm we added the right ones). If the list is long, we’ll scroll or limit height, but likely initial users have few agents.
Feedback/Confirmation: After clicking “Create Swarm,” a quick feedback can be given. Possibly using a small modal or toast that says “Swarm Created!” or visually transforming the summary card. For example, overlay a big checkmark on the card or turn the border green momentarily (success feedback). We can animate a check icon (like a black check that scales up then back down) to acknowledge the action.
Navigation Buttons: Each step has a Next or Continue button, and possibly a Back button on subsequent steps. We style them as before: large, uppercase, bold. The Next button uses a prominent color (maybe the same Diala blue or a green to indicate progression) with black text, and arrow icon UilArrowRight. The Back button (if present) could be a simpler text or secondary style button, possibly with UilArrowLeft. The final step’s primary button is “Create Swarm” instead of Next, and after creation “Done” or “Go to Swarms”. We ensure button states (disabled vs enabled) are visually distinct (grayed out or lower opacity when disabled, vibrant when enabled).
Backend Architecture (Convex & APIs)
Convex Swarm Creation: The completion of this onboarding triggers a Convex function (say createSwarm) to save the new swarm. This function will likely write to a Swarms table or collection, including fields: name, description, purpose, createdAt, and the list of agent IDs that belong to the swarm. In our mock data, a Swarm object includes id, name, description, purpose, agents[], totalCalls, successRate, created
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The Convex function will set initial metrics (totalCalls=0, successRate=0 or null, created=now). The list of agent IDs is crucial; those references link the swarm to actual agents. The function might also update each agent’s record to note they are part of this swarm (if the data model requires linking from agent side; otherwise, it’s enough to store agent IDs in swarm).
Convex Query Update: Once created, any Convex queries that fetch swarms (like the one powering the Swarms dashboard list) will include the new swarm. If the user is navigated to the swarms page after, they’ll see it without needing a manual refresh, thanks to Convex reactivity if set up (the useQuery hook would push updates). If not using real-time queries, we might refetch the swarms list on page load.
No External API Calls: Creating a swarm is an internal operation – no external APIs are needed. It’s mainly about organizing existing data. So the backend interaction is straightforward: a DB insert. One possible integration: if the swarm is conceptually tied to a “campaign” or call automation, we might schedule a task (like if “start immediately” was selected, we could kick off a search or call routine). However, those tasks belong more to the Calls module. The swarm itself is just a grouping, so we won’t call any outside service in this step.
Data Validation: The Convex function will validate that the swarm name is unique (or at least not blank). It may also ensure agent IDs exist and belong to the user’s account (to avoid any malicious insertion). Basic sanitization on name/description is done (trim whitespace, maybe limit length). If any issues, it returns an error that our frontend will handle (showing an error message on summary).
Link to Campaigns: If we did implement the “launch campaign” button, pressing that could call another Convex function or backend routine: e.g., create a new Call Campaign object with this swarm attached. But since that delves into the Calls domain, we might instead handle it on the frontend by navigation (and then in Calls page, user manually sets up a campaign). We can integrate lightly by passing parameters via URL (e.g., navigate to /dashboard/calls?tab=swarm&swarmId=X to pre-select that swarm in the calls interface). The code already allows switching tabs via URL param
file-fjpkexbjp2dfhprtk7mhwq
. We could extend it to accept a swarmId to auto-open a SwarmOverview or campaign modal. The SwarmOverviewModal exists in the code
file-fjpkexbjp2dfhprtk7mhwq
, perhaps used to show swarm’s call campaign details.
Convex Relationship: In the data model, the swarm might not physically “do” anything until used, but we might prepare an entry in a “SwarmCalls” or “Campaigns” table to accumulate stats. For now, we set up the basics: also consider if any initial stats should be computed. E.g., activeAgents count (the number of selected agents with status active). We can compute that in the function by checking each agent’s status (if accessible via a query of Agents table). In our static data, swarms have successRate and totalCalls aggregated
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Initially those are 0, but could we glean something? If the agents had previous calls, maybe totalCalls = sum of those agents’ totalCalls to date (giving a baseline). But probably simpler to start at 0 since as a new swarm it hasn’t done anything collectively.
Notification/Webhook: Not directly needed, but conceivably if the platform notifies an admin when a new swarm is created (for monitoring user engagement), a side-effect could be sending an event. That’s beyond core functionality but part of a robust backend integration if desired.
Resilience: The backend should handle partial failures gracefully. If, for example, one of the agent IDs is invalid (maybe the user deleted an agent mid-onboarding in another tab), the function would throw and the UI show an error. But such edge cases are rare. The function can also handle concurrency – though unlikely two swarms with same name are being created simultaneously by one user, the unique name check should handle it.
Design System & Neobrutalist Elements
The Swarms onboarding uses Diala’s design language to reinforce consistency:
Color Theme: Purple is the thematic color, echoing how the Swarms dashboard uses purple for the header and stat card for total swarms
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The onboarding background can be a purple tone (e.g. bg-purple-400 or a gradient of purple) with the same grid pattern overlay for texture
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. This differentiates it from other flows (Voice was blue, RAG yellow, etc.) while still being an “approved” palette color.
Typography & Copy: As always, uppercase bold headings for each step title. For example, “SWARM NAME” label above the input, or “SELECT AGENTS” as a section label, will be in a bold font (we might use small caps or just uppercase for labels). The main titles (like on the welcome screen or final screen) are large – possibly 5xl or 6xl size – ensuring the user immediately sees what the focus is (the name of the flow). Supporting text and descriptions use slightly lighter weight but still bold and high contrast (dark gray on light backgrounds, or black on colored backgrounds if legibility permits). The tone is motivational and action-oriented (e.g. using words like “team”, “campaign”, “powerful”).
Iconography: Many icons illustrate the points: UilLayerGroup or UilUsersAlt for swarm concept, UilRobot for agents in the list, UilPlus for adding new, etc. The design will often place these icons in squares or circles with contrasting background. For instance, in the page header for swarm onboarding, a purple circle with a white UilUsersAlt could float behind the title text as a design element. In lists, each agent might have an avatar circle with a robot icon (some differentiation if multiple – maybe different background colors as used in Active Agents card where index 0 is blue, 1 is purple
file-fjpkexbjp2dfhprtk7mhwq
, etc.). This adds visual variety and cues (like color-coding agents by purpose or status if possible).
Card Style: Just like other modules, cards have black borders and slight rotation. In fact, the swarm cards on dashboard are rotated depending on index
file-fjpkexbjp2dfhprtk7mhwq
 – in onboarding, we might rotate the main form card a bit for flair. But for user input, a slightly rotated form might be weird, so perhaps the form card stays straight to align input fields nicely (we can still rotate decorative elements). The final swarm summary card can be rotated to match how it’ll look among others. The heavy shadow on hover isn’t as relevant in static onboarding, but any clickable card (like agent select) will use the interactive shadow: normally 4px offset shadow, on hover increase to 6px to show it’s clickable
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. This gives that tangible feel.
Badges & Status indicators: We use the same badge styles as in the main app: e.g., purpose badges with white text on colored background, bordered in black
file-fjpkexbjp2dfhprtk7mhwq
, status dots for agents (tiny colored circles) – these appear in the swarms card design
file-fjpkexbjp2dfhprtk7mhwq
 and we mirror that in the agent list: a green dot next to an agent name for active, etc. Those dots can be simple <div className="w-2 h-2 bg-green-600 rounded-full animate-pulse"> as used in code
file-fjpkexbjp2dfhprtk7mhwq
. This subtle animation (pulse for active agent) is a nice touch to imply “liveliness” of active agents.
Accordion/Info Panels: If we have any explanatory text (like explaining distribution modes or swarm usage), we might tuck it in an accordion for cleanliness. The design system has an Accordion component as seen in transcripts flow
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. For example, a small accordion titled “How do swarms work?” could expand to show a short blurb: “All agents in a swarm call concurrently, dramatically increasing call volume. Monitor swarm performance in real time in the dashboard.” This educates without cluttering the main flow. Use of a bold question as trigger and normal text answer in content keeps style consistent
file-fjpkexbjp2dfhprtk7mhwq
.
Grid Layout & Responsiveness: The agent selection likely uses a grid. We’ll ensure on mobile that it becomes one column list (stacked agent cards) and on desktop two-column. The whole onboarding container is centered and scrollable if needed (min-h-screen ensures full height usage
file-fjpkexbjp2dfhprtk7mhwq
, and we add padding). Buttons are sized appropriately (full width on small screens, or inline on larger screens if space allows). All text remains readable on smaller devices by using relative units or tailwind’s responsive text classes (sm:text-lg vs md:text-xl etc., as seen in other flows
file-fjpkexbjp2dfhprtk7mhwq
).
State Management
State management for the Swarms onboarding will track the multi-step form and selections:
currentStep (number): As usual, to control which UI segment is shown. Steps 1 through 5 correspond to screens described. We initialize at 1 and increment with next/back.
Swarm form data states: swarmName (string), swarmPurpose (string) for the text fields
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Possibly swarmDescription if we want a longer description (it was present in code as well
file-fjpkexbjp2dfhprtk7mhwq
 but could be optional).
selectedAgentIds (array of strings): The IDs of agents chosen for the swarm. Toggled on selection. Alternatively, we could store a set or boolean map for agent IDs, but an array is fine. We’ll fill this if user had agents; if they add a new agent mid-process, we must ensure to update this list (like if CreateAgentModal returns a new agent, push it and mark selected).
If we have distribution mode or schedule: distributionMode (string, e.g. 'parallel' or 'serial'), startImmediately (bool) as an example. Defaults set accordingly (most likely parallel & true).
showCreateAgentModal (bool): If we allow agent creation inline, we manage that modal’s open state. On save, we get the new agent data – possibly via a callback or by monitoring a Convex mutation result. We then update the agent list state (which might be a local copy of available agents) and add its ID to selectedAgentIds. In code, we see an availableAgents list of mock new agents
file-fjpkexbjp2dfhprtk7mhwq
; for real data, we’d fetch the user’s agents from Convex at mount (or rely on what’s in context if user came from Agents section).
isSaving: A flag for when the swarm is being created. On clicking create, set true to disable the button and maybe show a spinner. After Convex returns success, set false and move to final step.
error: Any error message to display if creation fails (e.g. “Name already taken”). This could be shown in an alert style on the summary screen or as a modal.
We might also have a swarms state if we fetched existing swarms (not really needed for onboarding unless we want to ensure unique naming or reference). For simplicity, we don’t need it now.
All this state is local to the component. Since the data (agents list) might come from outside, we could fetch the list of agent objects at the start using a Convex query (useQuery(api.agents.list) for example). That returns an array of agent records. We then use that to render the selection options. This approach ensures we’re not using stale or mock data. Each agent record could include id, name, type/purpose, status, which is what we need for display
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. If the user came directly to swarm onboarding without any agents, this list would be empty and we’d handle that (maybe prompt to create agent first). But likely, we assume they have at least one agent (especially if this onboarding is accessed after Agents onboarding). Using React state for all of the above is sufficient. No complex state library is needed. If the app had a global context for user’s agents, we could tap into it to avoid refetching, but it’s fine to fetch within onboarding to decouple flows.
UX Interaction & Animations
Step Transitions: Just like Agents flow, transitions should be fluid. Pressing “Next” slides the current card left and brings the new card in from right (for example), creating a feeling of progress. Back does the reverse. This can be done with CSS transitions or a library for wizard flows. We’ll ensure focus moves to the first input of each new step for accessibility (e.g., when going to select agents, focus could go to the first agent card or heading so screen readers know context changed).
Selection Feedback: Clicking an agent card to select it will give immediate feedback: the card might do a quick jiggle or pop to indicate it’s been selected, and a checkmark appears. We could implement a tiny animation (scale up 1.05 then back) on selection. If deselecting, maybe a brief shake or the check disappears. Since multiple selections are allowed, each card acts like a toggle. We want this to be fun and clear – possibly a sound effect could even play on select (though that’s optional and needs sound library; probably skip for now).
Prevent Mistakes: If the user tries to proceed without any agents selected, the Next button will be disabled. We might also show a tooltip or text: “Select at least one agent to continue.” This avoids frustration. Similarly, if they try to create with an empty name, we disable create.
Back Navigation: Provide a Back button from steps 3 and 4 to allow corrections (maybe they want to rename the swarm or add a different agent). Back should preserve any inputs done (our state management ensures that).
Modal for New Agent: If used, the Create Agent modal might cover the screen. When it closes (after creating an agent), users return to the swarm flow seamlessly. We ensure to highlight that new agent in the list (maybe automatically select it and flash its card or scroll to it). This avoids the user feeling lost after adding.
Final Step Illustration: The last screen is an opportunity for a celebratory animation. We could animate multiple agent icons moving or a cluster icon getting a “success” highlight. Even a simple effect like confetti (small colored rectangles falling) could be triggered to celebrate creating a swarm. It adds delight and positive reinforcement. We must be careful not to overload – it should last a second or two then clear.
Guidance to Next Action: As mentioned, a big action button “Launch Campaign” or “View Swarm Dashboard” on the final screen guides the user on what to do with the swarm. If launching a campaign, clicking it provides feedback (maybe a loading if it takes time, or goes straight to calls page). If just viewing, it navigates quickly. In either case, we should also allow a neutral completion: maybe the top-right corner could have an “X” close button at all times to exit onboarding. If the user clicks that mid-way, we might ask confirmation (“Are you sure you want to exit? Progress will be lost.”) to avoid accidental loss. If they confirm, just navigate away (progress is ephemeral anyway).
Tooltips/Help: Certain terms might have tooltips. For example, when hovering over “Success Rate” in summary, it could show how that’s calculated (though initially zero). Or if distribution modes are present, a small ? icon next to each with more info. We can use a simple <Tooltip> from design system or a custom one (the title attribute or a popover on hover).
Responsive Adaptation: On smaller screens, the flow might not show as multi-column at the agent selection step. If it’s a list, each agent row could have a “Add” button or switch. We ensure those controls are large enough to tap. The continue button likely sticks to bottom for easy access. If needed, we implement a slight scroll on selecting an agent to bring the next one into view (if the keyboard popped up or something).
Technical Implementation Details
The Swarms onboarding is implemented similarly as a separate Next.js page (e.g. app/onboarding/swarms/page.tsx). Some specifics:
Data Fetching: We’ll use the Convex client to get the list of agents upon component mount. For example, const agents = useQuery(api.agents.listMine); which returns an array of agent objects. This hook will cause a re-render when data arrives. If Convex isn’t used for this, we could use a Next.js API route or directly fetch from an internal endpoint – but since Convex is central, we use it for consistency. We then keep that list in state (or just use it directly for rendering). We might also have const [agents, setAgents] = useState([]) and populate it via useEffect if not using useQuery. Either approach is fine.
Creating a Swarm: We’ll define a Convex mutation function to handle swarm creation (in the api.swarm namespace perhaps). This function could be generated via the Convex CLI. Our frontend calls it with the swarmData: name, purpose, agentIds, etc. Once awaited, if no error, we assume success (Convex would throw if error). We then set some local newSwarmId maybe if needed (for navigation), and call setCurrentStep(lastStepNumber) to show the final screen.
Error handling: If Convex throws (maybe name conflict or network error), we catch it and set an error state. The UI would then likely remain on the review step and display the error message somewhere (perhaps as a red text below the Create button or an alert box). The user can then adjust and retry.
Modal Integration: If we integrate CreateAgentModal from the existing codebase, we have to import it and use it. The code base shows a CreateAgentModal component being imported in Agents page
file-fjpkexbjp2dfhprtk7mhwq
. We can use it here. It likely expects props like onSave to receive the new agent data. We’d pass an onSave that takes the agent, closes the modal, and updates our state. The agent data might be an object with at least an id and name; if not, maybe the modal itself handles insertion to DB and triggers a re-fetch of agent list. We might rely on Convex reactivity: if agents.listMine query is active, after the new agent is saved via that modal (which likely also uses Convex), the query would update automatically to include the new agent. If so, our UI would update without manual intervention – neat. If not reactive, we can manually append.
Routing & Completion: After finishing, we likely navigate to /dashboard/swarms. We can use useRouter() for that. If we want to auto-open the SwarmOverview modal on the dashboard (to show details), we could append a query param like ?swarmId=new or something and have the swarms page check for it and open a modal (the swarms page code doesn’t explicitly handle query except maybe selection if any). Alternatively, simply landing on the swarms page and seeing the new entry at top (if we prepend it) is sufficient.
Conditional Steps: If the user has zero agents, step 3 might be skipped or replaced by a call-to-action to create an agent. In implementation, we could detect agents.length === 0 and instead of showing selection, show a special screen: “No agents yet. Let’s create one first!” with a button to launch CreateAgentModal. After they create one, that screen goes away and we essentially can skip to step 5 (because a swarm of one is now possible). But ideally, we expect them to have at least one agent from previous onboarding.
Testing the Flow: We test by simulating different conditions: user with one agent vs multiple. Ensure that if one agent, they can still create a “swarm” (though it’s somewhat an edge case, we allow it for completeness). The call to Convex with one agent is fine. Also test the new agent creation path. Because multiple asynchronous things happen, we ensure state updates correctly (maybe using useEffect to watch agents list and if it grows from 0 to 1, auto-select the new agent and advance the step if we were waiting).
Maintainability: We structure the code clearly with sub-components if needed (e.g., could break out AgentSelection as its own component that receives agent list and manages selection states). But given it’s not too complex, keeping it in one file is okay. Comments in code would explain each step’s block for clarity to future devs.
Integration Points
Swarms Dashboard Listing: The obvious integration is that the new swarm appears in the main Swarms dashboard (/dashboard/swarms). On that page, swarms are displayed in a grid with their stats
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Our newly created swarm will show up with 0 calls and baseline success rate in a card. Users can click it to see details (the app might navigate to /dashboard/swarms/[id] which has a detail page, as indicated by the folder structure
file-fjpkexbjp2dfhprtk7mhwq
). That detail page likely shows deeper analytics for the swarm (calls over time, agent contributions, etc.). Because our onboarding saved the swarm in the same store as those, everything is connected.
Live Call Monitor: If a swarm is used in a live campaign, the Live Call Monitor (and possibly SwarmOverview modal) will tap into the swarm’s data. For instance, if we open the SwarmOverviewModal in the Calls section, it might display which swarm and some stats. The code references a SwarmOverviewModal
file-fjpkexbjp2dfhprtk7mhwq
, likely to show details of a swarm’s campaign. That modal probably expects a Swarm object or ID. Since our swarm data structure matches, it can be used there seamlessly.
Calls Module (Campaigns): When a user wants to actually use the swarm, they’ll likely interact with the Calls module. Possibly, they’ll create a new campaign or search that leverages this swarm. The integration might be: on starting a campaign, the UI asks “Which swarm or agent to use?” If a swarm is chosen, then the call distribution logic uses all agents in that swarm. Under the hood, when dialing out, the system picks an available agent from that swarm for each call or parallel calls. The integration point is that the calls service (the part of backend that orchestrates calls, likely via Telnyx) needs to retrieve the list of agents in a swarm to assign calls. Since our swarm is stored, an API call from the call service (maybe a Convex function called by the serverless telephony function) can query by swarmId to get agent IDs, then fetch each agent’s config. This means the swarm’s existence and membership drive real behavior.
Analytics Aggregation: Over time, as calls are made, the system will update swarm metrics. E.g., increment totalCalls for the swarm, recalc successRate (maybe weighted average of member agents or outcome tracking). In the code, these seem to be handled possibly by accumulating data
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Our creation set them to 0, but as calls complete, perhaps a Convex mutation updates the swarm record (like swarm.totalCalls += 1, and successRate recalculated). Because the swarm grouping is known, any per-call record might carry a swarmId to facilitate such updates. Integration-wise, this means our swarm’s ID is used as a foreign key in call logs. By creating the swarm properly now, we ensure future processes have the needed reference.
UI Consistency: The onboarding uses the same components as the main app (e.g., same AgentCard style). If the design of AgentCard or Swarm card updates, these changes should reflect in onboarding too, maintaining an integrated feel. This is more a front-end integration but ensures that, for instance, if tomorrow they add a new field to swarms (like a “Team Lead” name or a color code), both the dashboard and onboarding could be updated to capture/display it.
Cross-module Onboarding Links: We’ve allowed the possibility of jumping to Agent creation from within Swarm onboarding. This is a cross-integration between two onboarding flows (Agents and Swarms). By doing so, we ensure the user doesn’t hit a dead-end if they need more agents. The integration is smooth because both use Convex – the new agent is saved and immediately accessible to the swarm flow. If multiple onboardings share components (like WelcomeCard, etc.), the design and behavior remain consistent.
User Guidance Continuity: After finishing swarm onboarding, the natural next step is to utilize the swarm. We integrated a pathway to the Calls module. This kind of integration ensures the user’s journey is continuous – they created something, now immediately see how to use it. It also means that the onboarding flows aren’t isolated tours; they feed actual objects into the system that subsequent modules recognize. The swarm’s name might even appear in notifications or suggestions, e.g., the Calls page might have a banner “New! Launch a campaign with Support Squad swarm.” This kind of cross-promotion can be built because the system knows a swarm was just created (via an event or just by checking that it exists).
Future Enhancements
Swarm Templates & AI Recommendations: In the future, Diala could provide pre-made swarm templates. For example, a template called “Appointment Army” (like seen in mocks
file-fjpkexbjp2dfhprtk7mhwq
) could come with recommended agent roles or even auto-generate a couple of agents specialized for that purpose. The onboarding could let the user pick a template at the start (“Choose a swarm template or create your own”). If they choose a template, we could auto-fill the swarm name, purpose, and even spin up new agents with appropriate settings. This would dramatically shorten setup: one click to get a fully functional swarm. It uses AI to provision agents – possibly generating custom prompts for each based on best practices. This enhancement would blur onboarding for Agents and Swarms together but yield a very powerful outcome quickly.
Dynamic Agent Suggestion: If a user has many agents, the system could suggest which agents to include in a new swarm based on their purpose or past performance. For instance, if the user’s agents have tags or specialization (sales vs support), and the swarm purpose is “Sales”, the UI might automatically check all sales-type agents for inclusion. It could also highlight an agent’s success rate or availability (if an agent is currently on a call or in training, maybe exclude or warn). As an enhancement, each agent card could show a metric (like success 92%
file-fjpkexbjp2dfhprtk7mhwq
) to inform selection.
Swarm Capacity Planning: Future iterations might allow setting the swarm’s campaign parameters right in onboarding. For example, “How many calls do you want this swarm to handle per day?” or “Target region for this swarm’s calls:”. This would tie into search (for leads) or into scheduling. While perhaps complex for initial onboarding, advanced users might appreciate configuring a swarm fully (like a campaign object) at creation. Eventually, the line between creating a swarm and launching a campaign might fade – one might do both in one flow.
Integration with Schedules/Calendar: A swarm might be scheduled to run at certain times (especially if using global agents). A future feature could let user set time windows (e.g., 9am-5pm weekdays). Onboarding could include picking a timezone or schedule for the swarm to be active. This would integrate with Diala’s job scheduler to only activate those agents in that window for outgoing calls. Not needed in MVP, but valuable for planning campaigns.
Swarm Collaboration Features: Perhaps in the future, swarms can have internal “chatter” or strategy – e.g., agents sharing information (one agent learns something on a call and another uses it). If so, onboarding might highlight that unique capability: “Agents in a swarm learn collectively. If one discovers a new objection handling tactic, all others adapt.” The UI might not change much for setup, but educational tooltips or an extra toggle like “Enable swarm intelligence sharing” could be present. This adds to the product’s differentiation if implemented.
Delete or Edit Swarm in Onboarding: Currently, onboarding is linear create. In the future, maybe support editing a swarm (via same UI). Or if they realize a mistake in final step, allow editing before leaving. Minor tweaks to flow to incorporate an “Edit” path might be beneficial.
Gamification: To encourage usage, perhaps show a benchmark: “Most users create 2 swarms to cover Sales and Support. You have 1 – consider creating another for support!”. Not directly part of a single onboarding flow, but a suggestion after completion. It could prompt to start the process again for a different purpose.
By evolving in these ways, the Swarms onboarding can become smarter and even more user-friendly, automating complex setup and showcasing the full power of coordinated AI agent teams.
AutoRag Module Onboarding Tendril
Purpose
The AutoRAG onboarding flow introduces users to Diala’s Retrieval-Augmented Generation system – essentially the feature that allows voice agents to leverage custom knowledge bases (documents, videos, websites) for more informed conversations. As a self-contained product-like experience, this flow walks the user through creating their first knowledge base (or “RAG workflow”) automatically. The purpose is twofold: first, to demonstrate how Diala can ingest large amounts of content and make it queryable by the AI agent, and second, to actually populate the user’s account with a useful knowledge base that their agents can use. It serves as a lead funnel by showcasing a high-value capability (integrating the company’s own data into AI calls), which can be a deciding factor for adoption. By the end of the onboarding, the user will have configured a data source (like a YouTube channel or a set of PDFs) and seen the system process it into a ready-to-use knowledge store. This flow ties into backend processes that perform web scraping, text chunking, embedding creation, and index building – but abstracts those technical steps into a user-friendly “autopilot” experience. The design and messaging reinforce that this is an automated, powerful feature that extends the intelligence of their voice agents.
Flow Steps
Welcome & Use Case Framing: On a bright orange (or gold) background – since RAG was associated with yellow in the main menu
file-fjpkexbjp2dfhprtk7mhwq
 – the user is greeted with “RAG System Setup”. The welcome text might say: “Welcome [Name]! Let’s give your voice agent a brain boost by feeding it your knowledge.” This sets the stage: the user is about to upload or connect some knowledge source. A succinct explanation is provided: “Diala’s AutoRAG will automatically retrieve data (like documents or videos) and teach your agent, so it can answer questions and handle specifics about your business.” A relevant icon appears, such as UilDatabase (database) combined with UilBrain (brain), signifying AI + data
file-fjpkexbjp2dfhprtk7mhwq
. The user clicks Get Started to proceed.
Choose Knowledge Source Type: The user is asked what kind of content they want to use as a knowledge base. Options are presented as large selection cards: YouTube (for videos/transcripts), Documents (PDFs, Word, etc.), Web Pages (URLs to scrape), or CSV/Knowledge base (if they have structured data). Each option card has an icon (UilYoutube for YouTube, UilFile for docs, UilLink for URLs, etc. as imported
file-fjpkexbjp2dfhprtk7mhwq
) and a brief description. For example: “YouTube Videos – ingest transcripts from a channel or playlist.” If needed, we restrict initial version to one type (say YouTube or Documents) to simplify, but showing all options indicates flexibility. The user clicks one; the selected card gets a bold highlight. We then show a Continue or automatically advance to next step.
Provide Source Details: Based on the type chosen, the user provides the input details:
If YouTube: an Input field for a Channel URL or Playlist URL (or individual video URL). Example placeholder: “https://youtube.com/@YourCompany/videos”. (In code, one of the workflows had type: 'youtube' with sources as channel URLs
file-fjpkexbjp2dfhprtk7mhwq
.) We also allow just a single video URL if they want to test with one video. Possibly also a number-of-videos setting (like how many recent videos to fetch – could default or be advanced setting).
If Documents: a file upload field (supporting multiple files). The UI could use the existing file upload component (maybe similar to File Upload Card used elsewhere
file-fjpkexbjp2dfhprtk7mhwq
). We’d list chosen files with their names and sizes.
If Web Pages: one or multiple URL inputs. Perhaps a textarea where they can paste several URLs or enter one by one, with an add button.
If Other: for a CSV or knowledge base, possibly just an upload or integration selection (maybe out of scope for now).
Each variant of this step shares a similar layout: a card with an icon corresponding to type in the header, a prompt to enter info, and maybe some validation (e.g. check if the URL is valid format). We also mention any limits (“We will process up to 100 videos” or “Max 20 documents” to set expectations). The design is user-friendly: e.g. for YouTube, after entering URL, we might fetch the channel name and show a preview (like channel title and thumbnail) to confirm correct link, using YouTube API or oEmbed. For documents, we might list the file names once uploaded. After input, the user clicks Next. If they leave it blank, Next disabled.
Configure Processing (Advanced): This step is optional/expandable for power users. We show some processing settings with sensible defaults: Chunk Size, Overlap, Embedding Model, Vector DB. In code, each RAG workflow had parameters like chunkSize, overlap, embeddingModel, vectorStore
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. To avoid scaring non-technical users, we can either hide these behind an “Advanced settings” accordion or pre-fill and explain them in simple terms. For example: “We’ll break documents into pieces of 500 words (Chunk Size) with a 50-word overlap to retain context. We’ll use OpenAI’s ADA embedding model to vectorize text, stored in a Pinecone vector database.” Each of those underlined terms could have a tooltip if hovered. If the user does open advanced, they can tweak: maybe a slider for chunk size (e.g. 256 to 1024), a dropdown for model (if they have their own key or want to use Open-source embedding model), and choice of vector DB (if Diala offers multiple or internal vs external). For most users, leaving defaults is fine; they might not even open this. But including it signals that the system is robust and configurable.
Launch Processing: Now the user begins the ingestion. A summary is shown: e.g. “Source: YouTube channel @YourCompany (45 videos)\nChunking: 512 tokens, Overlap: 50\nModel: text-embedding-ada-002, Store: Pinecone”. They hit Start Processing (or similar action label, maybe “Build Knowledge Base”). Upon clicking, the UI transitions into a processing state. This can either be on the same screen or navigate to the next “Progress” step. Preferably, we go to a dedicated progress view so we can show ongoing status.
Processing Progress: The user sees a live progress dashboard of the RAG workflow. This is where the automation feels real. We can emulate the UI of the AutoRAG dashboard but in a focused way for this workflow. For example, display a list of steps with statuses: “1. Scraping content – 10/45 videos downloaded”, “2. Generating embeddings – 0/45 completed”, “3. Indexing vectors – pending”. As the backend does each stage, update the UI. The code’s RAGWorkflow status field goes through states like 'scraping', 'embedding', 'indexing', etc.
file-fjpkexbjp2dfhprtk7mhwq
. We reflect that: perhaps show a progress bar for each stage, or one overall progress bar. The top could have a big progress percentage (like "65% done"
file-fjpkexbjp2dfhprtk7mhwq
) and maybe an estimated time remaining (if provided by backend
file-fjpkexbjp2dfhprtk7mhwq
). For example, “Estimated time: ~10 minutes”. If the process is quick (for small input, maybe finishes in under a minute), the user will see a rapid update. If it’s longer, we might not expect them to wait on this page indefinitely. But since this is onboarding, perhaps we simulate or accelerate it. (One approach: if they provided a large source, we could either process a subset for demo or send an email when done, but that breaks the flow. Instead, perhaps limit to something manageable for onboarding demonstration, like one video or small doc.) In any case, during progress we keep the user engaged with some fun facts or tips: “Did you know? You can add more data sources later to enhance your agent’s knowledge.” or “Your agent will soon have X pages of data at its fingertips.” The style should remain brutalist: progress bars have black borders, segmented perhaps, and status text is bold. Icons like UilSync (spinner) or emojis (✓, ⚠️) mark each step’s status (completed, in progress, etc.).
Completion & Results: Once processing hits 100%, we display a success message: “Knowledge Base Ready!” and possibly some stats: e.g. “45 videos processed, 1.2M characters indexed, 10k embeddings generated, index size 120 MB”
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. These numbers make the user feel the scale of what just happened. We present the final RAG Workflow Summary – basically a card very similar to what the AutoRAG dashboard shows for a completed workflow
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
: name, type, maybe a list of source names, createdAt and duration. The UI could allow them to click “View Details” which might open the ViewRAGWorkflowModal (already in code
file-fjpkexbjp2dfhprtk7mhwq
) if they want more granular breakdown like which sources were included and any issues encountered. For onboarding, we might skip deep details but mention they can see them in the main AutoRAG section. The key action now is to link this knowledge base to an agent. We highlight: “Great! Now your voice agents can use this data. You can assign ‘Sales Training Videos’ to any agent via the Agents dashboard or when configuring an agent.” Possibly, we incorporate a quick step here: if the user has an agent already (likely yes), ask “Apply this knowledge to Agent X now?” with a toggle or button. If they opt in, we call a Convex function to attach this workflow ID to that agent’s ragSources. This one-click integration makes the payoff immediate: their agent is now smarter. (Alternatively, just instruct them how to do it and not actually implement in onboarding to keep things simpler.)
Finish: The onboarding concludes with a call-to-action: “Your RAG system is set up. Going forward, you can create more knowledge workflows in the AutoRAG dashboard.” A button Go to Knowledge Base takes them to /dashboard/auto-rag where they will see this workflow listed among others (with status completed). Or possibly a Go to Agent button if they linked it, to try it out in Playground or calls. The experience wraps up by reinforcing what was achieved: their voice agent now has custom knowledge, which is a powerful differentiator.
Components (Frontend)
Multi-Step Form with Conditional UI: The onboarding uses conditional rendering similar to previous flows. Up to step 5, it’s form-like, then steps 6-7 are more dynamic status display. We’ll have components for each major portion: e.g., <SourceTypeSelection> for step 2, <SourceInput> for step 3 that changes based on type, <AdvancedSettings> for step 4, and <RAGProgressDisplay> for steps 6-7 combined. Breaking them into sub-components can keep the code tidy.
Card & Accordion: The interface elements heavily rely on Cards for grouping content. The Advanced settings likely will be inside an Accordion since it’s optional – we can use the design system’s Accordion (with a trigger like “Advanced Options”) to hide or show chunk/model settings
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Similarly, if we want to show additional info (like list of files, or intermediate stats), we might use collapsible sections. Each Card’s header uses relevant icons: e.g. a small icon on left of title and maybe another on right as decoration (like the transcripts onboarding had icons in corners
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). For RAG: a database icon on left, and a search or file icon on right perhaps
file-fjpkexbjp2dfhprtk7mhwq
.
Progress Bars: The design system has a <Progress> component
file-fjpkexbjp2dfhprtk7mhwq
 which is styled with border and can accept a value. We will use that for the various progress visuals. In the code, they adjust CSS variable --progress-color to style it for positive, neutral, negative sentiments
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, similarly we might color code different stages (blue for scraping, green for indexing etc., or simply one color throughout). We’ll accompany each progress bar with labels and percentages (like “65%” done, as text inside or alongside).
Icons for Steps: Possibly use an icon per stage in the progress display: e.g., UilDownloadAlt for scraping/download, UilChartGrowth or UilBrain for embedding, UilDatabase for indexing, UilCheckCircle for completed
file-fjpkexbjp2dfhprtk7mhwq
. These help scan the statuses quickly. Completed steps might get a green check icon, current step a loading spinner (maybe a simple CSS animation rotating an icon or using an <svg> spinner).
Modal for Viewing Workflow: If the user clicks a “View Details” button, we can invoke the existing ViewRAGWorkflowModal component (imported as seen in code
file-fjpkexbjp2dfhprtk7mhwq
). This modal likely shows a deeper breakdown (maybe listing all source items and their status, or allows adjusting settings post-creation). It’s not mandatory to include in onboarding, but having that option demonstrates transparency of process. The modal would appear with the typical black border styling and overlay.
File Upload UI: For documents, the file upload can either use a simple <input type="file" multiple> (styled with our UI) or a fancier drag-and-drop area. Possibly in the components there is file-upload-card.tsx
file-fjpkexbjp2dfhprtk7mhwq
, which might handle showing an upload area. If available, we integrate that, which likely uses browser File API and shows file preview in a card. Since this is onboarding, not the main app, we can do a simpler approach if time – but consistent styling is key. So likely we use FileUploadCard from custom components for consistency.
Select for Model/DB: The embedding model and vector DB choices (if exposed) can use the design system’s <Select> component
file-fjpkexbjp2dfhprtk7mhwq
 for a dropdown of options. We ensure it’s styled with the same black border and perhaps a custom arrow icon (or default, but inside a card it should look fine).
Link to Agent Option: If we add the “link to agent” step, we could have a small section at the end: either a dropdown of agents to choose one to link (if multiple) or if only one agent exists, a simple yes/no toggle to link it. This might be a <Switch> component from the UI kit
file-fjpkexbjp2dfhprtk7mhwq
 for a boolean, or just a checkbox with label “Attach this knowledge base to Alice (AI Sales Agent) now”. Simplicity is fine. On toggle true, we store the intent and execute it as part of finishing.
Backend Architecture (Convex & APIs)
Initiating the RAG Workflow: When the user clicks “Start Processing,” we trigger a backend sequence. Likely, there is a Convex mutation or an API endpoint that creates a RAG workflow job and starts processing asynchronously. In the code, there are likely functions for createRAGWorkflow and then some background worker or schedule to do the actual scraping/embedding (perhaps using Convex scheduled functions or external server). The AutoRAG dashboard shows mock workflows with statuses
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We need to replicate that logic for real. Probably, on start, we insert a new RAGWorkflow record with status 'queued' or 'scraping', initial progress 0, and all those parameters. Then either a Convex backend worker kicks off (Convex allows async jobs) or we have an AWS Lambda or similar (if not purely Convex). Given the complexity, possibly an external service might handle it. But since this is a blueprint, we can assume Convex functions manage orchestration by calling out to required APIs/libraries.
For YouTube: The backend likely uses the YouTube Data API or a scraping library to fetch video IDs and transcripts. Indeed, the presence of youtube-transcript-fetcher.ts and youtube-transcript.ts in lib
file-fjpkexbjp2dfhprtk7mhwq
 suggests a backend implementation for grabbing transcripts. It might spawn a job to retrieve transcripts for each video. We have to handle potentially thousands of videos, so ideally this is done in an asynchronous, streaming manner. The onboarding doesn’t need to delve deep into how; it just triggers it and polls status.
For Documents: The backend would accept file uploads (maybe already uploaded via the file upload component to some storage like S3 or Convex storage). Then it reads each file (text extraction for PDFs, etc.), splits text into chunks, gets embeddings (via OpenAI or others), and stores them.
For Web pages: Use an HTTP client to fetch HTML and strip text. Possibly use readability libraries or site-specific scrapers.
The backend then interacts with a Vector Store (like Pinecone, Weaviate, Chroma, Qdrant as seen in data
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). For example, using Pinecone’s API to upsert vectors. The chosen store (we’ll default to Pinecone for now, but the code had multiple, e.g., Weaviate
file-fjpkexbjp2dfhprtk7mhwq
, Qdrant
file-fjpkexbjp2dfhprtk7mhwq
, etc.). We will have API keys configured for these in settings (the settings page lists API keys for openai, etc.
file-fjpkexbjp2dfhprtk7mhwq
, likely Pinecone too). The Convex function would use those keys from environment variables to connect.
This heavy-lifting is mostly on backend. The onboarding’s Convex mutation may quickly return a job ID or initial status, after which we rely on polling.
Polling Mechanism: To update the progress UI, the frontend could call a Convex query periodically to get the latest workflow status. The code for transcripts used setInterval to poll job status every 2 seconds
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We can do similar: after starting, store the workflowId (Convex doc id) and poll via useQuery (Convex queries update in real-time when data changes, so we might not even need manual polling if Convex subscriptions can push updates to the client on document change; that would be ideal – as soon as we update progress in DB, the UI updates). If Convex real-time subscription is available, we’d just use a useQuery(api.ragWorkflow.get, {id}) and have the progressbars reflect that object’s fields. If not using subscription, then a setInterval calling a getStatus(jobId) Convex function is fine. (The code suggests fetchYoutubeTranscript returned a jobId and they polled getJobStatus
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, so likely use similar pattern here if not full real-time).
Updating Progress in Backend: The backend that’s processing will periodically update the workflow record in Convex: e.g., after downloading each video, increment contentProcessed and progress (maybe progress is computed as a percentage across phases). The code’s mock has progress field separate from content counts
file-fjpkexbjp2dfhprtk7mhwq
. Possibly they define progress roughly as (completed phases / total phases * 100) or something plus finer granularity. We can decide a scheme: e.g., scraping 30%, embedding 50%, indexing 20% of total progress. Or simpler, just update progress as ratio of completed items to total items (once scraping done, you’re maybe 50% if embedding left, etc.). Either way, the UI will trust whatever progress value comes.
Completion: Once done, the Convex workflow record is set to status 'completed' and progress 100. Possibly store a completedAt timestamp
file-fjpkexbjp2dfhprtk7mhwq
 and maybe processingTime. The backend may also compile some stats (like we see number of embeddings, index size, etc. in stats
file-fjpkexbjp2dfhprtk7mhwq
). These come from the actual processing: count of chunks, size of index (perhaps returned by Pinecone), etc. Those are saved in the stats field of the record.
Error Handling: If something fails (e.g. a video couldn’t be downloaded or embedding API error), the backend should mark status 'failed' and possibly include an error message. The UI should detect status === 'failed' and inform user (maybe show a red message: “Processing failed: [error]. Please check your inputs or try again.”). And allow retry. We might skip deep error scenarios in onboarding narrative unless prompted, but we design for it.
Linking to Agent: If we implement that quick link, that would be another Convex mutation like attachKnowledgeToAgent(agentId, workflowId). It updates the agent’s ragSources array to include this workflow (similar to adding M1 and M4 in the example agent data
file-fjpkexbjp2dfhprtk7mhwq
). It might also trigger any re-training if necessary (though likely the heavy training is already done by building the vector DB, nothing more needed except the agent now knows to use it when answering questions).
Integration with Search & Chat: The outcome of this is a vector index with an ID (or some reference). The Convex DB might store the Pinecone index name or collection reference. When the agent (LLM) is conversing, it will use a function or API to query that index given a user question, retrieve relevant snippets, and include them in prompts (that’s how RAG works). So an integration point: the agent runtime needs to be aware of attached RAG workflows for an agent and know how to query them. Possibly convexEntryPoint or some config includes hooking into a retrieval function with the vector store. While not part of onboarding directly, our created knowledge base should seamlessly hook into those flows.
Resource Management: We should note that ingesting large data might be time-consuming. In an onboarding context, we might either restrict size or run an abbreviated process for demo. Perhaps behind the scenes, we limit to first N videos or first M pages of a doc to finish in a minute or two. This way the user can see completion without waiting hours. In real use, bigger jobs might run longer asynchronously (the user could leave and come back). But for the guided flow, a controlled shorter path is beneficial. Implementation could detect “onboarding mode” vs “full mode” by the type of call or a flag, but ideally the user used a reasonably sized input. If not, we could inform them: “We’ll process the first 10 items now for this demo. You can process all data later in the main AutoRAG section.” This ensures the user sees success quickly.
Design System & Neobrutalist Elements
AutoRAG onboarding uses a bold, utilitarian style to demystify a complex process:
Color and Imagery: An orange/yellow palette invokes the idea of knowledge and highlights (fitting since knowledge base often highlighted in yellow, and it was used for RAG in menu
file-fjpkexbjp2dfhprtk7mhwq
). The background can have the usual grid pattern, with maybe a subtle motif like database icons or circuitry watermark to imply data. We want to visually convey “automation” and “data crunching.” Brutalist design might incorporate a schematic-like aesthetic – maybe using monospace font for step labels to mimic code (optional).
Layout: Steps 2-4 are basically form inputs on cards, which we style with the same off-kilter geometry. The “Choose Source Type” could use a horizontal row of option cards or a column; probably a row that wraps on mobile. Each option card might rotate slightly on hover as a playful cue. For instance, when you hover “YouTube”, it tilts less (straightens) and shadow intensifies indicating selectability (the main menu had such an effect
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
).
Illustrative Icons: On the source selection cards, along with the Unicons we might add small illustrations or emojis to make them distinct (like a YouTube logo or document icon). But sticking to the icon set might be cleaner. The icons used (UilYoutube, UilFile, UilLink, etc.) are line icons – to make them pop, we can place them in a colored circle with black border as a sort of icon badge on each card. E.g., a red circle behind the YouTube icon (since YouTube is red), a blue behind Link icon, etc., still all with black outlines to remain in style.
Progress Visualization: Neobrutalism can take the normally hidden technical details and surface them in a raw form – which is exactly what showing chunk counts and index size is. Presenting these numbers in a table or list with bold labels and monospace numbers could be very on-theme (like an old computer printout vibe, but with modern layout). We could style the progress text as bullet lists with square bullets (or small black squares) to emphasize each stat. The progress bars have thick borders and solid fills. Because they will animate (growing width), we can even quantize them (like a choppy growth) to fit the retro-tech aesthetic. But smooth is fine too.
Feedback & Warnings: If advanced settings are opened, any warnings (like “smaller chunks improve accuracy but increase embedding count”) can be shown in a small italic text or a caution icon (UilExclamationTriangle in yellow
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). We ensure those are styled consistently with the rest (maybe a yellow background highlight behind the warning text).
Spacing and Fonts: We maintain generous spacing so complex info doesn’t overwhelm. Each phase in progress perhaps in its own card or clearly separated section with margins. Use of font weight: Titles heavy, details medium-bold. Possibly use a different font for code-y stuff if available (though likely stick to Inter or Noyh Bold, maybe with letterspacing to give a typewritten feel).
Consistent Components: The advanced settings might reuse components also found on the actual AutoRAG page. For example, if the main AutoRAG dashboard has a modal for editing settings (SettingsRAGWorkflowModal
file-fjpkexbjp2dfhprtk7mhwq
), maybe the onboarding’s advanced section is similar but inline. The progress display in onboarding could mirror the dashboard’s layout of the workflow cards (the Agent Performance card and others in auto-rag page are an example of info display
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, though that’s performance, not needed here). But possibly the progress view in onboarding is simpler than the full dashboard, focusing only on one workflow.
Motion: Could incorporate some quirky animations like rotating gear icons or a ASCII-style loading bar animation in text for fun while processing. But given we have real progress bars, we might skip gimmicks and let the live update itself be the “animation”.
Mobile: Ensure even on mobile the progress info is legible – maybe stack phases vertically rather than side by side. Possibly hide advanced settings by default (mobile users likely skip fiddling with those). Buttons remain accessible at bottom. The grid for source type becomes a vertical list on mobile to avoid squishing.
State Management
Managing state for AutoRAG onboarding involves both immediate form state and reacting to asynchronous status updates:
currentStep (number): Controls which view to show (1 = welcome, 2 = choose type, 3 = input details, 4 = advanced, 5 = ready to start, 6 = in progress, 7 = completed). Some steps might be combined in implementation (e.g., step 5 could be merged with 6 as part of starting processing). We increment as user proceeds, but also might jump steps (e.g., if user skips advanced, currentStep might jump from 3 to 5).
selectedSourceType (string): “youtube”, “documents”, “urls”, etc. Determines which input fields to show in step 3.
sourceInput (varies): The actual data for the source. If YouTube, a youtubeUrl (string). If documents, a list of File objects or file references (or perhaps just an array of filenames if uploaded separately). If URLs, maybe a string of newline-separated URLs or an array. We manage those accordingly. For file uploads using a controlled component, we might store files in state (FileList or Array of File).
advancedSettings object: containing values like chunkSize (number), overlap (number), embeddingModel (string), vectorStore (string). These default to something (512, 50, “text-embedding-ada-002”, “pinecone”). If user changes via inputs, we update the state. If they never open advanced, we keep defaults.
workflowId (string or null): The identifier of the created workflow job after starting. We get this when initiating the process (Convex function return). This is used to fetch status updates.
workflowStatus (object or specific fields): We could maintain a state for the status (like “scraping”, “embedding”, etc.), progress percentage, and stats. But since we likely will use a Convex query that gives us the whole workflow object (with fields like status, progress, stats), we might not need to manually store each – we can rely on that reactive data. However, to make it simpler, we can have: currentStatus (string), progress (number), and perhaps workflowStats (object with totalContent, contentProcessed, embeddings, etc.). These will get updated on each poll tick. In a React component using useQuery, we wouldn’t need explicit state; the query result itself is stateful. Alternatively, we do useState and update via polling function.
error (string or null): To store any error message if the process fails. If not null, we’ll show an error state (and maybe allow retry – possibly just reusing the same inputs and hitting start again).
isProcessing (bool): Indicates that the job is running. It flips on when user starts and off when done or failed. It helps in UI logic (e.g., show cancel option maybe). We could allow a “Cancel processing” button if feasible, which would require backend to stop tasks. That might be advanced, so likely skip cancel in MVP. But if we did, isProcessing and a Convex function to cancel would be needed.
selectedAgentToAttach (string or null): If we offer attaching to agent, track which agent (or a boolean if just attaching to default). If the user toggles attach, we store the agentId (maybe default to first agent if they have one).
The interplay: The early states (selectedSourceType, sourceInput, advancedSettings) feed into the Convex call when starting. After that, the state is driven by backend responses. We use useEffect to start polling when isProcessing becomes true or currentStep enters progress stage. We also use useEffect or useQuery to react to data changes. If using polling, the transcripts example can be adapted: call a getWorkflowStatus(workflowId) periodically, update state accordingly. Possibly incorporate a setTimeout to stop polling after X time or when completed to avoid infinite loops. The transcripts flow did a setTimeout to stop polling after 60s
file-fjpkexbjp2dfhprtk7mhwq
. We can do similarly if concerned about long tasks in onboarding. If the task isn’t done in that timeframe, we could inform the user that it will continue in background and they can check the AutoRAG dashboard later (meaning we end onboarding here with a message “We’ll keep working on it!”). But ideally our tasks in onboarding are short. One nuance: if using Convex real-time subscription, then no need for manual interval – useQuery will push updates as the Convex doc changes. That’s neat and simpler: just keep showing the query data in UI, and when status === 'completed', we know it’s done. We should still include a timeout for failsafe maybe.
UX Interaction & Animations
Guided Input: Each input step will focus the relevant field automatically. E.g., after choosing YouTube and clicking next, the URL input is auto-focused so they can start typing without extra click. When they finish typing and hit enter, we can treat that as clicking Next (for convenience). If file upload, we support drag-drop; when they drop files, we might auto-advance (if we can guess they’re done selecting, or still require clicking Next explicitly – probably require explicit Next to confirm).
File Upload Feedback: If they add a file, maybe show a thumbnail or icon with name. Possibly a small animation of the file icon dropping into a database icon to symbolize ingestion could be a delightful detail. If they remove a file, animate it fading out.
Progress Live Updates: As content is processed, we visually update progress bars. We might animate the bar width transition for smoothness. Also possibly flash or highlight when one stage completes (like turn the bar green and show a checkmark). If using textual status (e.g., “Scraping (10/10) ✓”), the checkmark or green color appears when done. We can sequentially reveal the next stage as the previous finishes, to keep focus. For example, initially show “Scraping... [progress bar]” and “Embedding... (waiting)” grayed out. Once scraping done, mark it complete and then animate enabling the “Embedding” section (maybe ungray it or slide it in). This storyboard style keeps the user engaged, watching each stage tick off.
Notifications: If the process completes quickly, we’ll still show the final message. If it’s slow and maybe user navigates away (though in onboarding they likely won’t), maybe we could implement a browser notification or email. But since it’s in-flow, we focus on on-page updates.
Attach to Agent UI: If they choose to attach to an agent, we could do something like a toggle or prompt after completion: “Apply to agent now?” For simplicity, maybe present it as a checkbox on final screen, before finishing, or a prompt: “Would you like to attach these knowledge sources to any agent?” with a dropdown of agent names and a button “Attach”. If they do it, we give quick feedback (“Attached to Agent Alpha ✓”). This is a minor interaction but a nice closure.
Complete & Next Suggestions: On finishing, beyond the immediate CTA to go to the AutoRAG dashboard or agent, we might include a subtle suggestion: “You can create another workflow for a different source (e.g., upload your product manuals) in the AutoRAG section.” or “Your knowledge base will continuously improve as you add more sources.” – encouraging future use. In terms of interaction, maybe a button “Create Another Knowledge Base” that directly takes them to auto-rag page’s create modal. But probably unnecessary in onboarding itself.
Error Case: If an error occurs mid-process, we should stop updates and show a visible error message on the progress screen. Possibly overlay a semi-transparent red shade on the progress section or replace it with a red card that says “There was an error processing your data.” If we have details, include them. Provide options: “Retry” (which might just rerun the Convex function, maybe starting from scratch or continuing where left off if possible) or “Cancel” (which ends onboarding, maybe direct to support). The user could adjust input (maybe the URL was wrong) and try again. So we likely allow them to go back to step 3 or 2. That means if error happens, we might set currentStep back to an earlier one or keep at progress with an active Back button. Ensuring this flow doesn’t break the component requires careful state resets (e.g., clearing workflowId if we’re starting over).
Time-out / Background: If we implement that 60s timeout like transcripts did
file-fjpkexbjp2dfhprtk7mhwq
, when it triggers, we should communicate it: “This is taking longer than usual. We’ll continue processing in the background. You can find the results in the AutoRAG dashboard later.” and then maybe we finish the onboarding with an incomplete status. But a better approach might be to continue polling longer, or encourage the user they can leave and it’ll continue. Up to the product decision – likely keep it interactive as long as possible.
Technical Implementation Details
The AutoRAG onboarding page (app/onboarding/rag/page.tsx) in the provided code is very basic and labeled “coming soon”
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We will revamp it with above logic:
Conditional Rendering and Components: The component can use a simple switch on currentStep to decide what to show. Or break each part into separate components and include conditionally. For example:
jsx
Copy
Edit
{currentStep === 2 && <SourceTypeSelection onSelect={handleTypeSelect} />}
{currentStep === 3 && selectedSourceType === 'youtube' && <YouTubeInput ... />}
...
{currentStep === 6 && <RAGProgressView workflow={workflow} />}
Alternatively, embed logic directly. Splitting is cleaner for readability.
Convex interactions: We likely create two functions in Convex: startRagWorkflow and getRagStatus (unless using realtime). startRagWorkflow will insert the doc and possibly initiate an async process. Convex supports the concept of “actions” for long-running or external work (similar to serverless functions). We might implement the actual scraping/embedding in a Convex Action so it doesn’t block the mutation. The mutation returns the new workflow id, then the action (running asynchronously) updates the document as it goes. This design fits Convex’s model (actions can call external APIs like YouTube or Pinecone, while mutations can update DB). In code, youtubeTranscriptActions.fetchYoutubeTranscript suggests an action was used for transcripts fetching
file-fjpkexbjp2dfhprtk7mhwq
. We’d have analogous ragWorkflowActions.startProcessing. Implementation aside, from the front-end we just call one function and get an id. Then either poll a Convex query or directly subscribe. Possibly they have a useQuery that can filter by id (like useQuery(api.rag.getWorkflow, { id })).
Real-time update possibility: If we use useQuery for a single workflow doc by id, as the Convex actions update that doc’s fields, the query result updates. We then simply use that object’s fields in the UI. We need to ensure the UI updates smoothly; likely fine as Convex pushes updates via websockets.
Resource cleanup: If component unmounts (user leaves page mid-process), we might want to handle that. If they come back later to auto-rag page, they should see the workflow still processing or done. That’s fine since it’s in DB. No need to cancel if user leaves (the process continues on server).
Data limits: If concerned about load, one might incorporate usage limits (like how transcripts limited hours). But for blueprint, we skip that complexity.
Testing with dummy vs real: For development, one can test with small inputs or stubbed processing to ensure UI flows as expected. Or have a mode where instead of calling actual Pinecone, we simulate a delay and then fill fields. This is more for development ease. In production it would call real.
Integration Points
AutoRAG Dashboard: The knowledge base created in onboarding shows up on the main AutoRAG page (/dashboard/auto-rag). There, the user can manage it further – e.g., view details, adjust settings, or delete it. The code’s workflows state on that page contains several sample workflows
file-fjpkexbjp2dfhprtk7mhwq
. Our new one would be appended to such a list. If using Convex queries on that page, the new doc automatically appears (assuming the query fetches all workflows for that user). If the onboarding ended with the job not fully complete, the user might see it in progress in the dashboard (with a spinner icon or progress bar). When done, they’d see it turn to completed. So the integration ensures continuity: onboarding created an object that persists in the user’s account.
Agents Integration: The main reason to build RAG is for agents to use it. So integration wise, each agent can link to any number of RAG workflows. The agent data structure included ragSources array of {id, name, description} for modules
file-fjpkexbjp2dfhprtk7mhwq
. Perhaps each RAG workflow corresponds to a module entry. If so, part of our Convex attachKnowledgeToAgent might append something to that array with the workflow id and maybe name or a short description. Then, when the agent is making calls, the conversation logic sees it has ragSources and knows to query those. Possibly the conversation engine has a step: if agent has ragSources, do a vector DB similarity search on the user query and include top results in the LLM prompt. All that logic would lie in the workflow engine (maybe in WorkflowEngine.ts or similar
file-fjpkexbjp2dfhprtk7mhwq
). We ensure that by properly saving the data, that pipeline can retrieve it.
External Services Setup: Pinecone or vector store credentials must be configured. The onboarding doesn’t directly ask for API keys (we assume they were input in the Settings module beforehand – the Settings page did show placeholders for keys
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). If the user hasn’t set those up, the processing might fail (no API key). We might foresee this and if keys are missing, warn at step 4: e.g., “Please add your OpenAI API key in Settings before proceeding.” or ideally, Diala could allow limited use with its own keys. If not, integration with the Settings is needed (maybe the onboarding checks if keys exist via a convex query to a Key store and if not, either prompt the user to go to settings or allow them to input keys inline – but that complicates flow, so perhaps we assume keys exist as part of initial onboarding steps).
Background Processing: If a workflow is long-running beyond the onboarding, the system handles it gracefully. The integration here is that the backend will continue updating the record. If the user closes the app, the next time they open AutoRAG, they see the status thanks to that integration. If email notifications are a feature, perhaps an email could be sent when done (not in this flow’s scope, but overall integration to alert user when long jobs complete).
Search & Call integration: The knowledge base might also be used in the prospect Search & Call context. For example, if they scraped competitor websites in AutoRAG (like the “Competitor Analysis” workflow in sample
file-fjpkexbjp2dfhprtk7mhwq
), that could feed into how the agent talks about competition during calls. It’s all part of enriching agent dialogues. Not a direct integration to show in UI, but conceptually important: the data ingested via AutoRAG is accessible across Diala’s functionalities (calls, analytics maybe if queries asked, etc.).
Edge integration – Analytics: Potentially, the platform’s analytics could incorporate knowledge base usage (e.g., how often did the agent pull info from the KB during calls). If so, linking the workflow ID to transcripts or QA logs is needed, but that’s deep integration beyond initial onboarding.
Future Enhancements
Multiple Sources per Workflow: In the future, we may let users combine source types in one go (the code already had type: 'mixed' for workflows that have multiple kinds
file-fjpkexbjp2dfhprtk7mhwq
). Onboarding could allow selecting multiple sources: e.g., YouTube + a PDF at once to create a unified knowledge base. The UI could be a multi-step selection (“Add another source?” repeatedly). This could be powerful but might overload the initial user. Perhaps after first source, we could say “Add another source to this knowledge base or finish.”. This enhancement aligns with robust usage but might be a later iteration once users understand basics.
Scheduled Auto-Updates: A future feature might allow a workflow to periodically update (e.g., re-scan a YouTube channel for new videos weekly). Onboarding could mention or allow enabling “Auto-sync”. For instance, after processing a YouTube channel, ask “Keep this knowledge base updated with new videos from this channel?” and if yes, mark the workflow as recurring. The backend then would periodically run a job to fetch new content. Not trivial to implement but extremely useful (knowledge stays fresh). The UI element could be a toggle or frequency select on final step or in advanced settings.
Quality Feedback Loop: Integration with agent performance – maybe future onboarding or usage of AutoRAG will highlight how the knowledge base improves call outcomes. Over time, maybe the system can recommend sources to add based on questions the agent couldn’t answer. For example, if in calls agents often say “I don’t have information on X,” the system might suggest uploading docs about X. This is beyond the initial onboarding, but a possible system enhancement: showing suggestions in AutoRAG dashboard like “Customers often ask about pricing details. Consider uploading your pricing FAQ.”.
UI Enhancements: Could add visualizations, e.g., a graph showing how many embeddings or content added over time, or a treemap of content by source. In onboarding, probably not needed, but in main section it would be cool.
Collaboration: Maybe allow multiple team members to contribute sources. Not directly onboarding matter, but if in future an org has multiple users, one user’s onboarding could show existing knowledge bases and suggest linking to those instead of duplicating efforts.
Security & Privacy features: Possibly later allow setting certain knowledge bases as sensitive (require certain call contexts to use, etc.). Not likely needed to mention in onboarding, but might be part of advanced settings in future (like a toggle “This data is confidential” which then might restrict it to internal calls or something).
The AutoRAG onboarding, by implementing these enhancements down the line, would continue to demystify a complex AI capability in a user-centric way, and ensure users get the maximum value by keeping their AI agents informed and up-to-date with minimal effort.
Calls Module Onboarding Tendril
Purpose
The Calls onboarding flow is designed to help users set up and understand Diala’s calling system, which encompasses automated campaigns and real-time call monitoring. This module is one of the most critical, as it directly deals with the core functionality: making and managing calls with AI agents. The onboarding serves as an interactive tutorial that leads the user through configuring an Automated Call Campaign (sometimes referred to as “Search & Call”) – from defining a target audience, to launching the calls, to monitoring results. By doing so, it demonstrates how Diala can scale their outreach or handle inbound calls with AI at the helm. It’s a self-contained feature: even without prior context, a user could complete this flow and get a tangible outcome (like a scheduled campaign or an ongoing call simulation). It connects deeply to Diala’s infrastructure, triggering backend processes such as lead searching (for prospects), call scheduling via telephony APIs, and analytics tracking. The onboarding covers both the setup aspect (which ties into the prospect search engine) and the live aspect (which ties into the call engine and analytics), thereby giving a holistic view of the Calls module. Ultimately, it aims to convert a first-time user into someone confident enough to run their own AI-driven call campaign, seeing firsthand the system’s capabilities in action.
Flow Steps
Welcome & Campaign Intro: The user is welcomed to the Calls module with a vibrant violet background (matching the “Search & Call” onboarding color from the main menu
file-fjpkexbjp2dfhprtk7mhwq
, which was violet). The title might be “Intelligent Calling Campaign Setup” with icons like UilPhone and UilSearch crossing each other
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The welcome message frames the scenario: “Ready to supercharge your outreach? Let’s set up an AI-powered calling campaign that finds prospects and dials them automatically.” This immediately communicates that the system can both search for leads and call them – a unique value proposition. If the user’s name is known, include it ("Welcome back, [Name]!"). A brief description explains: “In a few steps, you’ll configure a campaign: define who to contact, let Diala find contacts, and have your AI agent call them, all on autopilot.” This sets expectations and perhaps alleviates intimidation. A Start or Configure Campaign button leads to the next step.
Define Campaign Criteria: This step gathers parameters for finding prospects or defining the call list. It likely corresponds to the fields in the HuntConfigurationModal (the code references location, businessType, keywords, etc. when saving a search workflow
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). We present a form with fields such as: Location (text or dropdown for region), Industry/Business Type (dropdown or text, e.g. “Software” or “Restaurants”), Keywords (tags or comma-separated terms describing target profiles), Include LinkedIn (a toggle to indicate whether to use LinkedIn data as well), and Search Depth (how many pages or how deep to search – could be simplified to e.g. “Broad vs Focused” search). These inputs define the scope of prospecting. The UI could arrange them in sections: e.g., “Who to search for” (industry, keywords), “Where” (location), “Data sources” (a checkbox for LinkedIn or other directories), and “Volume” (search depth or number of prospects desired, could be implied by depth). This is a bit complex, but we can keep it user-friendly: use plain language labels like “Target Industry”, “Target Location”, “Keywords (e.g. CFO, IT security)”, “Search extensively (yes/no)”. Each field might have helper text or placeholders. For instance, Location placeholder “e.g. San Francisco, CA or ‘USA’”, keywords placeholder “e.g. fintech, SaaS”. If the user doesn’t know what to put, we might allow defaults or skip (but ideally these are required to get meaningful results). We validate critical ones (perhaps require at least one of location or industry or keywords). Once filled, Next button is enabled.
Select/Confirm AI Agent & Swarm: Now the user specifies which AI agent (or swarm of agents) will make the calls. If the user has multiple agents or any swarms (from previous modules), we list them. Possibly two tabs: one for single Agent, one for Swarm, or a unified list labeling which is which. For example, a radio list: “Call with Diala-Tone (Sales Agent)” vs “Call with Sales Battalion (Swarm)”. If the user only has one agent and no swarms, we auto-select that and just show it (with an option to confirm or change later). If they have none (unlikely if they did earlier onboarding, but possible), we could embed a quick prompt to create an agent (similar to how Swarms onboarding handled no agents – but if reaching here with none, perhaps direct them to Agent onboarding first). Let’s assume they have at least one agent. The UI shows agent name, purpose, maybe current status (should be active), and even performance stats to give confidence (like success rate). If swarms are present, show those similarly (with number of agents indicated). The user picks one. The design can use cards or just a select dropdown. Cards would be nice: e.g., a card with agent name and avatar, when selected, highlighted. A UilRobot icon for agent, UilUsersAlt for swarm could distinguish them. After selection, Next.
Phone Number & Schedule: The user chooses what number to call from (and potentially when to run the campaign). If Diala integrates with phone number management (the code has a PhoneNumber interface with many fields
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 and a list of mock numbers
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
), the user likely has one or more phone numbers (or SIP endpoints) configured to make calls. We present a dropdown or list of their available outbound numbers (with displayName if given, and perhaps type like PSTN or SIP)
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. If they only have one, just show it. If none, instruct them to add one in Settings or present a note that Diala will use a default demo number. For onboarding, we might not require an actual number if it’s a demo (the call could be simulated), but at least show the concept. Next, scheduling: do they want to start calls immediately or later? Provide an option: Start now (immediate) or Schedule later (pick a date/time or time window). For simplicity, default to now. If scheduling is important (maybe not in MVP), we include a date-time picker or a simple “Tomorrow 9am” preset. Another possible input: Max Calls or duration (how long to run). But could keep it open-ended for demo. Perhaps incorporate a safety like “Limit to 10 calls” for demo. But not to overwhelm user, we might hide advanced scheduling and just go with immediate. The UI for selecting phone number could be a Card list like on the Numbers tab of Calls dashboard, showing the number and name (e.g. “Main Sales Line: +1 555 123-4567”
file-fjpkexbjp2dfhprtk7mhwq
). Selected gets bold outline. If scheduling, maybe a small card with clock icon and a couple of options.
Review Campaign Setup: Now we compile all chosen settings into a summary for confirmation. The card might read: “Campaign: Find [keywords] in [location] in [industry] and call using [Agent/Swarm Name] from [Phone Number], starting [Now/later].”. Essentially a sentence or bullet list of the plan. Example: “Prospects: Fintech, SaaS companies in San Francisco, CA. Agent: Diala-Tone (AI Sales Agent). Calls from: +1 (555) 123-4567 (Main Sales Line). Start: Immediately.” We present this in a nicely formatted way, maybe using badges or icons next to each piece of info (location pin icon next to location, briefcase icon for industry, user icon for agent, phone icon for number, clock for schedule). The user verifies all good. If something is off, they can go Back to change. If okay, they hit Launch Campaign (or Start Calling). This triggers the backend to commence the search and call operations. Transition to next step.
Prospect Search & Call Progress: After launching, the onboarding flow shows a dynamic view combining lead search progress and call execution status. This could be split into two areas: Prospect Finding and Live Calls. Initially, the system will be searching for businesses that match criteria. We display something akin to a loading list: e.g., “Searching for businesses...” with maybe a spinner. As results come in, we populate a table or list of prospects (like company names or contacts). The code’s SearchWorkflow has stats like pagesFound, businessesExtracted, validated, etc.
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We can show some of those: “Pages scanned: X, Businesses found: Y, Valid phone numbers: Z” updating in real-time. Each found contact could briefly flash on screen like "Found: Acme Corp, SF – number +1 415 ...". This gives user the sense that the system is actively discovering leads. Simultaneously or after a short delay, the calling begins: perhaps once a few numbers are identified, the AI agent starts dialing. We then show a Live Call Monitor section. For example, show an entry "Calling Acme Corp... (ringing)" with a status icon, then "Connected – Talking" when the agent is on call, etc. The code references LiveCallMonitorModal and CallAnalyticsModal
file-fjpkexbjp2dfhprtk7mhwq
 – those are for the full dashboard, but we mimic a mini version. Potentially, show one or two concurrent calls (if swarm, multiple calls at once). Represent them as small cards or lines with Agent name, prospect name, call status. For realism, we can simulate a short conversation: e.g., after "Talking" for a few seconds, mark "Call Ended – Outcome: Interested (scheduled follow-up)" or "Not interested". These outcomes could be random or predetermined. It demonstrates that the system not only calls but also tracks outcomes (success, no pickup, etc.). We can incorporate some analytics visuals if time – like a tiny chart for pick-up rate, but likely not needed in onboarding, a simple textual log suffices. If possible, integrate audio or transcript snippet: maybe display one line the agent said and the response (like a snippet: Agent: "Hello, is this John from Acme Corp?" – no need for actual audio, just text to illustrate conversation). This might be too deep, but even a static “Transcript available” icon shown could hint at capabilities. The key is to amaze the user that in moments the system found leads and is actually executing calls.
Campaign Outcome Summary: After simulating a few calls (maybe we say it called 5 contacts for demonstration), we present a brief summary of the campaign’s initial outcomes. For example: “Campaign Complete (Demo)\n5 Calls placed, 3 answered, 2 voicemails.\n1 Interested lead, 1 Follow-up scheduled.” This summary would normally accumulate as more calls run, but for onboarding we end it here. We might clarify: “(In a real campaign, calls would continue until the list is exhausted or stopped.)” Then we highlight that the user can monitor everything in the Calls dashboard going forward. Possibly show them where (like "In the Calls dashboard, you'll find Live Call Monitoring and detailed analytics for each campaign.").
Wrap-up and Next Steps: The final screen congratulates the user: “Your first AI-driven call campaign is set up!”. It provides guidance: “You can view and manage campaigns in the Calls section. Analytics for this campaign will update in real-time – check the dashboard to see results and listen to call recordings or read transcripts.” We then offer navigation: a Go to Calls Dashboard button to exit onboarding into /dashboard/calls (perhaps pre-selecting the "calls" or "agents" tab using the query param logic in code
file-fjpkexbjp2dfhprtk7mhwq
). Another possible button: View Campaign Analytics which could open the CallAnalyticsModal for this campaign (if we had an ID; maybe not needed for onboarding, better to just go to dashboard). Also possibly suggest: “You can also create campaigns without the search step if you have your own call lists.” (Because maybe Diala can also import contacts – but not covered here, just a note that search is optional in future usage). Finally, emphasize that the Agents, Swarms, RAG all come together in Calls: their agent was used, knowledge base (if attached) would be used in call, swarm if chosen, etc. This ties the onboarding threads together conceptually.
Components (Frontend)
Form Inputs and Cards: For campaign criteria (step 2), we’ll use various form controls. Likely simple <Input> for location and keywords, and maybe a <Select> or set of radio for industry (we could supply a list of common industries as options to ease input). We may also use <Select> or <Combobox> for some fields. All these go inside a Card with a header “Prospect Search Criteria” or similar. Each field label is bold uppercase (like “LOCATION”, “INDUSTRY”), and use placeholders as described. Possibly group location & industry on one row if space (md screens), otherwise stack. Use our UI input with thick borders. If LinkedIn is a toggle, use a <Switch> component (with label “Include LinkedIn data”). Search depth could be a slider (like “broad <-> deep” search), but simpler: maybe a dropdown: Basic (find ~50 prospects) vs Extensive (find ~200). The code shows searchDepth parameter
file-fjpkexbjp2dfhprtk7mhwq
; define maybe Low/Medium/High mapping to pages. Keep default medium.
Agent/Swarm Selection: Use either radio buttons or card selection. We could implement similar to earlier flows – e.g., in Agents onboarding we had to select voice agent and pitch, here we select calling agent. If using card, each agent card shows name and perhaps a small subtext (like purpose or swarm size). Could reuse AgentCard display but modified for selection. If many options, a simple radio in a list might suffice too. But visual card toggles would be nice. If we foresee adding new from here, we might add a small button "Create new agent" if needed, but likely not for onboarding.
Phone Numbers & Schedule: The phone numbers might be presented in a dropdown or list. The code’s numbers list has fields like provider, status, etc.
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We just need number and name for display. Possibly show status if relevant (most will be active). We could present each as “Name – Number (Type)”, e.g., "Main Sales Line – +1 555 123-4567 (pstn)". Selection via radio or dropdown. For scheduling, a simple pair of radio: “Start Now” vs “Schedule for Later”. If they pick later, reveal a datetime picker (the design system might not have one prebuilt; we can use a native input type="datetime-local" styled appropriately or just a couple of selects for date and time). But to avoid complexity, we might skip detailed scheduling in onboarding and assume Now. If scheduling later, we won’t actually wait, it's just for demonstration, so perhaps skip to not confuse. (Alternatively, if user picks schedule later, we just say "Campaign scheduled for X. We'll notify you when calls start." in summary, but we won't simulate calls now – that could be an alternate path. But doing an immediate demo is more exciting, so likely default to immediate run).
Progress Display Components: For searching prospects and live calls, we might create small sub-components or just structure it clearly in JSX:
A <div> for search progress containing maybe a <Progress> bar or just textual counters. Possibly represent it like an expanding list: we add an <li> for each found business (like a mini log). But could also just update numeric stats. A fun way: have a list area with a fixed height and overflow, each found company name appended to the list (scrolling as it grows). That gives a visual of accumulation. But maybe just counters is fine given time. We'll at least show something like “Found X prospects so far...” updating.
A <div> for active calls. Could show one call at a time if sequential or multiple if parallel (especially if swarm chosen, could do 2-3 concurrently). Each call could be a Card or row with: prospect name, call status, maybe a timer. Represent statuses by icons (phone ringing icon for dialing, phone with waves for in-call, check or cross for outcome). The code’s AgentCall interface had fields for answered, pickedUp etc. which suggests tracking performance by agent
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 – but for live display, we use simpler terms. Possibly incorporate CallStatusIndicator or similar if exists; if not, static. After finishing calls, we list outcome (like "Interested" or "No answer"). We might use colored badges for outcomes (green "Interested", red "No Answer", yellow "Voicemail").
Possibly a simplified timeline: we don’t need actual timeline UI, linear updates suffice.
Modal vs Inline: We will likely do this progress view inline in the onboarding page (like transcripts onboarding did everything inline). However, if we had done a separate modal for call monitoring, that’s more for the full dashboard. Here we integrate it into the guided experience.
Summary/Analytics: At the end, the summary can just be text and badges on a card. If we wanted, we could embed a mini chart – but charts might be too heavy for onboarding. The main app likely has analytics charts (volume over time, etc.). We can simply list key numbers as done above. Perhaps in bold with icons: e.g., a phone icon with "5 Calls", a user icon with "1 Lead". If the CallAnalyticsModal is easily pluggable, we could show it, but that’s probably too much detail. Better to funnel them to the actual Calls dashboard for full analytics if interested.
Backend Architecture (Convex & APIs)
The Calls onboarding initiates a complex orchestration that involves: web search for contacts, and connecting calls via telephony. Breaking it down:
Search Workflow (Prospect Hunting): This likely corresponds to what the code refers to as a SearchWorkflow (the structure with parameters and stats saved when using HuntConfigurationModal)
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The onboarding will call a Convex function or action to start this search. Similar to AutoRAG, it might create a workflow record and perform asynchronously. Possibly they have a scrapper integrated (maybe something like SerpAPI or custom headless browser calls to Google/Bing or LinkedIn search). The backend will take the criteria (location, industry, keywords) and generate search queries. For each result page, it scrapes company info and tries to extract phone numbers (maybe through the company’s website or directories). It populates a list of found businesses with numbers (businessesExtracted, businessesValidated in stats). It probably also cross-checks with LinkedIn if that option is true (e.g., to gather company size or contact names). This is a non-trivial pipeline: it could involve multiple external APIs (Google Custom Search API for web, LinkedIn API or unofficial scraping for LinkedIn, and maybe a third party for phone validation). For blueprint, assume the backend can handle it and just call an action startProspectHunt returning a workflow id.
Call Campaign Initiation: In parallel or once some contacts are found, the system will start dialing. This involves the telephony integration (Telnyx or another provider) and the agent AI for voice. Likely, they have a call control service that the Agents backend triggers. Possibly the architecture: Diala’s server initiates outbound calls via Telnyx API (by sending a call command with the chosen caller ID number and target number). When the call connects, Telnyx streams audio to/from Diala’s media server (or directly to an AI service). Diala uses Deepgram (for speech-to-text) to transcribe the callee, and uses OpenAI (or similar) to generate agent responses, and ElevenLabs to synthesize the agent’s voice, streaming audio back via Telnyx to the callee. This is all happening in real time, likely orchestrated by a state machine (maybe that’s what convexEntryPoint in agent data references – which conversation workflow to run
file-fjpkexbjp2dfhprtk7mhwq
). For onboarding, we don’t actually perform calls but simulate. However, if the platform is fully functional, one could theoretically do a real call to a test number. But safer to simulate for new users, unless they want to test with their own number. Possibly an enhancement: allow the user to put their own number to receive a demo call from the agent. But that might be risky or too early (and need phone verification etc.). So we simulate. The backend might provide a mode where calls are not actually sent to PSTN but recorded as if done (a dry-run mode). Or we just create dummy call logs.
Convex Data Updates: As calls are “made”, the system would create Call records, update agent stats (pickedUp++ etc.), and generate transcripts and analytics. The code shows in calls page, an AgentCall structure used for a table (calls made, answered, etc.)
file-fjpkexbjp2dfhprtk7mhwq
. The onboarding likely won’t write to the real DB for everything (unless we do a real call). If simulating, we can either push some fake data into the Convex DB (like increment some counters) or just locally simulate. Perhaps better to keep it local for onboarding, not to pollute actual analytics with fake data. The integration can remain conceptual (we describe that these stats would reflect in the real system). If we did decide to involve real data, we’d maybe create a “demo campaign” entry marked as such, but likely not.
Ending the Campaign: In a real scenario, the campaign would continue calling until the prospect list is exhausted or user stops it. For onboarding, we plan a short run. So the backend process for search can be stopped early. Perhaps we only gather 5 prospects then stop. The call process calls those 5 then stops. The Convex should handle halting further calls. Or we treat it as a demo mode within the onboarding code, not on backend – more likely, simulate entirely on front without contacting external. But since we do want to show some realistic asynchronous behavior (like search progress), maybe use a dummy timeline with timeouts to append results. Or spin up a Convex action that yields logs for a short time.
Tracking & Logging: The system likely has logs for each call (transcripts, outcomes). Possibly stored in a Calls collection. If we were doing this fully, we’d want to create those logs. But again, as a demo maybe not persistent. Could just generate ephemeral data. The integration is more about showing what will happen.
Notifications and Real usage: If a user truly set up a campaign for later (schedule), the backend would schedule those calls at that time (maybe using a Convex scheduled function or an external scheduler). Not needed in onboarding, but the architecture supports it if implemented.
Edge Cases: If the user defines a very broad search, real search could take time. Onboarding might either limit scope or inform the user partial results. We likely limit behind the scenes. Perhaps the Convex function for demo purposely only searches a couple of pages to be quick.
Convex Integration in UI: We may choose to use useQuery or polling to reflect search progress in UI. Similar to how we approached RAG. For search, maybe easier to poll a status (like how many found). If we treat it as part of a SearchWorkflow, we could have a record updated and subscribe. Or simply simulate with setInterval incrementing a counter in local state (since we don’t intend to store real results, simulation might be entirely front-end). Actually, simpler: do it front-end. When user hits launch, we can use setTimeouts to simulate asynchronous events:
After 1 sec, show first prospect found.
After 2 sec, show second, and start first call.
After 3 sec, show third, first call ends, second call starts...
etc.
This avoids heavy backend complexity for a demo. But if the system is ready and we want to truly show it, we could do a scaled-down real thing. It's a decision: fidelity vs complexity. Possibly simulation is fine for onboarding.
Telnyx/Telephony Integration: If it were real, the system would use Telnyx API (as keys were in settings
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). It would create call sessions. Telnyx would send webhooks to Diala on call status changes, which Diala’s Convex or serverless functions handle, updating call status (like pickedUp or not, etc.), and instructing the media pipeline. This likely not done via Convex (real-time media might be handled by a separate service, but logging could update Convex). For blueprint, we don't dive too deep, just know calls would be placed.
Design System & Neobrutalist Elements
Visual Style: The violet theme (as used in SearchCalled) gives a distinct identity to the Calls onboarding. Violet and black combination in backgrounds, with white text on violet for contrast, was used in the searchcalled placeholder
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We continue that. The grid background or cross-hatch from others remains (just adjusting opacity maybe to 0.1 like others did
file-fjpkexbjp2dfhprtk7mhwq
). Titles like "SEARCH & CALL SETUP" in huge black letters on card headers maintain the brutalist impact
file-fjpkexbjp2dfhprtk7mhwq
.
High-contrast Info Blocks: The progress display can employ brutalist info blocks: e.g., for each call status, use a white or light-gray background block with black border showing the call details, overlaying on a darker background. The transcripts UI had similar blocks for info (like the red blurred backdrop with white info cards inside
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 for training data info). We can do similarly: the Live Calls area could be a card with a semi-transparent background to stand out on the violet backdrop, containing inner bordered sections for each call (like [20†L6170-L6178] where stats were in a blurred panel). Actually, in swarms card, they have those translucent stat blocks
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, which is a nice style. We could mimic that for "CALLS: 5" and "SUCCESS: 60%" etc., but for now just straightforward.
Typography for Metrics: Use large numeric displays where possible. E.g., the number of calls or leads can be big to celebrate success. Possibly style "5 Calls" as text with class "text-4xl font-black". Surrounding context smaller. The stat cards in design system might be reused (StatCard component was used in various places
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). Actually, we could use a couple of <StatCard>s for final summary: one for Calls Made (with icon UilPhoneVolume
file-fjpkexbjp2dfhprtk7mhwq
), one for Interested Leads (with icon UilCommentAlt or UilThumbsUp). But that might break the narrative flow to suddenly show stat cards. Instead incorporate these stats into our summary card as text with icons.
Movement & Quirky Elements: We can lean into the "busy activity" vibe. For instance, might use the animate-pulse class on the status dot of an active call (like how active agent dot pulses green
file-fjpkexbjp2dfhprtk7mhwq
). For ringing, maybe animate an icon or wiggle a phone emoji. These small touches fit the playful brutalist approach.
Consistent Buttons: All calls to action (Launch Campaign, etc.) are big, bold, possibly with icons. For example, Launch Campaign button might show an analytics icon UilAnalytics as used in the searchcalled stub
file-fjpkexbjp2dfhprtk7mhwq
. Actually, in searchcalled they had "Configure Search" with an analytics icon
file-fjpkexbjp2dfhprtk7mhwq
. We could follow that: maybe a phone icon on the launch button. Keep button styling same: background Diala blue (0,82,255) or maybe a contrasting color like green to indicate go. But likely they used blue for key actions.
Notification of Steps: Possibly use step indicator (like OnboardingNav) here too if needed, but it's probably straightforward enough without. If used, update steps like "Criteria", "Agent", "Number", "Review", etc. Could help but might clutter UI since steps are many (maybe 5 visible steps plus progress). Could compress some logically (like treat search & call as one step in nav since they happen automatically after launch). Might omit nav for simplicity.
Emotion and Tone: Keep the copy confident and encouraging, reflecting the bold design. E.g., on launch: "Stand by as Diala hunts for prospects and dials out..." – kind of an excited tone. Brutalist design often pairs with straightforward, sometimes cheeky text. We can infuse a bit of excitement: e.g., when showing an interested lead: "🤝 Got one! Acme Corp wants a callback." (Using an emoji like handshake can be within style boundaries as a pop of personality.) Use sparingly to not deviate from professional feel, but one or two could lighten it.
State Management
currentStep (number): from welcome (1) to review (5). After launching, we might either set currentStep to 6 or just to a special "inProgress" state beyond the normal count, since it's not a typical step you navigate back from, it's a running state. But we could treat it as step 6.
Form fields: location, industry, keywords, includeLinkedIn (bool), searchDepth. These capture user input from step 2.
selectedCaller (agent or swarm identifier): could be something like { type: 'agent', id: 'a1' } or if swarm, { type: 'swarm', id: 's1' }. Or simpler, we can use one variable for either, since an agent and swarm have distinct id formatting or we keep a separate bool useSwarm. For front-end, maybe easier to maintain selectedAgentId and selectedSwarmId mutually exclusive (only one will be non-null). The UI ensures they pick one option or the other.
selectedNumberId (or the actual number string): from step 4. And possibly scheduleTime if they chose later (or a flag startNow).
campaignLaunched (bool): flips true once they hit Launch. This triggers the simulation or actual process.
Simulation state: If we simulate in front-end, we manage:
prospectsFound (array of objects or names) – each time we "find" one, we push to this array via setState, causing UI to list them.
calls (array of call objects) – each with prospect, status, outcome. As we simulate calls starting and ending, we update this array or specific call objects. Could hold only active calls, or active + finished. Perhaps have separate arrays activeCalls and completedCalls for clarity. Or one array with a status property and we render accordingly.
prospectCount, callCount, etc. – or derive from above arrays lengths.
If doing an automated timed simulation, might not need these to be reactive to user input, just internal for simulation logic. But to display dynamic updates, we store them and use re-renders.
If we attempted to do some real backend:
searchWorkflowId and status fields like earlier flows.
campaignId for the calling campaign.
But probably not for simulation.
We also track showAnalyticsModal or such if user triggers a deeper view. But likely not.
Back navigation states: allow editing previous steps, so keep input states around and just hide/show sections by currentStep. If currentStep changes, form states remain intact so user can modify and go forward again.
Possibly isCalling or callPhase to indicate we are in the calling phase (to maybe differentiate search vs call updates if needed). But can deduce from currentStep or launched flag.
If any error, e.g., no prospects found (in simulation we won't let that happen, but in real could), could have an error state or message. But in demo, skip error.
UX Interaction & Animations
Simulated Timing: We want a brisk but not instantaneous demo. Use short delays (maybe total of 15-20 seconds for the whole progress). That’s long enough to observe but short enough not to bore. E.g., find first prospect at 1s, then every 1s find another until 5. Start call at 2s, etc. If swarm selected, maybe show two calls at once after a few seconds, to illustrate parallel. We can use setTimeout or a series of them for each event, or a single interval that increments a counter step and triggers events at certain counts. Keep track to clear timers if component unmounted unexpectedly.
Scroll: If listing prospects or calls, ensure the container scrolls as items overflow (with a nice scroll style inside a card). Or auto-scroll to bottom as new items append, so the latest is visible. Use a ref to a list end and call scrollIntoView. Minor detail but improves feel of a live feed.
Call Status Transitions: Perhaps use color or icon changes: e.g., initially a call entry is gray "Dialing", then when "Talking", highlight it or show a small waveform icon. Then outcome with a colored badge. We might animate the change (fade from "Dialing" to "Talking", etc.). Could do a quick fade or slide. CSS transitions on text or a class change suffice.
Sound Effects (optional): If we wanted to be fancy, playing a subtle dial tone or ring sound during the demo could make it immersive. But likely avoid to not startle user or require audio permissions. Perhaps not.
Interactive during simulation: We likely won't allow user to intervene in calls (like no "hang up" or "pause" in onboarding). They just watch. That’s fine. Possibly provide a "Skip demo" or "Finish early" if they don’t want to wait all 20 seconds. Could just have a "Finish" button become visible after launch, which they can click to skip to summary. Not critical but user-friendly if someone is impatient.
Back disabled after launch: Once launched, going back doesn’t make sense as it’s in progress. We might lock navigation and hide or disable the Back button from that point. Or if we allowed schedule later and they want to change mind, theoretically they could go back before launching. But after launching, not.
Completing: When simulation is done, maybe briefly flash a "Campaign Completed" message or icon. Then auto-advance to final summary after a 2-second pause, giving them time to see the last events. Or directly show final summary as part of the progress section turning into summary. Could also scroll up or collapse the progress feed to focus on summary. Possibly we could overlay a semi-transparent celebration (like a checkmark or confetti) when done.
Confetti/Fireworks: Since this is the culminating multi-module demo basically, a tad of celebration could be warranted. E.g., a quick confetti burst when finalizing summary (choose a library or simple falling shapes as earlier idea). If other onboardings didn't do it, maybe we skip for consistency, but this is arguably the biggest "wow" moment, so a little confetti might be nice. Ensure it doesn't hinder reading summary.
Guide to Dashboard: The final step should clearly indicate where to go for more. Possibly highlight "Calls" in the side nav if we had control (like adding a glowing effect around the Calls icon in the sidebar). That would be a nice touch: since the user likely sees the dashboard layout (if onboarding is a page in the app), we could momentarily highlight the navigation item or relevant UI piece. But implementing that might be tricky in isolation, and maybe not expected. Instead, a textual prompt and a button should suffice.
Technical Implementation Details
We’ll implement the simulation timeline in the React component. For example:
jsx
Copy
Edit
useEffect(() => {
  if (campaignLaunched) {
    // simulate finding prospects and calling
    const events = [
      { t: 1000, action: () => addProspect("Acme Corp") },
      { t: 2000, action: () => { addProspect("Beta Inc"); startCall("Acme Corp"); } },
      // ... more events
    ];
    events.forEach(evt => {
       setTimeout(evt.action, evt.t);
    });
  }
}, [campaignLaunched]);
We define helper functions addProspect(name) (push into state) and startCall(name) (push into calls array with status "Dialing", then possibly schedule another event for call connected, ended, etc.). We chain timeouts to simulate call progression: e.g., after startCall, setTimeout in 2s to mark it connected, then in another 3s to mark ended with outcome. That nested but manageable for a few calls. Or store these events in the events timeline as well.
Ensure to clear timeouts if component unmounts using useEffect return () => clearTimeout for each or keep references. But likely not needed since user will either finish or navigate in app after. Still, best practice.
The final summary can be triggered by a final event in timeline (like at t=15000ms, set a state simulationDone = true or directly set currentStep to 7). Or we could derive simulationDone by seeing all calls completed if we track number to make vs made. Simpler: schedule a final event to finalize.
The timeline approach means adjusting if user picks swarm (calls concurrently) vs single (calls sequentially). For swarm, maybe we simulate two parallel calls at once. For sequential (single agent), one finishes then next starts. We can branch the event list depending on selected agent vs swarm. E.g., if swarm, events show two startCalls quickly and maybe overlapping durations. If agent, space them out. This is nuance but to reflect difference. Could do just 2 calls sequential for simplicity regardless, but showing parallel would highlight swarm benefit. Since we have swarms module done, user might try that. We'll implement a basic difference: if swarm chosen, start 2 calls around the same time.
If we ever integrated actual backend call placement (like to the user's phone as a demo), we would involve connecting to Telnyx and actually making a call. That’s heavy and risk of user not picking up or cost. So skip in onboarding.
For integrity, if we did call backend for search, we would have to feed results back to UI similarly, but it's easier to simulate that too. So no Convex calls for actual search. The only maybe Convex call we could do is to create a campaign entry if we wanted it to show up in their real calls list, but might be unnecessary. Perhaps we won't create actual DB records at all – onboarding can exist in a sandbox. That means after finishing, if user goes to Calls dashboard, they might not see the "demo campaign" in logs (which is fine, it was just a tutorial). However, one could argue it might be nice to actually create it so they see something. But since the demo was partial (maybe only 5 calls) and not real, leaving no trace might be cleaner to avoid confusion. We'll just tell them how to do it for real. If we wanted, we could create a dummy "Completed Demo Campaign" record in Convex, but it might clutter their actual analytics. I'd lean not.
So basically the onboarding calls no backend except maybe checking if they have an agent/number to list them. We can fetch agents and numbers via Convex queries at start (like for Swarms we did). If none found where expected, we adjust UI accordingly (like prompt to set up). But likely they do from earlier flows.
Integration with context: since the onboarding is part of app, we have access to whatever providers. The numbers might be in a store or not. If not, we can use a query or simulate one number if none. Possibly the settings page is where API keys/numbers are added, but on initial account maybe there's one default or user input some. If not, we might have to tell them to add one. But for onboarding, maybe we assume they've added a number (in reality, maybe they haven't, but it complicates flow to diverge into adding number). Alternatively, if none, allow them to proceed with a "demo number" that isn't real. We'll do that: show something like "Demo Line (virtual)" if no real number.
Ensure the component is resilient: if no agent, we might create a default. But since we had Agents onboarding, user likely did that first. If not, we should at least require an agent. Could detect and if none, show a message: "Please create an AI agent first to use in calls" with a link to Agents onboarding. But since the user specifically asked for an onboarding for calls, presumably the idea is they'd go through voice agent first anyway. We'll still handle gracefully by maybe creating a quick agent behind scenes named "Demo Agent" if absolutely needed (but that is messy). Simpler: if no agents, just instruct then exit or skip certain steps. But I'd expect sequence: voice agent onboarding -> transcripts (optional) -> swarms -> RAG -> calls. Not guaranteed though. Perhaps call onboarding should encourage using a swarm or agent created. We will assume at least one.
All in all, the calls onboarding logic is mostly front-end simulation orchestrated by timed events, integrated with existing data (agents, numbers) for user-specific context.
Integration Points
Agents & Swarms: The call onboarding directly uses the outputs of previous ones. The chosen agent or swarm in step 3 is exactly one the user configured. If they created a custom agent in Agents onboarding (with a certain persona), they will now use it here – making the earlier work tangible (integration of modules). If they created a swarm (with multiple agents), they can choose it, and the simulation will illustrate parallel calls, reinforcing why swarms matter. In summary, the onboarding flows aren’t siloed: this final Calls flow ties them together, showing the agent or swarm using presumably the knowledge base (if RAG was done) to handle calls, etc. We might mention implicitly that the agent could use the knowledge base – e.g., if they had done RAG, maybe mention on an answered call: "Agent used knowledge base info to answer a question" in passing. But that might be too detailed. Still, conceptually, yes integration: if an agent had ragSources, the call engine would query them to answer specific questions.
Phone Numbers (Telephony Integration): If the user had added multiple numbers in settings (maybe local vs toll-free, etc.), they appear here. If not, the system might have assigned a default. The integration with Telnyx is underlying but not directly visible except via the number selection. Later, on the Calls dashboard (numbers tab), they can manage these numbers (like purchase new, etc.). Onboarding touches just the selection. If the user chooses an inactive or maintenance number (in data, one was maintenance
file-fjpkexbjp2dfhprtk7mhwq
), ideally we filter those out. We'll assume active.
Calls Dashboard: After onboarding, if the user goes to the Calls dashboard, they might expect to see evidence of what happened. As decided, we probably won’t create actual logs, so they might see nothing there except their agent stats (calls=0 still). To prevent confusion, our final text should clarify the campaign was a demo. Possibly encourage them to run a real one. Or as an alternative, we could allow them to actually finalize the config into a real campaign. For example: after the demo, we could say "Now that you've seen how it works, do you want to run this campaign for real?" If yes, then actually use the same criteria to start a real search & call in the background. That could be an advanced option – might overwhelm. But could be offered if keys/numbers are all set. Might skip. Just signpost that to do it for real, they can go to Calls and launch a campaign (maybe by using the same form there). Actually, the calls page might have the hunts/campaign UI. The code suggests it has a tab for calls and possibly an area for launching new ones (the modals we saw). The HuntConfigurationModal and workflow modals in code likely handle the real thing. So the user will use those next time.
Analytics and transcripts: The calls module integrates with transcripts (recordings) and analytics. We likely mention transcripts (maybe "transcripts of each call are saved in the Transcripts section") tying that piece in. And analytics (like success rate, average call time in Agents or overall in Calls). For integration, possibly after running some calls, the agent's success rate stat could update. But since we didn't actually update DB, in real usage it would. The final summary could hint "Your agent's performance metrics will update based on these calls, visible in Agents Analytics." So it's known that these modules feed each other.
External CRM or follow-ups: Not directly shown, but an integration could be exporting leads or scheduling follow-ups. If Diala had a CRM integration, it might push the interested lead info somewhere. Out of scope for onboarding though.
Ending Conditions: The user might wonder, does the campaign stop automatically or run continuously? We should integrate that knowledge: probably it stops after finishing found leads. If user wants to stop early, the real system likely has a stop button in the Live Monitor. In onboarding, we didn't simulate that interaction. But maybe mention "You can pause or stop campaigns anytime from the dashboard." to integrate that piece of UI concept.
Data Persistence: The prospect search might produce data (the found leads). In a real scenario, the system might store those leads in a database or at least as part of the campaign record (for future reference or reuse). We are not doing that in onboarding, but the integration point exists – if user runs an actual campaign after, those leads would possibly appear in a table in the Calls section (maybe under a "Recent Calls" or some CRM-ish list). The onboarding could mention "All contacts dialed will be recorded, so you can review and export them." if true. But if not implemented fully, skip.
Wrap Up Integration: The calls onboarding is basically the culmination of all previous features working together – it's where Agents, Swarms, RAG, etc., come to life. So it inherently integrates everything. Our narrative and UI hints can point out these ties. E.g., if RAG was done, maybe in a call we simulate the agent giving a detailed answer thanks to the knowledge. But that might be lost on users who didn't do RAG. So maybe not mention it explicitly; but if the user did do RAG, they'd probably connect the dots themselves (and if they didn't, mention might confuse). So best to not mention RAG in call onboarding text unless sure. Possibly neutral mention: "Your agent can answer detailed questions thanks to its training" – that could apply generally (training could mean transcripts or RAG). We do have to keep it accessible even if they skipped transcripts or rag.
Future Real Integration: If the user runs a real campaign after, the flow would be similar but possibly slower (calls take minutes, etc.). The onboarding prepared them for what to expect, making the actual process less opaque.
Future Enhancements
Personalized Demo Calls: As touched on, a possible future feature is letting a new user receive a demo call from their AI agent. For example, after setting up, instead of a simulation in-app, ask "Would you like to receive a demo call from your AI agent right now at your phone?" If they enter their number and accept, the system could actually call them using the configured agent to run through a short script. This is a powerful demo because hearing the AI voice live is convincing. However, it requires a phone number, user consent, and uses actual telephony credits. As an enhancement, it could be offered if the user profile has a verified phone. It's a more visceral integration of the technology into onboarding.
Campaign Templates: Provide templates for common campaigns (similar to how we discussed swarm templates). E.g., "Follow-up Campaign" or "Welcome Call Campaign" pre-filling certain parameters. Onboarding could ask "What kind of campaign do you want?" up front, and if they select a template, many fields auto-fill (maybe skip search if they already have contacts, or use certain keywords). Not in initial, but a future user who logs in might select a template and launch a campaign in one minute. Onboarding could then adapt to show only minimal steps for that template.
Integration with CRM/Contacts upload: In future, not all campaigns will involve searching for leads; some might call existing customer lists. Onboarding in the future might branch: "Do you want to find new prospects or call your own list?" If the latter, they'd upload contacts or select from an integrated CRM. We could add a step to import a CSV of phone numbers for example. This would showcase versatility. It's more complex, so for now we focused on search. But adding that path eventually will cover more use cases.
More Realistic Call Simulation: Later, we might incorporate actual transcript segments or TTS in the onboarding to let user hear how an AI call sounds. Perhaps an animation of a waveform when agent speaking, and text of response. Or even audio playback if they click. This could be a built-in recording rather than generating on the fly, just to show quality of AI voice. It'd impress users but need audio handling. Possibly a "Listen to sample call" button at the end as an extra.
Dashboard Tour Mode: After campaign creation, the onboarding could seamlessly transition into a mini tour of the Calls dashboard (highlighting the Live Monitor tab, Analytics tab). This would directly integrate training with the actual interface. E.g., overlay arrows or highlights on the actual dashboard elements. This crosses from the "onboarding flow" into in-app guidance territory (like guided tours). Could be valuable but requires a framework to highlight UI elements. A future enhancement could unify these, but for now our flows have been separate pages. In lieu, textual guidance and a button to go see the results is what we do.
Continuous Onboarding Flow: Perhaps envision an even larger flow that ties all modules sequentially. E.g., a new user could optionally go through voice agent -> RAG -> swarm -> calls in one guided sequence (with ability to skip parts). This would be a meta-onboarding. Currently they are separate flows, but one could imagine combining them with branching. It's advanced but maybe later a "Setup Wizard" that covers everything in an optimal order. Our separate blueprints could then be interconnected steps of one super-flow. Implementation aside, conceptually it may help some users who want a full setup at once.
Monitoring & Tuning: A possible advanced onboarding extension is after a few calls, showing how to adjust on the fly. For instance, if the campaign isn't yielding success, how to tweak criteria or switch agent. That might be beyond initial onboarding, but for continuous improvement. Possibly an in-app hint later, not in first-run onboarding.
Gamification: Could add some gamification: e.g., show an achievement "First Campaign Launched!" and track how many leads/ calls achieved. These sort of things (badges, progress) encourage usage. Not directly part of onboarding blueprint, but an idea for user retention.
Multi-language or International campaign settings: If expanding globally, onboarding might ask for timezones, multi-lingual agent usage, etc. Could be future fields (like if calling different countries, choose agent languages accordingly). Now likely not needed.
Incorporating Feedback: After calls, maybe prompt user "How did your agent do on these calls?" gather feedback to refine agent prompt. That closes a learning loop. Possibly later a quick thumbs up/down after hearing a sample call.
All these enhancements aim to deepen the user's engagement and success with the Calls module after the initial onboarding, ensuring they not only try it out but also incorporate it effectively into their workflow.The following sections provide detailed onboarding tendril blueprints for each major module in the Diala dashboard. Each onboarding flow is designed as a self-contained, product-like experience that introduces the module’s capabilities, guides the user through setup, and seamlessly ties into Diala’s backend infrastructure. The design follows Diala’s neobrutalist aesthetic – bold typography, high-contrast color blocks, heavy borders, and slightly askew card layouts – ensuring a consistent look and feel across the app. Each blueprint covers the module’s Purpose, step-by-step Flow, key Frontend Components, Backend Architecture (Convex functions and external API calls), references to the Design System, State Management approach, UX Interaction details, Technical Implementation, system Integration Points, and potential Future Enhancements.
Agents Module Onboarding Tendril
Purpose
The Agents onboarding flow introduces new users to creating and configuring an AI Voice Agent – a virtual caller that can engage customers. It serves as a lead funnel by demonstrating how easily a custom voice agent can be set up and deployed. The tendril’s goal is to have the user create their first agent profile (with a persona, voice, and language) and understand the agent’s role in Diala. This guided setup connects to Diala’s core systems by actually saving the agent configuration to the backend and preparing it for use in calls. By the end, users see a preview of their agent in action (e.g. a simulated call greeting), which both educates and entices them to continue using the platform.
Flow Steps
Welcome & Agent Naming: The user is greeted with a welcome card on a distinctive background (e.g. vibrant blue, consistent with the voice agent theme
file-fjpkexbjp2dfhprtk7mhwq
). They’re prompted to enter their name or company name, which personalizes the experience (“Welcome, [Name]! Let’s set up your first Voice Agent.”). This step hooks the user and transitions into agent creation.
Define Agent Profile: The user inputs basic details for the agent: Agent Name (e.g. “Diala-Tone” or a custom name) and Purpose (a high-level role like Sales, Support, etc.). This might be presented on a tilted card with input fields (using the design system’s <Input> component for text
file-fjpkexbjp2dfhprtk7mhwq
). For guidance, the UI can show an example agent profile snippet (name, purpose, short description) in a stylized way. A Continue button (disabled until required fields are filled) moves to the next step.
Select Voice & Language: The user chooses the voice characteristics for the agent. This includes selecting a Language (with accent) and a Voice Style. For example, they might pick English (US) vs English (UK), and a voice persona like Friendly or Professional. The UI could display a grid of voice cards: each card showing a language flag and voice name. When a card is selected, it gets a bold black border and maybe a subtle pulse animation. In parallel, the user selects the Voice Provider or TTS engine if applicable (e.g. a default like ElevenLabs or an open-source voice). Both selectedLanguage and selectedVoiceAgent state must be chosen before continuing (the Next button becomes enabled only when both are set
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). This step leverages the neobrutalist style by showing each option as a chunky card or button with high contrast (e.g. a button with a background color representing language, and an icon like UilLanguage for languages
file-fjpkexbjp2dfhprtk7mhwq
). Audio previews can be offered – e.g. a small play icon to hear a sample of the voice – using a <Button size="icon"> with the UilPlay icon.
Configure Agent Behavior: The user configures the agent’s behavior and knowledge. This includes setting a Personality/Pitch and connecting any knowledge base. For Personality, the UI might offer presets like “Customer Support – Empathetic” or “Sales – Persuasive”, which map to different system prompts and tonalities. (In code, this was indicated by selectedPitch such as “customer-support”
file-fjpkexbjp2dfhprtk7mhwq
.) The design can use another grid of cards for tone/pitch options, each with an icon (e.g. UilCommentDots for conversational tone). The user also sees an option to attach a Retrieval-Augmented Generation Knowledge Base to the agent – for example, a toggle or button to link to RAG content. If they have no custom data yet, this section will be informational: “Your agent will use general knowledge and example scripts. (Integrate custom knowledge via AutoRAG later.)” If a knowledge base exists, they can select it here (the UI might list available RAG workflows or documents).
Summary & Launch: A summary screen appears on a rotated card that recaps the agent’s settings – e.g. “Agent: Diala-Tone\nLanguage: English (US)\nVoice Style: Professional\nPurpose: Sales & Discovery\nTone: Friendly” – styled in bold text with colored badges or icons next to each attribute (language flag, voice icon, etc.).
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The user confirms everything looks good. On confirmation, the agent profile is saved via a Convex function (e.g. createAgent mutation on the backend). Right after saving, the onboarding triggers a quick demo: the agent “calls” the user. For instance, a modal or next screen shows a Call Simulation – the agent’s avatar and name, a phone icon, and a transcript bubble where the agent says a greeting (“Hello, this is Diala-Tone, your new AI sales agent. Ready to make some calls!”). If audio is available, it plays using the chosen voice. This delightful finish demonstrates the agent in action and closes the loop on creation.
Components (Frontend)
WelcomeCard: Reused from existing onboarding components (like the one used in Voice and RAG flows) to capture the user’s name
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. It’s a branded card with input and a bold welcome message.
Card & CardContent: The design heavily uses the <Card> component with <CardHeader> and <CardContent> from the design system
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Each step’s UI is typically contained in one or more cards that may be rotated slightly (using utility classes like rotate-1 or -rotate-1 for a playful effect). For example, the profile form appears in a card with a slight tilt and thick black border.
Input Fields and Buttons: The agent profile form uses <Input> for text (name, purpose) and <Textarea> if a longer description is needed. These inputs use the neobrutalist style – e.g. tall input boxes with a 4px black border and bold placeholder text
file-fjpkexbjp2dfhprtk7mhwq
. Buttons throughout are high-contrast and bold: primary actions in bright colors (e.g. yellow or Diala blue) with uppercase labels and black text for contrast
file-fjpkexbjp2dfhprtk7mhwq
. Icons from Unicons (like UilPlus for add, UilArrowRight for continue) are placed inside buttons or headings to add visual cues.
Agent Preview Card: A custom component (could be similar to an AgentCard used in the dashboard
file-fjpkexbjp2dfhprtk7mhwq
) shows a mini profile of the configured agent. It might display the agent’s name, an avatar (perhaps a robot icon like UilRobot in a colored circle
file-fjpkexbjp2dfhprtk7mhwq
), and tags for language, voice, etc. This appears in the summary step to review settings.
OnboardingNav (Step Indicator): A step navigation UI at the bottom or top shows progress (e.g. “Step 3 of 5: Voice & Language”). This can be implemented by the <OnboardingNav> component similar to what the voice onboarding used
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. It displays steps as numbered circles or tabs, with completed ones filled in (using background color and a checkmark overlay)
file-fjpkexbjp2dfhprtk7mhwq
, and the current step highlighted (scale up and drop-shadow effect in CSS
file-fjpkexbjp2dfhprtk7mhwq
). Users can see how many steps remain and click on previous completed steps to review or edit (the nav allows backward navigation by calling onStepChange for earlier steps
file-fjpkexbjp2dfhprtk7mhwq
).
Modal/Dialog Components: If the final demo is shown, a modal overlay might be used – e.g. a <CallSimulationModal> (similar in structure to the provided CallAnalyticsModal or LiveCallMonitorModal used elsewhere
file-fjpkexbjp2dfhprtk7mhwq
) that appears with the agent on a “call.” Alternatively, the simulation can be inline on the final screen with a phone UI card.
Backend Architecture (Convex & APIs)
Convex Functions: When the user completes the agent setup, a Convex mutation (e.g. api.agent.createAgent) is invoked to save the agent profile to the database. This would include fields like name, description, language, voice settings, system prompt (derived from purpose/tone), etc., as seen in the voiceAgents data structure
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The Convex backend ensures the agent is persisted and accessible across the app (e.g. it will show up in the main Agents dashboard list and be used in calls).
AI Voice Services: If the onboarding demo includes generating an audio greeting, backend calls will be made to external APIs. Specifically, the system would use the chosen TTS (Text-to-Speech) provider to synthesize the agent’s greeting. For example, if ElevenLabs is configured (noted by an API key in settings
file-fjpkexbjp2dfhprtk7mhwq
), the app would send the greeting text and voice parameters to ElevenLabs API and stream back audio. Similarly, if the demo included an AI-crafted greeting text, an OpenAI API call could produce it (using the agent’s system prompt as context). For simplicity, the greeting might be pre-scripted, but a more dynamic approach could involve an OpenAI Chat completion to produce a personalized welcome line.
Convex Real-time updates: Although not critical in onboarding, if any step needed to fetch options (say available languages or voices), a Convex query could supply those. In our case, language and voice lists are mostly static, so they can be hardcoded or fetched from a static endpoint. However, linking to RAG sources might involve a Convex query to list available knowledge bases (RAG workflows stored in Convex) and a mutation to attach one to the agent profile.
Data Model: The new agent’s data structure includes performance stats and lastActive timestamps as placeholders
file-fjpkexbjp2dfhprtk7mhwq
 – these will initially be empty or default. The creation function may initialize some of these (e.g. set status: 'active' and zero counts). This integration ensures that immediately after onboarding, the user can navigate to the Agents dashboard and see their new agent listed with default metrics.
No Calls in Onboarding: Importantly, the “simulated call” at the end is not a real phone call but a local demo. Thus, no actual telephony API (like Telnyx) is invoked during onboarding (avoiding costs or complexities). The real Telnyx integration (for actual calls) happens when the user later uses the Calls module or playground. The onboarding just hints at that by using a fake call interface.
Analytics Hook: Optionally, the onboarding could log an event via Convex or an analytics service indicating a new agent was created (useful for the app to track conversion from onboarding to actual usage).
Design System & Neobrutalist Elements
The Agents onboarding UI adheres to the same design principles found elsewhere in Diala’s app for consistency and brand identity:
Typography: All titles and labels use the Noyh Bold font (imported at app level
file-fjpkexbjp2dfhprtk7mhwq
) in uppercase with heavy weight
file-fjpkexbjp2dfhprtk7mhwq
. For example, step titles like “Voice & Language” or the final “Agent Ready” are rendered in large, black uppercase text for emphasis. Supporting text uses bold sans-serif in slightly smaller size (often gray for secondary text).
Color Blocks: Each step likely features a signature background color overlay. Since Voice Agent is core to Diala, a blue theme (RGB 0,82,255) was used in the main onboarding menu
file-fjpkexbjp2dfhprtk7mhwq
 and could carry into this flow. We’ll use a blue backdrop with a subtle grid pattern (achieved by CSS gradients as seen in other onboardings
file-fjpkexbjp2dfhprtk7mhwq
). Cards themselves use white or light-gray backgrounds, with splashes of color for icons or badges (e.g. yellow or purple badges for language/purpose). All colors are high-saturation and flat, aligning with neobrutalism.
Borders & Shadows: All interactive elements (cards, buttons, modals) have thick black borders (4px) and offset shadows. For instance, cards use border-4 border-black with a drop shadow that mimics a hand-drawn offset (e.g. shadow-[6px_6px_0_rgba(0,0,0,1)] on hover to deepen the effect
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). This gives a slightly 3D, layered look. Inactive states might use lighter shadows (4px offset) and hover states increase to 6px or 8px, as consistently done in the app
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
.
Playful Layout: Many cards are deliberately rotated a few degrees (transform rotate-1 or -rotate-1)
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 to convey a creative, less regimented vibe. In this onboarding, perhaps the welcome card is rotated one way, the next form card rotates opposite, etc., alternating for each step. This matches the style in the transcripts onboarding where multiple cards have alternating rotations
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Despite the skew, alignment of form elements inside is maintained for usability.
Icons and Badges: Unicons icons are used generously to label sections (e.g. UilRobot for Agent info, UilMicrophone or UilVoice for voice selection, UilSetting or UilSlider for settings step)
file-fjpkexbjp2dfhprtk7mhwq
. Icons appear in solid color blocks with black outlines to form “neo-brutalist badges.” For example, a small square with a black border containing a yellow background and a black icon can precede a heading like “Select Language” – reinforcing meaning with color (yellow often used for knowledge/RAG, blue for voice, etc.). Badges in text (like showing agent status or language) are styled via the design system’s <Badge> component with custom classes for border and background
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
.
Overall, the design ensures the agent onboarding feels like part of the same family as the Voice, RAG, and Transcripts flows – bold, engaging, and a bit quirky – while clearly communicating the function of each UI element.
State Management
The onboarding component uses React useState hooks to manage form data and step transitions. Key state variables include:
currentStep (number): Tracks which step of the flow the user is on (initially 1). The component conditionally renders different content based on this
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The state is advanced via setCurrentStep(n) when the user clicks “Continue” on a step after validation.
Form fields like agentName, agentPurpose, selectedLanguage, selectedVoice, selectedTone: Strings (or IDs) captured from user input. For example, when the user types the agent’s name, the agentName state updates onChange. Selecting a language sets selectedLanguage (toggling logic ensures clicking the same option twice can deselect if needed
file-fjpkexbjp2dfhprtk7mhwq
). These states enable/disable the Next button for that step (e.g., require non-empty name, or require both language and voice selected as seen with combined condition !selectedAudio || !selectedLanguage in the Voice flow
file-fjpkexbjp2dfhprtk7mhwq
 – similarly, Agents flow will ensure required inputs are present).
showDemoModal (boolean): Controls whether the final call simulation modal is shown. Initially false, it flips to true once the agent is saved and the user triggers the demo. The modal component likely uses its own internal state for things like playback progress or closing.
Possibly isSaving or isLoading: A flag while the agent profile is being saved to the backend. Upon clicking “Finish” on the summary, isSaving can be set true and a loading spinner or a disabled state shown on the button to indicate processing. Once the Convex function returns success, isSaving goes false and we proceed to demo.
We may also maintain an object like formData that aggregates all agent fields, but given the few inputs, individual useState hooks are sufficient. Alternatively, a single useReducer or form library could manage the multi-step form data, but that might be overkill here.
The state is lifted within the onboarding component itself since it’s a dedicated flow. If needed, some global state (context or store) could provide user-specific info like the user’s organization or list of existing knowledge bases for RAG selection, but such data can also be fetched on the fly via Convex queries at the moment of needing them.
UX Interaction & Animations
This onboarding emphasizes a smooth, instructive user experience with interactive feedback at each step:
Progressive Disclosure: Only one step’s inputs are shown at a time, reducing cognitive load. The transition from one step to the next can be enhanced with a brief animation – e.g. the current card slides out or fades, and the next card slides in. The neobrutalist style can be extended to animations by making them slightly bouncy or offset (to match the playful theme).
Button States & Wobble: To encourage user action, the “Continue” button on each step might use the wobble animation (keyframes) once the step’s requirements are met. In the voice onboarding code, for example, the next-step button gained a wobble effect when prerequisites were selected
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We can apply similar here: after the user fills agent name and purpose, the “Next” button could wobble to draw attention. Likewise, on the voice selection step, once a language and voice are picked, the continue button pulses or shakes lightly
file-fjpkexbjp2dfhprtk7mhwq
. These animations use CSS (@keyframes) and inline styles toggled by state conditions.
Visual Feedback on Selection: Selecting a voice or language card provides immediate feedback: the card’s background may turn a brighter shade, an icon or border indicates selection (for instance, a checkmark or the border color changes from black to a highlight color). The code’s isSelected prop for language options shows a possible pattern
file-fjpkexbjp2dfhprtk7mhwq
. We’ll implement it such that clicking a card toggles its isSelected state and triggers a re-render with the new style (and maybe a short scale-up animation to emphasize activation).
Modals & Overlays: The final call demo uses a modal overlay that darkens the background (e.g. semi-transparent black overlay) and pops a centered card mimicking a phone call UI. The user can interact by pressing a “Play” button to hear the greeting again, or “End Demo” to close. This modal respects the design system (close button is a bold “X” icon, and the modal card has the thick border and drop shadow). It also ensures focus trapping (so that keyboard users are kept within the modal until it’s closed).
Error Handling: If any backend call fails (e.g. the agent save or the TTS fetch), the UI will show a brief error message – likely in a modal or banner. For instance, if saving fails, we might keep the user on summary step and show a red-outlined alert card saying “Error: Could not save agent. Please try again.” with a retry button. Errors during the demo (like audio failing to load) might be caught and shown as a notification (“Demo unavailable right now”). These edge cases ensure the user isn’t left confused if something goes wrong.
Skip Option: Although not always necessary, a “Skip Demo” or “Finish Setup” option can be provided for users who don’t want to go through the entire flow. This would likely appear on the summary screen. If clicked, it would still save the agent and then navigate the user to the main dashboard (Agents list) without showing the call simulation. It’s a small UX consideration to respect user’s time.
Throughout, the interactions aim to educate (by guiding input choices and showing outcomes) and delight (with fun visuals and a gratifying final preview of their AI agent coming to life).
Technical Implementation Details
Under the hood, the Agents onboarding is implemented as a React functional component (e.g. AgentsOnboarding.tsx) within the Next.js app, likely under the /onboarding/agents route. Key technical aspects include:
It is a client-side component ('use client' at top) because it manages interactive state and uses hooks, similar to other onboarding pages
file-fjpkexbjp2dfhprtk7mhwq
. This means it’s part of a Next.js page that doesn’t use Server-Side Rendering – appropriate since the content is highly dynamic and user-specific.
Routing & Isolation: This onboarding flow is separate from the main dashboard. It can be launched from a welcome screen (like the “Choose Your Path” menu
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). That screen routes to /onboarding/voice, /onboarding/rag, etc., and we would add a tile for Agents if not already present. Because it’s self-contained, users can experience it without having any agents; it essentially acts as a wizard that will populate the Agents module. It could also be re-used as a “Add New Agent” wizard if needed (triggered from the Agents dashboard via a “Create Agent” button
file-fjpkexbjp2dfhprtk7mhwq
).
Reusing Components: We leverage some existing components from the codebase to avoid reinventing the wheel. For example, the <CreateAgentModal> logic (if it exists) could be repurposed within the flow. In the provided code, there is create-agent-modal.tsx (likely a form for adding an agent)
file-fjpkexbjp2dfhprtk7mhwq
. Instead of using it as a modal, we can extract its form fields and validation logic for our multi-step process. Similarly, the <AgentCard> component (used to display agent info on the dashboard
file-fjpkexbjp2dfhprtk7mhwq
) can be used to render the preview in the summary. This reduces duplication and ensures consistency – the agent created in onboarding will look the same as those in the main app.
Convex Integration: We use the Convex React client, which is already set up in the app’s context (<Providers> component wraps the app with ConvexProvider
file-fjpkexbjp2dfhprtk7mhwq
). To call the backend, we use the useAction or useMutation hook from Convex. For example: const saveAgent = useAction(api.agents.create);. On the final step, await saveAgent(agentData) is called. The Convex function agents.create would handle inserting into the database. If Convex returns the new agent’s ID or object, we could use it to update local state or navigate to the agent’s detail page post-onboarding (for now, probably not needed during onboarding, but available).
Audio Playback: To implement the voice demo, the component may use the Web Audio API or simply an HTML5 <audio> element. The audio source would be the URL or binary returned from the TTS API call. A simpler approach: call a backend endpoint (Convex or Next.js API route) that returns an audio file URL after generating it. Then set that URL as the src of an audio element in the modal and call audio.play(). The UI includes controls for play/pause (maybe using a custom <Button> with play/pause icons UilPlay/UilPause for styling
file-fjpkexbjp2dfhprtk7mhwq
, but under the hood controlling the audio element). We also handle the cleanup – when the modal closes, we stop the audio and release the object URL if one was created.
Testing & Edge Cases: We ensure the multi-step form has validation at each step (e.g., required fields not empty, selections made). We also ensure that going “Back” to a previous step (if we allow it via the step indicator or a Back button) repopulates the form with the earlier inputs (since state is still retained). The component’s state persists as long as the user is on the onboarding route; if they refresh, they’d start over (which is acceptable). If needed, we could persist partial progress in localStorage or Convex to resume, but given the short flow, that’s optional.
Completion Path: Upon finishing, beyond showing the demo, we’ll likely programmatically navigate the user to the main app (maybe to the Agents dashboard page) when they exit the onboarding. This can be done with Next.js useRouter() to push a new route (e.g., router.push('/dashboard/agents')). We saw similar usage on the main onboarding selection page to route to the chosen path
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Here, after agent creation, a call like router.push('/dashboard/agents?new=1') could be used to both navigate and perhaps trigger a highlight on the new agent.
By engineering the onboarding in this way, we ensure it’s not just a dummy tutorial – it actually creates data and uses the real services, making the experience authentic. Yet it’s contained enough that a user can go through it without prior setup and end up with a tangible result (their custom AI agent ready to work).
Integration Points
The Agents onboarding ties into Diala’s broader infrastructure at several points to make the experience meaningful:
Agents Dashboard: The primary integration is that the created agent becomes part of the main Agents module. After onboarding, the new agent appears in the “Active Voice Agents” list on the dashboard, complete with the stats (mostly zeros or default) and status active
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The agent’s configuration (prompt, voice, etc.) is stored such that if the user opens the Agent Detail Modal later (accessible via the Agents page
file-fjpkexbjp2dfhprtk7mhwq
), they will see what they set up during onboarding. In this way, the onboarding acts as an alternate entry to the same agent creation process that an advanced user could do manually.
Calls Module: Any agent created is immediately available for use in calls and campaigns. For example, if the user proceeds to the Calls dashboard and starts a new call or campaign, they can select this agent to handle calls. The agent’s convexEntryPoint or identifier might be used by the call system to invoke the right logic (we see something like convexEntryPoint: 'agents.salesAlpha' in the calls data
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, implying each agent may have an identifier used in call workflows). Our created agent would have its unique entry point.
RAG (AutoRAG) Integration: If the user attached a knowledge base or if Diala automatically uses YouTube transcripts to train agents, that connection happens via the agent’s profile. The example agent data has a ragSources array listing modules or workflows
file-fjpkexbjp2dfhprtk7mhwq
. In onboarding, we might not fill this unless the user explicitly chose a knowledge base. But the structure is there – if later the user runs an AutoRAG workflow to build a knowledge base, they could link it to the agent (likely through an agent settings modal). Conversely, if this agent had been created by an advanced user, they might have chosen some default content to train on. The onboarding could behind the scenes flag that this new agent should train on some basic YouTube transcript set for their industry – for instance, if they chose Sales purpose, the system might automatically kick off a training job using a preset YouTube playlist for sales calls. (This is a speculative integration: to truly feed into backend workflows, an event could be emitted to start an AutoRAG job for the agent).
External Telephony: While no real call is made in onboarding, the agent is configured with everything needed to perform on actual calls. This means the conversation engine (OpenAI GPT model) and TTS/STT services can already work with it. When a real call comes in or out with this agent, the call pipeline will load the agent’s profile from the database (including voice settings and prompt) and spin up the necessary services (e.g. allocate a Deepgram transcription stream and an ElevenLabs voice for it). In short, the output of onboarding is not just UI fluff – it directly feeds the runtime behavior of the voice agent.
Design System & Theming: On a more front-end integration note, the onboarding uses the same design system components (cards, buttons, etc.), which means any updates to global styles (like a change in the Button component styling) will automatically reflect in the onboarding. For example, if the team updates the <Button> to a new hover effect, the onboarding’s buttons (being instances of that component) get the update too. This tight integration ensures consistency and reduces maintenance.
Analytics & User Onboarding Funnel: From a product perspective, this module ties into the user onboarding funnel. Likely, completion of the Agents onboarding is a key milestone (it shows the user has created an AI agent). This event might be tracked via an analytics integration (Segment, Mixpanel, etc.). If the platform has a “Setup Progress” checklist, the event of agent creation could check off an item like “✅ Create your first Voice Agent.” This is not a direct user-facing integration, but worth noting as part of the holistic experience.
Future Agent Enhancements: The agent created here will also benefit from any future system improvements. For instance, if later the platform adds an Agent Training feature (where the agent improves from call transcripts), all agents including this one would be part of that. The onboarding doesn’t need to handle that now, but it means the agent’s ID and records are ready to accumulate call data, training data, performance metrics (success rate, avg call time updated over time
file-fjpkexbjp2dfhprtk7mhwq
), etc., as the user uses it in real calls.
Future Enhancements
In future iterations of the Agents onboarding, several improvements and extensions could further increase its power and appeal:
Deeper Customization Steps: Additional steps could be introduced for fine-tuning the agent. For example, a step to upload a custom script or greeting for the agent (if the company wants a specific intro line), or to choose a persona image or avatar that will represent the agent in dashboards and calls. Currently, agents likely use default icons
file-fjpkexbjp2dfhprtk7mhwq
, but custom avatars (even just an emoji or generated icon) could make them more relatable.
Interactive Voice Tuning: We could let users test the voice during onboarding. A mini step where after selecting voice and language, the user can type a sentence and hear it spoken in the chosen voice. This try-and-adjust loop would utilize the TTS and allow them to toggle voice settings (pitch, speed) if the system supports it. It would showcase the realistic voice capabilities (one of Diala’s selling points is “realistic background sounds” and presumably natural speech
file-fjpkexbjp2dfhprtk7mhwq
). If background noise effects are available (as hinted by “background sounds”
file-fjpkexbjp2dfhprtk7mhwq
), the onboarding could allow choosing a background ambience for calls (e.g. office noise vs. quiet studio) to simulate realism – a fun feature for power users.
Agent Skill Configuration: Beyond purpose, let users pick “skills” or scenarios the agent is optimized for. For instance, checkboxes for Handling Objections, Appointment Setting, Demo Scheduling. These could correspond to different preset prompts or connect to specific RAG knowledge packs. In code, we see ragSources referencing things like "Objection Mindset" or "Price Objections" modules
file-fjpkexbjp2dfhprtk7mhwq
. A future onboarding step could list such modules for the chosen purpose and let the user toggle them. The agent would then automatically have those knowledge packs linked. This makes the agent immediately more powerful and tailored.
Integration with Contact Data: As Diala evolves, the onboarding might import some user-specific context. For example, if the user’s company has a set of FAQs or a product catalog, an advanced onboarding could prompt: “Want to train your agent on your own data? Upload a PDF or enter a website.” This would effectively initiate an AutoRAG process during agent creation. It blurs the line between the Agents and AutoRAG modules, but if done in a guided way, it could be extremely sticky – the user sees their agent learn their material in real-time. This might be a branching path: a simple agent creation vs. an advanced one with custom data ingestion.
Multi-Agent Onboarding: Down the line, if a user needs a team of agents (for different roles), the onboarding could allow creating multiple agents in one flow. For example, after creating one agent, it asks “Do you also need a support agent?” and could quickly replicate the process with slightly different defaults. However, this might be beyond the initial scope – typically one agent is enough to show value, and the Swarms module covers grouping multiple agents.
Guided Training Mode: Provide an option at the end of onboarding to “Practice a call with your agent.” This would switch to a Playground or test call mode where the user can speak or type and the agent responds, without involving actual phone lines. It’s like a training sandbox so the user gains confidence in the agent. This could reuse the Playground infrastructure in a focused way. It’s a logical next step after hearing the greeting: let the user try a sample conversation and give a thumbs-up/down if the agent’s response was good. Those signals could in the future fine-tune the agent (via reinforcement learning or by adjusting settings).
Onboarding Hints in Dashboard: Once the agent is created, subtle hints or tooltips in the main dashboard could appear (just-in-time education). For example, on the Agents page, highlight the “Active Voice Agents” card
file-fjpkexbjp2dfhprtk7mhwq
 and say “Here’s your new agent. Click to view details or edit.” On the Calls page, highlight how to start a call with that agent. This isn’t part of the onboarding UI itself, but a continuation of the user journey. Implementing this might involve setting a flag that the user completed agent onboarding and then showing a one-time tooltip in relevant places.
By implementing these future enhancements, the Agents onboarding would not only create an agent but also ensure the user fully understands and utilizes it, increasing the likelihood of Diala’s adoption in their workflow.
Swarms Module Onboarding Tendril
Purpose
The Swarms onboarding flow guides users through creating and managing Agent Swarms, which are groups of AI agents working in concert. In Diala, swarms represent powerful teams of voice agents deployed for coordinated campaigns (e.g. a sales “swarm” making calls in parallel, or a support “swarm” handling a surge of inquiries). This onboarding acts as a self-contained feature to set up a swarm from scratch, showcasing how grouping agents can amplify results. It serves as a lead funnel by illustrating a high-impact use case: rather than a single agent, imagine a “battalion” of agents – this drives home the scale Diala enables. The flow’s purpose is to get the user to create their first swarm (naming it, choosing its mission, and selecting member agents), while seamlessly plugging into backend systems by actually provisioning that swarm (persisting in the database and ready to be used in campaigns). In doing so, it also educates the user on the benefits of teamwork among AI agents (coordination, aggregated analytics, etc.).
Flow Steps
Welcome & Concept Intro: The user is welcomed with a card on a vibrant purple backdrop (purple is used in the app to denote swarms or grouping
file-fjpkexbjp2dfhprtk7mhwq
). A bold title might say “Agent Swarms Setup” with an icon like UilLayerGroup (stack of layers) to signify a group
file-fjpkexbjp2dfhprtk7mhwq
. The welcome message acknowledges the user (“Welcome back, [Name]!”) and teases the benefit: “Let’s group your agents into a powerful Swarm for coordinated calling.” A brief description explains what swarms are (e.g. “Swarms allow multiple voice agents to work together on targeted campaigns – think of it as your AI call team.”). A Get Started button moves to the next step.
Swarm Details (Name & Purpose): The user enters key info for the swarm. This is similar to creating a team: Swarm Name (e.g. “Sales Battalion”, “Support Squad”) and Purpose/Mission (a one-word or short phrase like “Outbound Sales” or “Customer Support”). These inputs are captured with text fields. For inspiration, the UI might show examples or even allow choosing from templates (like a dropdown of common swarm types: Sales, Support, Appointment Setting). Each template could pre-fill the purpose and even suggest which agents to include, simplifying the process for new users. The interface uses two input fields with labels and possibly helper text (e.g. “Give your swarm a memorable name. This will be visible in analytics and reports.”). The design uses the neobrutalist card style – perhaps a rotated white card on the purple background, with black bordered inputs. A continue button becomes active once a name is entered (purpose might be optional or default to the template chosen).
Select Member Agents: Now the user selects which voice agents will be in this swarm. The onboarding lists the user’s available agents in a scrollable list or grid. Each agent is displayed similarly to the Agents dashboard’s AgentCard
file-fjpkexbjp2dfhprtk7mhwq
, showing name, role/purpose, and status (active/training) with a status dot (green for active, orange for training, etc. as per getStatusColor logic
file-fjpkexbjp2dfhprtk7mhwq
). The user can toggle inclusion by clicking an agent card – selected ones are highlighted (e.g. outlined in the swarm’s color or with a checkmark). If the user doesn’t have multiple agents yet (quite possible if they only created one in the prior onboarding), this step can encourage them: “You have 1 agent. Create more to build a larger swarm.” It might allow proceeding with just one agent (technically a swarm of one), but ideally the user would have at least two. The flow might integrate with the Agents onboarding: for instance, right here provide a shortcut “+ Create New Agent” (maybe a small button with UilPlus
file-fjpkexbjp2dfhprtk7mhwq
). If clicked, it could pop open the CreateAgentModal or even link to the Agents onboarding. This way, users not only create a swarm but potentially another agent in the process. After selecting agents, the user clicks Next. (If none selected, we’d show a warning “Please select at least one agent” and disable advance.)
Configure Swarm Settings: This step (optional/advanced) allows tweaking how the swarm operates. For example, if Diala supports call distribution modes (round-robin, simultaneous, etc.), the user could choose one. Or if the swarm is for a campaign, maybe set a campaign goal like number of calls per agent, or schedule (run swarm now vs later). In the initial implementation, we might keep it simple: default settings that all agents will call from a shared number pool and start immediately when tasked. We can present a summary: “This swarm will use all selected agents concurrently for higher throughput.” with an info icon UilInfoCircle that on hover explains details (e.g., “Agents will automatically coordinate to avoid calling the same contact twice,” etc.). If there are any parameters (like max calls per agent or time windows), they can be sliders or toggles here. Keeping it minimal ensures users aren’t overwhelmed – they can accept defaults and proceed.
Review & Create Swarm: A summary card is shown, combining all chosen details. For example, “Swarm: Support Squad\nPurpose: Customer Support\nAgents: 2 agents (Echo-Diala, Diala-Belle)”, along with the swarm color or icon. The UI might present a stylized “swarm card” similar to how swarms appear in the dashboard grid
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 – i.e., the card header has the swarm name with purpose badges, and a list of agent names underneath
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. This gives a realistic preview. The user confirms by clicking Create Swarm. On click, a Convex function is called to save the swarm. After a quick success confirmation (perhaps the card transforms visually or a checkmark animation shows), the onboarding transitions to a final screen.
Swarm Ready (Demo/Next Steps): The final screen celebrates the creation. It might say “‘Support Squad’ is ready!” with a swarm icon. While there isn’t an obvious “demo” for a swarm like there was for a single agent, we can demonstrate the concept: for instance, simulate a live swarm dashboard view. We could show a mock realtime status: “Agents online: 2/2. Calls today: 0. Ready to deploy.” Or animate two agent avatars calling in parallel. This is more of a visualization than an interactive demo. Another approach: prompt the user to launch a campaign with this swarm (which segues into the Calls module). For example, a big button “Start a Calling Campaign with this Swarm” could be displayed. Clicking it could end the onboarding and navigate to the Calls dashboard’s campaign creation section (prefilling the swarm selection). If we include that, it directly drives the user to use the swarm immediately. Otherwise, simply instruct: “You can now use ‘Support Squad’ in your calls and campaigns. Visit the Calls section to deploy your swarm.”. Finally, a Done or Go to Dashboard button will exit the onboarding, bringing the user to /dashboard/swarms where they can see their newly created swarm in the list.
Components (Frontend)
Cards with Headers: Each step’s UI is primarily contained in a Card. The Swarm Details card might have a header with the UilUsersAlt icon (group of people) next to “Create New Swarm” in bold
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Using CardHeader and CardContent, we ensure consistent padding and border usage. The header background could use a light purple or appropriate thematic color. We saw in the swarms dashboard, card headers are color-coded by purpose (Discovery = purple, Support = green, etc.)
file-fjpkexbjp2dfhprtk7mhwq
. For onboarding, since purpose isn’t decided until input, we might keep the header a neutral or the base purple for all.
Form Inputs: The Name and Purpose fields use the standard <Input> for text. We may reuse the UI from the swarms dashboard’s Create Swarm modal if it exists. There’s likely a CreateSwarmModal given the code references setShowCreateModal(true)
file-fjpkexbjp2dfhprtk7mhwq
 and presumably a corresponding component. If available, we can extract its inner form. If not, implementing two <Input> components with labels is straightforward. To maintain style, we give them a class for thick border and maybe slightly rounded corners (in code, modals and inputs often use classes like rounded-[3px] border-4 border-black
file-fjpkexbjp2dfhprtk7mhwq
).
Agent Selection List: This is a critical component. It could be a custom list with checkboxes or toggle buttons, but a more visual approach: show each agent in a mini-card format. We can use the existing <AgentCard> or create a simplified version for listing. Possibly, a two-column grid if there are several agents. Each agent card includes the agent’s name and an icon or avatar, plus perhaps a small badge for status (active/inactive) as seen in swarms dashboard listing
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The design system’s <Badge variant="outline"> with colored background is used in the swarms card to show agent status
file-fjpkexbjp2dfhprtk7mhwq
, we can mimic that here or simply use an icon + text. Selecting an agent card can internally toggle a boolean, and we visually indicate selection by changing its style (e.g., add bg-yellow-100 or some highlight, and a checkmark in a corner). We might incorporate a small <Checkbox> component on each card for clarity (with a custom styled checkbox that fits the brutalist theme: perhaps a black border square that fills with a black check when selected).
Swarm Settings controls: If we include any (like a distribution mode), we can use radio buttons or a <Select> dropdown from the design system for simplicity. For example, a label “Call Distribution:” with options Simultaneous vs Sequential. These can be radio inputs styled with black border and a dot, or buttons that toggle. The design system likely has a Toggle or Switch component (like <Switch> or using UilToggleOn/Off icons
file-fjpkexbjp2dfhprtk7mhwq
) to represent boolean settings. We could use those if relevant (e.g., a switch for “Start immediately” vs scheduled).
Summary Card: This card will look much like a Swarm card in the dashboard. In fact, we might directly use the layout from the Swarms grid: a card with a colored header containing the swarm name and purpose badges, and inside a list of agents. The difference is in onboarding it’s static info for confirmation. We’ll use <Badge> elements for purpose and agent count as done in the real card
file-fjpkexbjp2dfhprtk7mhwq
. Each agent could be listed simply as name (with or without type). We can borrow the snippet that lists agents in a swarm card
file-fjpkexbjp2dfhprtk7mhwq
 but simplify (maybe omit the status badges here for brevity, or include them if we want to confirm we added the right ones). If the list is long, we’ll scroll or limit height, but likely initial users have few agents.
Feedback/Confirmation: After clicking “Create Swarm,” a quick feedback can be given. Possibly using a small modal or toast that says “Swarm Created!” or visually transforming the summary card. For example, overlay a big checkmark on the card or turn the border green momentarily (success feedback). We can animate a check icon (like a black check that scales up then back down) to acknowledge the action.
Navigation Buttons: Each step has a Next or Continue button, and possibly a Back button on subsequent steps. We style them as before: large, uppercase, bold. The Next button uses a prominent color (maybe the same Diala blue or a green to indicate progression) with black text, and arrow icon UilArrowRight. The Back button (if present) could be a simpler text or secondary style button, possibly with UilArrowLeft. The final step’s primary button is “Create Swarm” instead of Next, and after creation “Done” or “Go to Swarms”. We ensure button states (disabled vs enabled) are visually distinct (grayed out or lower opacity when disabled, vibrant when enabled).
Backend Architecture (Convex & APIs)
Convex Swarm Creation: The completion of this onboarding triggers a Convex function (say createSwarm) to save the new swarm. This function will likely write to a Swarms table or collection, including fields: name, description, purpose, createdAt, and the list of agent IDs that belong to the swarm. In our mock data, a Swarm object includes id, name, description, purpose, agents[], totalCalls, successRate, created
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The Convex function will set initial metrics (totalCalls=0, successRate=0 or null, created=now). The list of agent IDs is crucial; those references link the swarm to actual agents. The function might also update each agent’s record to note they are part of this swarm (if the data model requires linking from agent side; otherwise, it’s enough to store agent IDs in swarm).
Convex Query Update: Once created, any Convex queries that fetch swarms (like the one powering the Swarms dashboard list) will include the new swarm. If the user is navigated to the swarms page after, they’ll see it without needing a manual refresh, thanks to Convex reactivity if set up (the useQuery hook would push updates). If not using real-time queries, we might refetch the swarms list on page load.
No External API Calls: Creating a swarm is an internal operation – no external APIs are needed. It’s mainly about organizing existing data. So the backend interaction is straightforward: a DB insert. One possible integration: if the swarm is conceptually tied to a “campaign” or call automation, we might schedule a task (like if “start immediately” was selected, we could kick off a search or call routine). However, those tasks belong more to the Calls module. The swarm itself is just a grouping, so we won’t call any outside service in this step.
Data Validation: The Convex function will validate that the swarm name is unique (or at least not blank). It may also ensure agent IDs exist and belong to the user’s account (to avoid any malicious insertion). Basic sanitization on name/description is done (trim whitespace, maybe limit length). If any issues, it returns an error that our frontend will handle (showing an error message on summary).
Link to Campaigns: If we did implement the “launch campaign” button, pressing that could call another Convex function or backend routine: e.g., create a new Call Campaign object with this swarm attached. But since that delves into the Calls domain, we might instead handle it on the frontend by navigation (and then in Calls page, user manually sets up a campaign). We can integrate lightly by passing parameters via URL (e.g., navigate to /dashboard/calls?tab=swarm&swarmId=X to pre-select that swarm in the calls interface). The code already allows switching tabs via URL param
file-fjpkexbjp2dfhprtk7mhwq
. We could extend it to accept a swarmId to auto-open a SwarmOverview or campaign modal. The SwarmOverviewModal exists in the code
file-fjpkexbjp2dfhprtk7mhwq
, perhaps used to show swarm’s call campaign details.
Convex Relationship: In the data model, the swarm might not physically “do” anything until used, but we might prepare an entry in a “SwarmCalls” or “Campaigns” table to accumulate stats. For now, we set up the basics: also consider if any initial stats should be computed. E.g., activeAgents count (the number of selected agents with status active). We can compute that in the function by checking each agent’s status (if accessible via a query of Agents table). In our static data, swarms have successRate and totalCalls aggregated
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Initially those are 0, but could we glean something? If the agents had previous calls, maybe totalCalls = sum of those agents’ totalCalls to date (giving a baseline). But probably simpler to start at 0 since as a new swarm it hasn’t done anything collectively.
Notification/Webhook: Not directly needed, but conceivably if the platform notifies an admin when a new swarm is created (for monitoring user engagement), a side-effect could be sending an event. That’s beyond core functionality but part of a robust backend integration if desired.
Resilience: The backend should handle partial failures gracefully. If, for example, one of the agent IDs is invalid (maybe the user deleted an agent mid-onboarding in another tab), the function would throw and the UI show an error. But such edge cases are rare. The function can also handle concurrency – though unlikely two swarms with same name are being created simultaneously by one user, the unique name check should handle it.
Design System & Neobrutalist Elements
The Swarms onboarding uses Diala’s design language to reinforce consistency:
Color Theme: Purple is the thematic color, echoing how the Swarms dashboard uses purple for the header and stat card for total swarms
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The onboarding background can be a purple tone (e.g. bg-purple-400 or a gradient of purple) with the same grid pattern overlay for texture
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. This differentiates it from other flows (Voice was blue, RAG yellow, etc.) while still being an “approved” palette color.
Typography & Copy: As always, uppercase bold headings for each step title. For example, “SWARM NAME” label above the input, or “SELECT AGENTS” as a section label, will be in a bold font (we might use small caps or just uppercase for labels). The main titles (like on the welcome screen or final screen) are large – possibly 5xl or 6xl size – ensuring the user immediately sees what the focus is (the name of the flow). Supporting text and descriptions use slightly lighter weight but still bold and high contrast (dark gray on light backgrounds, or black on colored backgrounds if legibility permits). The tone is motivational and action-oriented (e.g. using words like “team”, “campaign”, “powerful”).
Iconography: Many icons illustrate the points: UilLayerGroup or UilUsersAlt for swarm concept, UilRobot for agents in the list, UilPlus for adding new, etc. The design will often place these icons in squares or circles with contrasting background. For instance, in the page header for swarm onboarding, a purple circle with a white UilUsersAlt could float behind the title text as a design element. In lists, each agent might have an avatar circle with a robot icon (some differentiation if multiple – maybe different background colors as used in Active Agents card where index 0 is blue, 1 is purple
file-fjpkexbjp2dfhprtk7mhwq
, etc.). This adds visual variety and cues (like color-coding agents by purpose or status if possible).
Card Style: Just like other modules, cards have black borders and slight rotation. In fact, the swarm cards on dashboard are rotated depending on index
file-fjpkexbjp2dfhprtk7mhwq
 – in onboarding, we might rotate the main form card a bit for flair. But for user input, a slightly rotated form might be weird, so perhaps the form card stays straight to align input fields nicely (we can still rotate decorative elements). The final swarm summary card can be rotated to match how it’ll look among others. The heavy shadow on hover isn’t as relevant in static onboarding, but any clickable card (like agent select) will use the interactive shadow: normally 4px offset shadow, on hover increase to 6px to show it’s clickable
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. This gives that tangible feel.
Badges & Status indicators: We use the same badge styles as in the main app: e.g., purpose badges with white text on colored background, bordered in black
file-fjpkexbjp2dfhprtk7mhwq
, status dots for agents (tiny colored circles) – these appear in the swarms card design
file-fjpkexbjp2dfhprtk7mhwq
 and we mirror that in the agent list: a green dot next to an agent name for active, etc. Those dots can be simple <div className="w-2 h-2 bg-green-600 rounded-full animate-pulse"> as used in code
file-fjpkexbjp2dfhprtk7mhwq
. This subtle animation (pulse for active agent) is a nice touch to imply “liveliness” of active agents.
Accordion/Info Panels: If we have any explanatory text (like explaining distribution modes or swarm usage), we might tuck it in an accordion for cleanliness. The design system has an Accordion component as seen in transcripts flow
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. For example, a small accordion titled “How do swarms work?” could expand to show a short blurb: “All agents in a swarm call concurrently, dramatically increasing call volume. Monitor swarm performance in real time in the dashboard.” This educates without cluttering the main flow. Use of a bold question as trigger and normal text answer in content keeps style consistent
file-fjpkexbjp2dfhprtk7mhwq
.
Grid Layout & Responsiveness: The agent selection likely uses a grid. We’ll ensure on mobile that it becomes one column list (stacked agent cards) and on desktop two-column. The whole onboarding container is centered and scrollable if needed (min-h-screen ensures full height usage
file-fjpkexbjp2dfhprtk7mhwq
, and we add padding). Buttons are sized appropriately (full width on small screens, or inline on larger screens if space allows). All text remains readable on smaller devices by using relative units or tailwind’s responsive text classes (sm:text-lg vs md:text-xl etc., as seen in other flows
file-fjpkexbjp2dfhprtk7mhwq
).
State Management
State management for the Swarms onboarding will track the multi-step form and selections:
currentStep (number): As usual, to control which UI segment is shown. Steps 1 through 5 correspond to screens described. We initialize at 1 and increment with next/back.
Swarm form data states: swarmName (string), swarmPurpose (string) for the text fields
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Possibly swarmDescription if we want a longer description (it was present in code as well
file-fjpkexbjp2dfhprtk7mhwq
 but could be optional).
selectedAgentIds (array of strings): The IDs of agents chosen for the swarm. Toggled on selection. Alternatively, we could store a set or boolean map for agent IDs, but an array is fine. We’ll fill this if user had agents; if they add a new agent mid-process, we must ensure to update this list (like if CreateAgentModal returns a new agent, push it and mark selected).
If we have distribution mode or schedule: distributionMode (string, e.g. 'parallel' or 'serial'), startImmediately (bool) as an example. Defaults set accordingly (most likely parallel & true).
showCreateAgentModal (bool): If we allow agent creation inline, we manage that modal’s open state. On save, we get the new agent data – possibly via a callback or by monitoring a Convex mutation result. We then update the agent list state (which might be a local copy of available agents) and add its ID to selectedAgentIds. In code, we see an availableAgents list of mock new agents
file-fjpkexbjp2dfhprtk7mhwq
; for real data, we’d fetch the user’s agents from Convex at mount (or rely on what’s in context if user came from Agents section).
isSaving: A flag for when the swarm is being created. On clicking create, set true to disable the button and maybe show a spinner. After Convex returns success, set false and move to final step.
error: Any error message to display if creation fails (e.g. “Name already taken”). This could be shown in an alert style on the summary screen or as a modal.
We might also have a swarms state if we fetched existing swarms (not really needed for onboarding unless we want to ensure unique naming or reference). For simplicity, we don’t need it now.
All this state is local to the component. Since the data (agents list) might come from outside, we could fetch the list of agent objects at the start using a Convex query (useQuery(api.agents.list) for example). That returns an array of agent records. We then use that to render the selection options. This approach ensures we’re not using stale or mock data. Each agent record could include id, name, type/purpose, status, which is what we need for display
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. If the user came directly to swarm onboarding without any agents, this list would be empty and we’d handle that (maybe prompt to create agent first). But likely, we assume they have at least one agent (especially if this onboarding is accessed after Agents onboarding). Using React state for all of the above is sufficient. No complex state library is needed. If the app had a global context for user’s agents, we could tap into it to avoid refetching, but it’s fine to fetch within onboarding to decouple flows.
UX Interaction & Animations
Step Transitions: Just like Agents flow, transitions should be fluid. Pressing “Next” slides the current card left and brings the new card in from right (for example), creating a feeling of progress. Back does the reverse. This can be done with CSS transitions or a library for wizard flows. We’ll ensure focus moves to the first input of each new step for accessibility (e.g., when going to select agents, focus could go to the first agent card or heading so screen readers know context changed).
Selection Feedback: Clicking an agent card to select it will give immediate feedback: the card might do a quick jiggle or pop to indicate it’s been selected, and a checkmark appears. We could implement a tiny animation (scale up 1.05 then back) on selection. If deselecting, maybe a brief shake or the check disappears. Since multiple selections are allowed, each card acts like a toggle. We want this to be fun and clear – possibly a sound effect could even play on select (though that’s optional and needs sound library; probably skip for now).
Prevent Mistakes: If the user tries to proceed without any agents selected, the Next button will be disabled. We might also show a tooltip or text: “Select at least one agent to continue.” This avoids frustration. Similarly, if they try to create with an empty name, we disable create.
Back Navigation: Provide a Back button from steps 3 and 4 to allow corrections (maybe they want to rename the swarm or add a different agent). Back should preserve any inputs done (our state management ensures that).
Modal for New Agent: If used, the Create Agent modal might cover the screen. When it closes (after creating an agent), users return to the swarm flow seamlessly. We ensure to highlight that new agent in the list (maybe automatically select it and flash its card or scroll to it). This avoids the user feeling lost after adding.
Final Step Illustration: The last screen is an opportunity for a celebratory animation. We could animate multiple agent icons moving or a cluster icon getting a “success” highlight. Even a simple effect like confetti (small colored rectangles falling) could be triggered to celebrate creating a swarm. It adds delight and positive reinforcement. We must be careful not to overload – it should last a second or two then clear.
Guidance to Next Action: As mentioned, a big action button “Launch Campaign” or “View Swarm Dashboard” on the final screen guides the user on what to do with the swarm. If launching a campaign, clicking it provides feedback (maybe a loading if it takes time, or goes straight to calls page). If just viewing, it navigates quickly. In either case, we should also allow a neutral completion: maybe the top-right corner could have an “X” close button at all times to exit onboarding. If the user clicks that mid-way, we might ask confirmation (“Are you sure you want to exit? Progress will be lost.”) to avoid accidental loss. If they confirm, just navigate away (progress is ephemeral anyway).
Tooltips/Help: Certain terms might have tooltips. For example, when hovering over “Success Rate” in summary, it could show how that’s calculated (though initially zero). Or if distribution modes are present, a small ? icon next to each with more info. We can use a simple <Tooltip> from design system or a custom one (the title attribute or a popover on hover).
Responsive Adaptation: On smaller screens, the flow might not show as multi-column at the agent selection step. If it’s a list, each agent row could have a “Add” button or switch. We ensure those controls are large enough to tap. The continue button likely sticks to bottom for easy access. If needed, we implement a slight scroll on selecting an agent to bring the next one into view (if the keyboard popped up or something).
Technical Implementation Details
The Swarms onboarding is implemented similarly as a separate Next.js page (e.g. app/onboarding/swarms/page.tsx). Some specifics:
Data Fetching: We’ll use the Convex client to get the list of agents upon component mount. For example, const agents = useQuery(api.agents.listMine); which returns an array of agent objects. This hook will cause a re-render when data arrives. If Convex isn’t used for this, we could use a Next.js API route or directly fetch from an internal endpoint – but since Convex is central, we use it for consistency. We then keep that list in state (or just use it directly for rendering). We might also have const [agents, setAgents] = useState([]) and populate it via useEffect if not using useQuery. Either approach is fine.
Creating a Swarm: We’ll define a Convex mutation function to handle swarm creation (in the api.swarm namespace perhaps). This function could be generated via the Convex CLI. Our frontend calls it with the swarmData: name, purpose, agentIds, etc. Once awaited, if no error, we assume success (Convex would throw if error). We then set some local newSwarmId maybe if needed (for navigation), and call setCurrentStep(lastStepNumber) to show the final screen.
Error handling: If Convex throws (maybe name conflict or network error), we catch it and set an error state. The UI would then likely remain on the review step and display the error message somewhere (perhaps as a red text below the Create button or an alert box). The user can then adjust and retry.
Modal Integration: If we integrate CreateAgentModal from the existing codebase, we have to import it and use it. The code base shows a CreateAgentModal component being imported in Agents page
file-fjpkexbjp2dfhprtk7mhwq
. We can use it here. It likely expects props like onSave to receive the new agent data. We’d pass an onSave that takes the agent, closes the modal, and updates our state. The agent data might be an object with at least an id and name; if not, maybe the modal itself handles insertion to DB and triggers a re-fetch of agent list. We might rely on Convex reactivity: if agents.listMine query is active, after the new agent is saved via that modal (which likely also uses Convex), the query would update automatically to include the new agent. If so, our UI would update without manual intervention – neat. If not reactive, we can manually append.
Routing & Completion: After finishing, we likely navigate to /dashboard/swarms. We can use useRouter() for that. If we want to auto-open the SwarmOverview modal on the dashboard (to show details), we could append a query param like ?swarmId=new or something and have the swarms page check for it and open a modal (the swarms page code doesn’t explicitly handle query except maybe selection if any). Alternatively, simply landing on the swarms page and seeing the new entry at top (if we prepend it) is sufficient.
Conditional Steps: If the user has zero agents, step 3 might be skipped or replaced by a call-to-action to create an agent. In implementation, we could detect agents.length === 0 and instead of showing selection, show a special screen: “No agents yet. Let’s create one first!” with a button to launch CreateAgentModal. After they create one, that screen goes away and we essentially can skip to step 5 (because a swarm of one is now possible). But ideally, we expect them to have at least one agent from previous onboarding.
Testing the Flow: We test by simulating different conditions: user with one agent vs multiple. Ensure that if one agent, they can still create a “swarm” (though it’s somewhat an edge case, we allow it for completeness). The call to Convex with one agent is fine. Also test the new agent creation path. Because multiple asynchronous things happen, we ensure state updates correctly (maybe using useEffect to watch agents list and if it grows from 0 to 1, auto-select the new agent and advance the step if we were waiting).
Maintainability: We structure the code clearly with sub-components if needed (e.g., could break out AgentSelection as its own component that receives agent list and manages selection states). But given it’s not too complex, keeping it in one file is okay. Comments in code would explain each step’s block for clarity to future devs.
Integration Points
Swarms Dashboard Listing: The obvious integration is that the new swarm appears in the main Swarms dashboard (/dashboard/swarms). On that page, swarms are displayed in a grid with their stats
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Our newly created swarm will show up with 0 calls and baseline success rate in a card. Users can click it to see details (the app might navigate to /dashboard/swarms/[id] which has a detail page, as indicated by the folder structure
file-fjpkexbjp2dfhprtk7mhwq
). That detail page likely shows deeper analytics for the swarm (calls over time, agent contributions, etc.). Because our onboarding saved the swarm in the same store as those, everything is connected.
Live Call Monitor: If a swarm is used in a live campaign, the Live Call Monitor (and possibly SwarmOverview modal) will tap into the swarm’s data. For instance, if we open the SwarmOverviewModal in the Calls section, it might display which swarm and some stats. The code references a SwarmOverviewModal
file-fjpkexbjp2dfhprtk7mhwq
, likely to show details of a swarm’s campaign. That modal probably expects a Swarm object or ID. Since our swarm data structure matches, it can be used there seamlessly.
Calls Module (Campaigns): When a user wants to actually use the swarm, they’ll likely interact with the Calls module. Possibly, they’ll create a new campaign or search that leverages this swarm. The integration might be: on starting a campaign, the UI asks “Which swarm or agent to use?” If a swarm is chosen, then the call distribution logic uses all agents in that swarm. Under the hood, when dialing out, the system picks an available agent from that swarm for each call or parallel calls. The integration point is that the calls service (the part of backend that orchestrates calls, likely via Telnyx) needs to retrieve the list of agents in a swarm to assign calls. Since our swarm is stored, an API call from the call service (maybe a Convex function called by the serverless telephony function) can query by swarmId to get agent IDs, then fetch each agent’s config. This means the swarm’s existence and membership drive real behavior.
Analytics Aggregation: Over time, as calls are made, the system will update swarm metrics. E.g., increment totalCalls for the swarm, recalc successRate (maybe weighted average of member agents or outcome tracking). In the code, these seem to be handled possibly by accumulating data
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Our creation set them to 0, but as calls complete, perhaps a Convex mutation updates the swarm record (like swarm.totalCalls += 1, and successRate recalculated). Because the swarm grouping is known, any per-call record might carry a swarmId to facilitate such updates. Integration-wise, this means our swarm’s ID is used as a foreign key in call logs. By creating the swarm properly now, we ensure future processes have the needed reference.
UI Consistency: The onboarding uses the same components as the main app (e.g., same AgentCard style). If the design of AgentCard or Swarm card updates, these changes should reflect in onboarding too, maintaining an integrated feel. This is more a front-end integration but ensures that, for instance, if tomorrow they add a new field to swarms (like a “Team Lead” name or a color code), both the dashboard and onboarding could be updated to capture/display it.
Cross-module Onboarding Links: We’ve allowed the possibility of jumping to Agent creation from within Swarm onboarding. This is a cross-integration between two onboarding flows (Agents and Swarms). By doing so, we ensure the user doesn’t hit a dead-end if they need more agents. The integration is smooth because both use Convex – the new agent is saved and immediately accessible to the swarm flow. If multiple onboardings share components (like WelcomeCard, etc.), the design and behavior remain consistent.
User Guidance Continuity: After finishing swarm onboarding, the natural next step is to utilize the swarm. We integrated a pathway to the Calls module. This kind of integration ensures the user’s journey is continuous – they created something, now immediately see how to use it. It also means that the onboarding flows aren’t isolated tours; they feed actual objects into the system that subsequent modules recognize. The swarm’s name might even appear in notifications or suggestions, e.g., the Calls page might have a banner “New! Launch a campaign with Support Squad swarm.” This kind of cross-promotion can be built because the system knows a swarm was just created (via an event or just by checking that it exists).
Future Enhancements
Swarm Templates & AI Recommendations: In the future, Diala could provide pre-made swarm templates. For example, a template called “Appointment Army” (like seen in mocks
file-fjpkexbjp2dfhprtk7mhwq
) could come with recommended agent roles or even auto-generate a couple of agents specialized for that purpose. The onboarding could let the user pick a template at the start (“Choose a swarm template or create your own”). If they choose a template, we could auto-fill the swarm name, purpose, and even spin up new agents with appropriate settings. This would dramatically shorten setup: one click to get a fully functional swarm. It uses AI to provision agents – possibly generating custom prompts for each based on best practices. This enhancement would blur onboarding for Agents and Swarms together but yield a very powerful outcome quickly.
Dynamic Agent Suggestion: If a user has many agents, the system could suggest which agents to include in a new swarm based on their purpose or past performance. For instance, if the user’s agents have tags or specialization (sales vs support), and the swarm purpose is “Sales”, the UI might automatically check all sales-type agents for inclusion. It could also highlight an agent’s success rate or availability (if an agent is currently on a call or in training, maybe exclude or warn). As an enhancement, each agent card could show a metric (like success 92%
file-fjpkexbjp2dfhprtk7mhwq
) to inform selection.
Swarm Capacity Planning: Future iterations might allow setting the swarm’s campaign parameters right in onboarding. For example, “How many calls do you want this swarm to handle per day?” or “Target region for this swarm’s calls:”. This would tie into search (for leads) or into scheduling. While perhaps complex for initial onboarding, advanced users might appreciate configuring a swarm fully (like a campaign object) at creation. Eventually, the line between creating a swarm and launching a campaign might fade – one might do both in one flow.
Integration with Schedules/Calendar: A swarm might be scheduled to run at certain times (especially if using global agents). A future feature could let user set time windows (e.g., 9am-5pm weekdays). Onboarding could include picking a timezone or schedule for the swarm to be active. This would integrate with Diala’s job scheduler to only activate those agents in that window for outgoing calls. Not needed in MVP, but valuable for planning campaigns.
Swarm Collaboration Features: Perhaps in the future, swarms can have internal “chatter” or strategy – e.g., agents sharing information (one agent learns something on a call and another uses it). If so, onboarding might highlight that unique capability: “Agents in a swarm learn collectively. If one discovers a new objection handling tactic, all others adapt.” The UI might not change much for setup, but educational tooltips or an extra toggle like “Enable swarm intelligence sharing” could be present. This adds to the product’s differentiation if implemented.
Delete or Edit Swarm in Onboarding: Currently, onboarding is linear create. In the future, maybe support editing a swarm (via same UI). Or if they realize a mistake in final step, allow editing before leaving. Minor tweaks to flow to incorporate an “Edit” path might be beneficial.
Gamification: To encourage usage, perhaps show a benchmark: “Most users create 2 swarms to cover Sales and Support. You have 1 – consider creating another for support!”. Not directly part of a single onboarding flow, but a suggestion after completion. It could prompt to start the process again for a different purpose.
By evolving in these ways, the Swarms onboarding can become smarter and even more user-friendly, automating complex setup and showcasing the full power of coordinated AI agent teams.
AutoRag Module Onboarding Tendril
Purpose
The AutoRAG onboarding flow introduces users to Diala’s Retrieval-Augmented Generation system – essentially the feature that allows voice agents to leverage custom knowledge bases (documents, videos, websites) for more informed conversations. As a self-contained product-like experience, this flow walks the user through creating their first knowledge base (or “RAG workflow”) automatically. The purpose is twofold: first, to demonstrate how Diala can ingest large amounts of content and make it queryable by the AI agent, and second, to actually populate the user’s account with a useful knowledge base that their agents can use. It serves as a lead funnel by showcasing a high-value capability (integrating the company’s own data into AI calls), which can be a deciding factor for adoption. By the end of the onboarding, the user will have configured a data source (like a YouTube channel or a set of PDFs) and seen the system process it into a ready-to-use knowledge store. This flow ties into backend processes that perform web scraping, text chunking, embedding creation, and index building – but abstracts those technical steps into a user-friendly “autopilot” experience. The design and messaging reinforce that this is an automated, powerful feature that extends the intelligence of their voice agents.
Flow Steps
Welcome & Use Case Framing: On a bright orange (or gold) background – since RAG was associated with yellow in the main menu
file-fjpkexbjp2dfhprtk7mhwq
 – the user is greeted with “RAG System Setup”. The welcome text might say: “Welcome [Name]! Let’s give your voice agent a brain boost by feeding it your knowledge.” This sets the stage: the user is about to upload or connect some knowledge source. A succinct explanation is provided: “Diala’s AutoRAG will automatically retrieve data (like documents or videos) and teach your agent, so it can answer questions and handle specifics about your business.” A relevant icon appears, such as UilDatabase (database) combined with UilBrain (brain), signifying AI + data
file-fjpkexbjp2dfhprtk7mhwq
. The user clicks Get Started to proceed.
Choose Knowledge Source Type: The user is asked what kind of content they want to use as a knowledge base. Options are presented as large selection cards: YouTube (for videos/transcripts), Documents (PDFs, Word, etc.), Web Pages (URLs to scrape), or CSV/Knowledge base (if they have structured data). Each option card has an icon (UilYoutube for YouTube, UilFile for docs, UilLink for URLs, etc. as imported
file-fjpkexbjp2dfhprtk7mhwq
) and a brief description. For example: “YouTube Videos – ingest transcripts from a channel or playlist.” If needed, we restrict initial version to one type (say YouTube or Documents) to simplify, but showing all options indicates flexibility. The user clicks one; the selected card gets a bold highlight. We then show a Continue or automatically advance to next step.
Provide Source Details: Based on the type chosen, the user provides the input details:
If YouTube: an Input field for a Channel URL or Playlist URL (or individual video URL). Example placeholder: “https://youtube.com/@YourCompany/videos”. (In code, one of the workflows had type: 'youtube' with sources as channel URLs
file-fjpkexbjp2dfhprtk7mhwq
.) We also allow just a single video URL if they want to test with one video. Possibly also a number-of-videos setting (like how many recent videos to fetch – could default or be advanced setting).
If Documents: a file upload field (supporting multiple files). The UI could use the existing file upload component (maybe similar to File Upload Card used elsewhere
file-fjpkexbjp2dfhprtk7mhwq
). We’d list chosen files with their names and sizes.
If Web Pages: one or multiple URL inputs. Perhaps a textarea where they can paste several URLs or enter one by one, with an add button.
If Other: for a CSV or knowledge base, possibly just an upload or integration selection (maybe out of scope for now).
Each variant of this step shares a similar layout: a card with an icon corresponding to type in the header, a prompt to enter info, and maybe some validation (e.g. check if the URL is valid format). We also mention any limits (“We will process up to 100 videos” or “Max 20 documents” to set expectations). The design is user-friendly: e.g. for YouTube, after entering URL, we might fetch the channel name and show a preview (like channel title and thumbnail) to confirm correct link, using YouTube API or oEmbed. For documents, we might list the file names once uploaded. After input, the user clicks Next. If they leave it blank, Next disabled.
Configure Processing (Advanced): This step is optional/expandable for power users. We show some processing settings with sensible defaults: Chunk Size, Overlap, Embedding Model, Vector DB. In code, each RAG workflow had parameters like chunkSize, overlap, embeddingModel, vectorStore
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. To avoid scaring non-technical users, we can either hide these behind an “Advanced settings” accordion or pre-fill and explain them in simple terms. For example: “We’ll break documents into pieces of 500 words (Chunk Size) with a 50-word overlap to retain context. We’ll use OpenAI’s ADA embedding model to vectorize text, stored in a Pinecone vector database.” Each of those underlined terms could have a tooltip if hovered. If the user does open advanced, they can tweak: maybe a slider for chunk size (e.g. 256 to 1024), a dropdown for model (if they have their own key or want to use Open-source embedding model), and choice of vector DB (if Diala offers multiple or internal vs external). For most users, leaving defaults is fine; they might not even open this. But including it signals that the system is robust and configurable.
Launch Processing: Now the user begins the ingestion. A summary is shown: e.g. “Source: YouTube channel @YourCompany (45 videos)\nChunking: 512 tokens, Overlap: 50\nModel: text-embedding-ada-002, Store: Pinecone”. They hit Start Processing (or similar action label, maybe “Build Knowledge Base”). Upon clicking, the UI transitions into a processing state. This can either be on the same screen or navigate to the next “Progress” step. Preferably, we go to a dedicated progress view so we can show ongoing status.
Processing Progress: The user sees a live progress dashboard of the RAG workflow. This is where the automation feels real. We can emulate the UI of the AutoRAG dashboard but in a focused way for this workflow. For example, display a list of steps with statuses: “1. Scraping content – 10/45 videos downloaded”, “2. Generating embeddings – 0/45 completed”, “3. Indexing vectors – pending”. As the backend does each stage, update the UI. The code’s RAGWorkflow status field goes through states like 'scraping', 'embedding', 'indexing', etc.
file-fjpkexbjp2dfhprtk7mhwq
. We reflect that: perhaps show a progress bar for each stage, or one overall progress bar. The top could have a big progress percentage (like "65% done"
file-fjpkexbjp2dfhprtk7mhwq
) and maybe an estimated time remaining (if provided by backend
file-fjpkexbjp2dfhprtk7mhwq
). For example, “Estimated time: ~10 minutes”. If the process is quick (for small input, maybe finishes in under a minute), the user will see a rapid update. If it’s longer, we might not expect them to wait on this page indefinitely. But since this is onboarding, perhaps we simulate or accelerate it. (One approach: if they provided a large source, we could either process a subset for demo or send an email when done, but that breaks the flow. Instead, perhaps limit to something manageable for onboarding demonstration, like one video or small doc.) In any case, during progress we keep the user engaged with some fun facts or tips: “Did you know? You can add more data sources later to enhance your agent’s knowledge.” or “Your agent will soon have X pages of data at its fingertips.” The style should remain brutalist: progress bars have black borders, segmented perhaps, and status text is bold. Icons like UilSync (spinner) or emojis (✓, ⚠️) mark each step’s status (completed, in progress, etc.).
Completion & Results: Once processing hits 100%, we display a success message: “Knowledge Base Ready!” and possibly some stats: e.g. “45 videos processed, 1.2M characters indexed, 10k embeddings generated, index size 120 MB”
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. These numbers make the user feel the scale of what just happened. We present the final RAG Workflow Summary – basically a card very similar to what the AutoRAG dashboard shows for a completed workflow
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
: name, type, maybe a list of source names, createdAt and duration. The UI could allow them to click “View Details” which might open the ViewRAGWorkflowModal (already in code
file-fjpkexbjp2dfhprtk7mhwq
) if they want more granular breakdown like which sources were included and any issues encountered. For onboarding, we might skip deep details but mention they can see them in the main AutoRAG section. The key action now is to link this knowledge base to an agent. We highlight: “Great! Now your voice agents can use this data. You can assign ‘Sales Training Videos’ to any agent via the Agents dashboard or when configuring an agent.” Possibly, we incorporate a quick step here: if the user has an agent already (likely yes), ask “Apply this knowledge to Agent X now?” with a toggle or button. If they opt in, we call a Convex function to attach this workflow ID to that agent’s ragSources. This one-click integration makes the payoff immediate: their agent is now smarter. (Alternatively, just instruct them how to do it and not actually implement in onboarding to keep things simpler.)
Finish: The onboarding concludes with a call-to-action: “Your RAG system is set up. Going forward, you can create more knowledge workflows in the AutoRAG dashboard.” A button Go to Knowledge Base takes them to /dashboard/auto-rag where they will see this workflow listed among others (with status completed). Or possibly a Go to Agent button if they linked it, to try it out in Playground or calls. The experience wraps up by reinforcing what was achieved: their voice agent now has custom knowledge, which is a powerful differentiator.
Components (Frontend)
Multi-Step Form with Conditional UI: The onboarding uses conditional rendering similar to previous flows. Up to step 5, it’s form-like, then steps 6-7 are more dynamic status display. We’ll have components for each major portion: e.g., <SourceTypeSelection> for step 2, <SourceInput> for step 3 that changes based on type, <AdvancedSettings> for step 4, and <RAGProgressDisplay> for steps 6-7 combined. Breaking them into sub-components can keep the code tidy.
Card & Accordion: The interface elements heavily rely on Cards for grouping content. The Advanced settings likely will be inside an Accordion since it’s optional – we can use the design system’s Accordion (with a trigger like “Advanced Options”) to hide or show chunk/model settings
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Similarly, if we want to show additional info (like list of files, or intermediate stats), we might use collapsible sections. Each Card’s header uses relevant icons: e.g. a small icon on left of title and maybe another on right as decoration (like the transcripts onboarding had icons in corners
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). For RAG: a database icon on left, and a search or file icon on right perhaps
file-fjpkexbjp2dfhprtk7mhwq
.
Progress Bars: The design system has a <Progress> component
file-fjpkexbjp2dfhprtk7mhwq
 which is styled with border and can accept a value. We will use that for the various progress visuals. In the code, they adjust CSS variable --progress-color to style it for positive, neutral, negative sentiments
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, similarly we might color code different stages (blue for scraping, green for indexing etc., or simply one color throughout). We’ll accompany each progress bar with labels and percentages (like “65%” done, as text inside or alongside).
Icons for Steps: Possibly use an icon per stage in the progress display: e.g., UilDownloadAlt for scraping/download, UilChartGrowth or UilBrain for embedding, UilDatabase for indexing, UilCheckCircle for completed
file-fjpkexbjp2dfhprtk7mhwq
. These help scan the statuses quickly. Completed steps might get a green check icon, current step a loading spinner (maybe a simple CSS animation rotating an icon or using an <svg> spinner).
Modal for Viewing Workflow: If the user clicks a “View Details” button, we can invoke the existing ViewRAGWorkflowModal component (imported as seen in code
file-fjpkexbjp2dfhprtk7mhwq
). This modal likely shows a deeper breakdown (maybe listing all source items and their status, or allows adjusting settings post-creation). It’s not mandatory to include in onboarding, but having that option demonstrates transparency of process. The modal would appear with the typical black border styling and overlay.
File Upload UI: For documents, the file upload can either use a simple <input type="file" multiple> (styled with our UI) or a fancier drag-and-drop area. Possibly in the components there is file-upload-card.tsx
file-fjpkexbjp2dfhprtk7mhwq
, which might handle showing an upload area. If available, we integrate that, which likely uses browser File API and shows file preview in a card. Since this is onboarding, not the main app, we can do a simpler approach if time – but consistent styling is key. So likely we use FileUploadCard from custom components for consistency.
Select for Model/DB: The embedding model and vector DB choices (if exposed) can use the design system’s <Select> component
file-fjpkexbjp2dfhprtk7mhwq
 for a dropdown of options. We ensure it’s styled with the same black border and perhaps a custom arrow icon (or default, but inside a card it should look fine).
Link to Agent Option: If we add the “link to agent” step, we could have a small section at the end: either a dropdown of agents to choose one to link (if multiple) or if only one agent exists, a simple yes/no toggle to link it. This might be a <Switch> component from the UI kit
file-fjpkexbjp2dfhprtk7mhwq
 for a boolean, or just a checkbox with label “Attach this knowledge base to Alice (AI Sales Agent) now”. Simplicity is fine. On toggle true, we store the intent and execute it as part of finishing.
Backend Architecture (Convex & APIs)
Initiating the RAG Workflow: When the user clicks “Start Processing,” we trigger a backend sequence. Likely, there is a Convex mutation or an API endpoint that creates a RAG workflow job and starts processing asynchronously. In the code, there are likely functions for createRAGWorkflow and then some background worker or schedule to do the actual scraping/embedding (perhaps using Convex scheduled functions or external server). The AutoRAG dashboard shows mock workflows with statuses
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We need to replicate that logic for real. Probably, on start, we insert a new RAGWorkflow record with status 'queued' or 'scraping', initial progress 0, and all those parameters. Then either a Convex backend worker kicks off (Convex allows async jobs) or we have an AWS Lambda or similar (if not purely Convex). Given the complexity, possibly an external service might handle it. But since this is a blueprint, we can assume Convex functions manage orchestration by calling out to required APIs/libraries.
For YouTube: The backend likely uses the YouTube Data API or a scraping library to fetch video IDs and transcripts. Indeed, the presence of youtube-transcript-fetcher.ts and youtube-transcript.ts in lib
file-fjpkexbjp2dfhprtk7mhwq
 suggests a backend implementation for grabbing transcripts. It might spawn a job to retrieve transcripts for each video. We have to handle potentially thousands of videos, so ideally this is done in an asynchronous, streaming manner. The onboarding doesn’t need to delve deep into how; it just triggers it and polls status.
For Documents: The backend would accept file uploads (maybe already uploaded via the file upload component to some storage like S3 or Convex storage). Then it reads each file (text extraction for PDFs, etc.), splits text into chunks, gets embeddings (via OpenAI or others), and stores them.
For Web pages: Use an HTTP client to fetch HTML and strip text. Possibly use readability libraries or site-specific scrapers.
The backend then interacts with a Vector Store (like Pinecone, Weaviate, Chroma, Qdrant as seen in data
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). For example, using Pinecone’s API to upsert vectors. The chosen store (we’ll default to Pinecone for now, but the code had multiple, e.g., Weaviate
file-fjpkexbjp2dfhprtk7mhwq
, Qdrant
file-fjpkexbjp2dfhprtk7mhwq
, etc.). We will have API keys configured for these in settings (the settings page lists API keys for openai, etc.
file-fjpkexbjp2dfhprtk7mhwq
, likely Pinecone too). The Convex function would use those keys from environment variables to connect.
This heavy-lifting is mostly on backend. The onboarding’s Convex mutation may quickly return a job ID or initial status, after which we rely on polling.
Polling Mechanism: To update the progress UI, the frontend could call a Convex query periodically to get the latest workflow status. The code for transcripts used setInterval to poll job status every 2 seconds
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We can do similar: after starting, store the workflowId (Convex doc id) and poll via useQuery (Convex queries update in real-time when data changes, so we might not even need manual polling if Convex subscriptions can push updates to the client on document change; that would be ideal – as soon as we update progress in DB, the UI updates). If Convex real-time subscription is available, we’d just use a useQuery(api.ragWorkflow.get, {id}) and have the progressbars reflect that object’s fields. If not using subscription, then a setInterval calling a getStatus(jobId) Convex function is fine. (The code suggests fetchYoutubeTranscript returned a jobId and they polled getJobStatus
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, so likely use similar pattern here if not full real-time).
Updating Progress in Backend: The backend that’s processing will periodically update the workflow record in Convex: e.g., after downloading each video, increment contentProcessed and progress (maybe progress is computed as a percentage across phases). The code’s mock has progress field separate from content counts
file-fjpkexbjp2dfhprtk7mhwq
. Possibly they define progress roughly as (completed phases / total phases * 100) or something plus finer granularity. We can decide a scheme: e.g., scraping 30%, embedding 50%, indexing 20% of total progress. Or simpler, just update progress as ratio of completed items to total items (once scraping done, you’re maybe 50% if embedding left, etc.). Either way, the UI will trust whatever progress value comes.
Completion: Once done, the Convex workflow record is set to status 'completed' and progress 100. Possibly store a completedAt timestamp
file-fjpkexbjp2dfhprtk7mhwq
 and maybe processingTime. The backend may also compile some stats (like we see number of embeddings, index size, etc. in stats
file-fjpkexbjp2dfhprtk7mhwq
). These come from the actual processing: count of chunks, size of index (perhaps returned by Pinecone), etc. Those are saved in the stats field of the record.
Error Handling: If something fails (e.g. a video couldn’t be downloaded or embedding API error), the backend should mark status 'failed' and possibly include an error message. The UI should detect status === 'failed' and inform user (maybe show a red message: “Processing failed: [error]. Please check your inputs or try again.”). And allow retry. We might skip deep error scenarios in onboarding narrative unless prompted, but we design for it.
Linking to Agent: If we implement that quick link, that would be another Convex mutation like attachKnowledgeToAgent(agentId, workflowId). It updates the agent’s ragSources array to include this workflow (similar to adding M1 and M4 in the example agent data
file-fjpkexbjp2dfhprtk7mhwq
). It might also trigger any re-training if necessary (though likely the heavy training is already done by building the vector DB, nothing more needed except the agent now knows to use it when answering questions).
Integration with Search & Chat: The outcome of this is a vector index with an ID (or some reference). The Convex DB might store the Pinecone index name or collection reference. When the agent (LLM) is conversing, it will use a function or API to query that index given a user question, retrieve relevant snippets, and include them in prompts (that’s how RAG works). So an integration point: the agent runtime needs to be aware of attached RAG workflows for an agent and know how to query them. Possibly convexEntryPoint or some config includes hooking into a retrieval function with the vector store. While not part of onboarding directly, our created knowledge base should seamlessly hook into those flows.
Resource Management: We should note that ingesting large data might be time-consuming. In an onboarding context, we might either restrict size or run an abbreviated process for demo. Perhaps behind the scenes, we limit to first N videos or first M pages of a doc to finish in a minute or two. This way the user can see completion without waiting hours. In real use, bigger jobs might run longer asynchronously (the user could leave and come back). But for the guided flow, a controlled shorter path is beneficial. Implementation could detect “onboarding mode” vs “full mode” by the type of call or a flag, but ideally the user used a reasonably sized input. If not, we could inform them: “We’ll process the first 10 items now for this demo. You can process all data later in the main AutoRAG section.” This ensures the user sees success quickly.
Design System & Neobrutalist Elements
AutoRAG onboarding uses a bold, utilitarian style to demystify a complex process:
Color and Imagery: An orange/yellow palette invokes the idea of knowledge and highlights (fitting since knowledge base often highlighted in yellow, and it was used for RAG in menu
file-fjpkexbjp2dfhprtk7mhwq
). The background can have the usual grid pattern, with maybe a subtle motif like database icons or circuitry watermark to imply data. We want to visually convey “automation” and “data crunching.” Brutalist design might incorporate a schematic-like aesthetic – maybe using monospace font for step labels to mimic code (optional).
Layout: Steps 2-4 are basically form inputs on cards, which we style with the same off-kilter geometry. The “Choose Source Type” could use a horizontal row of option cards or a column; probably a row that wraps on mobile. Each option card might rotate slightly on hover as a playful cue. For instance, when you hover “YouTube”, it tilts less (straightens) and shadow intensifies indicating selectability (the main menu had such an effect
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
).
Illustrative Icons: On the source selection cards, along with the Unicons we might add small illustrations or emojis to make them distinct (like a YouTube logo or document icon). But sticking to the icon set might be cleaner. The icons used (UilYoutube, UilFile, UilLink, etc.) are line icons – to make them pop, we can place them in a colored circle with black border as a sort of icon badge on each card. E.g., a red circle behind the YouTube icon (since YouTube is red), a blue behind Link icon, etc., still all with black outlines to remain in style.
Progress Visualization: Neobrutalism can take the normally hidden technical details and surface them in a raw form – which is exactly what showing chunk counts and index size is. Presenting these numbers in a table or list with bold labels and monospace numbers could be very on-theme (like an old computer printout vibe, but with modern layout). We could style the progress text as bullet lists with square bullets (or small black squares) to emphasize each stat. The progress bars have thick borders and solid fills. Because they will animate (growing width), we can even quantize them (like a choppy growth) to fit the retro-tech aesthetic. But smooth is fine too.
Feedback & Warnings: If advanced settings are opened, any warnings (like “smaller chunks improve accuracy but increase embedding count”) can be shown in a small italic text or a caution icon (UilExclamationTriangle in yellow
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). We ensure those are styled consistently with the rest (maybe a yellow background highlight behind the warning text).
Spacing and Fonts: We maintain generous spacing so complex info doesn’t overwhelm. Each phase in progress perhaps in its own card or clearly separated section with margins. Use of font weight: Titles heavy, details medium-bold. Possibly use a different font for code-y stuff if available (though likely stick to Inter or Noyh Bold, maybe with letterspacing to give a typewritten feel).
Consistent Components: The advanced settings might reuse components also found on the actual AutoRAG page. For example, if the main AutoRAG dashboard has a modal for editing settings (SettingsRAGWorkflowModal
file-fjpkexbjp2dfhprtk7mhwq
), maybe the onboarding’s advanced section is similar but inline. The progress display in onboarding could mirror the dashboard’s layout of the workflow cards (the Agent Performance card and others in auto-rag page are an example of info display
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, though that’s performance, not needed here). But possibly the progress view in onboarding is simpler than the full dashboard, focusing only on one workflow.
Motion: Could incorporate some quirky animations like rotating gear icons or a ASCII-style loading bar animation in text for fun while processing. But given we have real progress bars, we might skip gimmicks and let the live update itself be the “animation”.
Mobile: Ensure even on mobile the progress info is legible – maybe stack phases vertically rather than side by side. Possibly hide advanced settings by default (mobile users likely skip fiddling with those). Buttons remain accessible at bottom. The grid for source type becomes a vertical list on mobile to avoid squishing.
State Management
Managing state for AutoRAG onboarding involves both immediate form state and reacting to asynchronous status updates:
currentStep (number): Controls which view to show (1 = welcome, 2 = choose type, 3 = input details, 4 = advanced, 5 = ready to start, 6 = in progress, 7 = completed). Some steps might be combined in implementation (e.g., step 5 could be merged with 6 as part of starting processing). We increment as user proceeds, but also might jump steps (e.g., if user skips advanced, currentStep might jump from 3 to 5).
selectedSourceType (string): “youtube”, “documents”, “urls”, etc. Determines which input fields to show in step 3.
sourceInput (varies): The actual data for the source. If YouTube, a youtubeUrl (string). If documents, a list of File objects or file references (or perhaps just an array of filenames if uploaded separately). If URLs, maybe a string of newline-separated URLs or an array. We manage those accordingly. For file uploads using a controlled component, we might store files in state (FileList or Array of File).
advancedSettings object: containing values like chunkSize (number), overlap (number), embeddingModel (string), vectorStore (string). These default to something (512, 50, “text-embedding-ada-002”, “pinecone”). If user changes via inputs, we update the state. If they never open advanced, we keep defaults.
workflowId (string or null): The identifier of the created workflow job after starting. We get this when initiating the process (Convex function return). This is used to fetch status updates.
workflowStatus (object or specific fields): We could maintain a state for the status (like “scraping”, “embedding”, etc.), progress percentage, and stats. But since we likely will use a Convex query that gives us the whole workflow object (with fields like status, progress, stats), we might not need to manually store each – we can rely on that reactive data. However, to make it simpler, we can have: currentStatus (string), progress (number), and perhaps workflowStats (object with totalContent, contentProcessed, embeddings, etc.). These will get updated on each poll tick. In a React component using useQuery, we wouldn’t need explicit state; the query result itself is stateful. Alternatively, we do useState and update via polling function.
error (string or null): To store any error message if the process fails. If not null, we’ll show an error state (and maybe allow retry – possibly just reusing the same inputs and hitting start again).
isProcessing (bool): Indicates that the job is running. It flips on when user starts and off when done or failed. It helps in UI logic (e.g., show cancel option maybe). We could allow a “Cancel processing” button if feasible, which would require backend to stop tasks. That might be advanced, so likely skip cancel in MVP. But if we did, isProcessing and a Convex function to cancel would be needed.
selectedAgentToAttach (string or null): If we offer attaching to agent, track which agent (or a boolean if just attaching to default). If the user toggles attach, we store the agentId (maybe default to first agent if they have one).
The interplay: The early states (selectedSourceType, sourceInput, advancedSettings) feed into the Convex call when starting. After that, the state is driven by backend responses. We use useEffect to start polling when isProcessing becomes true or currentStep enters progress stage. We also use useEffect or useQuery to react to data changes. If using polling, the transcripts example can be adapted: call a getWorkflowStatus(workflowId) periodically, update state accordingly. Possibly incorporate a setTimeout to stop polling after X time or when completed to avoid infinite loops. The transcripts flow did a setTimeout to stop polling after 60s
file-fjpkexbjp2dfhprtk7mhwq
. We can do similarly if concerned about long tasks in onboarding. If the task isn’t done in that timeframe, we could inform the user that it will continue in background and they can check the AutoRAG dashboard later (meaning we end onboarding here with a message “We’ll keep working on it!”). But ideally our tasks in onboarding are short. One nuance: if using Convex real-time subscription, then no need for manual interval – useQuery will push updates as the Convex doc changes. That’s neat and simpler: just keep showing the query data in UI, and when status === 'completed', we know it’s done. We should still include a timeout for failsafe maybe.
UX Interaction & Animations
Guided Input: Each input step will focus the relevant field automatically. E.g., after choosing YouTube and clicking next, the URL input is auto-focused so they can start typing without extra click. When they finish typing and hit enter, we can treat that as clicking Next (for convenience). If file upload, we support drag-drop; when they drop files, we might auto-advance (if we can guess they’re done selecting, or still require clicking Next explicitly – probably require explicit Next to confirm).
File Upload Feedback: If they add a file, maybe show a thumbnail or icon with name. Possibly a small animation of the file icon dropping into a database icon to symbolize ingestion could be a delightful detail. If they remove a file, animate it fading out.
Progress Live Updates: As content is processed, we visually update progress bars. We might animate the bar width transition for smoothness. Also possibly flash or highlight when one stage completes (like turn the bar green and show a checkmark). If using textual status (e.g., “Scraping (10/10) ✓”), the checkmark or green color appears when done. We can sequentially reveal the next stage as the previous finishes, to keep focus. For example, initially show “Scraping... [progress bar]” and “Embedding... (waiting)” grayed out. Once scraping done, mark it complete and then animate enabling the “Embedding” section (maybe ungray it or slide it in). This storyboard style keeps the user engaged, watching each stage tick off.
Notifications: If the process completes quickly, we’ll still show the final message. If it’s slow and maybe user navigates away (though in onboarding they likely won’t), maybe we could implement a browser notification or email. But since it’s in-flow, we focus on on-page updates.
Attach to Agent UI: If they choose to attach to an agent, we could do something like a toggle or prompt after completion: “Apply to agent now?” For simplicity, maybe present it as a checkbox on final screen, before finishing, or a prompt: “Would you like to attach these knowledge sources to any agent?” with a dropdown of agent names and a button “Attach”. If they do it, we give quick feedback (“Attached to Agent Alpha ✓”). This is a minor interaction but a nice closure.
Complete & Next Suggestions: On finishing, beyond the immediate CTA to go to the AutoRAG dashboard or agent, we might include a subtle suggestion: “You can create another workflow for a different source (e.g., upload your product manuals) in the AutoRAG section.” or “Your knowledge base will continuously improve as you add more sources.” – encouraging future use. In terms of interaction, maybe a button “Create Another Knowledge Base” that directly takes them to auto-rag page’s create modal. But probably unnecessary in onboarding itself.
Error Case: If an error occurs mid-process, we should stop updates and show a visible error message on the progress screen. Possibly overlay a semi-transparent red shade on the progress section or replace it with a red card that says “There was an error processing your data.” If we have details, include them. Provide options: “Retry” (which might just rerun the Convex function, maybe starting from scratch or continuing where left off if possible) or “Cancel” (which ends onboarding, maybe direct to support). The user could adjust input (maybe the URL was wrong) and try again. So we likely allow them to go back to step 3 or 2. That means if error happens, we might set currentStep back to an earlier one or keep at progress with an active Back button. Ensuring this flow doesn’t break the component requires careful state resets (e.g., clearing workflowId if we’re starting over).
Time-out / Background: If we implement that 60s timeout like transcripts did
file-fjpkexbjp2dfhprtk7mhwq
, when it triggers, we should communicate it: “This is taking longer than usual. We’ll continue processing in the background. You can find the results in the AutoRAG dashboard later.” and then maybe we finish the onboarding with an incomplete status. But a better approach might be to continue polling longer, or encourage the user they can leave and it’ll continue. Up to the product decision – likely keep it interactive as long as possible.
Technical Implementation Details
The AutoRAG onboarding page (app/onboarding/rag/page.tsx) in the provided code is very basic and labeled “coming soon”
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We will revamp it with above logic:
Conditional Rendering and Components: The component can use a simple switch on currentStep to decide what to show. Or break each part into separate components and include conditionally. For example:
jsx
Copy
Edit
{currentStep === 2 && <SourceTypeSelection onSelect={handleTypeSelect} />}
{currentStep === 3 && selectedSourceType === 'youtube' && <YouTubeInput ... />}
...
{currentStep === 6 && <RAGProgressView workflow={workflow} />}
Alternatively, embed logic directly. Splitting is cleaner for readability.
Convex interactions: We likely create two functions in Convex: startRagWorkflow and getRagStatus (unless using realtime). startRagWorkflow will insert the doc and possibly initiate an async process. Convex supports the concept of “actions” for long-running or external work (similar to serverless functions). We might implement the actual scraping/embedding in a Convex Action so it doesn’t block the mutation. The mutation returns the new workflow id, then the action (running asynchronously) updates the document as it goes. This design fits Convex’s model (actions can call external APIs like YouTube or Pinecone, while mutations can update DB). In code, youtubeTranscriptActions.fetchYoutubeTranscript suggests an action was used for transcripts fetching
file-fjpkexbjp2dfhprtk7mhwq
. We’d have analogous ragWorkflowActions.startProcessing. Implementation aside, from the front-end we just call one function and get an id. Then either poll a Convex query or directly subscribe. Possibly they have a useQuery that can filter by id (like useQuery(api.rag.getWorkflow, { id })).
Real-time update possibility: If we use useQuery for a single workflow doc by id, as the Convex actions update that doc’s fields, the query result updates. We then simply use that object’s fields in the UI. We need to ensure the UI updates smoothly; likely fine as Convex pushes updates via websockets.
Resource cleanup: If component unmounts (user leaves page mid-process), we might want to handle that. If they come back later to auto-rag page, they should see the workflow still processing or done. That’s fine since it’s in DB. No need to cancel if user leaves (the process continues on server).
Data limits: If concerned about load, one might incorporate usage limits (like how transcripts limited hours). But for blueprint, we skip that complexity.
Testing with dummy vs real: For development, one can test with small inputs or stubbed processing to ensure UI flows as expected. Or have a mode where instead of calling actual Pinecone, we simulate a delay and then fill fields. This is more for development ease. In production it would call real.
Integration Points
AutoRAG Dashboard: The knowledge base created in onboarding shows up on the main AutoRAG page (/dashboard/auto-rag). There, the user can manage it further – e.g., view details, adjust settings, or delete it. The code’s workflows state on that page contains several sample workflows
file-fjpkexbjp2dfhprtk7mhwq
. Our new one would be appended to such a list. If using Convex queries on that page, the new doc automatically appears (assuming the query fetches all workflows for that user). If the onboarding ended with the job not fully complete, the user might see it in progress in the dashboard (with a spinner icon or progress bar). When done, they’d see it turn to completed. So the integration ensures continuity: onboarding created an object that persists in the user’s account.
Agents Integration: The main reason to build RAG is for agents to use it. So integration wise, each agent can link to any number of RAG workflows. The agent data structure included ragSources array of {id, name, description} for modules
file-fjpkexbjp2dfhprtk7mhwq
. Perhaps each RAG workflow corresponds to a module entry. If so, part of our Convex attachKnowledgeToAgent might append something to that array with the workflow id and maybe name or a short description. Then, when the agent is making calls, the conversation logic sees it has ragSources and knows to query those. Possibly the conversation engine has a step: if agent has ragSources, do a vector DB similarity search on the user query and include top results in the LLM prompt. All that logic would lie in the workflow engine (maybe in WorkflowEngine.ts or similar
file-fjpkexbjp2dfhprtk7mhwq
). We ensure that by properly saving the data, that pipeline can retrieve it.
External Services Setup: Pinecone or vector store credentials must be configured. The onboarding doesn’t directly ask for API keys (we assume they were input in the Settings module beforehand – the Settings page did show placeholders for keys
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). If the user hasn’t set those up, the processing might fail (no API key). We might foresee this and if keys are missing, warn at step 4: e.g., “Please add your OpenAI API key in Settings before proceeding.” or ideally, Diala could allow limited use with its own keys. If not, integration with the Settings is needed (maybe the onboarding checks if keys exist via a convex query to a Key store and if not, either prompt the user to go to settings or allow them to input keys inline – but that complicates flow, so perhaps we assume keys exist as part of initial onboarding steps).
Background Processing: If a workflow is long-running beyond the onboarding, the system handles it gracefully. The integration here is that the backend will continue updating the record. If the user closes the app, the next time they open AutoRAG, they see the status thanks to that integration. If email notifications are a feature, perhaps an email could be sent when done (not in this flow’s scope, but overall integration to alert user when long jobs complete).
Search & Call integration: The knowledge base might also be used in the prospect Search & Call context. For example, if they scraped competitor websites in AutoRAG (like the “Competitor Analysis” workflow in sample
file-fjpkexbjp2dfhprtk7mhwq
), that could feed into how the agent talks about competition during calls. It’s all part of enriching agent dialogues. Not a direct integration to show in UI, but conceptually important: the data ingested via AutoRAG is accessible across Diala’s functionalities (calls, analytics maybe if queries asked, etc.).
Edge integration – Analytics: Potentially, the platform’s analytics could incorporate knowledge base usage (e.g., how often did the agent pull info from the KB during calls). If so, linking the workflow ID to transcripts or QA logs is needed, but that’s deep integration beyond initial onboarding.
Future Enhancements
Multiple Sources per Workflow: In the future, we may let users combine source types in one go (the code already had type: 'mixed' for workflows that have multiple kinds
file-fjpkexbjp2dfhprtk7mhwq
). Onboarding could allow selecting multiple sources: e.g., YouTube + a PDF at once to create a unified knowledge base. The UI could be a multi-step selection (“Add another source?” repeatedly). This could be powerful but might overload the initial user. Perhaps after first source, we could say “Add another source to this knowledge base or finish.”. This enhancement aligns with robust usage but might be a later iteration once users understand basics.
Scheduled Auto-Updates: A future feature might allow a workflow to periodically update (e.g., re-scan a YouTube channel for new videos weekly). Onboarding could mention or allow enabling “Auto-sync”. For instance, after processing a YouTube channel, ask “Keep this knowledge base updated with new videos from this channel?” and if yes, mark the workflow as recurring. The backend then would periodically run a job to fetch new content. Not trivial to implement but extremely useful (knowledge stays fresh). The UI element could be a toggle or frequency select on final step or in advanced settings.
Quality Feedback Loop: Integration with agent performance – maybe future onboarding or usage of AutoRAG will highlight how the knowledge base improves call outcomes. Over time, maybe the system can recommend sources to add based on questions the agent couldn’t answer. For example, if in calls agents often say “I don’t have information on X,” the system might suggest uploading docs about X. This is beyond the initial onboarding, but a possible system enhancement: showing suggestions in AutoRAG dashboard like “Customers often ask about pricing details. Consider uploading your pricing FAQ.”.
UI Enhancements: Could add visualizations, e.g., a graph showing how many embeddings or content added over time, or a treemap of content by source. In onboarding, probably not needed, but in main section it would be cool.
Collaboration: Maybe allow multiple team members to contribute sources. Not directly onboarding matter, but if in future an org has multiple users, one user’s onboarding could show existing knowledge bases and suggest linking to those instead of duplicating efforts.
Security & Privacy features: Possibly later allow setting certain knowledge bases as sensitive (require certain call contexts to use, etc.). Not likely needed to mention in onboarding, but might be part of advanced settings in future (like a toggle “This data is confidential” which then might restrict it to internal calls or something).
The AutoRAG onboarding, by implementing these enhancements down the line, would continue to demystify a complex AI capability in a user-centric way, and ensure users get the maximum value by keeping their AI agents informed and up-to-date with minimal effort.
Calls Module Onboarding Tendril
Purpose
The Calls onboarding flow is designed to help users set up and understand Diala’s calling system, which encompasses automated campaigns and real-time call monitoring. This module is one of the most critical, as it directly deals with the core functionality: making and managing calls with AI agents. The onboarding serves as an interactive tutorial that leads the user through configuring an Automated Call Campaign (sometimes referred to as “Search & Call”) – from defining a target audience, to launching the calls, to monitoring results. By doing so, it demonstrates how Diala can scale their outreach or handle inbound calls with AI at the helm. It’s a self-contained feature: even without prior context, a user could complete this flow and get a tangible outcome (like a scheduled campaign or an ongoing call simulation). It connects deeply to Diala’s infrastructure, triggering backend processes such as lead searching (for prospects), call scheduling via telephony APIs, and analytics tracking. The onboarding covers both the setup aspect (which ties into the prospect search engine) and the live aspect (which ties into the call engine and analytics), thereby giving a holistic view of the Calls module. Ultimately, it aims to convert a first-time user into someone confident enough to run their own AI-driven call campaign, seeing firsthand the system’s capabilities in action.
Flow Steps
Welcome & Campaign Intro: The user is welcomed to the Calls module with a vibrant violet background (matching the “Search & Call” onboarding color from the main menu
file-fjpkexbjp2dfhprtk7mhwq
, which was violet). The title might be “Intelligent Calling Campaign Setup” with icons like UilPhone and UilSearch crossing each other
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The welcome message frames the scenario: “Ready to supercharge your outreach? Let’s set up an AI-powered calling campaign that finds prospects and dials them automatically.” This immediately communicates that the system can both search for leads and call them – a unique value proposition. If the user’s name is known, include it ("Welcome back, [Name]!"). A brief description explains: “In a few steps, you’ll configure a campaign: define who to contact, let Diala find contacts, and have your AI agent call them, all on autopilot.” This sets expectations and perhaps alleviates intimidation. A Start or Configure Campaign button leads to the next step.
Define Campaign Criteria: This step gathers parameters for finding prospects or defining the call list. It likely corresponds to the fields in the HuntConfigurationModal (the code references location, businessType, keywords, etc. when saving a search workflow
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). We present a form with fields such as: Location (text or dropdown for region), Industry/Business Type (dropdown or text, e.g. “Software” or “Restaurants”), Keywords (tags or comma-separated terms describing target profiles), Include LinkedIn (a toggle to indicate whether to use LinkedIn data as well), and Search Depth (how many pages or how deep to search – could be simplified to e.g. “Broad vs Focused” search). These inputs define the scope of prospecting. The UI could arrange them in sections: e.g., “Who to search for” (industry, keywords), “Where” (location), “Data sources” (a checkbox for LinkedIn or other directories), and “Volume” (search depth or number of prospects desired, could be implied by depth). This is a bit complex, but we can keep it user-friendly: use plain language labels like “Target Industry”, “Target Location”, “Keywords (e.g. CFO, IT security)”, “Search extensively (yes/no)”. Each field might have helper text or placeholders. For instance, Location placeholder “e.g. San Francisco, CA or ‘USA’”, keywords placeholder “e.g. fintech, SaaS”. If the user doesn’t know what to put, we might allow defaults or skip (but ideally these are required to get meaningful results). We validate critical ones (perhaps require at least one of location or industry or keywords). Once filled, Next button is enabled.
Select/Confirm AI Agent & Swarm: Now the user specifies which AI agent (or swarm of agents) will make the calls. If the user has multiple agents or any swarms (from previous modules), we list them. Possibly two tabs: one for single Agent, one for Swarm, or a unified list labeling which is which. For example, a radio list: “Call with Diala-Tone (Sales Agent)” vs “Call with Sales Battalion (Swarm)”. If the user only has one agent and no swarms, we auto-select that and just show it (with an option to confirm or change later). If they have none (unlikely if they did earlier onboarding, but possible), we could embed a quick prompt to create an agent (similar to how Swarms onboarding handled no agents – but if reaching here with none, perhaps direct them to Agent onboarding first). Let’s assume they have at least one agent. The UI shows agent name, purpose, maybe current status (should be active), and even performance stats to give confidence (like success rate). If swarms are present, show those similarly (with number of agents indicated). The user picks one. The design can use cards or just a select dropdown. Cards would be nice: e.g., a card with agent name and avatar, when selected, highlighted. A UilRobot icon for agent, UilUsersAlt for swarm could distinguish them. After selection, Next.
Phone Number & Schedule: The user chooses what number to call from (and potentially when to run the campaign). If Diala integrates with phone number management (the code has a PhoneNumber interface with many fields
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 and a list of mock numbers
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
), the user likely has one or more phone numbers (or SIP endpoints) configured to make calls. We present a dropdown or list of their available outbound numbers (with displayName if given, and perhaps type like PSTN or SIP)
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. If they only have one, just show it. If none, instruct them to add one in Settings or present a note that Diala will use a default demo number. For onboarding, we might not require an actual number if it’s a demo (the call could be simulated), but at least show the concept. Next, scheduling: do they want to start calls immediately or later? Provide an option: Start now (immediate) or Schedule later (pick a date/time or time window). For simplicity, default to now. If scheduling is important (maybe not in MVP), we include a date-time picker or a simple “Tomorrow 9am” preset. Another possible input: Max Calls or duration (how long to run). But could keep it open-ended for demo. Perhaps incorporate a safety like “Limit to 10 calls” for demo. But not to overwhelm user, we might hide advanced scheduling and just go with immediate. The UI for selecting phone number could be a Card list like on the Numbers tab of Calls dashboard, showing the number and name (e.g. “Main Sales Line: +1 555 123-4567”
file-fjpkexbjp2dfhprtk7mhwq
). Selected gets bold outline. If scheduling, maybe a small card with clock icon and a couple of options.
Review Campaign Setup: Now we compile all chosen settings into a summary for confirmation. The card might read: “Campaign: Find [keywords] in [location] in [industry] and call using [Agent/Swarm Name] from [Phone Number], starting [Now/later].”. Essentially a sentence or bullet list of the plan. Example: “Prospects: Fintech, SaaS companies in San Francisco, CA. Agent: Diala-Tone (AI Sales Agent). Calls from: +1 (555) 123-4567 (Main Sales Line). Start: Immediately.” We present this in a nicely formatted way, maybe using badges or icons next to each piece of info (location pin icon next to location, briefcase icon for industry, user icon for agent, phone icon for number, clock for schedule). The user verifies all good. If something is off, they can go Back to change. If okay, they hit Launch Campaign (or Start Calling). This triggers the backend to commence the search and call operations. Transition to next step.
Prospect Search & Call Progress: After launching, the onboarding flow shows a dynamic view combining lead search progress and call execution status. This could be split into two areas: Prospect Finding and Live Calls. Initially, the system will be searching for businesses that match criteria. We display something akin to a loading list: e.g., “Searching for businesses...” with maybe a spinner. As results come in, we populate a table or list of prospects (like company names or contacts). The code’s SearchWorkflow has stats like pagesFound, businessesExtracted, validated, etc.
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We can show some of those: “Pages scanned: X, Businesses found: Y, Valid phone numbers: Z” updating in real-time. Each found contact could briefly flash on screen like "Found: Acme Corp, SF – number +1 415 ...". This gives user the sense that the system is actively discovering leads. Simultaneously or after a short delay, the calling begins: perhaps once a few numbers are identified, the AI agent starts dialing. We then show a Live Call Monitor section. For example, show an entry "Calling Acme Corp... (ringing)" with a status icon, then "Connected – Talking" when the agent is on call, etc. The code references LiveCallMonitorModal and CallAnalyticsModal
file-fjpkexbjp2dfhprtk7mhwq
 – those are for the full dashboard, but we mimic a mini version. Potentially, show one or two concurrent calls (if swarm, multiple calls at once). Represent them as small cards or lines with Agent name, prospect name, call status. For realism, we can simulate a short conversation: e.g., after "Talking" for a few seconds, mark "Call Ended – Outcome: Interested (scheduled follow-up)" or "Not interested". These outcomes could be random or predetermined. It demonstrates that the system not only calls but also tracks outcomes (success, no pickup, etc.). We can incorporate some analytics visuals if time – like a tiny chart for pick-up rate, but likely not needed in onboarding, a simple textual log suffices. If possible, integrate audio or transcript snippet: maybe display one line the agent said and the response (like a snippet: Agent: "Hello, is this John from Acme Corp?" – no need for actual audio, just text to illustrate conversation). This might be too deep, but even a static “Transcript available” icon shown could hint at capabilities. The key is to amaze the user that in moments the system found leads and is actually executing calls.
Campaign Outcome Summary: After simulating a few calls (maybe we say it called 5 contacts for demonstration), we present a brief summary of the campaign’s initial outcomes. For example: “Campaign Complete (Demo)\n5 Calls placed, 3 answered, 2 voicemails.\n1 Interested lead, 1 Follow-up scheduled.” This summary would normally accumulate as more calls run, but for onboarding we end it here. We might clarify: “(In a real campaign, calls would continue until the list is exhausted or stopped.)” Then we highlight that the user can monitor everything in the Calls dashboard going forward. Possibly show them where (like "In the Calls dashboard, you'll find Live Call Monitoring and detailed analytics for each campaign.").
Wrap-up and Next Steps: The final screen congratulates the user: “Your first AI-driven call campaign is set up!”. It provides guidance: “You can view and manage campaigns in the Calls section. Analytics for this campaign will update in real-time – check the dashboard to see results and listen to call recordings or read transcripts.” We then offer navigation: a Go to Calls Dashboard button to exit onboarding into /dashboard/calls (perhaps pre-selecting the "calls" or "agents" tab using the query param logic in code
file-fjpkexbjp2dfhprtk7mhwq
). Another possible button: View Campaign Analytics which could open the CallAnalyticsModal for this campaign (if we had an ID; maybe not needed for onboarding, better to just go to dashboard). Also possibly suggest: “You can also create campaigns without the search step if you have your own call lists.” (Because maybe Diala can also import contacts – but not covered here, just a note that search is optional in future usage). Finally, emphasize that the Agents, Swarms, RAG all come together in Calls: their agent was used, knowledge base (if attached) would be used in call, swarm if chosen, etc. This ties the onboarding threads together conceptually.
Components (Frontend)
Form Inputs and Cards: For campaign criteria (step 2), we’ll use various form controls. Likely simple <Input> for location and keywords, and maybe a <Select> or set of radio for industry (we could supply a list of common industries as options to ease input). We may also use <Select> or <Combobox> for some fields. All these go inside a Card with a header “Prospect Search Criteria” or similar. Each field label is bold uppercase (like “LOCATION”, “INDUSTRY”), and use placeholders as described. Possibly group location & industry on one row if space (md screens), otherwise stack. Use our UI input with thick borders. If LinkedIn is a toggle, use a <Switch> component (with label “Include LinkedIn data”). Search depth could be a slider (like “broad <-> deep” search), but simpler: maybe a dropdown: Basic (find ~50 prospects) vs Extensive (find ~200). The code shows searchDepth parameter
file-fjpkexbjp2dfhprtk7mhwq
; define maybe Low/Medium/High mapping to pages. Keep default medium.
Agent/Swarm Selection: Use either radio buttons or card selection. We could implement similar to earlier flows – e.g., in Agents onboarding we had to select voice agent and pitch, here we select calling agent. If using card, each agent card shows name and perhaps a small subtext (like purpose or swarm size). Could reuse AgentCard display but modified for selection. If many options, a simple radio in a list might suffice too. But visual card toggles would be nice. If we foresee adding new from here, we might add a small button "Create new agent" if needed, but likely not for onboarding.
Phone Numbers & Schedule: The phone numbers might be presented in a dropdown or list. The code’s numbers list has fields like provider, status, etc.
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We just need number and name for display. Possibly show status if relevant (most will be active). We could present each as “Name – Number (Type)”, e.g., "Main Sales Line – +1 555 123-4567 (pstn)". Selection via radio or dropdown. For scheduling, a simple pair of radio: “Start Now” vs “Schedule for Later”. If they pick later, reveal a datetime picker (the design system might not have one prebuilt; we can use a native input type="datetime-local" styled appropriately or just a couple of selects for date and time). But to avoid complexity, we might skip detailed scheduling in onboarding and assume Now. If scheduling later, we won’t actually wait, it's just for demonstration, so perhaps skip to not confuse. (Alternatively, if user picks schedule later, we just say "Campaign scheduled for X. We'll notify you when calls start." in summary, but we won't simulate calls now – that could be an alternate path. But doing an immediate demo is more exciting, so likely default to immediate run).
Progress Display Components: For searching prospects and live calls, we might create small sub-components or just structure it clearly in JSX:
A <div> for search progress containing maybe a <Progress> bar or just textual counters. Possibly represent it like an expanding list: we add an <li> for each found business (like a mini log). But could also just update numeric stats. A fun way: have a list area with a fixed height and overflow, each found company name appended to the list (scrolling as it grows). That gives a visual of accumulation. But maybe just counters is fine given time. We'll at least show something like “Found X prospects so far...” updating.
A <div> for active calls. Could show one call at a time if sequential or multiple if parallel (especially if swarm chosen, could do 2-3 concurrently). Each call could be a Card or row with: prospect name, call status, maybe a timer. Represent statuses by icons (phone ringing icon for dialing, phone with waves for in-call, check or cross for outcome). The code’s AgentCall interface had fields for answered, pickedUp etc. which suggests tracking performance by agent
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 – but for live display, we use simpler terms. Possibly incorporate CallStatusIndicator or similar if exists; if not, static. After finishing calls, we list outcome (like "Interested" or "No answer"). We might use colored badges for outcomes (green "Interested", red "No Answer", yellow "Voicemail").
Possibly a simplified timeline: we don’t need actual timeline UI, linear updates suffice.
Modal vs Inline: We will likely do this progress view inline in the onboarding page (like transcripts onboarding did everything inline). However, if we had done a separate modal for call monitoring, that’s more for the full dashboard. Here we integrate it into the guided experience.
Summary/Analytics: At the end, the summary can just be text and badges on a card. If we wanted, we could embed a mini chart – but charts might be too heavy for onboarding. The main app likely has analytics charts (volume over time, etc.). We can simply list key numbers as done above. Perhaps in bold with icons: e.g., a phone icon with "5 Calls", a user icon with "1 Lead". If the CallAnalyticsModal is easily pluggable, we could show it, but that’s probably too much detail. Better to funnel them to the actual Calls dashboard for full analytics if interested.
Backend Architecture (Convex & APIs)
The Calls onboarding initiates a complex orchestration that involves: web search for contacts, and connecting calls via telephony. Breaking it down:
Search Workflow (Prospect Hunting): This likely corresponds to what the code refers to as a SearchWorkflow (the structure with parameters and stats saved when using HuntConfigurationModal)
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The onboarding will call a Convex function or action to start this search. Similar to AutoRAG, it might create a workflow record and perform asynchronously. Possibly they have a scrapper integrated (maybe something like SerpAPI or custom headless browser calls to Google/Bing or LinkedIn search). The backend will take the criteria (location, industry, keywords) and generate search queries. For each result page, it scrapes company info and tries to extract phone numbers (maybe through the company’s website or directories). It populates a list of found businesses with numbers (businessesExtracted, businessesValidated in stats). It probably also cross-checks with LinkedIn if that option is true (e.g., to gather company size or contact names). This is a non-trivial pipeline: it could involve multiple external APIs (Google Custom Search API for web, LinkedIn API or unofficial scraping for LinkedIn, and maybe a third party for phone validation). For blueprint, assume the backend can handle it and just call an action startProspectHunt returning a workflow id.
Call Campaign Initiation: In parallel or once some contacts are found, the system will start dialing. This involves the telephony integration (Telnyx or another provider) and the agent AI for voice. Likely, they have a call control service that the Agents backend triggers. Possibly the architecture: Diala’s server initiates outbound calls via Telnyx API (by sending a call command with the chosen caller ID number and target number). When the call connects, Telnyx streams audio to/from Diala’s media server (or directly to an AI service). Diala uses Deepgram (for speech-to-text) to transcribe the callee, and uses OpenAI (or similar) to generate agent responses, and ElevenLabs to synthesize the agent’s voice, streaming audio back via Telnyx to the callee. This is all happening in real time, likely orchestrated by a state machine (maybe that’s what convexEntryPoint in agent data references – which conversation workflow to run
file-fjpkexbjp2dfhprtk7mhwq
). For onboarding, we don’t actually perform calls but simulate. However, if the platform is fully functional, one could theoretically do a real call to a test number. But safer to simulate for new users, unless they want to test with their own number. Possibly an enhancement: allow the user to put their own number to receive a demo call from the agent. But that might be risky or too early (and need phone verification etc.). So we simulate. The backend might provide a mode where calls are not actually sent to PSTN but recorded as if done (a dry-run mode). Or we just create dummy call logs.
Convex Data Updates: As calls are “made”, the system would create Call records, update agent stats (pickedUp++ etc.), and generate transcripts and analytics. The code shows in calls page, an AgentCall structure used for a table (calls made, answered, etc.)
file-fjpkexbjp2dfhprtk7mhwq
. The onboarding likely won’t write to the real DB for everything (unless we do a real call). If simulating, we can either push some fake data into the Convex DB (like increment some counters) or just locally simulate. Perhaps better to keep it local for onboarding, not to pollute actual analytics with fake data. The integration can remain conceptual (we describe that these stats would reflect in the real system). If we did decide to involve real data, we’d maybe create a “demo campaign” entry marked as such, but likely not.
Ending the Campaign: In a real scenario, the campaign would continue calling until the prospect list is exhausted or user stops it. For onboarding, we plan a short run. So the backend process for search can be stopped early. Perhaps we only gather 5 prospects then stop. The call process calls those 5 then stops. The Convex should handle halting further calls. Or we treat it as a demo mode within the onboarding code, not on backend – more likely, simulate entirely on front without contacting external. But since we do want to show some realistic asynchronous behavior (like search progress), maybe use a dummy timeline with timeouts to append results. Or spin up a Convex action that yields logs for a short time.
Tracking & Logging: The system likely has logs for each call (transcripts, outcomes). Possibly stored in a Calls collection. If we were doing this fully, we’d want to create those logs. But again, as a demo maybe not persistent. Could just generate ephemeral data. The integration is more about showing what will happen.
Notifications and Real usage: If a user truly set up a campaign for later (schedule), the backend would schedule those calls at that time (maybe using a Convex scheduled function or an external scheduler). Not needed in onboarding, but the architecture supports it if implemented.
Edge Cases: If the user defines a very broad search, real search could take time. Onboarding might either limit scope or inform the user partial results. We likely limit behind the scenes. Perhaps the Convex function for demo purposely only searches a couple of pages to be quick.
Convex Integration in UI: We may choose to use useQuery or polling to reflect search progress in UI. Similar to how we approached RAG. For search, maybe easier to poll a status (like how many found). If we treat it as part of a SearchWorkflow, we could have a record updated and subscribe. Or simply simulate with setInterval incrementing a counter in local state (since we don’t intend to store real results, simulation might be entirely front-end). Actually, simpler: do it front-end. When user hits launch, we can use setTimeouts to simulate asynchronous events:
After 1 sec, show first prospect found.
After 2 sec, show second, and start first call.
After 3 sec, show third, first call ends, second call starts...
etc.
This avoids heavy backend complexity for a demo. But if the system is ready and we want to truly show it, we could do a scaled-down real thing. It's a decision: fidelity vs complexity. Possibly simulation is fine for onboarding.
Telnyx/Telephony Integration: If it were real, the system would use Telnyx API (as keys were in settings
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). It would create call sessions. Telnyx would send webhooks to Diala on call status changes, which Diala’s Convex or serverless functions handle, updating call status (like pickedUp or not, etc.), and instructing the media pipeline. This likely not done via Convex (real-time media might be handled by a separate service, but logging could update Convex). For blueprint, we don't dive too deep, just know calls would be placed.
Design System & Neobrutalist Elements
Visual Style: The violet theme (as used in SearchCalled) gives a distinct identity to the Calls onboarding. Violet and black combination in backgrounds, with white text on violet for contrast, was used in the searchcalled placeholder
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We continue that. The grid background or cross-hatch from others remains (just adjusting opacity maybe to 0.1 like others did
file-fjpkexbjp2dfhprtk7mhwq
). Titles like "SEARCH & CALL SETUP" in huge black letters on card headers maintain the brutalist impact
file-fjpkexbjp2dfhprtk7mhwq
.
High-contrast Info Blocks: The progress display can employ brutalist info blocks: e.g., for each call status, use a white or light-gray background block with black border showing the call details, overlaying on a darker background. The transcripts UI had similar blocks for info (like the red blurred backdrop with white info cards inside
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 for training data info). We can do similarly: the Live Calls area could be a card with a semi-transparent background to stand out on the violet backdrop, containing inner bordered sections for each call (like [20†L6170-L6178] where stats were in a blurred panel). Actually, in swarms card, they have those translucent stat blocks
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, which is a nice style. We could mimic that for "CALLS: 5" and "SUCCESS: 60%" etc., but for now just straightforward.
Typography for Metrics: Use large numeric displays where possible. E.g., the number of calls or leads can be big to celebrate success. Possibly style "5 Calls" as text with class "text-4xl font-black". Surrounding context smaller. The stat cards in design system might be reused (StatCard component was used in various places
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). Actually, we could use a couple of <StatCard>s for final summary: one for Calls Made (with icon UilPhoneVolume
file-fjpkexbjp2dfhprtk7mhwq
), one for Interested Leads (with icon UilCommentAlt or UilThumbsUp). But that might break the narrative flow to suddenly show stat cards. Instead incorporate these stats into our summary card as text with icons.
Movement & Quirky Elements: We can lean into the "busy activity" vibe. For instance, might use the animate-pulse class on the status dot of an active call (like how active agent dot pulses green
file-fjpkexbjp2dfhprtk7mhwq
). For ringing, maybe animate an icon or wiggle a phone emoji. These small touches fit the playful brutalist approach.
Consistent Buttons: All calls to action (Launch Campaign, etc.) are big, bold, possibly with icons. For example, Launch Campaign button might show an analytics icon UilAnalytics as used in the searchcalled stub
file-fjpkexbjp2dfhprtk7mhwq
. Actually, in searchcalled they had "Configure Search" with an analytics icon
file-fjpkexbjp2dfhprtk7mhwq
. We could follow that: maybe a phone icon on the launch button. Keep button styling same: background Diala blue (0,82,255) or maybe a contrasting color like green to indicate go. But likely they used blue for key actions.
Notification of Steps: Possibly use step indicator (like OnboardingNav) here too if needed, but it's probably straightforward enough without. If used, update steps like "Criteria", "Agent", "Number", "Review", etc. Could help but might clutter UI since steps are many (maybe 5 visible steps plus progress). Could compress some logically (like treat search & call as one step in nav since they happen automatically after launch). Might omit nav for simplicity.
Emotion and Tone: Keep the copy confident and encouraging, reflecting the bold design. E.g., on launch: "Stand by as Diala hunts for prospects and dials out..." – kind of an excited tone. Brutalist design often pairs with straightforward, sometimes cheeky text. We can infuse a bit of excitement: e.g., when showing an interested lead: "🤝 Got one! Acme Corp wants a callback." (Using an emoji like handshake can be within style boundaries as a pop of personality.) Use sparingly to not deviate from professional feel, but one or two could lighten it.
State Management
currentStep (number): from welcome (1) to review (5). After launching, we might either set currentStep to 6 or just to a special "inProgress" state beyond the normal count, since it's not a typical step you navigate back from, it's a running state. But we could treat it as step 6.
Form fields: location, industry, keywords, includeLinkedIn (bool), searchDepth. These capture user input from step 2.
selectedCaller (agent or swarm identifier): could be something like { type: 'agent', id: 'a1' } or if swarm, { type: 'swarm', id: 's1' }. Or simpler, we can use one variable for either, since an agent and swarm have distinct id formatting or we keep a separate bool useSwarm. For front-end, maybe easier to maintain selectedAgentId and selectedSwarmId mutually exclusive (only one will be non-null). The UI ensures they pick one option or the other.
selectedNumberId (or the actual number string): from step 4. And possibly scheduleTime if they chose later (or a flag startNow).
campaignLaunched (bool): flips true once they hit Launch. This triggers the simulation or actual process.
Simulation state: If we simulate in front-end, we manage:
prospectsFound (array of objects or names) – each time we "find" one, we push to this array via setState, causing UI to list them.
calls (array of call objects) – each with prospect, status, outcome. As we simulate calls starting and ending, we update this array or specific call objects. Could hold only active calls, or active + finished. Perhaps have separate arrays activeCalls and completedCalls for clarity. Or one array with a status property and we render accordingly.
prospectCount, callCount, etc. – or derive from above arrays lengths.
If doing an automated timed simulation, might not need these to be reactive to user input, just internal for simulation logic. But to display dynamic updates, we store them and use re-renders.
If we attempted to do some real backend:
searchWorkflowId and status fields like earlier flows.
campaignId for the calling campaign.
But probably not for simulation.
We also track showAnalyticsModal or such if user triggers a deeper view. But likely not.
Back navigation states: allow editing previous steps, so keep input states around and just hide/show sections by currentStep. If currentStep changes, form states remain intact so user can modify and go forward again.
Possibly isCalling or callPhase to indicate we are in the calling phase (to maybe differentiate search vs call updates if needed). But can deduce from currentStep or launched flag.
If any error, e.g., no prospects found (in simulation we won't let that happen, but in real could), could have an error state or message. But in demo, skip error.
UX Interaction & Animations
Simulated Timing: We want a brisk but not instantaneous demo. Use short delays (maybe total of 15-20 seconds for the whole progress). That’s long enough to observe but short enough not to bore. E.g., find first prospect at 1s, then every 1s find another until 5. Start call at 2s, etc. If swarm selected, maybe show two calls at once after a few seconds, to illustrate parallel. We can use setTimeout or a series of them for each event, or a single interval that increments a counter step and triggers events at certain counts. Keep track to clear timers if component unmounted unexpectedly.
Scroll: If listing prospects or calls, ensure the container scrolls as items overflow (with a nice scroll style inside a card). Or auto-scroll to bottom as new items append, so the latest is visible. Use a ref to a list end and call scrollIntoView. Minor detail but improves feel of a live feed.
Call Status Transitions: Perhaps use color or icon changes: e.g., initially a call entry is gray "Dialing", then when "Talking", highlight it or show a small waveform icon. Then outcome with a colored badge. We might animate the change (fade from "Dialing" to "Talking", etc.). Could do a quick fade or slide. CSS transitions on text or a class change suffice.
Sound Effects (optional): If we wanted to be fancy, playing a subtle dial tone or ring sound during the demo could make it immersive. But likely avoid to not startle user or require audio permissions. Perhaps not.
Interactive during simulation: We likely won't allow user to intervene in calls (like no "hang up" or "pause" in onboarding). They just watch. That’s fine. Possibly provide a "Skip demo" or "Finish early" if they don’t want to wait all 20 seconds. Could just have a "Finish" button become visible after launch, which they can click to skip to summary. Not critical but user-friendly if someone is impatient.
Back disabled after launch: Once launched, going back doesn’t make sense as it’s in progress. We might lock navigation and hide or disable the Back button from that point. Or if we allowed schedule later and they want to change mind, theoretically they could go back before launching. But after launching, not.
Completing: When simulation is done, maybe briefly flash a "Campaign Completed" message or icon. Then auto-advance to final summary after a 2-second pause, giving them time to see the last events. Or directly show final summary as part of the progress section turning into summary. Could also scroll up or collapse the progress feed to focus on summary. Possibly we could overlay a semi-transparent celebration (like a checkmark or confetti) when done.
Confetti/Fireworks: Since this is the culminating multi-module demo basically, a tad of celebration could be warranted. E.g., a quick confetti burst when finalizing summary (choose a library or simple falling shapes as earlier idea). If other onboardings didn't do it, maybe we skip for consistency, but this is arguably the biggest "wow" moment, so a little confetti might be nice. Ensure it doesn't hinder reading summary.
Guide to Dashboard: The final step should clearly indicate where to go for more. Possibly highlight "Calls" in the side nav if we had control (like adding a glowing effect around the Calls icon in the sidebar). That would be a nice touch: since the user likely sees the dashboard layout (if onboarding is a page in the app), we could momentarily highlight the navigation item or relevant UI piece. But implementing that might be tricky in isolation, and maybe not expected. Instead, a textual prompt and a button should suffice.
Technical Implementation Details
We’ll implement the simulation timeline in the React component. For example:
jsx
Copy
Edit
useEffect(() => {
  if (campaignLaunched) {
    // simulate finding prospects and calling
    const events = [
      { t: 1000, action: () => addProspect("Acme Corp") },
      { t: 2000, action: () => { addProspect("Beta Inc"); startCall("Acme Corp"); } },
      // ... more events
    ];
    events.forEach(evt => {
       setTimeout(evt.action, evt.t);
    });
  }
}, [campaignLaunched]);
We define helper functions addProspect(name) (push into state) and startCall(name) (push into calls array with status "Dialing", then possibly schedule another event for call connected, ended, etc.). We chain timeouts to simulate call progression: e.g., after startCall, setTimeout in 2s to mark it connected, then in another 3s to mark ended with outcome. That nested but manageable for a few calls. Or store these events in the events timeline as well.
Ensure to clear timeouts if component unmounts using useEffect return () => clearTimeout for each or keep references. But likely not needed since user will either finish or navigate in app after. Still, best practice.
The final summary can be triggered by a final event in timeline (like at t=15000ms, set a state simulationDone = true or directly set currentStep to 7). Or we could derive simulationDone by seeing all calls completed if we track number to make vs made. Simpler: schedule a final event to finalize.
The timeline approach means adjusting if user picks swarm (calls concurrently) vs single (calls sequentially). For swarm, maybe we simulate two parallel calls at once. For sequential (single agent), one finishes then next starts. We can branch the event list depending on selected agent vs swarm. E.g., if swarm, events show two startCalls quickly and maybe overlapping durations. If agent, space them out. This is nuance but to reflect difference. Could do just 2 calls sequential for simplicity regardless, but showing parallel would highlight swarm benefit. Since we have swarms module done, user might try that. We'll implement a basic difference: if swarm chosen, start 2 calls around the same time.
If we ever integrated actual backend call placement (like to the user's phone as a demo), we would involve connecting to Telnyx and actually making a call. That’s heavy and risk of user not picking up or cost. So skip in onboarding.
For integrity, if we did call backend for search, we would have to feed results back to UI similarly, but it's easier to simulate that too. So no Convex calls for actual search. The only maybe Convex call we could do is to create a campaign entry if we wanted it to show up in their real calls list, but might be unnecessary. Perhaps we won't create actual DB records at all – onboarding can exist in a sandbox. That means after finishing, if user goes to Calls dashboard, they might not see the "demo campaign" in logs (which is fine, it was just a tutorial). However, one could argue it might be nice to actually create it so they see something. But since the demo was partial (maybe only 5 calls) and not real, leaving no trace might be cleaner to avoid confusion. We'll just tell them how to do it for real. If we wanted, we could create a dummy "Completed Demo Campaign" record in Convex, but it might clutter their actual analytics. I'd lean not.
So basically the onboarding calls no backend except maybe checking if they have an agent/number to list them. We can fetch agents and numbers via Convex queries at start (like for Swarms we did). If none found where expected, we adjust UI accordingly (like prompt to set up). But likely they do from earlier flows.
Integration with context: since the onboarding is part of app, we have access to whatever providers. The numbers might be in a store or not. If not, we can use a query or simulate one number if none. Possibly the settings page is where API keys/numbers are added, but on initial account maybe there's one default or user input some. If not, we might have to tell them to add one. But for onboarding, maybe we assume they've added a number (in reality, maybe they haven't, but it complicates flow to diverge into adding number). Alternatively, if none, allow them to proceed with a "demo number" that isn't real. We'll do that: show something like "Demo Line (virtual)" if no real number.
Ensure the component is resilient: if no agent, we might create a default. But since we had Agents onboarding, user likely did that first. If not, we should at least require an agent. Could detect and if none, show a message: "Please create an AI agent first to use in calls" with a link to Agents onboarding. But since the user specifically asked for an onboarding for calls, presumably the idea is they'd go through voice agent first anyway. We'll still handle gracefully by maybe creating a quick agent behind scenes named "Demo Agent" if absolutely needed (but that is messy). Simpler: if no agents, just instruct then exit or skip certain steps. But I'd expect sequence: voice agent onboarding -> transcripts (optional) -> swarms -> RAG -> calls. Not guaranteed though. Perhaps call onboarding should encourage using a swarm or agent created. We will assume at least one.
All in all, the calls onboarding logic is mostly front-end simulation orchestrated by timed events, integrated with existing data (agents, numbers) for user-specific context.
Integration Points
Agents & Swarms: The call onboarding directly uses the outputs of previous ones. The chosen agent or swarm in step 3 is exactly one the user configured. If they created a custom agent in Agents onboarding (with a certain persona), they will now use it here – making the earlier work tangible (integration of modules). If they created a swarm (with multiple agents), they can choose it, and the simulation will illustrate parallel calls, reinforcing why swarms matter. In summary, the onboarding flows aren’t siloed: this final Calls flow ties them together, showing the agent or swarm using presumably the knowledge base (if RAG was done) to handle calls, etc. We might mention implicitly that the agent could use the knowledge base – e.g., if they had done RAG, maybe mention on an answered call: "Agent used knowledge base info to answer a question" in passing. But that might be too detailed. Still, conceptually, yes integration: if an agent had ragSources, the call engine would query them to answer specific questions.
Phone Numbers (Telephony Integration): If the user had added multiple numbers in settings (maybe local vs toll-free, etc.), they appear here. If not, the system might have assigned a default. The integration with Telnyx is underlying but not directly visible except via the number selection. Later, on the Calls dashboard (numbers tab), they can manage these numbers (like purchase new, etc.). Onboarding touches just the selection. If the user chooses an inactive or maintenance number (in data, one was maintenance
file-fjpkexbjp2dfhprtk7mhwq
), ideally we filter those out. We'll assume active.
Calls Dashboard: After onboarding, if the user goes to the Calls dashboard, they might expect to see evidence of what happened. As decided, we probably won’t create actual logs, so they might see nothing there except their agent stats (calls=0 still). To prevent confusion, our final text should clarify the campaign was a demo. Possibly encourage them to run a real one. Or as an alternative, we could allow them to actually finalize the config into a real campaign. For example: after the demo, we could say "Now that you've seen how it works, do you want to run this campaign for real?" If yes, then actually use the same criteria to start a real search & call in the background. That could be an advanced option – might overwhelm. But could be offered if keys/numbers are all set. Might skip. Just signpost that to do it for real, they can go to Calls and launch a campaign (maybe by using the same form there). Actually, the calls page might have the hunts/campaign UI. The code suggests it has a tab for calls and possibly an area for launching new ones (the modals we saw). The HuntConfigurationModal and workflow modals in code likely handle the real thing. So the user will use those next time.
Analytics and transcripts: The calls module integrates with transcripts (recordings) and analytics. We likely mention transcripts (maybe "transcripts of each call are saved in the Transcripts section") tying that piece in. And analytics (like success rate, average call time in Agents or overall in Calls). For integration, possibly after running some calls, the agent's success rate stat could update. But since we didn't actually update DB, in real usage it would. The final summary could hint "Your agent's performance metrics will update based on these calls, visible in Agents Analytics." So it's known that these modules feed each other.
External CRM or follow-ups: Not directly shown, but an integration could be exporting leads or scheduling follow-ups. If Diala had a CRM integration, it might push the interested lead info somewhere. Out of scope for onboarding though.
Ending Conditions: The user might wonder, does the campaign stop automatically or run continuously? We should integrate that knowledge: probably it stops after finishing found leads. If user wants to stop early, the real system likely has a stop button in the Live Monitor. In onboarding, we didn't simulate that interaction. But maybe mention "You can pause or stop campaigns anytime from the dashboard." to integrate that piece of UI concept.
Data Persistence: The prospect search might produce data (the found leads). In a real scenario, the system might store those leads in a database or at least as part of the campaign record (for future reference or reuse). We are not doing that in onboarding, but the integration point exists – if user runs an actual campaign after, those leads would possibly appear in a table in the Calls section (maybe under a "Recent Calls" or some CRM-ish list). The onboarding could mention "All contacts dialed will be recorded, so you can review and export them." if true. But if not implemented fully, skip.
Wrap Up Integration: The calls onboarding is basically the culmination of all previous features working together – it's where Agents, Swarms, RAG, etc., come to life. So it inherently integrates everything. Our narrative and UI hints can point out these ties. E.g., if RAG was done, maybe in a call we simulate the agent giving a detailed answer thanks to the knowledge. But that might be lost on users who didn't do RAG. So maybe not mention it explicitly; but if the user did do RAG, they'd probably connect the dots themselves (and if they didn't, mention might confuse). So best to not mention RAG in call onboarding text unless sure. Possibly neutral mention: "Your agent can answer detailed questions thanks to its training" – that could apply generally (training could mean transcripts or RAG). We do have to keep it accessible even if they skipped transcripts or rag.
Future Real Integration: If the user runs a real campaign after, the flow would be similar but possibly slower (calls take minutes, etc.). The onboarding prepared them for what to expect, making the actual process less opaque.
Future Enhancements
Personalized Demo Calls: As touched on, a possible future feature is letting a new user receive a demo call from their AI agent. For example, after setting up, instead of a simulation in-app, ask "Would you like to receive a demo call from your AI agent right now at your phone?" If they enter their number and accept, the system could actually call them using the configured agent to run through a short script. This is a powerful demo because hearing the AI voice live is convincing. However, it requires a phone number, user consent, and uses actual telephony credits. As an enhancement, it could be offered if the user profile has a verified phone. It's a more visceral integration of the technology into onboarding.
Campaign Templates: Provide templates for common campaigns (similar to how we discussed swarm templates). E.g., "Follow-up Campaign" or "Welcome Call Campaign" pre-filling certain parameters. Onboarding could ask "What kind of campaign do you want?" up front, and if they select a template, many fields auto-fill (maybe skip search if they already have contacts, or use certain keywords). Not in initial, but a future user who logs in might select a template and launch a campaign in one minute. Onboarding could then adapt to show only minimal steps for that template.
Integration with CRM/Contacts upload: In future, not all campaigns will involve searching for leads; some might call existing customer lists. Onboarding in the future might branch: "Do you want to find new prospects or call your own list?" If the latter, they'd upload contacts or select from an integrated CRM. We could add a step to import a CSV of phone numbers for example. This would showcase versatility. It's more complex, so for now we focused on search. But adding that path eventually will cover more use cases.
More Realistic Call Simulation: Later, we might incorporate actual transcript segments or TTS in the onboarding to let user hear how an AI call sounds. Perhaps an animation of a waveform when agent speaking, and text of response. Or even audio playback if they click. This could be a built-in recording rather than generating on the fly, just to show quality of AI voice. It'd impress users but need audio handling. Possibly a "Listen to sample call" button at the end as an extra.
Dashboard Tour Mode: After campaign creation, the onboarding could seamlessly transition into a mini tour of the Calls dashboard (highlighting the Live Monitor tab, Analytics tab). This would directly integrate training with the actual interface. E.g., overlay arrows or highlights on the actual dashboard elements. This crosses from the "onboarding flow" into in-app guidance territory (like guided tours). Could be valuable but requires a framework to highlight UI elements. A future enhancement could unify these, but for now our flows have been separate pages. In lieu, textual guidance and a button to go see the results is what we do.
Continuous Onboarding Flow: Perhaps envision an even larger flow that ties all modules sequentially. E.g., a new user could optionally go through voice agent -> RAG -> swarm -> calls in one guided sequence (with ability to skip parts). This would be a meta-onboarding. Currently they are separate flows, but one could imagine combining them with branching. It's advanced but maybe later a "Setup Wizard" that covers everything in an optimal order. Our separate blueprints could then be interconnected steps of one super-flow. Implementation aside, conceptually it may help some users who want a full setup at once.
Monitoring & Tuning: A possible advanced onboarding extension is after a few calls, showing how to adjust on the fly. For instance, if the campaign isn't yielding success, how to tweak criteria or switch agent. That might be beyond initial onboarding, but for continuous improvement. Possibly an in-app hint later, not in first-run onboarding.
Gamification: Could add some gamification: e.g., show an achievement "First Campaign Launched!" and track how many leads/ calls achieved. These sort of things (badges, progress) encourage usage. Not directly part of onboarding blueprint, but an idea for user retention.
Multi-language or International campaign settings: If expanding globally, onboarding might ask for timezones, multi-lingual agent usage, etc. Could be future fields (like if calling different countries, choose agent languages accordingly). Now likely not needed.
Incorporating Feedback: After calls, maybe prompt user "How did your agent do on these calls?" gather feedback to refine agent prompt. That closes a learning loop. Possibly later a quick thumbs up/down after hearing a sample call.
All these enhancements aim to deepen the user's engagement and success with the Calls module after the initial onboarding, ensuring they not only try it out but also incorporate it effectively into their workflow.The following sections provide detailed onboarding tendril blueprints for each major module in the Diala dashboard. Each onboarding flow is designed as a self-contained, product-like experience that introduces the module’s capabilities, guides the user through setup, and seamlessly ties into Diala’s backend infrastructure. The design follows Diala’s neobrutalist aesthetic – bold typography, high-contrast color blocks, heavy borders, and slightly askew card layouts – ensuring a consistent look and feel across the app. Each blueprint covers the module’s Purpose, step-by-step Flow, key Frontend Components, Backend Architecture (Convex functions and external API calls), references to the Design System, State Management approach, UX Interaction details, Technical Implementation, system Integration Points, and potential Future Enhancements.
Agents Module Onboarding Tendril
Purpose
The Agents onboarding flow introduces new users to creating and configuring an AI Voice Agent – a virtual caller that can engage customers. It serves as a lead funnel by demonstrating how easily a custom voice agent can be set up and deployed. The tendril’s goal is to have the user create their first agent profile (with a persona, voice, and language) and understand the agent’s role in Diala. This guided setup connects to Diala’s core systems by actually saving the agent configuration to the backend and preparing it for use in calls. By the end, users see a preview of their agent in action (e.g. a simulated call greeting), which both educates and entices them to continue using the platform.
Flow Steps
Welcome & Agent Naming: The user is greeted with a welcome card on a distinctive background (e.g. vibrant blue, consistent with the voice agent theme
file-fjpkexbjp2dfhprtk7mhwq
). They’re prompted to enter their name or company name, which personalizes the experience (“Welcome, [Name]! Let’s set up your first Voice Agent.”). This step hooks the user and transitions into agent creation.
Define Agent Profile: The user inputs basic details for the agent: Agent Name (e.g. “Diala-Tone” or a custom name) and Purpose (a high-level role like Sales, Support, etc.). This might be presented on a tilted card with input fields (using the design system’s <Input> component for text
file-fjpkexbjp2dfhprtk7mhwq
). For guidance, the UI can show an example agent profile snippet (name, purpose, short description) in a stylized way. A Continue button (disabled until required fields are filled) moves to the next step.
Select Voice & Language: The user chooses the voice characteristics for the agent. This includes selecting a Language (with accent) and a Voice Style. For example, they might pick English (US) vs English (UK), and a voice persona like Friendly or Professional. The UI could display a grid of voice cards: each card showing a language flag and voice name. When a card is selected, it gets a bold black border and maybe a subtle pulse animation. In parallel, the user selects the Voice Provider or TTS engine if applicable (e.g. a default like ElevenLabs or an open-source voice). Both selectedLanguage and selectedVoiceAgent state must be chosen before continuing (the Next button becomes enabled only when both are set
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). This step leverages the neobrutalist style by showing each option as a chunky card or button with high contrast (e.g. a button with a background color representing language, and an icon like UilLanguage for languages
file-fjpkexbjp2dfhprtk7mhwq
). Audio previews can be offered – e.g. a small play icon to hear a sample of the voice – using a <Button size="icon"> with the UilPlay icon.
Configure Agent Behavior: The user configures the agent’s behavior and knowledge. This includes setting a Personality/Pitch and connecting any knowledge base. For Personality, the UI might offer presets like “Customer Support – Empathetic” or “Sales – Persuasive”, which map to different system prompts and tonalities. (In code, this was indicated by selectedPitch such as “customer-support”
file-fjpkexbjp2dfhprtk7mhwq
.) The design can use another grid of cards for tone/pitch options, each with an icon (e.g. UilCommentDots for conversational tone). The user also sees an option to attach a Retrieval-Augmented Generation Knowledge Base to the agent – for example, a toggle or button to link to RAG content. If they have no custom data yet, this section will be informational: “Your agent will use general knowledge and example scripts. (Integrate custom knowledge via AutoRAG later.)” If a knowledge base exists, they can select it here (the UI might list available RAG workflows or documents).
Summary & Launch: A summary screen appears on a rotated card that recaps the agent’s settings – e.g. “Agent: Diala-Tone\nLanguage: English (US)\nVoice Style: Professional\nPurpose: Sales & Discovery\nTone: Friendly” – styled in bold text with colored badges or icons next to each attribute (language flag, voice icon, etc.).
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The user confirms everything looks good. On confirmation, the agent profile is saved via a Convex function (e.g. createAgent mutation on the backend). Right after saving, the onboarding triggers a quick demo: the agent “calls” the user. For instance, a modal or next screen shows a Call Simulation – the agent’s avatar and name, a phone icon, and a transcript bubble where the agent says a greeting (“Hello, this is Diala-Tone, your new AI sales agent. Ready to make some calls!”). If audio is available, it plays using the chosen voice. This delightful finish demonstrates the agent in action and closes the loop on creation.
Components (Frontend)
WelcomeCard: Reused from existing onboarding components (like the one used in Voice and RAG flows) to capture the user’s name
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. It’s a branded card with input and a bold welcome message.
Card & CardContent: The design heavily uses the <Card> component with <CardHeader> and <CardContent> from the design system
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Each step’s UI is typically contained in one or more cards that may be rotated slightly (using utility classes like rotate-1 or -rotate-1 for a playful effect). For example, the profile form appears in a card with a slight tilt and thick black border.
Input Fields and Buttons: The agent profile form uses <Input> for text (name, purpose) and <Textarea> if a longer description is needed. These inputs use the neobrutalist style – e.g. tall input boxes with a 4px black border and bold placeholder text
file-fjpkexbjp2dfhprtk7mhwq
. Buttons throughout are high-contrast and bold: primary actions in bright colors (e.g. yellow or Diala blue) with uppercase labels and black text for contrast
file-fjpkexbjp2dfhprtk7mhwq
. Icons from Unicons (like UilPlus for add, UilArrowRight for continue) are placed inside buttons or headings to add visual cues.
Agent Preview Card: A custom component (could be similar to an AgentCard used in the dashboard
file-fjpkexbjp2dfhprtk7mhwq
) shows a mini profile of the configured agent. It might display the agent’s name, an avatar (perhaps a robot icon like UilRobot in a colored circle
file-fjpkexbjp2dfhprtk7mhwq
), and tags for language, voice, etc. This appears in the summary step to review settings.
OnboardingNav (Step Indicator): A step navigation UI at the bottom or top shows progress (e.g. “Step 3 of 5: Voice & Language”). This can be implemented by the <OnboardingNav> component similar to what the voice onboarding used
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. It displays steps as numbered circles or tabs, with completed ones filled in (using background color and a checkmark overlay)
file-fjpkexbjp2dfhprtk7mhwq
, and the current step highlighted (scale up and drop-shadow effect in CSS
file-fjpkexbjp2dfhprtk7mhwq
). Users can see how many steps remain and click on previous completed steps to review or edit (the nav allows backward navigation by calling onStepChange for earlier steps
file-fjpkexbjp2dfhprtk7mhwq
).
Modal/Dialog Components: If the final demo is shown, a modal overlay might be used – e.g. a <CallSimulationModal> (similar in structure to the provided CallAnalyticsModal or LiveCallMonitorModal used elsewhere
file-fjpkexbjp2dfhprtk7mhwq
) that appears with the agent on a “call.” Alternatively, the simulation can be inline on the final screen with a phone UI card.
Backend Architecture (Convex & APIs)
Convex Functions: When the user completes the agent setup, a Convex mutation (e.g. api.agent.createAgent) is invoked to save the agent profile to the database. This would include fields like name, description, language, voice settings, system prompt (derived from purpose/tone), etc., as seen in the voiceAgents data structure
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The Convex backend ensures the agent is persisted and accessible across the app (e.g. it will show up in the main Agents dashboard list and be used in calls).
AI Voice Services: If the onboarding demo includes generating an audio greeting, backend calls will be made to external APIs. Specifically, the system would use the chosen TTS (Text-to-Speech) provider to synthesize the agent’s greeting. For example, if ElevenLabs is configured (noted by an API key in settings
file-fjpkexbjp2dfhprtk7mhwq
), the app would send the greeting text and voice parameters to ElevenLabs API and stream back audio. Similarly, if the demo included an AI-crafted greeting text, an OpenAI API call could produce it (using the agent’s system prompt as context). For simplicity, the greeting might be pre-scripted, but a more dynamic approach could involve an OpenAI Chat completion to produce a personalized welcome line.
Convex Real-time updates: Although not critical in onboarding, if any step needed to fetch options (say available languages or voices), a Convex query could supply those. In our case, language and voice lists are mostly static, so they can be hardcoded or fetched from a static endpoint. However, linking to RAG sources might involve a Convex query to list available knowledge bases (RAG workflows stored in Convex) and a mutation to attach one to the agent profile.
Data Model: The new agent’s data structure includes performance stats and lastActive timestamps as placeholders
file-fjpkexbjp2dfhprtk7mhwq
 – these will initially be empty or default. The creation function may initialize some of these (e.g. set status: 'active' and zero counts). This integration ensures that immediately after onboarding, the user can navigate to the Agents dashboard and see their new agent listed with default metrics.
No Calls in Onboarding: Importantly, the “simulated call” at the end is not a real phone call but a local demo. Thus, no actual telephony API (like Telnyx) is invoked during onboarding (avoiding costs or complexities). The real Telnyx integration (for actual calls) happens when the user later uses the Calls module or playground. The onboarding just hints at that by using a fake call interface.
Analytics Hook: Optionally, the onboarding could log an event via Convex or an analytics service indicating a new agent was created (useful for the app to track conversion from onboarding to actual usage).
Design System & Neobrutalist Elements
The Agents onboarding UI adheres to the same design principles found elsewhere in Diala’s app for consistency and brand identity:
Typography: All titles and labels use the Noyh Bold font (imported at app level
file-fjpkexbjp2dfhprtk7mhwq
) in uppercase with heavy weight
file-fjpkexbjp2dfhprtk7mhwq
. For example, step titles like “Voice & Language” or the final “Agent Ready” are rendered in large, black uppercase text for emphasis. Supporting text uses bold sans-serif in slightly smaller size (often gray for secondary text).
Color Blocks: Each step likely features a signature background color overlay. Since Voice Agent is core to Diala, a blue theme (RGB 0,82,255) was used in the main onboarding menu
file-fjpkexbjp2dfhprtk7mhwq
 and could carry into this flow. We’ll use a blue backdrop with a subtle grid pattern (achieved by CSS gradients as seen in other onboardings
file-fjpkexbjp2dfhprtk7mhwq
). Cards themselves use white or light-gray backgrounds, with splashes of color for icons or badges (e.g. yellow or purple badges for language/purpose). All colors are high-saturation and flat, aligning with neobrutalism.
Borders & Shadows: All interactive elements (cards, buttons, modals) have thick black borders (4px) and offset shadows. For instance, cards use border-4 border-black with a drop shadow that mimics a hand-drawn offset (e.g. shadow-[6px_6px_0_rgba(0,0,0,1)] on hover to deepen the effect
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). This gives a slightly 3D, layered look. Inactive states might use lighter shadows (4px offset) and hover states increase to 6px or 8px, as consistently done in the app
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
.
Playful Layout: Many cards are deliberately rotated a few degrees (transform rotate-1 or -rotate-1)
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 to convey a creative, less regimented vibe. In this onboarding, perhaps the welcome card is rotated one way, the next form card rotates opposite, etc., alternating for each step. This matches the style in the transcripts onboarding where multiple cards have alternating rotations
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Despite the skew, alignment of form elements inside is maintained for usability.
Icons and Badges: Unicons icons are used generously to label sections (e.g. UilRobot for Agent info, UilMicrophone or UilVoice for voice selection, UilSetting or UilSlider for settings step)
file-fjpkexbjp2dfhprtk7mhwq
. Icons appear in solid color blocks with black outlines to form “neo-brutalist badges.” For example, a small square with a black border containing a yellow background and a black icon can precede a heading like “Select Language” – reinforcing meaning with color (yellow often used for knowledge/RAG, blue for voice, etc.). Badges in text (like showing agent status or language) are styled via the design system’s <Badge> component with custom classes for border and background
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
.
Overall, the design ensures the agent onboarding feels like part of the same family as the Voice, RAG, and Transcripts flows – bold, engaging, and a bit quirky – while clearly communicating the function of each UI element.
State Management
The onboarding component uses React useState hooks to manage form data and step transitions. Key state variables include:
currentStep (number): Tracks which step of the flow the user is on (initially 1). The component conditionally renders different content based on this
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The state is advanced via setCurrentStep(n) when the user clicks “Continue” on a step after validation.
Form fields like agentName, agentPurpose, selectedLanguage, selectedVoice, selectedTone: Strings (or IDs) captured from user input. For example, when the user types the agent’s name, the agentName state updates onChange. Selecting a language sets selectedLanguage (toggling logic ensures clicking the same option twice can deselect if needed
file-fjpkexbjp2dfhprtk7mhwq
). These states enable/disable the Next button for that step (e.g., require non-empty name, or require both language and voice selected as seen with combined condition !selectedAudio || !selectedLanguage in the Voice flow
file-fjpkexbjp2dfhprtk7mhwq
 – similarly, Agents flow will ensure required inputs are present).
showDemoModal (boolean): Controls whether the final call simulation modal is shown. Initially false, it flips to true once the agent is saved and the user triggers the demo. The modal component likely uses its own internal state for things like playback progress or closing.
Possibly isSaving or isLoading: A flag while the agent profile is being saved to the backend. Upon clicking “Finish” on the summary, isSaving can be set true and a loading spinner or a disabled state shown on the button to indicate processing. Once the Convex function returns success, isSaving goes false and we proceed to demo.
We may also maintain an object like formData that aggregates all agent fields, but given the few inputs, individual useState hooks are sufficient. Alternatively, a single useReducer or form library could manage the multi-step form data, but that might be overkill here.
The state is lifted within the onboarding component itself since it’s a dedicated flow. If needed, some global state (context or store) could provide user-specific info like the user’s organization or list of existing knowledge bases for RAG selection, but such data can also be fetched on the fly via Convex queries at the moment of needing them.
UX Interaction & Animations
This onboarding emphasizes a smooth, instructive user experience with interactive feedback at each step:
Progressive Disclosure: Only one step’s inputs are shown at a time, reducing cognitive load. The transition from one step to the next can be enhanced with a brief animation – e.g. the current card slides out or fades, and the next card slides in. The neobrutalist style can be extended to animations by making them slightly bouncy or offset (to match the playful theme).
Button States & Wobble: To encourage user action, the “Continue” button on each step might use the wobble animation (keyframes) once the step’s requirements are met. In the voice onboarding code, for example, the next-step button gained a wobble effect when prerequisites were selected
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We can apply similar here: after the user fills agent name and purpose, the “Next” button could wobble to draw attention. Likewise, on the voice selection step, once a language and voice are picked, the continue button pulses or shakes lightly
file-fjpkexbjp2dfhprtk7mhwq
. These animations use CSS (@keyframes) and inline styles toggled by state conditions.
Visual Feedback on Selection: Selecting a voice or language card provides immediate feedback: the card’s background may turn a brighter shade, an icon or border indicates selection (for instance, a checkmark or the border color changes from black to a highlight color). The code’s isSelected prop for language options shows a possible pattern
file-fjpkexbjp2dfhprtk7mhwq
. We’ll implement it such that clicking a card toggles its isSelected state and triggers a re-render with the new style (and maybe a short scale-up animation to emphasize activation).
Modals & Overlays: The final call demo uses a modal overlay that darkens the background (e.g. semi-transparent black overlay) and pops a centered card mimicking a phone call UI. The user can interact by pressing a “Play” button to hear the greeting again, or “End Demo” to close. This modal respects the design system (close button is a bold “X” icon, and the modal card has the thick border and drop shadow). It also ensures focus trapping (so that keyboard users are kept within the modal until it’s closed).
Error Handling: If any backend call fails (e.g. the agent save or the TTS fetch), the UI will show a brief error message – likely in a modal or banner. For instance, if saving fails, we might keep the user on summary step and show a red-outlined alert card saying “Error: Could not save agent. Please try again.” with a retry button. Errors during the demo (like audio failing to load) might be caught and shown as a notification (“Demo unavailable right now”). These edge cases ensure the user isn’t left confused if something goes wrong.
Skip Option: Although not always necessary, a “Skip Demo” or “Finish Setup” option can be provided for users who don’t want to go through the entire flow. This would likely appear on the summary screen. If clicked, it would still save the agent and then navigate the user to the main dashboard (Agents list) without showing the call simulation. It’s a small UX consideration to respect user’s time.
Throughout, the interactions aim to educate (by guiding input choices and showing outcomes) and delight (with fun visuals and a gratifying final preview of their AI agent coming to life).
Technical Implementation Details
Under the hood, the Agents onboarding is implemented as a React functional component (e.g. AgentsOnboarding.tsx) within the Next.js app, likely under the /onboarding/agents route. Key technical aspects include:
It is a client-side component ('use client' at top) because it manages interactive state and uses hooks, similar to other onboarding pages
file-fjpkexbjp2dfhprtk7mhwq
. This means it’s part of a Next.js page that doesn’t use Server-Side Rendering – appropriate since the content is highly dynamic and user-specific.
Routing & Isolation: This onboarding flow is separate from the main dashboard. It can be launched from a welcome screen (like the “Choose Your Path” menu
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). That screen routes to /onboarding/voice, /onboarding/rag, etc., and we would add a tile for Agents if not already present. Because it’s self-contained, users can experience it without having any agents; it essentially acts as a wizard that will populate the Agents module. It could also be re-used as a “Add New Agent” wizard if needed (triggered from the Agents dashboard via a “Create Agent” button
file-fjpkexbjp2dfhprtk7mhwq
).
Reusing Components: We leverage some existing components from the codebase to avoid reinventing the wheel. For example, the <CreateAgentModal> logic (if it exists) could be repurposed within the flow. In the provided code, there is create-agent-modal.tsx (likely a form for adding an agent)
file-fjpkexbjp2dfhprtk7mhwq
. Instead of using it as a modal, we can extract its form fields and validation logic for our multi-step process. Similarly, the <AgentCard> component (used to display agent info on the dashboard
file-fjpkexbjp2dfhprtk7mhwq
) can be used to render the preview in the summary. This reduces duplication and ensures consistency – the agent created in onboarding will look the same as those in the main app.
Convex Integration: We use the Convex React client, which is already set up in the app’s context (<Providers> component wraps the app with ConvexProvider
file-fjpkexbjp2dfhprtk7mhwq
). To call the backend, we use the useAction or useMutation hook from Convex. For example: const saveAgent = useAction(api.agents.create);. On the final step, await saveAgent(agentData) is called. The Convex function agents.create would handle inserting into the database. If Convex returns the new agent’s ID or object, we could use it to update local state or navigate to the agent’s detail page post-onboarding (for now, probably not needed during onboarding, but available).
Audio Playback: To implement the voice demo, the component may use the Web Audio API or simply an HTML5 <audio> element. The audio source would be the URL or binary returned from the TTS API call. A simpler approach: call a backend endpoint (Convex or Next.js API route) that returns an audio file URL after generating it. Then set that URL as the src of an audio element in the modal and call audio.play(). The UI includes controls for play/pause (maybe using a custom <Button> with play/pause icons UilPlay/UilPause for styling
file-fjpkexbjp2dfhprtk7mhwq
, but under the hood controlling the audio element). We also handle the cleanup – when the modal closes, we stop the audio and release the object URL if one was created.
Testing & Edge Cases: We ensure the multi-step form has validation at each step (e.g., required fields not empty, selections made). We also ensure that going “Back” to a previous step (if we allow it via the step indicator or a Back button) repopulates the form with the earlier inputs (since state is still retained). The component’s state persists as long as the user is on the onboarding route; if they refresh, they’d start over (which is acceptable). If needed, we could persist partial progress in localStorage or Convex to resume, but given the short flow, that’s optional.
Completion Path: Upon finishing, beyond showing the demo, we’ll likely programmatically navigate the user to the main app (maybe to the Agents dashboard page) when they exit the onboarding. This can be done with Next.js useRouter() to push a new route (e.g., router.push('/dashboard/agents')). We saw similar usage on the main onboarding selection page to route to the chosen path
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Here, after agent creation, a call like router.push('/dashboard/agents?new=1') could be used to both navigate and perhaps trigger a highlight on the new agent.
By engineering the onboarding in this way, we ensure it’s not just a dummy tutorial – it actually creates data and uses the real services, making the experience authentic. Yet it’s contained enough that a user can go through it without prior setup and end up with a tangible result (their custom AI agent ready to work).
Integration Points
The Agents onboarding ties into Diala’s broader infrastructure at several points to make the experience meaningful:
Agents Dashboard: The primary integration is that the created agent becomes part of the main Agents module. After onboarding, the new agent appears in the “Active Voice Agents” list on the dashboard, complete with the stats (mostly zeros or default) and status active
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The agent’s configuration (prompt, voice, etc.) is stored such that if the user opens the Agent Detail Modal later (accessible via the Agents page
file-fjpkexbjp2dfhprtk7mhwq
), they will see what they set up during onboarding. In this way, the onboarding acts as an alternate entry to the same agent creation process that an advanced user could do manually.
Calls Module: Any agent created is immediately available for use in calls and campaigns. For example, if the user proceeds to the Calls dashboard and starts a new call or campaign, they can select this agent to handle calls. The agent’s convexEntryPoint or identifier might be used by the call system to invoke the right logic (we see something like convexEntryPoint: 'agents.salesAlpha' in the calls data
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, implying each agent may have an identifier used in call workflows). Our created agent would have its unique entry point.
RAG (AutoRAG) Integration: If the user attached a knowledge base or if Diala automatically uses YouTube transcripts to train agents, that connection happens via the agent’s profile. The example agent data has a ragSources array listing modules or workflows
file-fjpkexbjp2dfhprtk7mhwq
. In onboarding, we might not fill this unless the user explicitly chose a knowledge base. But the structure is there – if later the user runs an AutoRAG workflow to build a knowledge base, they could link it to the agent (likely through an agent settings modal). Conversely, if this agent had been created by an advanced user, they might have chosen some default content to train on. The onboarding could behind the scenes flag that this new agent should train on some basic YouTube transcript set for their industry – for instance, if they chose Sales purpose, the system might automatically kick off a training job using a preset YouTube playlist for sales calls. (This is a speculative integration: to truly feed into backend workflows, an event could be emitted to start an AutoRAG job for the agent).
External Telephony: While no real call is made in onboarding, the agent is configured with everything needed to perform on actual calls. This means the conversation engine (OpenAI GPT model) and TTS/STT services can already work with it. When a real call comes in or out with this agent, the call pipeline will load the agent’s profile from the database (including voice settings and prompt) and spin up the necessary services (e.g. allocate a Deepgram transcription stream and an ElevenLabs voice for it). In short, the output of onboarding is not just UI fluff – it directly feeds the runtime behavior of the voice agent.
Design System & Theming: On a more front-end integration note, the onboarding uses the same design system components (cards, buttons, etc.), which means any updates to global styles (like a change in the Button component styling) will automatically reflect in the onboarding. For example, if the team updates the <Button> to a new hover effect, the onboarding’s buttons (being instances of that component) get the update too. This tight integration ensures consistency and reduces maintenance.
Analytics & User Onboarding Funnel: From a product perspective, this module ties into the user onboarding funnel. Likely, completion of the Agents onboarding is a key milestone (it shows the user has created an AI agent). This event might be tracked via an analytics integration (Segment, Mixpanel, etc.). If the platform has a “Setup Progress” checklist, the event of agent creation could check off an item like “✅ Create your first Voice Agent.” This is not a direct user-facing integration, but worth noting as part of the holistic experience.
Future Agent Enhancements: The agent created here will also benefit from any future system improvements. For instance, if later the platform adds an Agent Training feature (where the agent improves from call transcripts), all agents including this one would be part of that. The onboarding doesn’t need to handle that now, but it means the agent’s ID and records are ready to accumulate call data, training data, performance metrics (success rate, avg call time updated over time
file-fjpkexbjp2dfhprtk7mhwq
), etc., as the user uses it in real calls.
Future Enhancements
In future iterations of the Agents onboarding, several improvements and extensions could further increase its power and appeal:
Deeper Customization Steps: Additional steps could be introduced for fine-tuning the agent. For example, a step to upload a custom script or greeting for the agent (if the company wants a specific intro line), or to choose a persona image or avatar that will represent the agent in dashboards and calls. Currently, agents likely use default icons
file-fjpkexbjp2dfhprtk7mhwq
, but custom avatars (even just an emoji or generated icon) could make them more relatable.
Interactive Voice Tuning: We could let users test the voice during onboarding. A mini step where after selecting voice and language, the user can type a sentence and hear it spoken in the chosen voice. This try-and-adjust loop would utilize the TTS and allow them to toggle voice settings (pitch, speed) if the system supports it. It would showcase the realistic voice capabilities (one of Diala’s selling points is “realistic background sounds” and presumably natural speech
file-fjpkexbjp2dfhprtk7mhwq
). If background noise effects are available (as hinted by “background sounds”
file-fjpkexbjp2dfhprtk7mhwq
), the onboarding could allow choosing a background ambience for calls (e.g. office noise vs. quiet studio) to simulate realism – a fun feature for power users.
Agent Skill Configuration: Beyond purpose, let users pick “skills” or scenarios the agent is optimized for. For instance, checkboxes for Handling Objections, Appointment Setting, Demo Scheduling. These could correspond to different preset prompts or connect to specific RAG knowledge packs. In code, we see ragSources referencing things like "Objection Mindset" or "Price Objections" modules
file-fjpkexbjp2dfhprtk7mhwq
. A future onboarding step could list such modules for the chosen purpose and let the user toggle them. The agent would then automatically have those knowledge packs linked. This makes the agent immediately more powerful and tailored.
Integration with Contact Data: As Diala evolves, the onboarding might import some user-specific context. For example, if the user’s company has a set of FAQs or a product catalog, an advanced onboarding could prompt: “Want to train your agent on your own data? Upload a PDF or enter a website.” This would effectively initiate an AutoRAG process during agent creation. It blurs the line between the Agents and AutoRAG modules, but if done in a guided way, it could be extremely sticky – the user sees their agent learn their material in real-time. This might be a branching path: a simple agent creation vs. an advanced one with custom data ingestion.
Multi-Agent Onboarding: Down the line, if a user needs a team of agents (for different roles), the onboarding could allow creating multiple agents in one flow. For example, after creating one agent, it asks “Do you also need a support agent?” and could quickly replicate the process with slightly different defaults. However, this might be beyond the initial scope – typically one agent is enough to show value, and the Swarms module covers grouping multiple agents.
Guided Training Mode: Provide an option at the end of onboarding to “Practice a call with your agent.” This would switch to a Playground or test call mode where the user can speak or type and the agent responds, without involving actual phone lines. It’s like a training sandbox so the user gains confidence in the agent. This could reuse the Playground infrastructure in a focused way. It’s a logical next step after hearing the greeting: let the user try a sample conversation and give a thumbs-up/down if the agent’s response was good. Those signals could in the future fine-tune the agent (via reinforcement learning or by adjusting settings).
Onboarding Hints in Dashboard: Once the agent is created, subtle hints or tooltips in the main dashboard could appear (just-in-time education). For example, on the Agents page, highlight the “Active Voice Agents” card
file-fjpkexbjp2dfhprtk7mhwq
 and say “Here’s your new agent. Click to view details or edit.” On the Calls page, highlight how to start a call with that agent. This isn’t part of the onboarding UI itself, but a continuation of the user journey. Implementing this might involve setting a flag that the user completed agent onboarding and then showing a one-time tooltip in relevant places.
By implementing these future enhancements, the Agents onboarding would not only create an agent but also ensure the user fully understands and utilizes it, increasing the likelihood of Diala’s adoption in their workflow.
Swarms Module Onboarding Tendril
Purpose
The Swarms onboarding flow guides users through creating and managing Agent Swarms, which are groups of AI agents working in concert. In Diala, swarms represent powerful teams of voice agents deployed for coordinated campaigns (e.g. a sales “swarm” making calls in parallel, or a support “swarm” handling a surge of inquiries). This onboarding acts as a self-contained feature to set up a swarm from scratch, showcasing how grouping agents can amplify results. It serves as a lead funnel by illustrating a high-impact use case: rather than a single agent, imagine a “battalion” of agents – this drives home the scale Diala enables. The flow’s purpose is to get the user to create their first swarm (naming it, choosing its mission, and selecting member agents), while seamlessly plugging into backend systems by actually provisioning that swarm (persisting in the database and ready to be used in campaigns). In doing so, it also educates the user on the benefits of teamwork among AI agents (coordination, aggregated analytics, etc.).
Flow Steps
Welcome & Concept Intro: The user is welcomed with a card on a vibrant purple backdrop (purple is used in the app to denote swarms or grouping
file-fjpkexbjp2dfhprtk7mhwq
). A bold title might say “Agent Swarms Setup” with an icon like UilLayerGroup (stack of layers) to signify a group
file-fjpkexbjp2dfhprtk7mhwq
. The welcome message acknowledges the user (“Welcome back, [Name]!”) and teases the benefit: “Let’s group your agents into a powerful Swarm for coordinated calling.” A brief description explains what swarms are (e.g. “Swarms allow multiple voice agents to work together on targeted campaigns – think of it as your AI call team.”). A Get Started button moves to the next step.
Swarm Details (Name & Purpose): The user enters key info for the swarm. This is similar to creating a team: Swarm Name (e.g. “Sales Battalion”, “Support Squad”) and Purpose/Mission (a one-word or short phrase like “Outbound Sales” or “Customer Support”). These inputs are captured with text fields. For inspiration, the UI might show examples or even allow choosing from templates (like a dropdown of common swarm types: Sales, Support, Appointment Setting). Each template could pre-fill the purpose and even suggest which agents to include, simplifying the process for new users. The interface uses two input fields with labels and possibly helper text (e.g. “Give your swarm a memorable name. This will be visible in analytics and reports.”). The design uses the neobrutalist card style – perhaps a rotated white card on the purple background, with black bordered inputs. A continue button becomes active once a name is entered (purpose might be optional or default to the template chosen).
Select Member Agents: Now the user selects which voice agents will be in this swarm. The onboarding lists the user’s available agents in a scrollable list or grid. Each agent is displayed similarly to the Agents dashboard’s AgentCard
file-fjpkexbjp2dfhprtk7mhwq
, showing name, role/purpose, and status (active/training) with a status dot (green for active, orange for training, etc. as per getStatusColor logic
file-fjpkexbjp2dfhprtk7mhwq
). The user can toggle inclusion by clicking an agent card – selected ones are highlighted (e.g. outlined in the swarm’s color or with a checkmark). If the user doesn’t have multiple agents yet (quite possible if they only created one in the prior onboarding), this step can encourage them: “You have 1 agent. Create more to build a larger swarm.” It might allow proceeding with just one agent (technically a swarm of one), but ideally the user would have at least two. The flow might integrate with the Agents onboarding: for instance, right here provide a shortcut “+ Create New Agent” (maybe a small button with UilPlus
file-fjpkexbjp2dfhprtk7mhwq
). If clicked, it could pop open the CreateAgentModal or even link to the Agents onboarding. This way, users not only create a swarm but potentially another agent in the process. After selecting agents, the user clicks Next. (If none selected, we’d show a warning “Please select at least one agent” and disable advance.)
Configure Swarm Settings: This step (optional/advanced) allows tweaking how the swarm operates. For example, if Diala supports call distribution modes (round-robin, simultaneous, etc.), the user could choose one. Or if the swarm is for a campaign, maybe set a campaign goal like number of calls per agent, or schedule (run swarm now vs later). In the initial implementation, we might keep it simple: default settings that all agents will call from a shared number pool and start immediately when tasked. We can present a summary: “This swarm will use all selected agents concurrently for higher throughput.” with an info icon UilInfoCircle that on hover explains details (e.g., “Agents will automatically coordinate to avoid calling the same contact twice,” etc.). If there are any parameters (like max calls per agent or time windows), they can be sliders or toggles here. Keeping it minimal ensures users aren’t overwhelmed – they can accept defaults and proceed.
Review & Create Swarm: A summary card is shown, combining all chosen details. For example, “Swarm: Support Squad\nPurpose: Customer Support\nAgents: 2 agents (Echo-Diala, Diala-Belle)”, along with the swarm color or icon. The UI might present a stylized “swarm card” similar to how swarms appear in the dashboard grid
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 – i.e., the card header has the swarm name with purpose badges, and a list of agent names underneath
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. This gives a realistic preview. The user confirms by clicking Create Swarm. On click, a Convex function is called to save the swarm. After a quick success confirmation (perhaps the card transforms visually or a checkmark animation shows), the onboarding transitions to a final screen.
Swarm Ready (Demo/Next Steps): The final screen celebrates the creation. It might say “‘Support Squad’ is ready!” with a swarm icon. While there isn’t an obvious “demo” for a swarm like there was for a single agent, we can demonstrate the concept: for instance, simulate a live swarm dashboard view. We could show a mock realtime status: “Agents online: 2/2. Calls today: 0. Ready to deploy.” Or animate two agent avatars calling in parallel. This is more of a visualization than an interactive demo. Another approach: prompt the user to launch a campaign with this swarm (which segues into the Calls module). For example, a big button “Start a Calling Campaign with this Swarm” could be displayed. Clicking it could end the onboarding and navigate to the Calls dashboard’s campaign creation section (prefilling the swarm selection). If we include that, it directly drives the user to use the swarm immediately. Otherwise, simply instruct: “You can now use ‘Support Squad’ in your calls and campaigns. Visit the Calls section to deploy your swarm.”. Finally, a Done or Go to Dashboard button will exit the onboarding, bringing the user to /dashboard/swarms where they can see their newly created swarm in the list.
Components (Frontend)
Cards with Headers: Each step’s UI is primarily contained in a Card. The Swarm Details card might have a header with the UilUsersAlt icon (group of people) next to “Create New Swarm” in bold
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Using CardHeader and CardContent, we ensure consistent padding and border usage. The header background could use a light purple or appropriate thematic color. We saw in the swarms dashboard, card headers are color-coded by purpose (Discovery = purple, Support = green, etc.)
file-fjpkexbjp2dfhprtk7mhwq
. For onboarding, since purpose isn’t decided until input, we might keep the header a neutral or the base purple for all.
Form Inputs: The Name and Purpose fields use the standard <Input> for text. We may reuse the UI from the swarms dashboard’s Create Swarm modal if it exists. There’s likely a CreateSwarmModal given the code references setShowCreateModal(true)
file-fjpkexbjp2dfhprtk7mhwq
 and presumably a corresponding component. If available, we can extract its inner form. If not, implementing two <Input> components with labels is straightforward. To maintain style, we give them a class for thick border and maybe slightly rounded corners (in code, modals and inputs often use classes like rounded-[3px] border-4 border-black
file-fjpkexbjp2dfhprtk7mhwq
).
Agent Selection List: This is a critical component. It could be a custom list with checkboxes or toggle buttons, but a more visual approach: show each agent in a mini-card format. We can use the existing <AgentCard> or create a simplified version for listing. Possibly, a two-column grid if there are several agents. Each agent card includes the agent’s name and an icon or avatar, plus perhaps a small badge for status (active/inactive) as seen in swarms dashboard listing
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The design system’s <Badge variant="outline"> with colored background is used in the swarms card to show agent status
file-fjpkexbjp2dfhprtk7mhwq
, we can mimic that here or simply use an icon + text. Selecting an agent card can internally toggle a boolean, and we visually indicate selection by changing its style (e.g., add bg-yellow-100 or some highlight, and a checkmark in a corner). We might incorporate a small <Checkbox> component on each card for clarity (with a custom styled checkbox that fits the brutalist theme: perhaps a black border square that fills with a black check when selected).
Swarm Settings controls: If we include any (like a distribution mode), we can use radio buttons or a <Select> dropdown from the design system for simplicity. For example, a label “Call Distribution:” with options Simultaneous vs Sequential. These can be radio inputs styled with black border and a dot, or buttons that toggle. The design system likely has a Toggle or Switch component (like <Switch> or using UilToggleOn/Off icons
file-fjpkexbjp2dfhprtk7mhwq
) to represent boolean settings. We could use those if relevant (e.g., a switch for “Start immediately” vs scheduled).
Summary Card: This card will look much like a Swarm card in the dashboard. In fact, we might directly use the layout from the Swarms grid: a card with a colored header containing the swarm name and purpose badges, and inside a list of agents. The difference is in onboarding it’s static info for confirmation. We’ll use <Badge> elements for purpose and agent count as done in the real card
file-fjpkexbjp2dfhprtk7mhwq
. Each agent could be listed simply as name (with or without type). We can borrow the snippet that lists agents in a swarm card
file-fjpkexbjp2dfhprtk7mhwq
 but simplify (maybe omit the status badges here for brevity, or include them if we want to confirm we added the right ones). If the list is long, we’ll scroll or limit height, but likely initial users have few agents.
Feedback/Confirmation: After clicking “Create Swarm,” a quick feedback can be given. Possibly using a small modal or toast that says “Swarm Created!” or visually transforming the summary card. For example, overlay a big checkmark on the card or turn the border green momentarily (success feedback). We can animate a check icon (like a black check that scales up then back down) to acknowledge the action.
Navigation Buttons: Each step has a Next or Continue button, and possibly a Back button on subsequent steps. We style them as before: large, uppercase, bold. The Next button uses a prominent color (maybe the same Diala blue or a green to indicate progression) with black text, and arrow icon UilArrowRight. The Back button (if present) could be a simpler text or secondary style button, possibly with UilArrowLeft. The final step’s primary button is “Create Swarm” instead of Next, and after creation “Done” or “Go to Swarms”. We ensure button states (disabled vs enabled) are visually distinct (grayed out or lower opacity when disabled, vibrant when enabled).
Backend Architecture (Convex & APIs)
Convex Swarm Creation: The completion of this onboarding triggers a Convex function (say createSwarm) to save the new swarm. This function will likely write to a Swarms table or collection, including fields: name, description, purpose, createdAt, and the list of agent IDs that belong to the swarm. In our mock data, a Swarm object includes id, name, description, purpose, agents[], totalCalls, successRate, created
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The Convex function will set initial metrics (totalCalls=0, successRate=0 or null, created=now). The list of agent IDs is crucial; those references link the swarm to actual agents. The function might also update each agent’s record to note they are part of this swarm (if the data model requires linking from agent side; otherwise, it’s enough to store agent IDs in swarm).
Convex Query Update: Once created, any Convex queries that fetch swarms (like the one powering the Swarms dashboard list) will include the new swarm. If the user is navigated to the swarms page after, they’ll see it without needing a manual refresh, thanks to Convex reactivity if set up (the useQuery hook would push updates). If not using real-time queries, we might refetch the swarms list on page load.
No External API Calls: Creating a swarm is an internal operation – no external APIs are needed. It’s mainly about organizing existing data. So the backend interaction is straightforward: a DB insert. One possible integration: if the swarm is conceptually tied to a “campaign” or call automation, we might schedule a task (like if “start immediately” was selected, we could kick off a search or call routine). However, those tasks belong more to the Calls module. The swarm itself is just a grouping, so we won’t call any outside service in this step.
Data Validation: The Convex function will validate that the swarm name is unique (or at least not blank). It may also ensure agent IDs exist and belong to the user’s account (to avoid any malicious insertion). Basic sanitization on name/description is done (trim whitespace, maybe limit length). If any issues, it returns an error that our frontend will handle (showing an error message on summary).
Link to Campaigns: If we did implement the “launch campaign” button, pressing that could call another Convex function or backend routine: e.g., create a new Call Campaign object with this swarm attached. But since that delves into the Calls domain, we might instead handle it on the frontend by navigation (and then in Calls page, user manually sets up a campaign). We can integrate lightly by passing parameters via URL (e.g., navigate to /dashboard/calls?tab=swarm&swarmId=X to pre-select that swarm in the calls interface). The code already allows switching tabs via URL param
file-fjpkexbjp2dfhprtk7mhwq
. We could extend it to accept a swarmId to auto-open a SwarmOverview or campaign modal. The SwarmOverviewModal exists in the code
file-fjpkexbjp2dfhprtk7mhwq
, perhaps used to show swarm’s call campaign details.
Convex Relationship: In the data model, the swarm might not physically “do” anything until used, but we might prepare an entry in a “SwarmCalls” or “Campaigns” table to accumulate stats. For now, we set up the basics: also consider if any initial stats should be computed. E.g., activeAgents count (the number of selected agents with status active). We can compute that in the function by checking each agent’s status (if accessible via a query of Agents table). In our static data, swarms have successRate and totalCalls aggregated
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Initially those are 0, but could we glean something? If the agents had previous calls, maybe totalCalls = sum of those agents’ totalCalls to date (giving a baseline). But probably simpler to start at 0 since as a new swarm it hasn’t done anything collectively.
Notification/Webhook: Not directly needed, but conceivably if the platform notifies an admin when a new swarm is created (for monitoring user engagement), a side-effect could be sending an event. That’s beyond core functionality but part of a robust backend integration if desired.
Resilience: The backend should handle partial failures gracefully. If, for example, one of the agent IDs is invalid (maybe the user deleted an agent mid-onboarding in another tab), the function would throw and the UI show an error. But such edge cases are rare. The function can also handle concurrency – though unlikely two swarms with same name are being created simultaneously by one user, the unique name check should handle it.
Design System & Neobrutalist Elements
The Swarms onboarding uses Diala’s design language to reinforce consistency:
Color Theme: Purple is the thematic color, echoing how the Swarms dashboard uses purple for the header and stat card for total swarms
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The onboarding background can be a purple tone (e.g. bg-purple-400 or a gradient of purple) with the same grid pattern overlay for texture
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. This differentiates it from other flows (Voice was blue, RAG yellow, etc.) while still being an “approved” palette color.
Typography & Copy: As always, uppercase bold headings for each step title. For example, “SWARM NAME” label above the input, or “SELECT AGENTS” as a section label, will be in a bold font (we might use small caps or just uppercase for labels). The main titles (like on the welcome screen or final screen) are large – possibly 5xl or 6xl size – ensuring the user immediately sees what the focus is (the name of the flow). Supporting text and descriptions use slightly lighter weight but still bold and high contrast (dark gray on light backgrounds, or black on colored backgrounds if legibility permits). The tone is motivational and action-oriented (e.g. using words like “team”, “campaign”, “powerful”).
Iconography: Many icons illustrate the points: UilLayerGroup or UilUsersAlt for swarm concept, UilRobot for agents in the list, UilPlus for adding new, etc. The design will often place these icons in squares or circles with contrasting background. For instance, in the page header for swarm onboarding, a purple circle with a white UilUsersAlt could float behind the title text as a design element. In lists, each agent might have an avatar circle with a robot icon (some differentiation if multiple – maybe different background colors as used in Active Agents card where index 0 is blue, 1 is purple
file-fjpkexbjp2dfhprtk7mhwq
, etc.). This adds visual variety and cues (like color-coding agents by purpose or status if possible).
Card Style: Just like other modules, cards have black borders and slight rotation. In fact, the swarm cards on dashboard are rotated depending on index
file-fjpkexbjp2dfhprtk7mhwq
 – in onboarding, we might rotate the main form card a bit for flair. But for user input, a slightly rotated form might be weird, so perhaps the form card stays straight to align input fields nicely (we can still rotate decorative elements). The final swarm summary card can be rotated to match how it’ll look among others. The heavy shadow on hover isn’t as relevant in static onboarding, but any clickable card (like agent select) will use the interactive shadow: normally 4px offset shadow, on hover increase to 6px to show it’s clickable
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. This gives that tangible feel.
Badges & Status indicators: We use the same badge styles as in the main app: e.g., purpose badges with white text on colored background, bordered in black
file-fjpkexbjp2dfhprtk7mhwq
, status dots for agents (tiny colored circles) – these appear in the swarms card design
file-fjpkexbjp2dfhprtk7mhwq
 and we mirror that in the agent list: a green dot next to an agent name for active, etc. Those dots can be simple <div className="w-2 h-2 bg-green-600 rounded-full animate-pulse"> as used in code
file-fjpkexbjp2dfhprtk7mhwq
. This subtle animation (pulse for active agent) is a nice touch to imply “liveliness” of active agents.
Accordion/Info Panels: If we have any explanatory text (like explaining distribution modes or swarm usage), we might tuck it in an accordion for cleanliness. The design system has an Accordion component as seen in transcripts flow
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. For example, a small accordion titled “How do swarms work?” could expand to show a short blurb: “All agents in a swarm call concurrently, dramatically increasing call volume. Monitor swarm performance in real time in the dashboard.” This educates without cluttering the main flow. Use of a bold question as trigger and normal text answer in content keeps style consistent
file-fjpkexbjp2dfhprtk7mhwq
.
Grid Layout & Responsiveness: The agent selection likely uses a grid. We’ll ensure on mobile that it becomes one column list (stacked agent cards) and on desktop two-column. The whole onboarding container is centered and scrollable if needed (min-h-screen ensures full height usage
file-fjpkexbjp2dfhprtk7mhwq
, and we add padding). Buttons are sized appropriately (full width on small screens, or inline on larger screens if space allows). All text remains readable on smaller devices by using relative units or tailwind’s responsive text classes (sm:text-lg vs md:text-xl etc., as seen in other flows
file-fjpkexbjp2dfhprtk7mhwq
).
State Management
State management for the Swarms onboarding will track the multi-step form and selections:
currentStep (number): As usual, to control which UI segment is shown. Steps 1 through 5 correspond to screens described. We initialize at 1 and increment with next/back.
Swarm form data states: swarmName (string), swarmPurpose (string) for the text fields
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Possibly swarmDescription if we want a longer description (it was present in code as well
file-fjpkexbjp2dfhprtk7mhwq
 but could be optional).
selectedAgentIds (array of strings): The IDs of agents chosen for the swarm. Toggled on selection. Alternatively, we could store a set or boolean map for agent IDs, but an array is fine. We’ll fill this if user had agents; if they add a new agent mid-process, we must ensure to update this list (like if CreateAgentModal returns a new agent, push it and mark selected).
If we have distribution mode or schedule: distributionMode (string, e.g. 'parallel' or 'serial'), startImmediately (bool) as an example. Defaults set accordingly (most likely parallel & true).
showCreateAgentModal (bool): If we allow agent creation inline, we manage that modal’s open state. On save, we get the new agent data – possibly via a callback or by monitoring a Convex mutation result. We then update the agent list state (which might be a local copy of available agents) and add its ID to selectedAgentIds. In code, we see an availableAgents list of mock new agents
file-fjpkexbjp2dfhprtk7mhwq
; for real data, we’d fetch the user’s agents from Convex at mount (or rely on what’s in context if user came from Agents section).
isSaving: A flag for when the swarm is being created. On clicking create, set true to disable the button and maybe show a spinner. After Convex returns success, set false and move to final step.
error: Any error message to display if creation fails (e.g. “Name already taken”). This could be shown in an alert style on the summary screen or as a modal.
We might also have a swarms state if we fetched existing swarms (not really needed for onboarding unless we want to ensure unique naming or reference). For simplicity, we don’t need it now.
All this state is local to the component. Since the data (agents list) might come from outside, we could fetch the list of agent objects at the start using a Convex query (useQuery(api.agents.list) for example). That returns an array of agent records. We then use that to render the selection options. This approach ensures we’re not using stale or mock data. Each agent record could include id, name, type/purpose, status, which is what we need for display
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. If the user came directly to swarm onboarding without any agents, this list would be empty and we’d handle that (maybe prompt to create agent first). But likely, we assume they have at least one agent (especially if this onboarding is accessed after Agents onboarding). Using React state for all of the above is sufficient. No complex state library is needed. If the app had a global context for user’s agents, we could tap into it to avoid refetching, but it’s fine to fetch within onboarding to decouple flows.
UX Interaction & Animations
Step Transitions: Just like Agents flow, transitions should be fluid. Pressing “Next” slides the current card left and brings the new card in from right (for example), creating a feeling of progress. Back does the reverse. This can be done with CSS transitions or a library for wizard flows. We’ll ensure focus moves to the first input of each new step for accessibility (e.g., when going to select agents, focus could go to the first agent card or heading so screen readers know context changed).
Selection Feedback: Clicking an agent card to select it will give immediate feedback: the card might do a quick jiggle or pop to indicate it’s been selected, and a checkmark appears. We could implement a tiny animation (scale up 1.05 then back) on selection. If deselecting, maybe a brief shake or the check disappears. Since multiple selections are allowed, each card acts like a toggle. We want this to be fun and clear – possibly a sound effect could even play on select (though that’s optional and needs sound library; probably skip for now).
Prevent Mistakes: If the user tries to proceed without any agents selected, the Next button will be disabled. We might also show a tooltip or text: “Select at least one agent to continue.” This avoids frustration. Similarly, if they try to create with an empty name, we disable create.
Back Navigation: Provide a Back button from steps 3 and 4 to allow corrections (maybe they want to rename the swarm or add a different agent). Back should preserve any inputs done (our state management ensures that).
Modal for New Agent: If used, the Create Agent modal might cover the screen. When it closes (after creating an agent), users return to the swarm flow seamlessly. We ensure to highlight that new agent in the list (maybe automatically select it and flash its card or scroll to it). This avoids the user feeling lost after adding.
Final Step Illustration: The last screen is an opportunity for a celebratory animation. We could animate multiple agent icons moving or a cluster icon getting a “success” highlight. Even a simple effect like confetti (small colored rectangles falling) could be triggered to celebrate creating a swarm. It adds delight and positive reinforcement. We must be careful not to overload – it should last a second or two then clear.
Guidance to Next Action: As mentioned, a big action button “Launch Campaign” or “View Swarm Dashboard” on the final screen guides the user on what to do with the swarm. If launching a campaign, clicking it provides feedback (maybe a loading if it takes time, or goes straight to calls page). If just viewing, it navigates quickly. In either case, we should also allow a neutral completion: maybe the top-right corner could have an “X” close button at all times to exit onboarding. If the user clicks that mid-way, we might ask confirmation (“Are you sure you want to exit? Progress will be lost.”) to avoid accidental loss. If they confirm, just navigate away (progress is ephemeral anyway).
Tooltips/Help: Certain terms might have tooltips. For example, when hovering over “Success Rate” in summary, it could show how that’s calculated (though initially zero). Or if distribution modes are present, a small ? icon next to each with more info. We can use a simple <Tooltip> from design system or a custom one (the title attribute or a popover on hover).
Responsive Adaptation: On smaller screens, the flow might not show as multi-column at the agent selection step. If it’s a list, each agent row could have a “Add” button or switch. We ensure those controls are large enough to tap. The continue button likely sticks to bottom for easy access. If needed, we implement a slight scroll on selecting an agent to bring the next one into view (if the keyboard popped up or something).
Technical Implementation Details
The Swarms onboarding is implemented similarly as a separate Next.js page (e.g. app/onboarding/swarms/page.tsx). Some specifics:
Data Fetching: We’ll use the Convex client to get the list of agents upon component mount. For example, const agents = useQuery(api.agents.listMine); which returns an array of agent objects. This hook will cause a re-render when data arrives. If Convex isn’t used for this, we could use a Next.js API route or directly fetch from an internal endpoint – but since Convex is central, we use it for consistency. We then keep that list in state (or just use it directly for rendering). We might also have const [agents, setAgents] = useState([]) and populate it via useEffect if not using useQuery. Either approach is fine.
Creating a Swarm: We’ll define a Convex mutation function to handle swarm creation (in the api.swarm namespace perhaps). This function could be generated via the Convex CLI. Our frontend calls it with the swarmData: name, purpose, agentIds, etc. Once awaited, if no error, we assume success (Convex would throw if error). We then set some local newSwarmId maybe if needed (for navigation), and call setCurrentStep(lastStepNumber) to show the final screen.
Error handling: If Convex throws (maybe name conflict or network error), we catch it and set an error state. The UI would then likely remain on the review step and display the error message somewhere (perhaps as a red text below the Create button or an alert box). The user can then adjust and retry.
Modal Integration: If we integrate CreateAgentModal from the existing codebase, we have to import it and use it. The code base shows a CreateAgentModal component being imported in Agents page
file-fjpkexbjp2dfhprtk7mhwq
. We can use it here. It likely expects props like onSave to receive the new agent data. We’d pass an onSave that takes the agent, closes the modal, and updates our state. The agent data might be an object with at least an id and name; if not, maybe the modal itself handles insertion to DB and triggers a re-fetch of agent list. We might rely on Convex reactivity: if agents.listMine query is active, after the new agent is saved via that modal (which likely also uses Convex), the query would update automatically to include the new agent. If so, our UI would update without manual intervention – neat. If not reactive, we can manually append.
Routing & Completion: After finishing, we likely navigate to /dashboard/swarms. We can use useRouter() for that. If we want to auto-open the SwarmOverview modal on the dashboard (to show details), we could append a query param like ?swarmId=new or something and have the swarms page check for it and open a modal (the swarms page code doesn’t explicitly handle query except maybe selection if any). Alternatively, simply landing on the swarms page and seeing the new entry at top (if we prepend it) is sufficient.
Conditional Steps: If the user has zero agents, step 3 might be skipped or replaced by a call-to-action to create an agent. In implementation, we could detect agents.length === 0 and instead of showing selection, show a special screen: “No agents yet. Let’s create one first!” with a button to launch CreateAgentModal. After they create one, that screen goes away and we essentially can skip to step 5 (because a swarm of one is now possible). But ideally, we expect them to have at least one agent from previous onboarding.
Testing the Flow: We test by simulating different conditions: user with one agent vs multiple. Ensure that if one agent, they can still create a “swarm” (though it’s somewhat an edge case, we allow it for completeness). The call to Convex with one agent is fine. Also test the new agent creation path. Because multiple asynchronous things happen, we ensure state updates correctly (maybe using useEffect to watch agents list and if it grows from 0 to 1, auto-select the new agent and advance the step if we were waiting).
Maintainability: We structure the code clearly with sub-components if needed (e.g., could break out AgentSelection as its own component that receives agent list and manages selection states). But given it’s not too complex, keeping it in one file is okay. Comments in code would explain each step’s block for clarity to future devs.
Integration Points
Swarms Dashboard Listing: The obvious integration is that the new swarm appears in the main Swarms dashboard (/dashboard/swarms). On that page, swarms are displayed in a grid with their stats
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Our newly created swarm will show up with 0 calls and baseline success rate in a card. Users can click it to see details (the app might navigate to /dashboard/swarms/[id] which has a detail page, as indicated by the folder structure
file-fjpkexbjp2dfhprtk7mhwq
). That detail page likely shows deeper analytics for the swarm (calls over time, agent contributions, etc.). Because our onboarding saved the swarm in the same store as those, everything is connected.
Live Call Monitor: If a swarm is used in a live campaign, the Live Call Monitor (and possibly SwarmOverview modal) will tap into the swarm’s data. For instance, if we open the SwarmOverviewModal in the Calls section, it might display which swarm and some stats. The code references a SwarmOverviewModal
file-fjpkexbjp2dfhprtk7mhwq
, likely to show details of a swarm’s campaign. That modal probably expects a Swarm object or ID. Since our swarm data structure matches, it can be used there seamlessly.
Calls Module (Campaigns): When a user wants to actually use the swarm, they’ll likely interact with the Calls module. Possibly, they’ll create a new campaign or search that leverages this swarm. The integration might be: on starting a campaign, the UI asks “Which swarm or agent to use?” If a swarm is chosen, then the call distribution logic uses all agents in that swarm. Under the hood, when dialing out, the system picks an available agent from that swarm for each call or parallel calls. The integration point is that the calls service (the part of backend that orchestrates calls, likely via Telnyx) needs to retrieve the list of agents in a swarm to assign calls. Since our swarm is stored, an API call from the call service (maybe a Convex function called by the serverless telephony function) can query by swarmId to get agent IDs, then fetch each agent’s config. This means the swarm’s existence and membership drive real behavior.
Analytics Aggregation: Over time, as calls are made, the system will update swarm metrics. E.g., increment totalCalls for the swarm, recalc successRate (maybe weighted average of member agents or outcome tracking). In the code, these seem to be handled possibly by accumulating data
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Our creation set them to 0, but as calls complete, perhaps a Convex mutation updates the swarm record (like swarm.totalCalls += 1, and successRate recalculated). Because the swarm grouping is known, any per-call record might carry a swarmId to facilitate such updates. Integration-wise, this means our swarm’s ID is used as a foreign key in call logs. By creating the swarm properly now, we ensure future processes have the needed reference.
UI Consistency: The onboarding uses the same components as the main app (e.g., same AgentCard style). If the design of AgentCard or Swarm card updates, these changes should reflect in onboarding too, maintaining an integrated feel. This is more a front-end integration but ensures that, for instance, if tomorrow they add a new field to swarms (like a “Team Lead” name or a color code), both the dashboard and onboarding could be updated to capture/display it.
Cross-module Onboarding Links: We’ve allowed the possibility of jumping to Agent creation from within Swarm onboarding. This is a cross-integration between two onboarding flows (Agents and Swarms). By doing so, we ensure the user doesn’t hit a dead-end if they need more agents. The integration is smooth because both use Convex – the new agent is saved and immediately accessible to the swarm flow. If multiple onboardings share components (like WelcomeCard, etc.), the design and behavior remain consistent.
User Guidance Continuity: After finishing swarm onboarding, the natural next step is to utilize the swarm. We integrated a pathway to the Calls module. This kind of integration ensures the user’s journey is continuous – they created something, now immediately see how to use it. It also means that the onboarding flows aren’t isolated tours; they feed actual objects into the system that subsequent modules recognize. The swarm’s name might even appear in notifications or suggestions, e.g., the Calls page might have a banner “New! Launch a campaign with Support Squad swarm.” This kind of cross-promotion can be built because the system knows a swarm was just created (via an event or just by checking that it exists).
Future Enhancements
Swarm Templates & AI Recommendations: In the future, Diala could provide pre-made swarm templates. For example, a template called “Appointment Army” (like seen in mocks
file-fjpkexbjp2dfhprtk7mhwq
) could come with recommended agent roles or even auto-generate a couple of agents specialized for that purpose. The onboarding could let the user pick a template at the start (“Choose a swarm template or create your own”). If they choose a template, we could auto-fill the swarm name, purpose, and even spin up new agents with appropriate settings. This would dramatically shorten setup: one click to get a fully functional swarm. It uses AI to provision agents – possibly generating custom prompts for each based on best practices. This enhancement would blur onboarding for Agents and Swarms together but yield a very powerful outcome quickly.
Dynamic Agent Suggestion: If a user has many agents, the system could suggest which agents to include in a new swarm based on their purpose or past performance. For instance, if the user’s agents have tags or specialization (sales vs support), and the swarm purpose is “Sales”, the UI might automatically check all sales-type agents for inclusion. It could also highlight an agent’s success rate or availability (if an agent is currently on a call or in training, maybe exclude or warn). As an enhancement, each agent card could show a metric (like success 92%
file-fjpkexbjp2dfhprtk7mhwq
) to inform selection.
Swarm Capacity Planning: Future iterations might allow setting the swarm’s campaign parameters right in onboarding. For example, “How many calls do you want this swarm to handle per day?” or “Target region for this swarm’s calls:”. This would tie into search (for leads) or into scheduling. While perhaps complex for initial onboarding, advanced users might appreciate configuring a swarm fully (like a campaign object) at creation. Eventually, the line between creating a swarm and launching a campaign might fade – one might do both in one flow.
Integration with Schedules/Calendar: A swarm might be scheduled to run at certain times (especially if using global agents). A future feature could let user set time windows (e.g., 9am-5pm weekdays). Onboarding could include picking a timezone or schedule for the swarm to be active. This would integrate with Diala’s job scheduler to only activate those agents in that window for outgoing calls. Not needed in MVP, but valuable for planning campaigns.
Swarm Collaboration Features: Perhaps in the future, swarms can have internal “chatter” or strategy – e.g., agents sharing information (one agent learns something on a call and another uses it). If so, onboarding might highlight that unique capability: “Agents in a swarm learn collectively. If one discovers a new objection handling tactic, all others adapt.” The UI might not change much for setup, but educational tooltips or an extra toggle like “Enable swarm intelligence sharing” could be present. This adds to the product’s differentiation if implemented.
Delete or Edit Swarm in Onboarding: Currently, onboarding is linear create. In the future, maybe support editing a swarm (via same UI). Or if they realize a mistake in final step, allow editing before leaving. Minor tweaks to flow to incorporate an “Edit” path might be beneficial.
Gamification: To encourage usage, perhaps show a benchmark: “Most users create 2 swarms to cover Sales and Support. You have 1 – consider creating another for support!”. Not directly part of a single onboarding flow, but a suggestion after completion. It could prompt to start the process again for a different purpose.
By evolving in these ways, the Swarms onboarding can become smarter and even more user-friendly, automating complex setup and showcasing the full power of coordinated AI agent teams.
AutoRag Module Onboarding Tendril
Purpose
The AutoRAG onboarding flow introduces users to Diala’s Retrieval-Augmented Generation system – essentially the feature that allows voice agents to leverage custom knowledge bases (documents, videos, websites) for more informed conversations. As a self-contained product-like experience, this flow walks the user through creating their first knowledge base (or “RAG workflow”) automatically. The purpose is twofold: first, to demonstrate how Diala can ingest large amounts of content and make it queryable by the AI agent, and second, to actually populate the user’s account with a useful knowledge base that their agents can use. It serves as a lead funnel by showcasing a high-value capability (integrating the company’s own data into AI calls), which can be a deciding factor for adoption. By the end of the onboarding, the user will have configured a data source (like a YouTube channel or a set of PDFs) and seen the system process it into a ready-to-use knowledge store. This flow ties into backend processes that perform web scraping, text chunking, embedding creation, and index building – but abstracts those technical steps into a user-friendly “autopilot” experience. The design and messaging reinforce that this is an automated, powerful feature that extends the intelligence of their voice agents.
Flow Steps
Welcome & Use Case Framing: On a bright orange (or gold) background – since RAG was associated with yellow in the main menu
file-fjpkexbjp2dfhprtk7mhwq
 – the user is greeted with “RAG System Setup”. The welcome text might say: “Welcome [Name]! Let’s give your voice agent a brain boost by feeding it your knowledge.” This sets the stage: the user is about to upload or connect some knowledge source. A succinct explanation is provided: “Diala’s AutoRAG will automatically retrieve data (like documents or videos) and teach your agent, so it can answer questions and handle specifics about your business.” A relevant icon appears, such as UilDatabase (database) combined with UilBrain (brain), signifying AI + data
file-fjpkexbjp2dfhprtk7mhwq
. The user clicks Get Started to proceed.
Choose Knowledge Source Type: The user is asked what kind of content they want to use as a knowledge base. Options are presented as large selection cards: YouTube (for videos/transcripts), Documents (PDFs, Word, etc.), Web Pages (URLs to scrape), or CSV/Knowledge base (if they have structured data). Each option card has an icon (UilYoutube for YouTube, UilFile for docs, UilLink for URLs, etc. as imported
file-fjpkexbjp2dfhprtk7mhwq
) and a brief description. For example: “YouTube Videos – ingest transcripts from a channel or playlist.” If needed, we restrict initial version to one type (say YouTube or Documents) to simplify, but showing all options indicates flexibility. The user clicks one; the selected card gets a bold highlight. We then show a Continue or automatically advance to next step.
Provide Source Details: Based on the type chosen, the user provides the input details:
If YouTube: an Input field for a Channel URL or Playlist URL (or individual video URL). Example placeholder: “https://youtube.com/@YourCompany/videos”. (In code, one of the workflows had type: 'youtube' with sources as channel URLs
file-fjpkexbjp2dfhprtk7mhwq
.) We also allow just a single video URL if they want to test with one video. Possibly also a number-of-videos setting (like how many recent videos to fetch – could default or be advanced setting).
If Documents: a file upload field (supporting multiple files). The UI could use the existing file upload component (maybe similar to File Upload Card used elsewhere
file-fjpkexbjp2dfhprtk7mhwq
). We’d list chosen files with their names and sizes.
If Web Pages: one or multiple URL inputs. Perhaps a textarea where they can paste several URLs or enter one by one, with an add button.
If Other: for a CSV or knowledge base, possibly just an upload or integration selection (maybe out of scope for now).
Each variant of this step shares a similar layout: a card with an icon corresponding to type in the header, a prompt to enter info, and maybe some validation (e.g. check if the URL is valid format). We also mention any limits (“We will process up to 100 videos” or “Max 20 documents” to set expectations). The design is user-friendly: e.g. for YouTube, after entering URL, we might fetch the channel name and show a preview (like channel title and thumbnail) to confirm correct link, using YouTube API or oEmbed. For documents, we might list the file names once uploaded. After input, the user clicks Next. If they leave it blank, Next disabled.
Configure Processing (Advanced): This step is optional/expandable for power users. We show some processing settings with sensible defaults: Chunk Size, Overlap, Embedding Model, Vector DB. In code, each RAG workflow had parameters like chunkSize, overlap, embeddingModel, vectorStore
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. To avoid scaring non-technical users, we can either hide these behind an “Advanced settings” accordion or pre-fill and explain them in simple terms. For example: “We’ll break documents into pieces of 500 words (Chunk Size) with a 50-word overlap to retain context. We’ll use OpenAI’s ADA embedding model to vectorize text, stored in a Pinecone vector database.” Each of those underlined terms could have a tooltip if hovered. If the user does open advanced, they can tweak: maybe a slider for chunk size (e.g. 256 to 1024), a dropdown for model (if they have their own key or want to use Open-source embedding model), and choice of vector DB (if Diala offers multiple or internal vs external). For most users, leaving defaults is fine; they might not even open this. But including it signals that the system is robust and configurable.
Launch Processing: Now the user begins the ingestion. A summary is shown: e.g. “Source: YouTube channel @YourCompany (45 videos)\nChunking: 512 tokens, Overlap: 50\nModel: text-embedding-ada-002, Store: Pinecone”. They hit Start Processing (or similar action label, maybe “Build Knowledge Base”). Upon clicking, the UI transitions into a processing state. This can either be on the same screen or navigate to the next “Progress” step. Preferably, we go to a dedicated progress view so we can show ongoing status.
Processing Progress: The user sees a live progress dashboard of the RAG workflow. This is where the automation feels real. We can emulate the UI of the AutoRAG dashboard but in a focused way for this workflow. For example, display a list of steps with statuses: “1. Scraping content – 10/45 videos downloaded”, “2. Generating embeddings – 0/45 completed”, “3. Indexing vectors – pending”. As the backend does each stage, update the UI. The code’s RAGWorkflow status field goes through states like 'scraping', 'embedding', 'indexing', etc.
file-fjpkexbjp2dfhprtk7mhwq
. We reflect that: perhaps show a progress bar for each stage, or one overall progress bar. The top could have a big progress percentage (like "65% done"
file-fjpkexbjp2dfhprtk7mhwq
) and maybe an estimated time remaining (if provided by backend
file-fjpkexbjp2dfhprtk7mhwq
). For example, “Estimated time: ~10 minutes”. If the process is quick (for small input, maybe finishes in under a minute), the user will see a rapid update. If it’s longer, we might not expect them to wait on this page indefinitely. But since this is onboarding, perhaps we simulate or accelerate it. (One approach: if they provided a large source, we could either process a subset for demo or send an email when done, but that breaks the flow. Instead, perhaps limit to something manageable for onboarding demonstration, like one video or small doc.) In any case, during progress we keep the user engaged with some fun facts or tips: “Did you know? You can add more data sources later to enhance your agent’s knowledge.” or “Your agent will soon have X pages of data at its fingertips.” The style should remain brutalist: progress bars have black borders, segmented perhaps, and status text is bold. Icons like UilSync (spinner) or emojis (✓, ⚠️) mark each step’s status (completed, in progress, etc.).
Completion & Results: Once processing hits 100%, we display a success message: “Knowledge Base Ready!” and possibly some stats: e.g. “45 videos processed, 1.2M characters indexed, 10k embeddings generated, index size 120 MB”
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. These numbers make the user feel the scale of what just happened. We present the final RAG Workflow Summary – basically a card very similar to what the AutoRAG dashboard shows for a completed workflow
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
: name, type, maybe a list of source names, createdAt and duration. The UI could allow them to click “View Details” which might open the ViewRAGWorkflowModal (already in code
file-fjpkexbjp2dfhprtk7mhwq
) if they want more granular breakdown like which sources were included and any issues encountered. For onboarding, we might skip deep details but mention they can see them in the main AutoRAG section. The key action now is to link this knowledge base to an agent. We highlight: “Great! Now your voice agents can use this data. You can assign ‘Sales Training Videos’ to any agent via the Agents dashboard or when configuring an agent.” Possibly, we incorporate a quick step here: if the user has an agent already (likely yes), ask “Apply this knowledge to Agent X now?” with a toggle or button. If they opt in, we call a Convex function to attach this workflow ID to that agent’s ragSources. This one-click integration makes the payoff immediate: their agent is now smarter. (Alternatively, just instruct them how to do it and not actually implement in onboarding to keep things simpler.)
Finish: The onboarding concludes with a call-to-action: “Your RAG system is set up. Going forward, you can create more knowledge workflows in the AutoRAG dashboard.” A button Go to Knowledge Base takes them to /dashboard/auto-rag where they will see this workflow listed among others (with status completed). Or possibly a Go to Agent button if they linked it, to try it out in Playground or calls. The experience wraps up by reinforcing what was achieved: their voice agent now has custom knowledge, which is a powerful differentiator.
Components (Frontend)
Multi-Step Form with Conditional UI: The onboarding uses conditional rendering similar to previous flows. Up to step 5, it’s form-like, then steps 6-7 are more dynamic status display. We’ll have components for each major portion: e.g., <SourceTypeSelection> for step 2, <SourceInput> for step 3 that changes based on type, <AdvancedSettings> for step 4, and <RAGProgressDisplay> for steps 6-7 combined. Breaking them into sub-components can keep the code tidy.
Card & Accordion: The interface elements heavily rely on Cards for grouping content. The Advanced settings likely will be inside an Accordion since it’s optional – we can use the design system’s Accordion (with a trigger like “Advanced Options”) to hide or show chunk/model settings
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. Similarly, if we want to show additional info (like list of files, or intermediate stats), we might use collapsible sections. Each Card’s header uses relevant icons: e.g. a small icon on left of title and maybe another on right as decoration (like the transcripts onboarding had icons in corners
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). For RAG: a database icon on left, and a search or file icon on right perhaps
file-fjpkexbjp2dfhprtk7mhwq
.
Progress Bars: The design system has a <Progress> component
file-fjpkexbjp2dfhprtk7mhwq
 which is styled with border and can accept a value. We will use that for the various progress visuals. In the code, they adjust CSS variable --progress-color to style it for positive, neutral, negative sentiments
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, similarly we might color code different stages (blue for scraping, green for indexing etc., or simply one color throughout). We’ll accompany each progress bar with labels and percentages (like “65%” done, as text inside or alongside).
Icons for Steps: Possibly use an icon per stage in the progress display: e.g., UilDownloadAlt for scraping/download, UilChartGrowth or UilBrain for embedding, UilDatabase for indexing, UilCheckCircle for completed
file-fjpkexbjp2dfhprtk7mhwq
. These help scan the statuses quickly. Completed steps might get a green check icon, current step a loading spinner (maybe a simple CSS animation rotating an icon or using an <svg> spinner).
Modal for Viewing Workflow: If the user clicks a “View Details” button, we can invoke the existing ViewRAGWorkflowModal component (imported as seen in code
file-fjpkexbjp2dfhprtk7mhwq
). This modal likely shows a deeper breakdown (maybe listing all source items and their status, or allows adjusting settings post-creation). It’s not mandatory to include in onboarding, but having that option demonstrates transparency of process. The modal would appear with the typical black border styling and overlay.
File Upload UI: For documents, the file upload can either use a simple <input type="file" multiple> (styled with our UI) or a fancier drag-and-drop area. Possibly in the components there is file-upload-card.tsx
file-fjpkexbjp2dfhprtk7mhwq
, which might handle showing an upload area. If available, we integrate that, which likely uses browser File API and shows file preview in a card. Since this is onboarding, not the main app, we can do a simpler approach if time – but consistent styling is key. So likely we use FileUploadCard from custom components for consistency.
Select for Model/DB: The embedding model and vector DB choices (if exposed) can use the design system’s <Select> component
file-fjpkexbjp2dfhprtk7mhwq
 for a dropdown of options. We ensure it’s styled with the same black border and perhaps a custom arrow icon (or default, but inside a card it should look fine).
Link to Agent Option: If we add the “link to agent” step, we could have a small section at the end: either a dropdown of agents to choose one to link (if multiple) or if only one agent exists, a simple yes/no toggle to link it. This might be a <Switch> component from the UI kit
file-fjpkexbjp2dfhprtk7mhwq
 for a boolean, or just a checkbox with label “Attach this knowledge base to Alice (AI Sales Agent) now”. Simplicity is fine. On toggle true, we store the intent and execute it as part of finishing.
Backend Architecture (Convex & APIs)
Initiating the RAG Workflow: When the user clicks “Start Processing,” we trigger a backend sequence. Likely, there is a Convex mutation or an API endpoint that creates a RAG workflow job and starts processing asynchronously. In the code, there are likely functions for createRAGWorkflow and then some background worker or schedule to do the actual scraping/embedding (perhaps using Convex scheduled functions or external server). The AutoRAG dashboard shows mock workflows with statuses
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We need to replicate that logic for real. Probably, on start, we insert a new RAGWorkflow record with status 'queued' or 'scraping', initial progress 0, and all those parameters. Then either a Convex backend worker kicks off (Convex allows async jobs) or we have an AWS Lambda or similar (if not purely Convex). Given the complexity, possibly an external service might handle it. But since this is a blueprint, we can assume Convex functions manage orchestration by calling out to required APIs/libraries.
For YouTube: The backend likely uses the YouTube Data API or a scraping library to fetch video IDs and transcripts. Indeed, the presence of youtube-transcript-fetcher.ts and youtube-transcript.ts in lib
file-fjpkexbjp2dfhprtk7mhwq
 suggests a backend implementation for grabbing transcripts. It might spawn a job to retrieve transcripts for each video. We have to handle potentially thousands of videos, so ideally this is done in an asynchronous, streaming manner. The onboarding doesn’t need to delve deep into how; it just triggers it and polls status.
For Documents: The backend would accept file uploads (maybe already uploaded via the file upload component to some storage like S3 or Convex storage). Then it reads each file (text extraction for PDFs, etc.), splits text into chunks, gets embeddings (via OpenAI or others), and stores them.
For Web pages: Use an HTTP client to fetch HTML and strip text. Possibly use readability libraries or site-specific scrapers.
The backend then interacts with a Vector Store (like Pinecone, Weaviate, Chroma, Qdrant as seen in data
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). For example, using Pinecone’s API to upsert vectors. The chosen store (we’ll default to Pinecone for now, but the code had multiple, e.g., Weaviate
file-fjpkexbjp2dfhprtk7mhwq
, Qdrant
file-fjpkexbjp2dfhprtk7mhwq
, etc.). We will have API keys configured for these in settings (the settings page lists API keys for openai, etc.
file-fjpkexbjp2dfhprtk7mhwq
, likely Pinecone too). The Convex function would use those keys from environment variables to connect.
This heavy-lifting is mostly on backend. The onboarding’s Convex mutation may quickly return a job ID or initial status, after which we rely on polling.
Polling Mechanism: To update the progress UI, the frontend could call a Convex query periodically to get the latest workflow status. The code for transcripts used setInterval to poll job status every 2 seconds
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We can do similar: after starting, store the workflowId (Convex doc id) and poll via useQuery (Convex queries update in real-time when data changes, so we might not even need manual polling if Convex subscriptions can push updates to the client on document change; that would be ideal – as soon as we update progress in DB, the UI updates). If Convex real-time subscription is available, we’d just use a useQuery(api.ragWorkflow.get, {id}) and have the progressbars reflect that object’s fields. If not using subscription, then a setInterval calling a getStatus(jobId) Convex function is fine. (The code suggests fetchYoutubeTranscript returned a jobId and they polled getJobStatus
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, so likely use similar pattern here if not full real-time).
Updating Progress in Backend: The backend that’s processing will periodically update the workflow record in Convex: e.g., after downloading each video, increment contentProcessed and progress (maybe progress is computed as a percentage across phases). The code’s mock has progress field separate from content counts
file-fjpkexbjp2dfhprtk7mhwq
. Possibly they define progress roughly as (completed phases / total phases * 100) or something plus finer granularity. We can decide a scheme: e.g., scraping 30%, embedding 50%, indexing 20% of total progress. Or simpler, just update progress as ratio of completed items to total items (once scraping done, you’re maybe 50% if embedding left, etc.). Either way, the UI will trust whatever progress value comes.
Completion: Once done, the Convex workflow record is set to status 'completed' and progress 100. Possibly store a completedAt timestamp
file-fjpkexbjp2dfhprtk7mhwq
 and maybe processingTime. The backend may also compile some stats (like we see number of embeddings, index size, etc. in stats
file-fjpkexbjp2dfhprtk7mhwq
). These come from the actual processing: count of chunks, size of index (perhaps returned by Pinecone), etc. Those are saved in the stats field of the record.
Error Handling: If something fails (e.g. a video couldn’t be downloaded or embedding API error), the backend should mark status 'failed' and possibly include an error message. The UI should detect status === 'failed' and inform user (maybe show a red message: “Processing failed: [error]. Please check your inputs or try again.”). And allow retry. We might skip deep error scenarios in onboarding narrative unless prompted, but we design for it.
Linking to Agent: If we implement that quick link, that would be another Convex mutation like attachKnowledgeToAgent(agentId, workflowId). It updates the agent’s ragSources array to include this workflow (similar to adding M1 and M4 in the example agent data
file-fjpkexbjp2dfhprtk7mhwq
). It might also trigger any re-training if necessary (though likely the heavy training is already done by building the vector DB, nothing more needed except the agent now knows to use it when answering questions).
Integration with Search & Chat: The outcome of this is a vector index with an ID (or some reference). The Convex DB might store the Pinecone index name or collection reference. When the agent (LLM) is conversing, it will use a function or API to query that index given a user question, retrieve relevant snippets, and include them in prompts (that’s how RAG works). So an integration point: the agent runtime needs to be aware of attached RAG workflows for an agent and know how to query them. Possibly convexEntryPoint or some config includes hooking into a retrieval function with the vector store. While not part of onboarding directly, our created knowledge base should seamlessly hook into those flows.
Resource Management: We should note that ingesting large data might be time-consuming. In an onboarding context, we might either restrict size or run an abbreviated process for demo. Perhaps behind the scenes, we limit to first N videos or first M pages of a doc to finish in a minute or two. This way the user can see completion without waiting hours. In real use, bigger jobs might run longer asynchronously (the user could leave and come back). But for the guided flow, a controlled shorter path is beneficial. Implementation could detect “onboarding mode” vs “full mode” by the type of call or a flag, but ideally the user used a reasonably sized input. If not, we could inform them: “We’ll process the first 10 items now for this demo. You can process all data later in the main AutoRAG section.” This ensures the user sees success quickly.
Design System & Neobrutalist Elements
AutoRAG onboarding uses a bold, utilitarian style to demystify a complex process:
Color and Imagery: An orange/yellow palette invokes the idea of knowledge and highlights (fitting since knowledge base often highlighted in yellow, and it was used for RAG in menu
file-fjpkexbjp2dfhprtk7mhwq
). The background can have the usual grid pattern, with maybe a subtle motif like database icons or circuitry watermark to imply data. We want to visually convey “automation” and “data crunching.” Brutalist design might incorporate a schematic-like aesthetic – maybe using monospace font for step labels to mimic code (optional).
Layout: Steps 2-4 are basically form inputs on cards, which we style with the same off-kilter geometry. The “Choose Source Type” could use a horizontal row of option cards or a column; probably a row that wraps on mobile. Each option card might rotate slightly on hover as a playful cue. For instance, when you hover “YouTube”, it tilts less (straightens) and shadow intensifies indicating selectability (the main menu had such an effect
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
).
Illustrative Icons: On the source selection cards, along with the Unicons we might add small illustrations or emojis to make them distinct (like a YouTube logo or document icon). But sticking to the icon set might be cleaner. The icons used (UilYoutube, UilFile, UilLink, etc.) are line icons – to make them pop, we can place them in a colored circle with black border as a sort of icon badge on each card. E.g., a red circle behind the YouTube icon (since YouTube is red), a blue behind Link icon, etc., still all with black outlines to remain in style.
Progress Visualization: Neobrutalism can take the normally hidden technical details and surface them in a raw form – which is exactly what showing chunk counts and index size is. Presenting these numbers in a table or list with bold labels and monospace numbers could be very on-theme (like an old computer printout vibe, but with modern layout). We could style the progress text as bullet lists with square bullets (or small black squares) to emphasize each stat. The progress bars have thick borders and solid fills. Because they will animate (growing width), we can even quantize them (like a choppy growth) to fit the retro-tech aesthetic. But smooth is fine too.
Feedback & Warnings: If advanced settings are opened, any warnings (like “smaller chunks improve accuracy but increase embedding count”) can be shown in a small italic text or a caution icon (UilExclamationTriangle in yellow
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). We ensure those are styled consistently with the rest (maybe a yellow background highlight behind the warning text).
Spacing and Fonts: We maintain generous spacing so complex info doesn’t overwhelm. Each phase in progress perhaps in its own card or clearly separated section with margins. Use of font weight: Titles heavy, details medium-bold. Possibly use a different font for code-y stuff if available (though likely stick to Inter or Noyh Bold, maybe with letterspacing to give a typewritten feel).
Consistent Components: The advanced settings might reuse components also found on the actual AutoRAG page. For example, if the main AutoRAG dashboard has a modal for editing settings (SettingsRAGWorkflowModal
file-fjpkexbjp2dfhprtk7mhwq
), maybe the onboarding’s advanced section is similar but inline. The progress display in onboarding could mirror the dashboard’s layout of the workflow cards (the Agent Performance card and others in auto-rag page are an example of info display
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, though that’s performance, not needed here). But possibly the progress view in onboarding is simpler than the full dashboard, focusing only on one workflow.
Motion: Could incorporate some quirky animations like rotating gear icons or a ASCII-style loading bar animation in text for fun while processing. But given we have real progress bars, we might skip gimmicks and let the live update itself be the “animation”.
Mobile: Ensure even on mobile the progress info is legible – maybe stack phases vertically rather than side by side. Possibly hide advanced settings by default (mobile users likely skip fiddling with those). Buttons remain accessible at bottom. The grid for source type becomes a vertical list on mobile to avoid squishing.
State Management
Managing state for AutoRAG onboarding involves both immediate form state and reacting to asynchronous status updates:
currentStep (number): Controls which view to show (1 = welcome, 2 = choose type, 3 = input details, 4 = advanced, 5 = ready to start, 6 = in progress, 7 = completed). Some steps might be combined in implementation (e.g., step 5 could be merged with 6 as part of starting processing). We increment as user proceeds, but also might jump steps (e.g., if user skips advanced, currentStep might jump from 3 to 5).
selectedSourceType (string): “youtube”, “documents”, “urls”, etc. Determines which input fields to show in step 3.
sourceInput (varies): The actual data for the source. If YouTube, a youtubeUrl (string). If documents, a list of File objects or file references (or perhaps just an array of filenames if uploaded separately). If URLs, maybe a string of newline-separated URLs or an array. We manage those accordingly. For file uploads using a controlled component, we might store files in state (FileList or Array of File).
advancedSettings object: containing values like chunkSize (number), overlap (number), embeddingModel (string), vectorStore (string). These default to something (512, 50, “text-embedding-ada-002”, “pinecone”). If user changes via inputs, we update the state. If they never open advanced, we keep defaults.
workflowId (string or null): The identifier of the created workflow job after starting. We get this when initiating the process (Convex function return). This is used to fetch status updates.
workflowStatus (object or specific fields): We could maintain a state for the status (like “scraping”, “embedding”, etc.), progress percentage, and stats. But since we likely will use a Convex query that gives us the whole workflow object (with fields like status, progress, stats), we might not need to manually store each – we can rely on that reactive data. However, to make it simpler, we can have: currentStatus (string), progress (number), and perhaps workflowStats (object with totalContent, contentProcessed, embeddings, etc.). These will get updated on each poll tick. In a React component using useQuery, we wouldn’t need explicit state; the query result itself is stateful. Alternatively, we do useState and update via polling function.
error (string or null): To store any error message if the process fails. If not null, we’ll show an error state (and maybe allow retry – possibly just reusing the same inputs and hitting start again).
isProcessing (bool): Indicates that the job is running. It flips on when user starts and off when done or failed. It helps in UI logic (e.g., show cancel option maybe). We could allow a “Cancel processing” button if feasible, which would require backend to stop tasks. That might be advanced, so likely skip cancel in MVP. But if we did, isProcessing and a Convex function to cancel would be needed.
selectedAgentToAttach (string or null): If we offer attaching to agent, track which agent (or a boolean if just attaching to default). If the user toggles attach, we store the agentId (maybe default to first agent if they have one).
The interplay: The early states (selectedSourceType, sourceInput, advancedSettings) feed into the Convex call when starting. After that, the state is driven by backend responses. We use useEffect to start polling when isProcessing becomes true or currentStep enters progress stage. We also use useEffect or useQuery to react to data changes. If using polling, the transcripts example can be adapted: call a getWorkflowStatus(workflowId) periodically, update state accordingly. Possibly incorporate a setTimeout to stop polling after X time or when completed to avoid infinite loops. The transcripts flow did a setTimeout to stop polling after 60s
file-fjpkexbjp2dfhprtk7mhwq
. We can do similarly if concerned about long tasks in onboarding. If the task isn’t done in that timeframe, we could inform the user that it will continue in background and they can check the AutoRAG dashboard later (meaning we end onboarding here with a message “We’ll keep working on it!”). But ideally our tasks in onboarding are short. One nuance: if using Convex real-time subscription, then no need for manual interval – useQuery will push updates as the Convex doc changes. That’s neat and simpler: just keep showing the query data in UI, and when status === 'completed', we know it’s done. We should still include a timeout for failsafe maybe.
UX Interaction & Animations
Guided Input: Each input step will focus the relevant field automatically. E.g., after choosing YouTube and clicking next, the URL input is auto-focused so they can start typing without extra click. When they finish typing and hit enter, we can treat that as clicking Next (for convenience). If file upload, we support drag-drop; when they drop files, we might auto-advance (if we can guess they’re done selecting, or still require clicking Next explicitly – probably require explicit Next to confirm).
File Upload Feedback: If they add a file, maybe show a thumbnail or icon with name. Possibly a small animation of the file icon dropping into a database icon to symbolize ingestion could be a delightful detail. If they remove a file, animate it fading out.
Progress Live Updates: As content is processed, we visually update progress bars. We might animate the bar width transition for smoothness. Also possibly flash or highlight when one stage completes (like turn the bar green and show a checkmark). If using textual status (e.g., “Scraping (10/10) ✓”), the checkmark or green color appears when done. We can sequentially reveal the next stage as the previous finishes, to keep focus. For example, initially show “Scraping... [progress bar]” and “Embedding... (waiting)” grayed out. Once scraping done, mark it complete and then animate enabling the “Embedding” section (maybe ungray it or slide it in). This storyboard style keeps the user engaged, watching each stage tick off.
Notifications: If the process completes quickly, we’ll still show the final message. If it’s slow and maybe user navigates away (though in onboarding they likely won’t), maybe we could implement a browser notification or email. But since it’s in-flow, we focus on on-page updates.
Attach to Agent UI: If they choose to attach to an agent, we could do something like a toggle or prompt after completion: “Apply to agent now?” For simplicity, maybe present it as a checkbox on final screen, before finishing, or a prompt: “Would you like to attach these knowledge sources to any agent?” with a dropdown of agent names and a button “Attach”. If they do it, we give quick feedback (“Attached to Agent Alpha ✓”). This is a minor interaction but a nice closure.
Complete & Next Suggestions: On finishing, beyond the immediate CTA to go to the AutoRAG dashboard or agent, we might include a subtle suggestion: “You can create another workflow for a different source (e.g., upload your product manuals) in the AutoRAG section.” or “Your knowledge base will continuously improve as you add more sources.” – encouraging future use. In terms of interaction, maybe a button “Create Another Knowledge Base” that directly takes them to auto-rag page’s create modal. But probably unnecessary in onboarding itself.
Error Case: If an error occurs mid-process, we should stop updates and show a visible error message on the progress screen. Possibly overlay a semi-transparent red shade on the progress section or replace it with a red card that says “There was an error processing your data.” If we have details, include them. Provide options: “Retry” (which might just rerun the Convex function, maybe starting from scratch or continuing where left off if possible) or “Cancel” (which ends onboarding, maybe direct to support). The user could adjust input (maybe the URL was wrong) and try again. So we likely allow them to go back to step 3 or 2. That means if error happens, we might set currentStep back to an earlier one or keep at progress with an active Back button. Ensuring this flow doesn’t break the component requires careful state resets (e.g., clearing workflowId if we’re starting over).
Time-out / Background: If we implement that 60s timeout like transcripts did
file-fjpkexbjp2dfhprtk7mhwq
, when it triggers, we should communicate it: “This is taking longer than usual. We’ll continue processing in the background. You can find the results in the AutoRAG dashboard later.” and then maybe we finish the onboarding with an incomplete status. But a better approach might be to continue polling longer, or encourage the user they can leave and it’ll continue. Up to the product decision – likely keep it interactive as long as possible.
Technical Implementation Details
The AutoRAG onboarding page (app/onboarding/rag/page.tsx) in the provided code is very basic and labeled “coming soon”
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We will revamp it with above logic:
Conditional Rendering and Components: The component can use a simple switch on currentStep to decide what to show. Or break each part into separate components and include conditionally. For example:
jsx
Copy
Edit
{currentStep === 2 && <SourceTypeSelection onSelect={handleTypeSelect} />}
{currentStep === 3 && selectedSourceType === 'youtube' && <YouTubeInput ... />}
...
{currentStep === 6 && <RAGProgressView workflow={workflow} />}
Alternatively, embed logic directly. Splitting is cleaner for readability.
Convex interactions: We likely create two functions in Convex: startRagWorkflow and getRagStatus (unless using realtime). startRagWorkflow will insert the doc and possibly initiate an async process. Convex supports the concept of “actions” for long-running or external work (similar to serverless functions). We might implement the actual scraping/embedding in a Convex Action so it doesn’t block the mutation. The mutation returns the new workflow id, then the action (running asynchronously) updates the document as it goes. This design fits Convex’s model (actions can call external APIs like YouTube or Pinecone, while mutations can update DB). In code, youtubeTranscriptActions.fetchYoutubeTranscript suggests an action was used for transcripts fetching
file-fjpkexbjp2dfhprtk7mhwq
. We’d have analogous ragWorkflowActions.startProcessing. Implementation aside, from the front-end we just call one function and get an id. Then either poll a Convex query or directly subscribe. Possibly they have a useQuery that can filter by id (like useQuery(api.rag.getWorkflow, { id })).
Real-time update possibility: If we use useQuery for a single workflow doc by id, as the Convex actions update that doc’s fields, the query result updates. We then simply use that object’s fields in the UI. We need to ensure the UI updates smoothly; likely fine as Convex pushes updates via websockets.
Resource cleanup: If component unmounts (user leaves page mid-process), we might want to handle that. If they come back later to auto-rag page, they should see the workflow still processing or done. That’s fine since it’s in DB. No need to cancel if user leaves (the process continues on server).
Data limits: If concerned about load, one might incorporate usage limits (like how transcripts limited hours). But for blueprint, we skip that complexity.
Testing with dummy vs real: For development, one can test with small inputs or stubbed processing to ensure UI flows as expected. Or have a mode where instead of calling actual Pinecone, we simulate a delay and then fill fields. This is more for development ease. In production it would call real.
Integration Points
AutoRAG Dashboard: The knowledge base created in onboarding shows up on the main AutoRAG page (/dashboard/auto-rag). There, the user can manage it further – e.g., view details, adjust settings, or delete it. The code’s workflows state on that page contains several sample workflows
file-fjpkexbjp2dfhprtk7mhwq
. Our new one would be appended to such a list. If using Convex queries on that page, the new doc automatically appears (assuming the query fetches all workflows for that user). If the onboarding ended with the job not fully complete, the user might see it in progress in the dashboard (with a spinner icon or progress bar). When done, they’d see it turn to completed. So the integration ensures continuity: onboarding created an object that persists in the user’s account.
Agents Integration: The main reason to build RAG is for agents to use it. So integration wise, each agent can link to any number of RAG workflows. The agent data structure included ragSources array of {id, name, description} for modules
file-fjpkexbjp2dfhprtk7mhwq
. Perhaps each RAG workflow corresponds to a module entry. If so, part of our Convex attachKnowledgeToAgent might append something to that array with the workflow id and maybe name or a short description. Then, when the agent is making calls, the conversation logic sees it has ragSources and knows to query those. Possibly the conversation engine has a step: if agent has ragSources, do a vector DB similarity search on the user query and include top results in the LLM prompt. All that logic would lie in the workflow engine (maybe in WorkflowEngine.ts or similar
file-fjpkexbjp2dfhprtk7mhwq
). We ensure that by properly saving the data, that pipeline can retrieve it.
External Services Setup: Pinecone or vector store credentials must be configured. The onboarding doesn’t directly ask for API keys (we assume they were input in the Settings module beforehand – the Settings page did show placeholders for keys
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). If the user hasn’t set those up, the processing might fail (no API key). We might foresee this and if keys are missing, warn at step 4: e.g., “Please add your OpenAI API key in Settings before proceeding.” or ideally, Diala could allow limited use with its own keys. If not, integration with the Settings is needed (maybe the onboarding checks if keys exist via a convex query to a Key store and if not, either prompt the user to go to settings or allow them to input keys inline – but that complicates flow, so perhaps we assume keys exist as part of initial onboarding steps).
Background Processing: If a workflow is long-running beyond the onboarding, the system handles it gracefully. The integration here is that the backend will continue updating the record. If the user closes the app, the next time they open AutoRAG, they see the status thanks to that integration. If email notifications are a feature, perhaps an email could be sent when done (not in this flow’s scope, but overall integration to alert user when long jobs complete).
Search & Call integration: The knowledge base might also be used in the prospect Search & Call context. For example, if they scraped competitor websites in AutoRAG (like the “Competitor Analysis” workflow in sample
file-fjpkexbjp2dfhprtk7mhwq
), that could feed into how the agent talks about competition during calls. It’s all part of enriching agent dialogues. Not a direct integration to show in UI, but conceptually important: the data ingested via AutoRAG is accessible across Diala’s functionalities (calls, analytics maybe if queries asked, etc.).
Edge integration – Analytics: Potentially, the platform’s analytics could incorporate knowledge base usage (e.g., how often did the agent pull info from the KB during calls). If so, linking the workflow ID to transcripts or QA logs is needed, but that’s deep integration beyond initial onboarding.
Future Enhancements
Multiple Sources per Workflow: In the future, we may let users combine source types in one go (the code already had type: 'mixed' for workflows that have multiple kinds
file-fjpkexbjp2dfhprtk7mhwq
). Onboarding could allow selecting multiple sources: e.g., YouTube + a PDF at once to create a unified knowledge base. The UI could be a multi-step selection (“Add another source?” repeatedly). This could be powerful but might overload the initial user. Perhaps after first source, we could say “Add another source to this knowledge base or finish.”. This enhancement aligns with robust usage but might be a later iteration once users understand basics.
Scheduled Auto-Updates: A future feature might allow a workflow to periodically update (e.g., re-scan a YouTube channel for new videos weekly). Onboarding could mention or allow enabling “Auto-sync”. For instance, after processing a YouTube channel, ask “Keep this knowledge base updated with new videos from this channel?” and if yes, mark the workflow as recurring. The backend then would periodically run a job to fetch new content. Not trivial to implement but extremely useful (knowledge stays fresh). The UI element could be a toggle or frequency select on final step or in advanced settings.
Quality Feedback Loop: Integration with agent performance – maybe future onboarding or usage of AutoRAG will highlight how the knowledge base improves call outcomes. Over time, maybe the system can recommend sources to add based on questions the agent couldn’t answer. For example, if in calls agents often say “I don’t have information on X,” the system might suggest uploading docs about X. This is beyond the initial onboarding, but a possible system enhancement: showing suggestions in AutoRAG dashboard like “Customers often ask about pricing details. Consider uploading your pricing FAQ.”.
UI Enhancements: Could add visualizations, e.g., a graph showing how many embeddings or content added over time, or a treemap of content by source. In onboarding, probably not needed, but in main section it would be cool.
Collaboration: Maybe allow multiple team members to contribute sources. Not directly onboarding matter, but if in future an org has multiple users, one user’s onboarding could show existing knowledge bases and suggest linking to those instead of duplicating efforts.
Security & Privacy features: Possibly later allow setting certain knowledge bases as sensitive (require certain call contexts to use, etc.). Not likely needed to mention in onboarding, but might be part of advanced settings in future (like a toggle “This data is confidential” which then might restrict it to internal calls or something).
The AutoRAG onboarding, by implementing these enhancements down the line, would continue to demystify a complex AI capability in a user-centric way, and ensure users get the maximum value by keeping their AI agents informed and up-to-date with minimal effort.
Calls Module Onboarding Tendril
Purpose
The Calls onboarding flow is designed to help users set up and understand Diala’s calling system, which encompasses automated campaigns and real-time call monitoring. This module is one of the most critical, as it directly deals with the core functionality: making and managing calls with AI agents. The onboarding serves as an interactive tutorial that leads the user through configuring an Automated Call Campaign (sometimes referred to as “Search & Call”) – from defining a target audience, to launching the calls, to monitoring results. By doing so, it demonstrates how Diala can scale their outreach or handle inbound calls with AI at the helm. It’s a self-contained feature: even without prior context, a user could complete this flow and get a tangible outcome (like a scheduled campaign or an ongoing call simulation). It connects deeply to Diala’s infrastructure, triggering backend processes such as lead searching (for prospects), call scheduling via telephony APIs, and analytics tracking. The onboarding covers both the setup aspect (which ties into the prospect search engine) and the live aspect (which ties into the call engine and analytics), thereby giving a holistic view of the Calls module. Ultimately, it aims to convert a first-time user into someone confident enough to run their own AI-driven call campaign, seeing firsthand the system’s capabilities in action.
Flow Steps
Welcome & Campaign Intro: The user is welcomed to the Calls module with a vibrant violet background (matching the “Search & Call” onboarding color from the main menu
file-fjpkexbjp2dfhprtk7mhwq
, which was violet). The title might be “Intelligent Calling Campaign Setup” with icons like UilPhone and UilSearch crossing each other
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The welcome message frames the scenario: “Ready to supercharge your outreach? Let’s set up an AI-powered calling campaign that finds prospects and dials them automatically.” This immediately communicates that the system can both search for leads and call them – a unique value proposition. If the user’s name is known, include it ("Welcome back, [Name]!"). A brief description explains: “In a few steps, you’ll configure a campaign: define who to contact, let Diala find contacts, and have your AI agent call them, all on autopilot.” This sets expectations and perhaps alleviates intimidation. A Start or Configure Campaign button leads to the next step.
Define Campaign Criteria: This step gathers parameters for finding prospects or defining the call list. It likely corresponds to the fields in the HuntConfigurationModal (the code references location, businessType, keywords, etc. when saving a search workflow
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). We present a form with fields such as: Location (text or dropdown for region), Industry/Business Type (dropdown or text, e.g. “Software” or “Restaurants”), Keywords (tags or comma-separated terms describing target profiles), Include LinkedIn (a toggle to indicate whether to use LinkedIn data as well), and Search Depth (how many pages or how deep to search – could be simplified to e.g. “Broad vs Focused” search). These inputs define the scope of prospecting. The UI could arrange them in sections: e.g., “Who to search for” (industry, keywords), “Where” (location), “Data sources” (a checkbox for LinkedIn or other directories), and “Volume” (search depth or number of prospects desired, could be implied by depth). This is a bit complex, but we can keep it user-friendly: use plain language labels like “Target Industry”, “Target Location”, “Keywords (e.g. CFO, IT security)”, “Search extensively (yes/no)”. Each field might have helper text or placeholders. For instance, Location placeholder “e.g. San Francisco, CA or ‘USA’”, keywords placeholder “e.g. fintech, SaaS”. If the user doesn’t know what to put, we might allow defaults or skip (but ideally these are required to get meaningful results). We validate critical ones (perhaps require at least one of location or industry or keywords). Once filled, Next button is enabled.
Select/Confirm AI Agent & Swarm: Now the user specifies which AI agent (or swarm of agents) will make the calls. If the user has multiple agents or any swarms (from previous modules), we list them. Possibly two tabs: one for single Agent, one for Swarm, or a unified list labeling which is which. For example, a radio list: “Call with Diala-Tone (Sales Agent)” vs “Call with Sales Battalion (Swarm)”. If the user only has one agent and no swarms, we auto-select that and just show it (with an option to confirm or change later). If they have none (unlikely if they did earlier onboarding, but possible), we could embed a quick prompt to create an agent (similar to how Swarms onboarding handled no agents – but if reaching here with none, perhaps direct them to Agent onboarding first). Let’s assume they have at least one agent. The UI shows agent name, purpose, maybe current status (should be active), and even performance stats to give confidence (like success rate). If swarms are present, show those similarly (with number of agents indicated). The user picks one. The design can use cards or just a select dropdown. Cards would be nice: e.g., a card with agent name and avatar, when selected, highlighted. A UilRobot icon for agent, UilUsersAlt for swarm could distinguish them. After selection, Next.
Phone Number & Schedule: The user chooses what number to call from (and potentially when to run the campaign). If Diala integrates with phone number management (the code has a PhoneNumber interface with many fields
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 and a list of mock numbers
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
), the user likely has one or more phone numbers (or SIP endpoints) configured to make calls. We present a dropdown or list of their available outbound numbers (with displayName if given, and perhaps type like PSTN or SIP)
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. If they only have one, just show it. If none, instruct them to add one in Settings or present a note that Diala will use a default demo number. For onboarding, we might not require an actual number if it’s a demo (the call could be simulated), but at least show the concept. Next, scheduling: do they want to start calls immediately or later? Provide an option: Start now (immediate) or Schedule later (pick a date/time or time window). For simplicity, default to now. If scheduling is important (maybe not in MVP), we include a date-time picker or a simple “Tomorrow 9am” preset. Another possible input: Max Calls or duration (how long to run). But could keep it open-ended for demo. Perhaps incorporate a safety like “Limit to 10 calls” for demo. But not to overwhelm user, we might hide advanced scheduling and just go with immediate. The UI for selecting phone number could be a Card list like on the Numbers tab of Calls dashboard, showing the number and name (e.g. “Main Sales Line: +1 555 123-4567”
file-fjpkexbjp2dfhprtk7mhwq
). Selected gets bold outline. If scheduling, maybe a small card with clock icon and a couple of options.
Review Campaign Setup: Now we compile all chosen settings into a summary for confirmation. The card might read: “Campaign: Find [keywords] in [location] in [industry] and call using [Agent/Swarm Name] from [Phone Number], starting [Now/later].”. Essentially a sentence or bullet list of the plan. Example: “Prospects: Fintech, SaaS companies in San Francisco, CA. Agent: Diala-Tone (AI Sales Agent). Calls from: +1 (555) 123-4567 (Main Sales Line). Start: Immediately.” We present this in a nicely formatted way, maybe using badges or icons next to each piece of info (location pin icon next to location, briefcase icon for industry, user icon for agent, phone icon for number, clock for schedule). The user verifies all good. If something is off, they can go Back to change. If okay, they hit Launch Campaign (or Start Calling). This triggers the backend to commence the search and call operations. Transition to next step.
Prospect Search & Call Progress: After launching, the onboarding flow shows a dynamic view combining lead search progress and call execution status. This could be split into two areas: Prospect Finding and Live Calls. Initially, the system will be searching for businesses that match criteria. We display something akin to a loading list: e.g., “Searching for businesses...” with maybe a spinner. As results come in, we populate a table or list of prospects (like company names or contacts). The code’s SearchWorkflow has stats like pagesFound, businessesExtracted, validated, etc.
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We can show some of those: “Pages scanned: X, Businesses found: Y, Valid phone numbers: Z” updating in real-time. Each found contact could briefly flash on screen like "Found: Acme Corp, SF – number +1 415 ...". This gives user the sense that the system is actively discovering leads. Simultaneously or after a short delay, the calling begins: perhaps once a few numbers are identified, the AI agent starts dialing. We then show a Live Call Monitor section. For example, show an entry "Calling Acme Corp... (ringing)" with a status icon, then "Connected – Talking" when the agent is on call, etc. The code references LiveCallMonitorModal and CallAnalyticsModal
file-fjpkexbjp2dfhprtk7mhwq
 – those are for the full dashboard, but we mimic a mini version. Potentially, show one or two concurrent calls (if swarm, multiple calls at once). Represent them as small cards or lines with Agent name, prospect name, call status. For realism, we can simulate a short conversation: e.g., after "Talking" for a few seconds, mark "Call Ended – Outcome: Interested (scheduled follow-up)" or "Not interested". These outcomes could be random or predetermined. It demonstrates that the system not only calls but also tracks outcomes (success, no pickup, etc.). We can incorporate some analytics visuals if time – like a tiny chart for pick-up rate, but likely not needed in onboarding, a simple textual log suffices. If possible, integrate audio or transcript snippet: maybe display one line the agent said and the response (like a snippet: Agent: "Hello, is this John from Acme Corp?" – no need for actual audio, just text to illustrate conversation). This might be too deep, but even a static “Transcript available” icon shown could hint at capabilities. The key is to amaze the user that in moments the system found leads and is actually executing calls.
Campaign Outcome Summary: After simulating a few calls (maybe we say it called 5 contacts for demonstration), we present a brief summary of the campaign’s initial outcomes. For example: “Campaign Complete (Demo)\n5 Calls placed, 3 answered, 2 voicemails.\n1 Interested lead, 1 Follow-up scheduled.” This summary would normally accumulate as more calls run, but for onboarding we end it here. We might clarify: “(In a real campaign, calls would continue until the list is exhausted or stopped.)” Then we highlight that the user can monitor everything in the Calls dashboard going forward. Possibly show them where (like "In the Calls dashboard, you'll find Live Call Monitoring and detailed analytics for each campaign.").
Wrap-up and Next Steps: The final screen congratulates the user: “Your first AI-driven call campaign is set up!”. It provides guidance: “You can view and manage campaigns in the Calls section. Analytics for this campaign will update in real-time – check the dashboard to see results and listen to call recordings or read transcripts.” We then offer navigation: a Go to Calls Dashboard button to exit onboarding into /dashboard/calls (perhaps pre-selecting the "calls" or "agents" tab using the query param logic in code
file-fjpkexbjp2dfhprtk7mhwq
). Another possible button: View Campaign Analytics which could open the CallAnalyticsModal for this campaign (if we had an ID; maybe not needed for onboarding, better to just go to dashboard). Also possibly suggest: “You can also create campaigns without the search step if you have your own call lists.” (Because maybe Diala can also import contacts – but not covered here, just a note that search is optional in future usage). Finally, emphasize that the Agents, Swarms, RAG all come together in Calls: their agent was used, knowledge base (if attached) would be used in call, swarm if chosen, etc. This ties the onboarding threads together conceptually.
Components (Frontend)
Form Inputs and Cards: For campaign criteria (step 2), we’ll use various form controls. Likely simple <Input> for location and keywords, and maybe a <Select> or set of radio for industry (we could supply a list of common industries as options to ease input). We may also use <Select> or <Combobox> for some fields. All these go inside a Card with a header “Prospect Search Criteria” or similar. Each field label is bold uppercase (like “LOCATION”, “INDUSTRY”), and use placeholders as described. Possibly group location & industry on one row if space (md screens), otherwise stack. Use our UI input with thick borders. If LinkedIn is a toggle, use a <Switch> component (with label “Include LinkedIn data”). Search depth could be a slider (like “broad <-> deep” search), but simpler: maybe a dropdown: Basic (find ~50 prospects) vs Extensive (find ~200). The code shows searchDepth parameter
file-fjpkexbjp2dfhprtk7mhwq
; define maybe Low/Medium/High mapping to pages. Keep default medium.
Agent/Swarm Selection: Use either radio buttons or card selection. We could implement similar to earlier flows – e.g., in Agents onboarding we had to select voice agent and pitch, here we select calling agent. If using card, each agent card shows name and perhaps a small subtext (like purpose or swarm size). Could reuse AgentCard display but modified for selection. If many options, a simple radio in a list might suffice too. But visual card toggles would be nice. If we foresee adding new from here, we might add a small button "Create new agent" if needed, but likely not for onboarding.
Phone Numbers & Schedule: The phone numbers might be presented in a dropdown or list. The code’s numbers list has fields like provider, status, etc.
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We just need number and name for display. Possibly show status if relevant (most will be active). We could present each as “Name – Number (Type)”, e.g., "Main Sales Line – +1 555 123-4567 (pstn)". Selection via radio or dropdown. For scheduling, a simple pair of radio: “Start Now” vs “Schedule for Later”. If they pick later, reveal a datetime picker (the design system might not have one prebuilt; we can use a native input type="datetime-local" styled appropriately or just a couple of selects for date and time). But to avoid complexity, we might skip detailed scheduling in onboarding and assume Now. If scheduling later, we won’t actually wait, it's just for demonstration, so perhaps skip to not confuse. (Alternatively, if user picks schedule later, we just say "Campaign scheduled for X. We'll notify you when calls start." in summary, but we won't simulate calls now – that could be an alternate path. But doing an immediate demo is more exciting, so likely default to immediate run).
Progress Display Components: For searching prospects and live calls, we might create small sub-components or just structure it clearly in JSX:
A <div> for search progress containing maybe a <Progress> bar or just textual counters. Possibly represent it like an expanding list: we add an <li> for each found business (like a mini log). But could also just update numeric stats. A fun way: have a list area with a fixed height and overflow, each found company name appended to the list (scrolling as it grows). That gives a visual of accumulation. But maybe just counters is fine given time. We'll at least show something like “Found X prospects so far...” updating.
A <div> for active calls. Could show one call at a time if sequential or multiple if parallel (especially if swarm chosen, could do 2-3 concurrently). Each call could be a Card or row with: prospect name, call status, maybe a timer. Represent statuses by icons (phone ringing icon for dialing, phone with waves for in-call, check or cross for outcome). The code’s AgentCall interface had fields for answered, pickedUp etc. which suggests tracking performance by agent
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 – but for live display, we use simpler terms. Possibly incorporate CallStatusIndicator or similar if exists; if not, static. After finishing calls, we list outcome (like "Interested" or "No answer"). We might use colored badges for outcomes (green "Interested", red "No Answer", yellow "Voicemail").
Possibly a simplified timeline: we don’t need actual timeline UI, linear updates suffice.
Modal vs Inline: We will likely do this progress view inline in the onboarding page (like transcripts onboarding did everything inline). However, if we had done a separate modal for call monitoring, that’s more for the full dashboard. Here we integrate it into the guided experience.
Summary/Analytics: At the end, the summary can just be text and badges on a card. If we wanted, we could embed a mini chart – but charts might be too heavy for onboarding. The main app likely has analytics charts (volume over time, etc.). We can simply list key numbers as done above. Perhaps in bold with icons: e.g., a phone icon with "5 Calls", a user icon with "1 Lead". If the CallAnalyticsModal is easily pluggable, we could show it, but that’s probably too much detail. Better to funnel them to the actual Calls dashboard for full analytics if interested.
Backend Architecture (Convex & APIs)
The Calls onboarding initiates a complex orchestration that involves: web search for contacts, and connecting calls via telephony. Breaking it down:
Search Workflow (Prospect Hunting): This likely corresponds to what the code refers to as a SearchWorkflow (the structure with parameters and stats saved when using HuntConfigurationModal)
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. The onboarding will call a Convex function or action to start this search. Similar to AutoRAG, it might create a workflow record and perform asynchronously. Possibly they have a scrapper integrated (maybe something like SerpAPI or custom headless browser calls to Google/Bing or LinkedIn search). The backend will take the criteria (location, industry, keywords) and generate search queries. For each result page, it scrapes company info and tries to extract phone numbers (maybe through the company’s website or directories). It populates a list of found businesses with numbers (businessesExtracted, businessesValidated in stats). It probably also cross-checks with LinkedIn if that option is true (e.g., to gather company size or contact names). This is a non-trivial pipeline: it could involve multiple external APIs (Google Custom Search API for web, LinkedIn API or unofficial scraping for LinkedIn, and maybe a third party for phone validation). For blueprint, assume the backend can handle it and just call an action startProspectHunt returning a workflow id.
Call Campaign Initiation: In parallel or once some contacts are found, the system will start dialing. This involves the telephony integration (Telnyx or another provider) and the agent AI for voice. Likely, they have a call control service that the Agents backend triggers. Possibly the architecture: Diala’s server initiates outbound calls via Telnyx API (by sending a call command with the chosen caller ID number and target number). When the call connects, Telnyx streams audio to/from Diala’s media server (or directly to an AI service). Diala uses Deepgram (for speech-to-text) to transcribe the callee, and uses OpenAI (or similar) to generate agent responses, and ElevenLabs to synthesize the agent’s voice, streaming audio back via Telnyx to the callee. This is all happening in real time, likely orchestrated by a state machine (maybe that’s what convexEntryPoint in agent data references – which conversation workflow to run
file-fjpkexbjp2dfhprtk7mhwq
). For onboarding, we don’t actually perform calls but simulate. However, if the platform is fully functional, one could theoretically do a real call to a test number. But safer to simulate for new users, unless they want to test with their own number. Possibly an enhancement: allow the user to put their own number to receive a demo call from the agent. But that might be risky or too early (and need phone verification etc.). So we simulate. The backend might provide a mode where calls are not actually sent to PSTN but recorded as if done (a dry-run mode). Or we just create dummy call logs.
Convex Data Updates: As calls are “made”, the system would create Call records, update agent stats (pickedUp++ etc.), and generate transcripts and analytics. The code shows in calls page, an AgentCall structure used for a table (calls made, answered, etc.)
file-fjpkexbjp2dfhprtk7mhwq
. The onboarding likely won’t write to the real DB for everything (unless we do a real call). If simulating, we can either push some fake data into the Convex DB (like increment some counters) or just locally simulate. Perhaps better to keep it local for onboarding, not to pollute actual analytics with fake data. The integration can remain conceptual (we describe that these stats would reflect in the real system). If we did decide to involve real data, we’d maybe create a “demo campaign” entry marked as such, but likely not.
Ending the Campaign: In a real scenario, the campaign would continue calling until the prospect list is exhausted or user stops it. For onboarding, we plan a short run. So the backend process for search can be stopped early. Perhaps we only gather 5 prospects then stop. The call process calls those 5 then stops. The Convex should handle halting further calls. Or we treat it as a demo mode within the onboarding code, not on backend – more likely, simulate entirely on front without contacting external. But since we do want to show some realistic asynchronous behavior (like search progress), maybe use a dummy timeline with timeouts to append results. Or spin up a Convex action that yields logs for a short time.
Tracking & Logging: The system likely has logs for each call (transcripts, outcomes). Possibly stored in a Calls collection. If we were doing this fully, we’d want to create those logs. But again, as a demo maybe not persistent. Could just generate ephemeral data. The integration is more about showing what will happen.
Notifications and Real usage: If a user truly set up a campaign for later (schedule), the backend would schedule those calls at that time (maybe using a Convex scheduled function or an external scheduler). Not needed in onboarding, but the architecture supports it if implemented.
Edge Cases: If the user defines a very broad search, real search could take time. Onboarding might either limit scope or inform the user partial results. We likely limit behind the scenes. Perhaps the Convex function for demo purposely only searches a couple of pages to be quick.
Convex Integration in UI: We may choose to use useQuery or polling to reflect search progress in UI. Similar to how we approached RAG. For search, maybe easier to poll a status (like how many found). If we treat it as part of a SearchWorkflow, we could have a record updated and subscribe. Or simply simulate with setInterval incrementing a counter in local state (since we don’t intend to store real results, simulation might be entirely front-end). Actually, simpler: do it front-end. When user hits launch, we can use setTimeouts to simulate asynchronous events:
After 1 sec, show first prospect found.
After 2 sec, show second, and start first call.
After 3 sec, show third, first call ends, second call starts...
etc.
This avoids heavy backend complexity for a demo. But if the system is ready and we want to truly show it, we could do a scaled-down real thing. It's a decision: fidelity vs complexity. Possibly simulation is fine for onboarding.
Telnyx/Telephony Integration: If it were real, the system would use Telnyx API (as keys were in settings
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). It would create call sessions. Telnyx would send webhooks to Diala on call status changes, which Diala’s Convex or serverless functions handle, updating call status (like pickedUp or not, etc.), and instructing the media pipeline. This likely not done via Convex (real-time media might be handled by a separate service, but logging could update Convex). For blueprint, we don't dive too deep, just know calls would be placed.
Design System & Neobrutalist Elements
Visual Style: The violet theme (as used in SearchCalled) gives a distinct identity to the Calls onboarding. Violet and black combination in backgrounds, with white text on violet for contrast, was used in the searchcalled placeholder
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
. We continue that. The grid background or cross-hatch from others remains (just adjusting opacity maybe to 0.1 like others did
file-fjpkexbjp2dfhprtk7mhwq
). Titles like "SEARCH & CALL SETUP" in huge black letters on card headers maintain the brutalist impact
file-fjpkexbjp2dfhprtk7mhwq
.
High-contrast Info Blocks: The progress display can employ brutalist info blocks: e.g., for each call status, use a white or light-gray background block with black border showing the call details, overlaying on a darker background. The transcripts UI had similar blocks for info (like the red blurred backdrop with white info cards inside
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
 for training data info). We can do similarly: the Live Calls area could be a card with a semi-transparent background to stand out on the violet backdrop, containing inner bordered sections for each call (like [20†L6170-L6178] where stats were in a blurred panel). Actually, in swarms card, they have those translucent stat blocks
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
, which is a nice style. We could mimic that for "CALLS: 5" and "SUCCESS: 60%" etc., but for now just straightforward.
Typography for Metrics: Use large numeric displays where possible. E.g., the number of calls or leads can be big to celebrate success. Possibly style "5 Calls" as text with class "text-4xl font-black". Surrounding context smaller. The stat cards in design system might be reused (StatCard component was used in various places
file-fjpkexbjp2dfhprtk7mhwq
file-fjpkexbjp2dfhprtk7mhwq
). Actually, we could use a couple of <StatCard>s for final summary: one for Calls Made (with icon UilPhoneVolume
file-fjpkexbjp2dfhprtk7mhwq
), one for Interested Leads (with icon UilCommentAlt or UilThumbsUp). But that might break the narrative flow to suddenly show stat cards. Instead incorporate these stats into our summary card as text with icons.
Movement & Quirky Elements: We can lean into the "busy activity" vibe. For instance, might use the animate-pulse class on the status dot of an active call (like how active agent dot pulses green
file-fjpkexbjp2dfhprtk7mhwq
). For ringing, maybe animate an icon or wiggle a phone emoji. These small touches fit the playful brutalist approach.
Consistent Buttons: All calls to action (Launch Campaign, etc.) are big, bold, possibly with icons. For example, Launch Campaign button might show an analytics icon UilAnalytics as used in the searchcalled stub
file-fjpkexbjp2dfhprtk7mhwq
. Actually, in searchcalled they had "Configure Search" with an analytics icon
file-fjpkexbjp2dfhprtk7mhwq
. We could follow that: maybe a phone icon on the launch button. Keep button styling same: background Diala blue (0,82,255) or maybe a contrasting color like green to indicate go. But likely they used blue for key actions.
Notification of Steps: Possibly use step indicator (like OnboardingNav) here too if needed, but it's probably straightforward enough without. If used, update steps like "Criteria", "Agent", "Number", "Review", etc. Could help but might clutter UI since steps are many (maybe 5 visible steps plus progress). Could compress some logically (like treat search & call as one step in nav since they happen automatically after launch). Might omit nav for simplicity.
Emotion and Tone: Keep the copy confident and encouraging, reflecting the bold design. E.g., on launch: "Stand by as Diala hunts for prospects and dials out..." – kind of an excited tone. Brutalist design often pairs with straightforward, sometimes cheeky text. We can infuse a bit of excitement: e.g., when showing an interested lead: "🤝 Got one! Acme Corp wants a callback." (Using an emoji like handshake can be within style boundaries as a pop of personality.) Use sparingly to not deviate from professional feel, but one or two could lighten it.
State Management
currentStep (number): from welcome (1) to review (5). After launching, we might either set currentStep to 6 or just to a special "inProgress" state beyond the normal count, since it's not a typical step you navigate back from, it's a running state. But we could treat it as step 6.
Form fields: location, industry, keywords, includeLinkedIn (bool), searchDepth. These capture user input from step 2.
selectedCaller (agent or swarm identifier): could be something like { type: 'agent', id: 'a1' } or if swarm, { type: 'swarm', id: 's1' }. Or simpler, we can use one variable for either, since an agent and swarm have distinct id formatting or we keep a separate bool useSwarm. For front-end, maybe easier to maintain selectedAgentId and selectedSwarmId mutually exclusive (only one will be non-null). The UI ensures they pick one option or the other.
selectedNumberId (or the actual number string): from step 4. And possibly scheduleTime if they chose later (or a flag startNow).
campaignLaunched (bool): flips true once they hit Launch. This triggers the simulation or actual process.
Simulation state: If we simulate in front-end, we manage:
prospectsFound (array of objects or names) – each time we "find" one, we push to this array via setState, causing UI to list them.
calls (array of call objects) – each with prospect, status, outcome. As we simulate calls starting and ending, we update this array or specific call objects. Could hold only active calls, or active + finished. Perhaps have separate arrays activeCalls and completedCalls for clarity. Or one array with a status property and we render accordingly.
prospectCount, callCount, etc. – or derive from above arrays lengths.
If doing an automated timed simulation, might not need these to be reactive to user input, just internal for simulation logic. But to display dynamic updates, we store them and use re-renders.
If we attempted to do some real backend:
searchWorkflowId and status fields like earlier flows.
campaignId for the calling campaign.
But probably not for simulation.
We also track showAnalyticsModal or such if user triggers a deeper view. But likely not.
Back navigation states: allow editing previous steps, so keep input states around and just hide/show sections by currentStep. If currentStep changes, form states remain intact so user can modify and go forward again.
Possibly isCalling or callPhase to indicate we are in the calling phase (to maybe differentiate search vs call updates if needed). But can deduce from currentStep or launched flag.
If any error, e.g., no prospects found (in simulation we won't let that happen, but in real could), could have an error state or message. But in demo, skip error.
UX Interaction & Animations
Simulated Timing: We want a brisk but not instantaneous demo. Use short delays (maybe total of 15-20 seconds for the whole progress). That’s long enough to observe but short enough not to bore. E.g., find first prospect at 1s, then every 1s find another until 5. Start call at 2s, etc. If swarm selected, maybe show two calls at once after a few seconds, to illustrate parallel. We can use setTimeout or a series of them for each event, or a single interval that increments a counter step and triggers events at certain counts. Keep track to clear timers if component unmounted unexpectedly.
Scroll: If listing prospects or calls, ensure the container scrolls as items overflow (with a nice scroll style inside a card). Or auto-scroll to bottom as new items append, so the latest is visible. Use a ref to a list end and call scrollIntoView. Minor detail but improves feel of a live feed.
Call Status Transitions: Perhaps use color or icon changes: e.g., initially a call entry is gray "Dialing", then when "Talking", highlight it or show a small waveform icon. Then outcome with a colored badge. We might animate the change (fade from "Dialing" to "Talking", etc.). Could do a quick fade or slide. CSS transitions on text or a class change suffice.
Sound Effects (optional): If we wanted to be fancy, playing a subtle dial tone or ring sound during the demo could make it immersive. But likely avoid to not startle user or require audio permissions. Perhaps not.
Interactive during simulation: We likely won't allow user to intervene in calls (like no "hang up" or "pause" in onboarding). They just watch. That’s fine. Possibly provide a "Skip demo" or "Finish early" if they don’t want to wait all 20 seconds. Could just have a "Finish" button become visible after launch, which they can click to skip to summary. Not critical but user-friendly if someone is impatient.
Back disabled after launch: Once launched, going back doesn’t make sense as it’s in progress. We might lock navigation and hide or disable the Back button from that point. Or if we allowed schedule later and they want to change mind, theoretically they could go back before launching. But after launching, not.
Completing: When simulation is done, maybe briefly flash a "Campaign Completed" message or icon. Then auto-advance to final summary after a 2-second pause, giving them time to see the last events. Or directly show final summary as part of the progress section turning into summary. Could also scroll up or collapse the progress feed to focus on summary. Possibly we could overlay a semi-transparent celebration (like a checkmark or confetti) when done.
Confetti/Fireworks: Since this is the culminating multi-module demo basically, a tad of celebration could be warranted. E.g., a quick confetti burst when finalizing summary (choose a library or simple falling shapes as earlier idea). If other onboardings didn't do it, maybe we skip for consistency, but this is arguably the biggest "wow" moment, so a little confetti might be nice. Ensure it doesn't hinder reading summary.
Guide to Dashboard: The final step should clearly indicate where to go for more. Possibly highlight "Calls" in the side nav if we had control (like adding a glowing effect around the Calls icon in the sidebar). That would be a nice touch: since the user likely sees the dashboard layout (if onboarding is a page in the app), we could momentarily highlight the navigation item or relevant UI piece. But implementing that might be tricky in isolation, and maybe not expected. Instead, a textual prompt and a button should suffice.
Technical Implementation Details
We’ll implement the simulation timeline in the React component. For example:
jsx
Copy
Edit
useEffect(() => {
  if (campaignLaunched) {
    // simulate finding prospects and calling
    const events = [
      { t: 1000, action: () => addProspect("Acme Corp") },
      { t: 2000, action: () => { addProspect("Beta Inc"); startCall("Acme Corp"); } },
      // ... more events
    ];
    events.forEach(evt => {
       setTimeout(evt.action, evt.t);
    });
  }
}, [campaignLaunched]);
We define helper functions addProspect(name) (push into state) and startCall(name) (push into calls array with status "Dialing", then possibly schedule another event for call connected, ended, etc.). We chain timeouts to simulate call progression: e.g., after startCall, setTimeout in 2s to mark it connected, then in another 3s to mark ended with outcome. That nested but manageable for a few calls. Or store these events in the events timeline as well.
Ensure to clear timeouts if component unmounts using useEffect return () => clearTimeout for each or keep references. But likely not needed since user will either finish or navigate in app after. Still, best practice.
The final summary can be triggered by a final event in timeline (like at t=15000ms, set a state simulationDone = true or directly set currentStep to 7). Or we could derive simulationDone by seeing all calls completed if we track number to make vs made. Simpler: schedule a final event to finalize.
The timeline approach means adjusting if user picks swarm (calls concurrently) vs single (calls sequentially). For swarm, maybe we simulate two parallel calls at once. For sequential (single agent), one finishes then next starts. We can branch the event list depending on selected agent vs swarm. E.g., if swarm, events show two startCalls quickly and maybe overlapping durations. If agent, space them out. This is nuance but to reflect difference. Could do just 2 calls sequential for simplicity regardless, but showing parallel would highlight swarm benefit. Since we have swarms module done, user might try that. We'll implement a basic difference: if swarm chosen, start 2 calls around the same time.
If we ever integrated actual backend call placement (like to the user's phone as a demo), we would involve connecting to Telnyx and actually making a call. That’s heavy and risk of user not picking up or cost. So skip in onboarding.
For integrity, if we did call backend for search, we would have to feed results back to UI similarly, but it's easier to simulate that too. So no Convex calls for actual search. The only maybe Convex call we could do is to create a campaign entry if we wanted it to show up in their real calls list, but might be unnecessary. Perhaps we won't create actual DB records at all – onboarding can exist in a sandbox. That means after finishing, if user goes to Calls dashboard, they might not see the "demo campaign" in logs (which is fine, it was just a tutorial). However, one could argue it might be nice to actually create it so they see something. But since the demo was partial (maybe only 5 calls) and not real, leaving no trace might be cleaner to avoid confusion. We'll just tell them how to do it for real. If we wanted, we could create a dummy "Completed Demo Campaign" record in Convex, but it might clutter their actual analytics. I'd lean not.
So basically the onboarding calls no backend except maybe checking if they have an agent/number to list them. We can fetch agents and numbers via Convex queries at start (like for Swarms we did). If none found where expected, we adjust UI accordingly (like prompt to set up). But likely they do from earlier flows.
Integration with context: since the onboarding is part of app, we have access to whatever providers. The numbers might be in a store or not. If not, we can use a query or simulate one number if none. Possibly the settings page is where API keys/numbers are added, but on initial account maybe there's one default or user input some. If not, we might have to tell them to add one. But for onboarding, maybe we assume they've added a number (in reality, maybe they haven't, but it complicates flow to diverge into adding number). Alternatively, if none, allow them to proceed with a "demo number" that isn't real. We'll do that: show something like "Demo Line (virtual)" if no real number.
Ensure the component is resilient: if no agent, we might create a default. But since we had Agents onboarding, user likely did that first. If not, we should at least require an agent. Could detect and if none, show a message: "Please create an AI agent first to use in calls" with a link to Agents onboarding. But since the user specifically asked for an onboarding for calls, presumably the idea is they'd go through voice agent first anyway. We'll still handle gracefully by maybe creating a quick agent behind scenes named "Demo Agent" if absolutely needed (but that is messy). Simpler: if no agents, just instruct then exit or skip certain steps. But I'd expect sequence: voice agent onboarding -> transcripts (optional) -> swarms -> RAG -> calls. Not guaranteed though. Perhaps call onboarding should encourage using a swarm or agent created. We will assume at least one.
All in all, the calls onboarding logic is mostly front-end simulation orchestrated by timed events, integrated with existing data (agents, numbers) for user-specific context.
Integration Points
Agents & Swarms: The call onboarding directly uses the outputs of previous ones. The chosen agent or swarm in step 3 is exactly one the user configured. If they created a custom agent in Agents onboarding (with a certain persona), they will now use it here – making the earlier work tangible (integration of modules). If they created a swarm (with multiple agents), they can choose it, and the simulation will illustrate parallel calls, reinforcing why swarms matter. In summary, the onboarding flows aren’t siloed: this final Calls flow ties them together, showing the agent or swarm using presumably the knowledge base (if RAG was done) to handle calls, etc. We might mention implicitly that the agent could use the knowledge base – e.g., if they had done RAG, maybe mention on an answered call: "Agent used knowledge base info to answer a question" in passing. But that might be too detailed. Still, conceptually, yes integration: if an agent had ragSources, the call engine would query them to answer specific questions.
Phone Numbers (Telephony Integration): If the user had added multiple numbers in settings (maybe local vs toll-free, etc.), they appear here. If not, the system might have assigned a default. The integration with Telnyx is underlying but not directly visible except via the number selection. Later, on the Calls dashboard (numbers tab), they can manage these numbers (like purchase new, etc.). Onboarding touches just the selection. If the user chooses an inactive or maintenance number (in data, one was maintenance
file-fjpkexbjp2dfhprtk7mhwq
), ideally we filter those out. We'll assume active.
Calls Dashboard: After onboarding, if the user goes to the Calls dashboard, they might expect to see evidence of what happened. As decided, we probably won’t create actual logs, so they might see nothing there except their agent stats (calls=0 still). To prevent confusion, our final text should clarify the campaign was a demo. Possibly encourage them to run a real one. Or as an alternative, we could allow them to actually finalize the config into a real campaign. For example: after the demo, we could say "Now that you've seen how it works, do you want to run this campaign for real?" If yes, then actually use the same criteria to start a real search & call in the background. That could be an advanced option – might overwhelm. But could be offered if keys/numbers are all set. Might skip. Just signpost that to do it for real, they can go to Calls and launch a campaign (maybe by using the same form there). Actually, the calls page might have the hunts/campaign UI. The code suggests it has a tab for calls and possibly an area for launching new ones (the modals we saw). The HuntConfigurationModal and workflow modals in code likely handle the real thing. So the user will use those next time.
Analytics and transcripts: The calls module integrates with transcripts (recordings) and analytics. We likely mention transcripts (maybe "transcripts of each call are saved in the Transcripts section") tying that piece in. And analytics (like success rate, average call time in Agents or overall in Calls). For integration, possibly after running some calls, the agent's success rate stat could update. But since we didn't actually update DB, in real usage it would. The final summary could hint "Your agent's performance metrics will update based on these calls, visible in Agents Analytics." So it's known that these modules feed each other.
External CRM or follow-ups: Not directly shown, but an integration could be exporting leads or scheduling follow-ups. If Diala had a CRM integration, it might push the interested lead info somewhere. Out of scope for onboarding though.
Ending Conditions: The user might wonder, does the campaign stop automatically or run continuously? We should integrate that knowledge: probably it stops after finishing found leads. If user wants to stop early, the real system likely has a stop button in the Live Monitor. In onboarding, we didn't simulate that interaction. But maybe mention "You can pause or stop campaigns anytime from the dashboard." to integrate that piece of UI concept.
Data Persistence: The prospect search might produce data (the found leads). In a real scenario, the system might store those leads in a database or at least as part of the campaign record (for future reference or reuse). We are not doing that in onboarding, but the integration point exists – if user runs an actual campaign after, those leads would possibly appear in a table in the Calls section (maybe under a "Recent Calls" or some CRM-ish list). The onboarding could mention "All contacts dialed will be recorded, so you can review and export them." if true. But if not implemented fully, skip.
Wrap Up Integration: The calls onboarding is basically the culmination of all previous features working together – it's where Agents, Swarms, RAG, etc., come to life. So it inherently integrates everything. Our narrative and UI hints can point out these ties. E.g., if RAG was done, maybe in a call we simulate the agent giving a detailed answer thanks to the knowledge. But that might be lost on users who didn't do RAG. So maybe not mention it explicitly; but if the user did do RAG, they'd probably connect the dots themselves (and if they didn't, mention might confuse). So best to not mention RAG in call onboarding text unless sure. Possibly neutral mention: "Your agent can answer detailed questions thanks to its training" – that could apply generally (training could mean transcripts or RAG). We do have to keep it accessible even if they skipped transcripts or rag.
Future Real Integration: If the user runs a real campaign after, the flow would be similar but possibly slower (calls take minutes, etc.). The onboarding prepared them for what to expect, making the actual process less opaque.
Future Enhancements
Personalized Demo Calls: As touched on, a possible future feature is letting a new user receive a demo call from their AI agent. For example, after setting up, instead of a simulation in-app, ask "Would you like to receive a demo call from your AI agent right now at your phone?" If they enter their number and accept, the system could actually call them using the configured agent to run through a short script. This is a powerful demo because hearing the AI voice live is convincing. However, it requires a phone number, user consent, and uses actual telephony credits. As an enhancement, it could be offered if the user profile has a verified phone. It's a more visceral integration of the technology into onboarding.
Campaign Templates: Provide templates for common campaigns (similar to how we discussed swarm templates). E.g., "Follow-up Campaign" or "Welcome Call Campaign" pre-filling certain parameters. Onboarding could ask "What kind of campaign do you want?" up front, and if they select a template, many fields auto-fill (maybe skip search if they already have contacts, or use certain keywords). Not in initial, but a future user who logs in might select a template and launch a campaign in one minute. Onboarding could then adapt to show only minimal steps for that template.
Integration with CRM/Contacts upload: In future, not all campaigns will involve searching for leads; some might call existing customer lists. Onboarding in the future might branch: "Do you want to find new prospects or call your own list?" If the latter, they'd upload contacts or select from an integrated CRM. We could add a step to import a CSV of phone numbers for example. This would showcase versatility. It's more complex, so for now we focused on search. But adding that path eventually will cover more use cases.
More Realistic Call Simulation: Later, we might incorporate actual transcript segments or TTS in the onboarding to let user hear how an AI call sounds. Perhaps an animation of a waveform when agent speaking, and text of response. Or even audio playback if they click. This could be a built-in recording rather than generating on the fly, just to show quality of AI voice. It'd impress users but need audio handling. Possibly a "Listen to sample call" button at the end as an extra.
Dashboard Tour Mode: After campaign creation, the onboarding could seamlessly transition into a mini tour of the Calls dashboard (highlighting the Live Monitor tab, Analytics tab). This would directly integrate training with the actual interface. E.g., overlay arrows or highlights on the actual dashboard elements. This crosses from the "onboarding flow" into in-app guidance territory (like guided tours). Could be valuable but requires a framework to highlight UI elements. A future enhancement could unify these, but for now our flows have been separate pages. In lieu, textual guidance and a button to go see the results is what we do.
Continuous Onboarding Flow: Perhaps envision an even larger flow that ties all modules sequentially. E.g., a new user could optionally go through voice agent -> RAG -> swarm -> calls in one guided sequence (with ability to skip parts). This would be a meta-onboarding. Currently they are separate flows, but one could imagine combining them with branching. It's advanced but maybe later a "Setup Wizard" that covers everything in an optimal order. Our separate blueprints could then be interconnected steps of one super-flow. Implementation aside, conceptually it may help some users who want a full setup at once.
Monitoring & Tuning: A possible advanced onboarding extension is after a few calls, showing how to adjust on the fly. For instance, if the campaign isn't yielding success, how to tweak criteria or switch agent. That might be beyond initial onboarding, but for continuous improvement. Possibly an in-app hint later, not in first-run onboarding.
Gamification: Could add some gamification: e.g., show an achievement "First Campaign Launched!" and track how many leads/ calls achieved. These sort of things (badges, progress) encourage usage. Not directly part of onboarding blueprint, but an idea for user retention.
Multi-language or International campaign settings: If expanding globally, onboarding might ask for timezones, multi-lingual agent usage, etc. Could be future fields (like if calling different countries, choose agent languages accordingly). Now likely not needed.
Incorporating Feedback: After calls, maybe prompt user "How did your agent do on these calls?" gather feedback to refine agent prompt. That closes a learning loop. Possibly later a quick thumbs up/down after hearing a sample call.
All these enhancements aim to deepen the user's engagement and success with the Calls module after the initial onboarding, ensuring they not only try it out but also incorporate it effectively into their workflow.
