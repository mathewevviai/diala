Directory structure:
└── services/
    ├── adaptive_thresholding_manager.py
    ├── audio_preparation_service.py
    ├── audio_processor.py
    ├── audio_separation_service.py
    ├── batch_embedding_service.py
    ├── bulk_audio_processor.py
    ├── bulk_job_manager.py
    ├── BULK_JOB_MANAGER_USAGE.md
    ├── bulk_processing_service.py
    ├── bulk_workflow_orchestrator.py
    ├── chatterbox_client.py
    ├── chatterbox_service.py
    ├── comprehensive_audio_service.py
    ├── content_chunking_service.py
    ├── droplet_manager.py
    ├── eleven_labs_client.py
    ├── embedding_quality_assessor.py
    ├── enhanced_stream_simulation.py
    ├── fast_graph_optimizer.py
    ├── graph_based_clustering_engine.py
    ├── gstreamer_service.py
    ├── instagram_service.py
    ├── integrated_speaker_identifier.py
    ├── memory_efficient_speaker_manager.py
    ├── memory_monitor.py
    ├── modern_diarization_requirements.txt
    ├── modern_stateful_speaker_identifier.py
    ├── procedural_audio_service.py
    ├── prosody_analysis_service.py
    ├── quality_weighted_centroid.py
    ├── quality_weighted_centroid_manager.py
    ├── realtime_analysis_service.py
    ├── redis_client.py
    ├── speaker_embedding_service.py
    ├── speaker_merging.py
    ├── speaker_profile.py
    ├── speaker_pruning.py
    ├── stream_simulation_service.py
    ├── telephony_service.py
    ├── telephony_service_monitoring.py
    ├── telnyx_client.py
    ├── temporal_context_tracker.py
    ├── tiktok_service.py
    ├── tts_manager.py
    ├── tts_service.py
    ├── twitch_service.py
    ├── vector_database_examples.py
    ├── vector_database_service.py
    ├── vector_db_connectors.py
    ├── voice_clone_jobs.py
    ├── gemini/
    │   ├── __init__.py
    │   ├── config.py
    │   ├── embeddings_client.py
    │   ├── embeddings_service.py
    │   └── models.py
    └── jina/
        ├── __init__.py
        ├── config.py
        ├── embeddings_client.py
        ├── embeddings_service.py
        └── models.py

================================================
FILE: adaptive_thresholding_manager.py
================================================
#!/usr/bin/env python3
"""
Adaptive Thresholding Manager for Speaker Diarization

Implements per-speaker adaptive thresholds following:
- Park et al. (2022) "Adaptive Clustering for Online Speaker Diarization"

This module provides dynamic threshold adaptation for speaker identification
in streaming scenarios, improving accuracy across varying acoustic conditions
and speaker characteristics.
"""

import numpy as np
from typing import Dict, Optional
from collections import deque
import time
import logging

logger = logging.getLogger(__name__)


class AdaptiveThresholdingManager:
    """
    Manages per-speaker adaptive thresholds with confidence-weighted updates.
    
    Following Park et al. (2022), this class implements:
    1. Per-speaker threshold storage and management
    2. Confidence-weighted threshold adaptation with exponential moving average
    3. Temporal smoothing for threshold stability
    4. Base threshold initialization and adaptation rate parameters
    """
    
    def __init__(self,
                 base_threshold: float = 0.7,
                 adaptation_rate: float = 0.1,
                 smoothing_window: int = 5,
                 min_threshold: float = 0.4,
                 max_threshold: float = 0.9,
                 confidence_decay: float = 0.95,
                 temporal_smoothing_alpha: float = 0.3):
        """
        Initialize the adaptive thresholding manager.
        """
        self.base_threshold = base_threshold
        self.adaptation_rate = adaptation_rate
        self.smoothing_window = smoothing_window
        self.min_threshold = min_threshold
        self.max_threshold = max_threshold
        self.confidence_decay = confidence_decay
        self.temporal_smoothing_alpha = temporal_smoothing_alpha
        
        self.speaker_thresholds: Dict[str, float] = {}
        self.confidence_history: Dict[str, deque] = {}
        self.threshold_history: Dict[str, deque] = {}
        self.speaker_stats: Dict[str, Dict] = {}
        self.quality_weight_factor = 0.5
        
        logger.info(f"AdaptiveThresholdingManager initialized with base_threshold={base_threshold}")
    
    def get_threshold(self, speaker_id: str) -> float:
        """Get the current adaptive threshold for a specific speaker."""
        if speaker_id not in self.speaker_thresholds:
            self._initialize_speaker(speaker_id)
        return self.speaker_thresholds[speaker_id]
    
    def update_threshold(self, 
                        speaker_id: str, 
                        similarity: float, 
                        quality_score: float,
                        was_accepted: bool = True) -> None:
        """Update speaker-specific threshold using Park et al. confidence weighting."""
        if speaker_id not in self.speaker_thresholds:
            self._initialize_speaker(speaker_id)
        
        current_threshold = self.speaker_thresholds[speaker_id]
        
        confidence_weight = self._calculate_confidence_weight(
            speaker_id, similarity, quality_score, was_accepted
        )
        
        threshold_adjustment = self._calculate_threshold_adjustment(
            similarity, quality_score, was_accepted, confidence_weight
        )
        
        raw_new_threshold = current_threshold + (
            self.adaptation_rate * confidence_weight * threshold_adjustment
        )
        
        smoothed_threshold = self._apply_temporal_smoothing(
            speaker_id, raw_new_threshold
        )
        
        final_threshold = np.clip(
            smoothed_threshold, 
            self.min_threshold, 
            self.max_threshold
        )
        
        self.speaker_thresholds[speaker_id] = final_threshold
        self._update_speaker_statistics(speaker_id, similarity, quality_score, was_accepted)
        
        logger.debug(f"Updated threshold for {speaker_id}: {current_threshold:.3f} -> {final_threshold:.3f}")
    
    def should_accept_assignment(self, 
                                speaker_id: str, 
                                similarity: float, 
                                quality_score: float) -> bool:
        """Decision function with quality-aware threshold comparison."""
        threshold = self.get_threshold(speaker_id)
        quality_adjusted_threshold = self._get_quality_adjusted_threshold(
            threshold, quality_score
        )
        return similarity >= quality_adjusted_threshold
    
    def _initialize_speaker(self, speaker_id: str) -> None:
        """Initialize a new speaker with default parameters."""
        self.speaker_thresholds[speaker_id] = self.base_threshold
        self.confidence_history[speaker_id] = deque(maxlen=self.smoothing_window)
        self.threshold_history[speaker_id] = deque(maxlen=self.smoothing_window)
        self.speaker_stats[speaker_id] = {
            'total_comparisons': 0, 'accepted_comparisons': 0,
            'avg_similarity': 0.0, 'avg_quality': 0.0,
            'last_update_time': time.time()
        }
        logger.debug(f"Initialized new speaker: {speaker_id}")
    
    def _calculate_confidence_weight(self, 
                                   speaker_id: str, 
                                   similarity: float, 
                                   quality_score: float,
                                   was_accepted: bool) -> float:
        """Calculate confidence weight for threshold adaptation."""
        quality_confidence = quality_score
        history_confidence = self._calculate_history_confidence(speaker_id, was_accepted)
        similarity_confidence = min(1.0, similarity / self.base_threshold)
        
        confidence_weight = (
            0.4 * quality_confidence +
            0.3 * history_confidence +
            0.3 * similarity_confidence
        )
        return np.clip(confidence_weight, 0.1, 1.0)
    
    def _calculate_history_confidence(self, speaker_id: str, was_accepted: bool) -> float:
        """Calculate confidence based on recent decision history."""
        history = self.confidence_history.get(speaker_id)
        if not history:
            return 0.5
        
        recent_decisions = list(history)
        recent_decisions.append(1.0 if was_accepted else 0.0)
        
        weights = np.array([self.confidence_decay ** i for i in range(len(recent_decisions))])[::-1]
        return np.average(recent_decisions, weights=weights)

    def _calculate_threshold_adjustment(self, 
                                     similarity: float, 
                                     quality_score: float,
                                     was_accepted: bool,
                                     confidence_weight: float) -> float:
        """
        Calculate the magnitude and direction of threshold adjustment.
        """
        base_adjustment = 0.0
        
        if was_accepted:
            if quality_score > 0.8 and similarity > 0.9:
                # High quality, high similarity match. We can be more selective.
                base_adjustment = 0.02
            elif quality_score < 0.4:
                # Accepted a low-quality segment. This is risky (potential false positive).
                # Nudge the threshold higher to be more selective in the future.
                base_adjustment = 0.01
        else: # was_rejected
            if quality_score > 0.7 and similarity > 0.6:
                # Rejected a high-quality segment with decent similarity. We might be too strict.
                # Slightly larger adjustment to ensure it's measurable in tests.
                base_adjustment = -0.05
            elif quality_score < 0.3:
                # Correctly rejected a low-quality segment. Reinforce this behavior.
                base_adjustment = 0.01
        
        return base_adjustment * confidence_weight
    
    def _apply_temporal_smoothing(self, speaker_id: str, new_threshold: float) -> float:
        """Apply temporal smoothing using exponential moving average."""
        history = self.threshold_history[speaker_id]
        if not history:
            history.append(new_threshold)
            return new_threshold
        
        last_smoothed = history[-1]
        smoothed = (
            self.temporal_smoothing_alpha * new_threshold +
            (1 - self.temporal_smoothing_alpha) * last_smoothed
        )
        history.append(smoothed)
        return smoothed
    
    def _get_quality_adjusted_threshold(self, threshold: float, quality_score: float) -> float:
        """Adjust threshold for a single decision based on current audio quality."""
        if quality_score < 0.3:
            adjustment = -0.1
        elif quality_score < 0.5:
            adjustment = -0.05
        elif quality_score > 0.8:
            adjustment = 0.02
        else:
            adjustment = 0.0
        
        adjusted_threshold = threshold + (adjustment * self.quality_weight_factor)
        return np.clip(adjusted_threshold, self.min_threshold, self.max_threshold)
    
    def _update_speaker_statistics(self, 
                                 speaker_id: str, 
                                 similarity: float, 
                                 quality_score: float,
                                 was_accepted: bool) -> None:
        """Update speaker statistics for monitoring and adaptation."""
        stats = self.speaker_stats[speaker_id]
        stats['total_comparisons'] += 1
        if was_accepted:
            stats['accepted_comparisons'] += 1
        
        n = stats['total_comparisons']
        stats['avg_similarity'] = ((stats['avg_similarity'] * (n - 1) + similarity) / n)
        stats['avg_quality'] = ((stats['avg_quality'] * (n - 1) + quality_score) / n)
        
        self.confidence_history[speaker_id].append(1.0 if was_accepted else 0.0)
        stats['last_update_time'] = time.time()
    
    def get_speaker_statistics(self, speaker_id: str) -> Optional[Dict]:
        """Get statistics for a specific speaker."""
        stats = self.speaker_stats.get(speaker_id)
        if not stats:
            return None
        
        stats_copy = stats.copy()
        stats_copy['current_threshold'] = self.get_threshold(speaker_id)
        if stats_copy['total_comparisons'] > 0:
            stats_copy['acceptance_rate'] = (stats_copy['accepted_comparisons'] / stats_copy['total_comparisons'])
        else:
            stats_copy['acceptance_rate'] = 0.0
        return stats_copy
    
    def cleanup_inactive_speakers(self, inactive_threshold_seconds: float = 300) -> int:
        """Remove speakers that haven't been updated recently."""
        current_time = time.time()
        inactive_speakers = [
            spk_id for spk_id, stats in self.speaker_stats.items()
            if current_time - stats['last_update_time'] > inactive_threshold_seconds
        ]
        
        for speaker_id in inactive_speakers:
            del self.speaker_thresholds[speaker_id]
            del self.confidence_history[speaker_id]
            del self.threshold_history[speaker_id]
            del self.speaker_stats[speaker_id]
        
        if inactive_speakers:
            logger.info(f"Cleaned up {len(inactive_speakers)} inactive speakers")
        
        return len(inactive_speakers)


================================================
FILE: audio_preparation_service.py
================================================
"""
Audio Preparation Service

Provides unified audio preparation for different TTS providers,
including Whisper transcription, segmentation, and provider-specific preprocessing.
Based on Trelis TTS fine-tuning approach for optimal voice cloning results.
"""

import os
import json
import logging
import tempfile
import asyncio
import subprocess
import uuid
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
import soundfile as sf
import numpy as np
from src.services.audio_separation_service import audio_separation_service
from src.services.realtime_analysis_service import get_realtime_analysis_service
from src.services.comprehensive_audio_service import comprehensive_audio_service

logger = logging.getLogger(__name__)


class AudioPreparationService:
    """Unified service for preparing audio for various TTS providers"""
    
    def __init__(self):
        """Initialize the audio preparation service"""
        self.temp_dir = tempfile.mkdtemp(prefix="audio_prep_")
        
        logger.info("Audio Preparation Service initialized - Using Whisper v3 ASR via realtime service")
    
    async def prepare_audio(
        self,
        audio_path: str,
        provider: str = "chatterbox",
        config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Prepare audio for TTS provider with optional Whisper processing
        
        Args:
            audio_path: Path to input audio file
            provider: TTS provider name
            config: Preparation configuration
                - use_whisper: Enable Whisper transcription
                - segment_audio: Split into segments
                - max_segment_duration: Max segment length (seconds)
                - transcribe: Generate transcript
                - clean_silence: Remove silence
                - provider_specific: Provider-specific options
        
        Returns:
            Dictionary containing:
                - prepared_audio_path: Path to prepared audio
                - transcription: Optional transcript
                - segments: Optional list of segments
                - metadata: Additional preparation metadata
        """
        try:
            # Default configuration
            default_config = {
                "use_whisper": True,
                "segment_audio": True,
                "max_segment_duration": 30,
                "transcribe": True,
                "clean_silence": True,
                "provider_specific": {}
            }
            
            # Merge with provided config
            if config:
                default_config.update(config)
            config = default_config
            
            # Provider-specific preparation
            if provider == "chatterbox":
                return await self._prepare_for_chatterbox(audio_path, config)
            elif provider == "elevenlabs":
                return await self._prepare_for_elevenlabs(audio_path, config)
            elif provider == "transcription":
                return await self._prepare_for_transcription(audio_path, config)
            else:
                # Default preparation (basic conversion)
                return await self._prepare_default(audio_path, config)
                
        except Exception as e:
            logger.error(f"Error preparing audio: {str(e)}")
            raise
    
    async def _prepare_for_chatterbox(
        self,
        audio_path: str,
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Prepare audio specifically for Chatterbox TTS
        Using Trelis approach: Whisper transcription + segmentation
        """
        result = {
            "prepared_audio_path": audio_path,
            "transcription": None,
            "segments": [],
            "metadata": {}
        }
        
        try:
            # Step 1: Separate vocals if requested (for cleaner voice cloning)
            if config.get("separate_voices", True):
                logger.info("Extracting vocals for voice cloning")
                
                separation_config = {
                    "extract_vocals": True,
                    "min_speakers": 1,
                    "max_speakers": 1,  # Voice cloning is single speaker
                    "min_duration": 1.0
                }
                
                separation_result = await audio_separation_service.separate_and_diarize(
                    audio_path,
                    config=separation_config
                )
                
                # Use vocals for all subsequent processing
                if separation_result.get("vocals_path"):
                    audio_path = separation_result["vocals_path"]
                    result["metadata"]["vocals_extracted"] = True
                    logger.info(f"Using extracted vocals: {audio_path}")
            
            # Transcribe with Whisper v3 via realtime service
            logger.info(f"Transcribing audio with Whisper v3 ASR: {audio_path}")
            
            # Get realtime analysis service
            realtime_service = await get_realtime_analysis_service()
            
            # Load and process audio directly
            audio_data, sample_rate = sf.read(audio_path, dtype='int16')
            audio_bytes = audio_data.tobytes()
            
            # Process with realtime analysis service
            analysis_result = await realtime_service.process_sentiment_chunk(audio_bytes)
            
            # Create transcription result in expected format
            transcription_result = {
                "transcript": analysis_result.get("text", "").strip(),
                "language": "en",
                "sentiment": analysis_result.get("sentiment", "neutral"),
                "tokens": analysis_result.get("tokens", [])
            }
            
            # Store transcription
            result["transcription"] = transcription_result.get("transcript", "").strip()
            result["metadata"]["language"] = transcription_result.get("language", "en")
            
            # Process segments if enabled
            if config.get("segment_audio"):
                transcript_text = transcription_result.get("transcript", "")
                if transcript_text:
                    # Create basic segments for chatterbox processing
                    sentences = [s.strip() for s in transcript_text.split('.') if s.strip()]
                    segments = []
                    current_time = 0.0
                    audio_duration = len(audio_data) / sample_rate if len(audio_data) > 0 else 0
                    
                    for sentence in sentences:
                        if sentence:
                            # Estimate duration based on word count
                            word_count = len(sentence.split())
                            estimated_duration = min(word_count * 0.5, audio_duration - current_time)
                            
                            segments.append({
                                "text": sentence.strip(),
                                "start": current_time,
                                "end": current_time + estimated_duration,
                                "duration": estimated_duration,
                                "path": audio_path  # Use original audio for now
                            })
                            current_time += estimated_duration
                    
                    result["segments"] = segments
                    result["metadata"]["total_segments"] = len(segments)
                    result["metadata"]["total_duration"] = current_time
            
            # Additional Chatterbox-specific processing
            if config.get("clean_silence"):
                cleaned_path = await self._remove_silence(result["prepared_audio_path"])
                if cleaned_path != result["prepared_audio_path"]:
                    result["prepared_audio_path"] = cleaned_path
                    result["metadata"]["silence_removed"] = True
            
            # Ensure correct format for Chatterbox (24kHz, mono)
            final_path = await self._ensure_audio_format(
                result["prepared_audio_path"],
                sample_rate=24000,
                channels=1
            )
            result["prepared_audio_path"] = final_path
            
            # Log the complete processing chain
            logger.info(f"Chatterbox audio preparation complete: "
                       f"vocals_extracted={result['metadata'].get('vocals_extracted', False)}, "
                       f"segments={len(result['segments'])}, "
                       f"silence_removed={result['metadata'].get('silence_removed', False)}")
            
            return result
            
        except Exception as e:
            logger.error(f"Error in Chatterbox preparation: {str(e)}")
            raise
    
    async def _prepare_for_elevenlabs(
        self,
        audio_path: str,
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Prepare audio specifically for ElevenLabs
        (Placeholder for future implementation)
        """
        # ElevenLabs may have different requirements
        # For now, use default preparation
        return await self._prepare_default(audio_path, config)
    
    async def _prepare_for_transcription(
        self,
        audio_path: str,
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Prepare audio specifically for transcription
        Focused on getting the best transcription quality
        """
        result = {
            "prepared_audio_path": audio_path,
            "transcription": None,
            "segments": [],
            "metadata": {}
        }
        
        try:
            # Step 1: Separate vocals and identify speakers if requested
            if config.get("separate_voices", True) or config.get("identify_speakers", True):
                logger.info("Performing audio separation and speaker diarization")
                
                separation_config = {
                    "extract_vocals": config.get("separate_voices", True),
                    "min_speakers": config.get("min_speakers", 1),
                    "max_speakers": config.get("max_speakers", 10),
                    "min_duration": config.get("min_speaker_duration", 1.0)
                }
                
                separation_result = await audio_separation_service.separate_and_diarize(
                    audio_path,
                    config=separation_config
                )
                
                # Use vocals if extracted, otherwise use original
                if separation_result.get("vocals_path"):
                    audio_path = separation_result["vocals_path"]
                    result["metadata"]["vocals_extracted"] = True
                
                # Store diarization results
                result["diarization"] = separation_result.get("diarization")
                result["metadata"]["speakers_identified"] = separation_result.get("diarization", {}).get("speakers", 1)
            
            # Transcribe with Whisper v3 via realtime service
            logger.info(f"Transcribing audio with Whisper v3 ASR: {audio_path}")
            
            # Clean silence if requested (after vocal extraction for better results)
            if config.get("clean_silence", True):
                cleaned_path = await self._remove_silence(audio_path)
                audio_path = cleaned_path
                result["metadata"]["silence_removed"] = True
            
            # Ensure proper format for ASR (16kHz is optimal)
            formatted_path = await self._ensure_audio_format(
                audio_path,
                sample_rate=16000,  # Whisper prefers 16kHz for stable results
                channels=1
            )
            
            # Use the proven comprehensive audio processing approach
            logger.info("Using comprehensive audio processing service (based on working stream_simulation.py)")
            
            comprehensive_result = await comprehensive_audio_service.process_audio_comprehensive(
                audio_path=formatted_path,
                separate_speakers=config.get("identify_speakers", True),
                use_pyannote=True,  # Use pyannote diarization
                max_seconds=None
            )
            
            if "error" in comprehensive_result:
                logger.error(f"Comprehensive processing failed: {comprehensive_result['error']}")
                # Fallback to basic realtime service
                realtime_service = await get_realtime_analysis_service()
                audio_data, sample_rate = sf.read(formatted_path, dtype='int16')
                audio_bytes = audio_data.tobytes()
                analysis_result = await realtime_service.process_sentiment_chunk(audio_bytes)
                
                transcription_result = {
                    "transcript": analysis_result.get("text", "").strip(),
                    "language": "en",
                    "sentiment": analysis_result.get("sentiment", "neutral"),
                    "tokens": analysis_result.get("tokens", [])
                }
            else:
                # Use comprehensive results and format for Convex schema
                segments = comprehensive_result.get("segments", [])
                
                # Convert segments to Convex speaker format
                convex_speakers = []
                for segment in segments:
                    convex_speakers.append({
                        "speaker": segment.get("speaker", "Unknown"),
                        "start": segment.get("start_time", 0),
                        "end": segment.get("end_time", 0), 
                        "duration": segment.get("duration", 0),
                        "text": segment.get("text", ""),
                        "sentiment": segment.get("sentiment", "neutral"),
                        "speaker_similarity": segment.get("speaker_similarity", 0.0),
                        "langextract_analysis": segment.get("langextract_analysis", {}),
                        "emotion2vec": segment.get("emotion2vec", {})
                    })
                
                transcription_result = {
                    "transcript": comprehensive_result.get("transcript", "").strip(),
                    "language": comprehensive_result.get("language", "en"),
                    "segments": convex_speakers,  # Use formatted speaker data
                    "speakers": comprehensive_result.get("speakers", []),
                    "total_segments": comprehensive_result.get("total_segments", 0),
                    "processing_approach": comprehensive_result.get("processing_approach", "comprehensive")
                }
            
            # Store full transcription
            result["transcription"] = transcription_result.get("transcript", "").strip()
            result["metadata"]["language"] = transcription_result.get("language", "en")
            
            # Process segments with detailed information
            if config.get("segment_audio"):
                # Create basic segments from transcription
                transcript_text = transcription_result.get("transcript", "")
                if transcript_text:
                    # Simple sentence-based segmentation
                    sentences = [s.strip() for s in transcript_text.split('.') if s.strip()]
                    segments = []
                    current_time = 0.0
                    audio_duration = len(audio_data) / sample_rate if len(audio_data) > 0 else 0
                    
                    for i, sentence in enumerate(sentences):
                        if sentence:
                            # Estimate duration based on word count
                            word_count = len(sentence.split())
                            estimated_duration = min(word_count * 0.5, audio_duration - current_time)
                            
                            segment_data = {
                                "text": sentence.strip(),
                                "start": current_time,
                                "end": current_time + estimated_duration,
                                "duration": estimated_duration,
                                "speaker": "SPEAKER_00",
                                "sentiment": transcription_result.get("sentiment", "neutral")
                            }
                            segments.append(segment_data)
                            current_time += estimated_duration
                    
                    result["segments"] = segments
                    result["metadata"]["total_segments"] = len(segments)
                    
                    # Calculate total duration
                    if segments:
                        result["metadata"]["total_duration"] = segments[-1]["end"]
                    
                    # Group segments by speaker
                    result["segments_by_speaker"] = self._group_segments_by_speaker(segments)
            
            # Clean up temporary formatted file if different from input
            if formatted_path != audio_path and os.path.exists(formatted_path):
                os.unlink(formatted_path)
            
            result["prepared_audio_path"] = audio_path

            # Extra logging summary
            logger.info(
                "Transcription prepared (Whisper v3): length_chars=%d, speakers=%s",
                len(result.get("transcription") or ""),
                result.get("diarization", {}).get("speakers") if result.get("diarization") else "n/a",
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Error in transcription preparation: {str(e)}")
            raise
    
    async def _prepare_default(
        self,
        audio_path: str,
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Default audio preparation (basic conversion)"""
        result = {
            "prepared_audio_path": audio_path,
            "transcription": None,
            "segments": [],
            "metadata": {}
        }
        
        # Basic format conversion
        final_path = await self._ensure_audio_format(audio_path)
        result["prepared_audio_path"] = final_path
        
        return result
    
    async def _process_asr_segments(
        self,
        audio_path: str,
        segments: List[Dict[str, Any]],
        max_duration: int
    ) -> Dict[str, Any]:
        """
        Process ASR segments and create audio chunks
        Based on Trelis approach for optimal TTS training
        """
        chunks = []
        current_chunk = None
        
        # Group segments into chunks <= max_duration
        for segment in segments:
            start, end, text = segment["start"], segment["end"], segment["text"]
            duration = end - start
            
            # Skip segments that are too long individually
            if duration > max_duration:
                logger.warning(f"Skipping segment >={max_duration}s: {text[:50]}...")
                continue
            
            if current_chunk is None:
                current_chunk = {"start": start, "end": end, "text": text}
            elif (end - current_chunk["start"]) <= max_duration:
                # Extend current chunk
                current_chunk["end"] = end
                current_chunk["text"] += " " + text
            else:
                # Save current chunk and start new one
                chunks.append(current_chunk)
                current_chunk = {"start": start, "end": end, "text": text}
        
        # Don't forget the last chunk
        if current_chunk:
            chunks.append(current_chunk)
        
        logger.info(f"Created {len(chunks)} audio chunks from {len(segments)} segments")
        
        # Extract audio chunks
        chunk_files = []
        for i, chunk in enumerate(chunks):
            chunk_path = os.path.join(self.temp_dir, f"chunk_{uuid.uuid4().hex}.wav")
            
            # Use ffmpeg to extract chunk
            cmd = [
                "ffmpeg", "-loglevel", "error", "-y",
                "-i", audio_path,
                "-ss", str(chunk["start"]),
                "-to", str(chunk["end"]),
                "-ar", "24000",  # 24kHz for TTS
                "-ac", "1",      # Mono
                chunk_path
            ]
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            _, stderr = await process.communicate()
            
            if process.returncode != 0:
                logger.error(f"FFmpeg error: {stderr.decode()}")
                continue
            
            chunk_files.append({
                "path": chunk_path,
                "text": chunk["text"].strip(),
                "start": chunk["start"],
                "end": chunk["end"],
                "duration": chunk["end"] - chunk["start"]
            })
        
        # Merge chunks back into single file
        merged_path = os.path.join(self.temp_dir, f"prepared_{uuid.uuid4().hex}.wav")
        
        if chunk_files:
            # Create file list for ffmpeg concat
            list_path = os.path.join(self.temp_dir, "concat_list.txt")
            with open(list_path, 'w') as f:
                for chunk in chunk_files:
                    f.write(f"file '{chunk['path']}'\n")
            
            # Concatenate chunks
            cmd = [
                "ffmpeg", "-loglevel", "error", "-y",
                "-f", "concat",
                "-safe", "0",
                "-i", list_path,
                "-c", "copy",
                merged_path
            ]
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            await process.communicate()
            
            # Clean up temp files
            os.unlink(list_path)
            for chunk in chunk_files:
                if os.path.exists(chunk["path"]):
                    os.unlink(chunk["path"])
        else:
            # No valid chunks, use original
            merged_path = audio_path
        
        # Calculate total duration
        total_duration = sum(chunk["duration"] for chunk in chunk_files)
        
        return {
            "segments": chunk_files,
            "merged_audio_path": merged_path,
            "total_duration": total_duration
        }
    
    async def _remove_silence(self, audio_path: str, threshold_db: float = -40.0) -> str:
        """
        Remove silence from audio file
        
        Args:
            audio_path: Input audio path
            threshold_db: Silence threshold in dB
            
        Returns:
            Path to audio with silence removed
        """
        try:
            output_path = os.path.join(self.temp_dir, f"desilenced_{uuid.uuid4().hex}.wav")
            
            # Use ffmpeg to remove silence
            cmd = [
                "ffmpeg", "-loglevel", "error", "-y",
                "-i", audio_path,
                "-af", f"silenceremove=stop_periods=-1:stop_duration=0.5:stop_threshold={threshold_db}dB",
                output_path
            ]
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            _, stderr = await process.communicate()
            
            if process.returncode != 0:
                logger.error(f"Error removing silence: {stderr.decode()}")
                return audio_path
            
            return output_path
            
        except Exception as e:
            logger.error(f"Error in silence removal: {str(e)}")
            return audio_path
    
    async def _ensure_audio_format(
        self,
        audio_path: str,
        sample_rate: int = 24000,
        channels: int = 1,
        format: str = "wav"
    ) -> str:
        """
        Ensure audio is in the correct format
        
        Args:
            audio_path: Input audio path
            sample_rate: Target sample rate
            channels: Number of channels (1=mono, 2=stereo)
            format: Output format
            
        Returns:
            Path to formatted audio
        """
        try:
            # Check current format
            info = sf.info(audio_path)
            
            # If already correct format, return as-is
            if (info.samplerate == sample_rate and 
                info.channels == channels and 
                audio_path.endswith(f".{format}")):
                return audio_path
            
            # Convert to target format
            output_path = os.path.join(self.temp_dir, f"formatted_{uuid.uuid4().hex}.{format}")
            
            cmd = [
                "ffmpeg", "-loglevel", "error", "-y",
                "-i", audio_path,
                "-ar", str(sample_rate),
                "-ac", str(channels),
                output_path
            ]
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            _, stderr = await process.communicate()
            
            if process.returncode != 0:
                logger.error(f"Error converting audio format: {stderr.decode()}")
                return audio_path
            
            return output_path
            
        except Exception as e:
            logger.error(f"Error ensuring audio format: {str(e)}")
            return audio_path
    
    def _find_speaker_for_segment(
        self,
        whisper_segment: Dict[str, Any],
        diarization_segments: List[Dict[str, Any]]
    ) -> str:
        """
        Find the speaker for a Whisper segment based on diarization results
        
        Args:
            whisper_segment: Whisper transcription segment
            diarization_segments: Speaker diarization segments
            
        Returns:
            Speaker label
        """
        segment_start = whisper_segment["start"]
        segment_end = whisper_segment["end"]
        segment_mid = (segment_start + segment_end) / 2
        
        # Find speaker with most overlap
        best_speaker = "SPEAKER_UNKNOWN"
        best_overlap = 0
        
        for diar_seg in diarization_segments:
            # Calculate overlap
            overlap_start = max(segment_start, diar_seg["start"])
            overlap_end = min(segment_end, diar_seg["end"])
            overlap_duration = max(0, overlap_end - overlap_start)
            
            # Also check if segment midpoint falls within diarization segment
            midpoint_match = diar_seg["start"] <= segment_mid <= diar_seg["end"]
            
            # Prefer midpoint match, then overlap duration
            if midpoint_match and overlap_duration > 0:
                return diar_seg["speaker"]
            elif overlap_duration > best_overlap:
                best_overlap = overlap_duration
                best_speaker = diar_seg["speaker"]
        
        return best_speaker
    
    def _group_segments_by_speaker(
        self,
        segments: List[Dict[str, Any]]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Group transcription segments by speaker
        
        Args:
            segments: List of segments with speaker information
            
        Returns:
            Dictionary mapping speaker labels to their segments
        """
        grouped = {}
        
        for segment in segments:
            speaker = segment.get("speaker", "SPEAKER_UNKNOWN")
            if speaker not in grouped:
                grouped[speaker] = []
            grouped[speaker].append(segment)
        
        return grouped
    
    def cleanup(self):
        """Clean up temporary files"""
        try:
            import shutil
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                logger.info(f"Cleaned up temp directory: {self.temp_dir}")
        except Exception as e:
            logger.warning(f"Error cleaning up temp files: {str(e)}")
    
    def __del__(self):
        """Cleanup on deletion"""
        self.cleanup()


# Global instance
audio_preparation_service = AudioPreparationService()


================================================
FILE: audio_processor.py
================================================
"""
Audio Processing Service

Handles audio extraction from video files, format conversion,
and validation for voice cloning operations.
"""

import os
import tempfile
import asyncio
import subprocess
import logging
from pathlib import Path
from typing import Optional, Tuple
import aiofiles

logger = logging.getLogger(__name__)


class AudioProcessor:
    """Service for processing audio files for voice cloning."""
    
    # Supported input formats
    SUPPORTED_VIDEO_FORMATS = {'.mp4', '.mov', '.avi', '.webm', '.mkv', '.flv'}
    SUPPORTED_AUDIO_FORMATS = {'.mp3', '.wav', '.m4a', '.aac', '.ogg', '.flac'}
    
    # Output settings
    OUTPUT_FORMAT = 'mp3'
    OUTPUT_BITRATE = '192k'
    OUTPUT_SAMPLE_RATE = 24000  # Chatterbox uses 24kHz
    
    # Validation limits
    MAX_DURATION_SECONDS = 300  # 5 minutes max
    MIN_DURATION_SECONDS = 2    # 2 seconds minimum
    MAX_FILE_SIZE_MB = 50      # 50MB max file size
    
    async def process_file_for_cloning(self, file_path: str) -> str:
        """
        Process an audio or video file for voice cloning.
        
        Args:
            file_path: Path to the input file
            
        Returns:
            Path to the processed MP3 file
            
        Raises:
            ValueError: If file is invalid or processing fails
        """
        try:
            # Validate file exists and size
            if not os.path.exists(file_path):
                raise ValueError(f"File not found: {file_path}")
            
            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
            if file_size_mb > self.MAX_FILE_SIZE_MB:
                raise ValueError(f"File too large: {file_size_mb:.1f}MB (max {self.MAX_FILE_SIZE_MB}MB)")
            
            # Get file extension
            file_ext = Path(file_path).suffix.lower()
            
            # Check if it's already MP3
            if file_ext == '.mp3':
                # Validate duration and return as-is if valid
                duration = await self._get_audio_duration(file_path)
                self._validate_duration(duration)
                logger.info(f"File is already MP3 with duration {duration}s, using as-is")
                return file_path
            
            # Determine if video or audio
            is_video = file_ext in self.SUPPORTED_VIDEO_FORMATS
            is_audio = file_ext in self.SUPPORTED_AUDIO_FORMATS
            
            if not is_video and not is_audio:
                raise ValueError(f"Unsupported file format: {file_ext}")
            
            # Create output file
            output_file = tempfile.NamedTemporaryFile(
                suffix=f'.{self.OUTPUT_FORMAT}',
                delete=False
            )
            output_path = output_file.name
            output_file.close()
            
            # Extract/convert audio
            if is_video:
                logger.info(f"Extracting audio from video file: {file_path}")
                await self._extract_audio_from_video(file_path, output_path)
            else:
                logger.info(f"Converting audio file to MP3: {file_path}")
                await self._convert_audio_format(file_path, output_path)
            
            # Validate output
            duration = await self._get_audio_duration(output_path)
            self._validate_duration(duration)
            
            logger.info(f"Successfully processed audio: {output_path} (duration: {duration}s)")
            return output_path
            
        except Exception as e:
            logger.error(f"Error processing file for cloning: {str(e)}")
            raise
    
    async def _extract_audio_from_video(self, input_path: str, output_path: str):
        """Extract audio from video file using ffmpeg."""
        cmd = [
            'ffmpeg',
            '-i', input_path,
            '-vn',  # No video
            '-acodec', 'libmp3lame',  # MP3 codec
            '-ab', self.OUTPUT_BITRATE,  # Bitrate
            '-ar', str(self.OUTPUT_SAMPLE_RATE),  # Sample rate
            '-ac', '1',  # Mono audio (better for voice cloning)
            '-y',  # Overwrite output
            output_path
        ]
        
        await self._run_ffmpeg_command(cmd)
    
    async def _convert_audio_format(self, input_path: str, output_path: str):
        """Convert audio file to MP3 format."""
        cmd = [
            'ffmpeg',
            '-i', input_path,
            '-acodec', 'libmp3lame',
            '-ab', self.OUTPUT_BITRATE,
            '-ar', str(self.OUTPUT_SAMPLE_RATE),
            '-ac', '1',  # Mono audio
            '-y',
            output_path
        ]
        
        await self._run_ffmpeg_command(cmd)
    
    async def _get_audio_duration(self, file_path: str) -> float:
        """Get duration of audio file in seconds."""
        cmd = [
            'ffprobe',
            '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            file_path
        ]
        
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                raise ValueError(f"ffprobe failed: {stderr.decode()}")
            
            duration = float(stdout.decode().strip())
            return duration
            
        except Exception as e:
            logger.error(f"Error getting audio duration: {str(e)}")
            raise ValueError(f"Could not determine audio duration: {str(e)}")
    
    async def _run_ffmpeg_command(self, cmd: list):
        """Run ffmpeg command asynchronously."""
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                error_msg = stderr.decode()
                logger.error(f"ffmpeg error: {error_msg}")
                raise ValueError(f"Audio processing failed: {error_msg}")
            
            logger.debug(f"ffmpeg output: {stdout.decode()}")
            
        except Exception as e:
            logger.error(f"Error running ffmpeg: {str(e)}")
            raise ValueError(f"Audio processing failed: {str(e)}")
    
    def _validate_duration(self, duration: float):
        """Validate audio duration is within acceptable limits."""
        if duration < self.MIN_DURATION_SECONDS:
            raise ValueError(
                f"Audio too short: {duration:.1f}s (minimum {self.MIN_DURATION_SECONDS}s)"
            )
        
        if duration > self.MAX_DURATION_SECONDS:
            raise ValueError(
                f"Audio too long: {duration:.1f}s (maximum {self.MAX_DURATION_SECONDS}s)"
            )
    
    async def cleanup_temp_file(self, file_path: str):
        """Clean up temporary file if it exists."""
        try:
            if file_path and os.path.exists(file_path):
                os.unlink(file_path)
                logger.debug(f"Cleaned up temporary file: {file_path}")
        except Exception as e:
            logger.warning(f"Failed to clean up temp file {file_path}: {str(e)}")


# Global instance
audio_processor = AudioProcessor()


================================================
FILE: audio_separation_service.py
================================================
# FILE: src/services/audio_separation_service.py

"""
Audio Separation Service

Provides audio source separation using Demucs and speaker diarization using pyannote.audio
to improve transcription quality by isolating vocals and identifying multiple speakers.
"""

import os
import logging
import tempfile
import asyncio
import subprocess
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
import uuid
import json
import torch
import torchaudio
from pyannote.audio import Pipeline

logger = logging.getLogger(__name__)


class AudioSeparationService:
    """Service for audio source separation and speaker diarization"""
    
    def __init__(self):
        """Initialize the audio separation service"""
        self.demucs_model = None
        self.diarization_pipeline = None
        self.temp_dir = tempfile.mkdtemp(prefix="audio_sep_")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Get HuggingFace token for pyannote - load dynamically when needed
        self._hf_token = None
        
        logger.info(f"Audio Separation Service initialized - Device: {self.device}")
    
    @property
    def hf_token(self):
        """Lazy load the HuggingFace token when needed"""
        if self._hf_token is None:
            self._hf_token = os.getenv("HUGGINGFACE_TOKEN", os.getenv("HF_TOKEN"))
            if not self._hf_token:
                logger.warning("No HuggingFace token found. Speaker diarization will be limited.")
        return self._hf_token
            
    def _load_diarization_pipeline(self):
        """Loads the pyannote pipeline if not already loaded."""
        if self.diarization_pipeline is None:
            logger.info("Loading pyannote speaker diarization pipeline...")
            self.diarization_pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=self.hf_token
            ).to(torch.device(self.device))
            logger.info("Diarization pipeline loaded.")

    async def extract_vocals(
        self,
        audio_path: str,
        model_name: str = "htdemucs"
    ) -> str:
        """
        Extract vocals from audio using Demucs
        
        Args:
            audio_path: Path to input audio file
            model_name: Demucs model to use (htdemucs, htdemucs_ft, etc.)
            
        Returns:
            Path to extracted vocals audio file
        """
        try:
            logger.info(f"Extracting vocals from: {audio_path}")
            
            # Create output directory for separated tracks
            output_dir = os.path.join(self.temp_dir, f"separated_{uuid.uuid4().hex}")
            os.makedirs(output_dir, exist_ok=True)
            
            cmd = [
                "python", "-m", "demucs",
                "--two-stems=vocals",
                "-n", model_name,
                "-o", output_dir,
                "--device", self.device,
                audio_path
            ]
            
            if audio_path.lower().endswith('.mp3'):
                cmd.extend(["--mp3", "--mp3-bitrate", "320"])
            
            logger.info(f"Running Demucs: {' '.join(cmd)}")
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                error_msg = stderr.decode() if stderr else "Unknown error"
                logger.error(f"Demucs failed: {error_msg}")
                raise RuntimeError(f"Demucs separation failed: {error_msg}")
            
            vocals_path = None
            for root, _, files in os.walk(output_dir):
                for file in files:
                    if file.startswith("vocals"):
                        vocals_path = os.path.join(root, file)
                        break
                if vocals_path:
                    break
            
            if not vocals_path or not os.path.exists(vocals_path):
                raise FileNotFoundError(f"Vocals file not found in {output_dir}")
            
            logger.info(f"Successfully extracted vocals: {vocals_path}")
            return vocals_path
            
        except Exception as e:
            logger.error(f"Error extracting vocals: {str(e)}")
            raise

    async def diarize_from_waveform(
        self,
        waveform: torch.Tensor,
        sample_rate: int,
        min_speakers: int = 1,
        max_speakers: int = 10,
        min_duration: float = 1.0
    ) -> Dict[str, Any]:
        """
        Perform speaker diarization on an in-memory waveform.
        Expects a 16kHz mono waveform.
        """
        if sample_rate != 16000:
            raise ValueError(f"Diarization expects 16000 Hz, but received {sample_rate} Hz.")
            
        try:
            logger.info(f"Performing speaker diarization on waveform of shape {waveform.shape}")

            if not self.hf_token:
                logger.warning("No HuggingFace token available. Returning single speaker. Set HUGGINGFACE_TOKEN or HF_TOKEN environment variable.")
                duration = waveform.shape[1] / sample_rate
                return {
                    "speakers": 1,
                    "segments": [{"speaker": "SPEAKER_00", "start": 0.0, "end": float(duration), "confidence": 1.0}]
                }

            self._load_diarization_pipeline()

            logger.info("Running speaker diarization...")
            audio_data_dict = {'waveform': waveform, 'sample_rate': sample_rate}
            diarization = self.diarization_pipeline(
                audio_data_dict,
                min_speakers=min_speakers,
                max_speakers=max_speakers
            )

            segments, speakers = [], set()
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                if turn.duration < min_duration:
                    continue
                segments.append({
                    "speaker": speaker, "start": float(turn.start), "end": float(turn.end),
                    "duration": float(turn.duration), "confidence": 0.95
                })
                speakers.add(speaker)
            
            segments.sort(key=lambda x: x["start"])
            
            result = {
                "speakers": len(speakers), "speaker_labels": sorted(list(speakers)),
                "segments": segments, "total_segments": len(segments)
            }
            
            logger.info(f"Diarization complete: {len(speakers)} speakers, {len(segments)} segments")
            return result

        except Exception as e:
            logger.error(f"Error in speaker diarization from waveform: {str(e)}", exc_info=True)
            duration = waveform.shape[1] / sample_rate
            return {
                "speakers": 1, "segments": [{"speaker": "SPEAKER_00", "start": 0.0, "end": float(duration), "confidence": 0.5}]
            }

    async def separate_and_diarize(
        self,
        audio_path: str,
        config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Combined audio separation and speaker diarization
        
        Args:
            audio_path: Path to input audio file
            config: Configuration dict with options:
                - extract_vocals: Whether to extract vocals (default: True)
                - min_speakers: Minimum speakers to detect
                - max_speakers: Maximum speakers to detect
                - min_duration: Minimum speaker segment duration
        
        Returns:
            Dict containing vocals_path, diarization results, and metadata
        """
        try:
            # Default configuration
            default_config = {
                "extract_vocals": True,
                "min_speakers": 1,
                "max_speakers": 10,
                "min_duration": 1.0
            }
            
            if config:
                default_config.update(config)
            config = default_config
            
            result = {
                "vocals_path": None,
                "diarization": None,
                "metadata": {}
            }
            
            # Step 1: Extract vocals if requested
            if config.get("extract_vocals", True):
                logger.info("Extracting vocals for cleaner processing")
                vocals_path = await self.extract_vocals(audio_path)
                result["vocals_path"] = vocals_path
                result["metadata"]["vocals_extracted"] = True
                audio_for_diarization = vocals_path
            else:
                audio_for_diarization = audio_path
            
            # Step 2: Perform speaker diarization
            logger.info("Performing speaker diarization")
            diarization_result = await self.diarize_speakers(
                audio_for_diarization,
                min_speakers=config.get("min_speakers", 1),
                max_speakers=config.get("max_speakers", 10),
                min_duration=config.get("min_duration", 1.0)
            )
            
            result["diarization"] = diarization_result
            result["metadata"]["speakers_identified"] = diarization_result.get("speakers", 1)
            
            return result
            
        except Exception as e:
            logger.error(f"Error in separate_and_diarize: {str(e)}")
            raise

    # The original diarize_speakers can now be a simple wrapper for backwards compatibility
    async def diarize_speakers(
        self,
        audio_path: str,
        min_speakers: int = 1,
        max_speakers: int = 10,
        min_duration: float = 1.0
    ) -> Dict[str, Any]:
        """
        Perform speaker diarization on audio from a file path.
        """
        waveform, sample_rate = torchaudio.load(audio_path)
        # Resample to 16kHz for the pipeline
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
            waveform = resampler(waveform)
        
        # Ensure mono
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
            
        return await self.diarize_from_waveform(waveform, 16000, min_speakers, max_speakers, min_duration)

    def cleanup(self):
        """Clean up temporary files"""
        try:
            import shutil
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                logger.info(f"Cleaned up temp directory: {self.temp_dir}")
        except Exception as e:
            logger.warning(f"Error cleaning up temp files: {str(e)}")

    def __del__(self):
        """Cleanup on deletion"""
        self.cleanup()

# Global instance
audio_separation_service = AudioSeparationService()



================================================
FILE: batch_embedding_service.py
================================================
"""
Batch Embedding Service for Efficient Processing

This service provides batch processing capabilities for both Jina AI and Gemini embeddings,
with optimizations for rate limiting, memory management, and export preparation.
"""

import asyncio
import logging
import time
import json
import numpy as np
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import pickle
from pathlib import Path

from .jina.embeddings_service import JinaEmbeddingsService
from .jina.config import JinaConfig
from .gemini.embeddings_service import GeminiEmbeddingsService
from .gemini.config import GeminiConfig

logger = logging.getLogger(__name__)

class EmbeddingProvider(Enum):
    """Supported embedding providers"""
    JINA = "jina"
    GEMINI = "gemini"

class ExportFormat(Enum):
    """Supported export formats"""
    JSON = "json"
    NUMPY = "numpy"
    PICKLE = "pickle"
    CSV = "csv"

@dataclass
class BatchConfig:
    """Configuration for batch processing"""
    provider: EmbeddingProvider
    batch_size: int = 50
    max_concurrent_batches: int = 3
    rate_limit_delay: float = 1.0
    retry_attempts: int = 3
    retry_delay: float = 2.0
    memory_limit_mb: int = 1024
    enable_progress_tracking: bool = True

@dataclass
class BatchResult:
    """Result of batch processing operation"""
    success: bool
    total_items: int
    processed_items: int
    failed_items: int
    embeddings: List[List[float]]
    metadata: Dict[str, Any]
    processing_time: float
    errors: List[str]

@dataclass
class ExportData:
    """Data structure for embedding exports"""
    embeddings: List[List[float]]
    texts: List[str]
    metadata: Dict[str, Any]
    provider: str
    model: str
    dimensions: int
    timestamp: float
    total_items: int

class BatchEmbeddingService:
    """
    High-performance batch embedding service with multi-provider support
    """
    
    def __init__(
        self,
        jina_config: Optional[JinaConfig] = None,
        gemini_config: Optional[GeminiConfig] = None
    ):
        """
        Initialize batch embedding service
        
        Args:
            jina_config: Optional Jina configuration
            gemini_config: Optional Gemini configuration
        """
        logger.info("Initializing BatchEmbeddingService...")
        
        # Initialize services
        self.jina_service = JinaEmbeddingsService(jina_config) if jina_config or self._has_jina_config() else None
        self.gemini_service = GeminiEmbeddingsService(gemini_config) if gemini_config or self._has_gemini_config() else None
        
        # Rate limiting state
        self._rate_limits = {
            EmbeddingProvider.JINA: {"last_request": 0, "request_count": 0},
            EmbeddingProvider.GEMINI: {"last_request": 0, "request_count": 0}
        }
        
        # Progress tracking
        self._progress_callbacks = []
        
        logger.info(f"BatchEmbeddingService initialized - Jina: {bool(self.jina_service)}, Gemini: {bool(self.gemini_service)}")
    
    def _has_jina_config(self) -> bool:
        """Check if Jina configuration is available"""
        try:
            config = JinaConfig()
            return bool(config.api_key)
        except:
            return False
    
    def _has_gemini_config(self) -> bool:
        """Check if Gemini configuration is available"""
        try:
            config = GeminiConfig()
            return bool(config.api_key)
        except:
            return False
    
    def add_progress_callback(self, callback):
        """Add progress tracking callback"""
        self._progress_callbacks.append(callback)
    
    def _notify_progress(self, processed: int, total: int, provider: str):
        """Notify progress callbacks"""
        for callback in self._progress_callbacks:
            try:
                callback(processed, total, provider)
            except Exception as e:
                logger.warning(f"Progress callback error: {e}")
    
    async def process_jina_batch(
        self,
        texts: List[str],
        config: Optional[BatchConfig] = None
    ) -> BatchResult:
        """
        Process texts using Jina AI embeddings with optimized batching
        
        Args:
            texts: List of texts to embed
            config: Batch processing configuration
            
        Returns:
            BatchResult with embeddings and metadata
        """
        if not self.jina_service:
            raise ValueError("Jina service not available - check API configuration")
        
        config = config or BatchConfig(provider=EmbeddingProvider.JINA)
        start_time = time.time()
        
        logger.info(f"Starting Jina batch processing for {len(texts)} texts")
        
        try:
            # Optimize batch size based on content
            optimized_batch_size = self.optimize_batch_size(texts, config.batch_size, EmbeddingProvider.JINA)
            
            # Process in batches with rate limiting
            all_embeddings = []
            processed_count = 0
            errors = []
            
            for i in range(0, len(texts), optimized_batch_size):
                batch = texts[i:i + optimized_batch_size]
                
                # Handle rate limiting
                await self.handle_rate_limits(EmbeddingProvider.JINA, config.rate_limit_delay)
                
                try:
                    # Process batch with retry logic
                    batch_embeddings = await self._process_batch_with_retry(
                        self.jina_service.embed_documents,
                        batch,
                        config.retry_attempts,
                        config.retry_delay
                    )
                    
                    all_embeddings.extend(batch_embeddings)
                    processed_count += len(batch)
                    
                    # Update progress
                    if config.enable_progress_tracking:
                        self._notify_progress(processed_count, len(texts), "jina")
                    
                    logger.debug(f"Processed Jina batch {i//optimized_batch_size + 1}, total: {processed_count}/{len(texts)}")
                    
                except Exception as e:
                    error_msg = f"Batch {i//optimized_batch_size + 1} failed: {str(e)}"
                    errors.append(error_msg)
                    logger.error(error_msg)
                    
                    # Add empty embeddings for failed batch
                    all_embeddings.extend([[]] * len(batch))
            
            processing_time = time.time() - start_time
            
            result = BatchResult(
                success=len(errors) == 0,
                total_items=len(texts),
                processed_items=processed_count,
                failed_items=len(texts) - processed_count,
                embeddings=all_embeddings,
                metadata={
                    "provider": "jina",
                    "model": "jina-embeddings-v4",
                    "batch_size": optimized_batch_size,
                    "dimensions": 1024
                },
                processing_time=processing_time,
                errors=errors
            )
            
            logger.info(f"Jina batch processing completed in {processing_time:.2f}s - Success: {result.success}")
            return result
            
        except Exception as e:
            logger.error(f"Jina batch processing failed: {e}")
            return BatchResult(
                success=False,
                total_items=len(texts),
                processed_items=0,
                failed_items=len(texts),
                embeddings=[],
                metadata={"provider": "jina", "error": str(e)},
                processing_time=time.time() - start_time,
                errors=[str(e)]
            )
    
    async def process_gemini_batch(
        self,
        texts: List[str],
        task_type: str = "RETRIEVAL_DOCUMENT",
        output_dimensionality: Optional[int] = None,
        config: Optional[BatchConfig] = None
    ) -> BatchResult:
        """
        Process texts using Gemini embeddings with optimized batching
        
        Args:
            texts: List of texts to embed
            task_type: Gemini task type for optimization
            output_dimensionality: Optional dimension truncation
            config: Batch processing configuration
            
        Returns:
            BatchResult with embeddings and metadata
        """
        if not self.gemini_service:
            raise ValueError("Gemini service not available - check API configuration")
        
        config = config or BatchConfig(provider=EmbeddingProvider.GEMINI)
        start_time = time.time()
        
        logger.info(f"Starting Gemini batch processing for {len(texts)} texts")
        
        try:
            # Optimize batch size based on content and rate limits
            optimized_batch_size = self.optimize_batch_size(texts, config.batch_size, EmbeddingProvider.GEMINI)
            
            # Process in batches with rate limiting
            all_embeddings = []
            processed_count = 0
            errors = []
            
            for i in range(0, len(texts), optimized_batch_size):
                batch = texts[i:i + optimized_batch_size]
                
                # Handle rate limiting (Gemini has stricter limits)
                await self.handle_rate_limits(EmbeddingProvider.GEMINI, config.rate_limit_delay)
                
                try:
                    # Process batch with retry logic
                    if task_type == "RETRIEVAL_DOCUMENT":
                        batch_embeddings = await self._process_batch_with_retry(
                            lambda x: self.gemini_service.embed_documents(x, output_dimensionality),
                            batch,
                            config.retry_attempts,
                            config.retry_delay
                        )
                    elif task_type == "RETRIEVAL_QUERY":
                        batch_embeddings = await self._process_batch_with_retry(
                            lambda x: self.gemini_service.embed_queries(x, output_dimensionality),
                            batch,
                            config.retry_attempts,
                            config.retry_delay
                        )
                    elif task_type == "CLASSIFICATION":
                        batch_embeddings = await self._process_batch_with_retry(
                            lambda x: self.gemini_service.embed_for_classification(x, output_dimensionality),
                            batch,
                            config.retry_attempts,
                            config.retry_delay
                        )
                    elif task_type == "CLUSTERING":
                        batch_embeddings = await self._process_batch_with_retry(
                            lambda x: self.gemini_service.embed_for_clustering(x, output_dimensionality),
                            batch,
                            config.retry_attempts,
                            config.retry_delay
                        )
                    else:
                        # Default to document embedding
                        batch_embeddings = await self._process_batch_with_retry(
                            lambda x: self.gemini_service.embed_documents(x, output_dimensionality),
                            batch,
                            config.retry_attempts,
                            config.retry_delay
                        )
                    
                    all_embeddings.extend(batch_embeddings)
                    processed_count += len(batch)
                    
                    # Update progress
                    if config.enable_progress_tracking:
                        self._notify_progress(processed_count, len(texts), "gemini")
                    
                    logger.debug(f"Processed Gemini batch {i//optimized_batch_size + 1}, total: {processed_count}/{len(texts)}")
                    
                except Exception as e:
                    error_msg = f"Batch {i//optimized_batch_size + 1} failed: {str(e)}"
                    errors.append(error_msg)
                    logger.error(error_msg)
                    
                    # Add empty embeddings for failed batch
                    all_embeddings.extend([[]] * len(batch))
            
            processing_time = time.time() - start_time
            
            result = BatchResult(
                success=len(errors) == 0,
                total_items=len(texts),
                processed_items=processed_count,
                failed_items=len(texts) - processed_count,
                embeddings=all_embeddings,
                metadata={
                    "provider": "gemini",
                    "model": "gemini-embedding-exp-03-07",
                    "batch_size": optimized_batch_size,
                    "task_type": task_type,
                    "dimensions": output_dimensionality or 3072
                },
                processing_time=processing_time,
                errors=errors
            )
            
            logger.info(f"Gemini batch processing completed in {processing_time:.2f}s - Success: {result.success}")
            return result
            
        except Exception as e:
            logger.error(f"Gemini batch processing failed: {e}")
            return BatchResult(
                success=False,
                total_items=len(texts),
                processed_items=0,
                failed_items=len(texts),
                embeddings=[],
                metadata={"provider": "gemini", "error": str(e)},
                processing_time=time.time() - start_time,
                errors=[str(e)]
            )
    
    async def _process_batch_with_retry(
        self,
        process_func,
        batch: List[str],
        max_retries: int,
        retry_delay: float
    ) -> List[List[float]]:
        """
        Process a batch with retry logic
        
        Args:
            process_func: Function to process the batch
            batch: Batch of texts
            max_retries: Maximum retry attempts
            retry_delay: Delay between retries
            
        Returns:
            List of embeddings
        """
        for attempt in range(max_retries + 1):
            try:
                return await process_func(batch)
            except Exception as e:
                if attempt == max_retries:
                    raise e
                
                logger.warning(f"Batch processing attempt {attempt + 1} failed: {e}, retrying in {retry_delay}s")
                await asyncio.sleep(retry_delay * (attempt + 1))  # Exponential backoff
        
        return []
    
    def optimize_batch_size(
        self,
        texts: List[str],
        base_batch_size: int,
        provider: EmbeddingProvider
    ) -> int:
        """
        Optimize batch size based on content characteristics and provider limits
        
        Args:
            texts: List of texts to analyze
            base_batch_size: Base batch size
            provider: Embedding provider
            
        Returns:
            Optimized batch size
        """
        if not texts:
            return base_batch_size
        
        # Calculate average text length
        avg_length = sum(len(text) for text in texts) / len(texts)
        
        # Provider-specific optimizations
        if provider == EmbeddingProvider.JINA:
            # Jina has 8K token limit, batch size based on text length
            if avg_length > 2000:
                return min(base_batch_size // 2, 25)  # Reduce for long texts
            elif avg_length < 500:
                return min(base_batch_size * 2, 200)  # Increase for short texts
            else:
                return base_batch_size
                
        elif provider == EmbeddingProvider.GEMINI:
            # Gemini has stricter rate limits but 8K context
            if avg_length > 2000:
                return min(base_batch_size // 3, 20)  # More conservative for long texts
            elif avg_length < 500:
                return min(base_batch_size, 50)  # Conservative increase
            else:
                return min(base_batch_size, 30)  # Default conservative
        
        return base_batch_size
    
    async def handle_rate_limits(self, provider: EmbeddingProvider, delay: float):
        """
        Handle rate limiting for embedding providers
        
        Args:
            provider: Embedding provider
            delay: Base delay between requests
        """
        current_time = time.time()
        rate_info = self._rate_limits[provider]
        
        # Calculate required delay based on rate limits
        if provider == EmbeddingProvider.JINA:
            # Jina: 600 requests/minute for free tier
            min_interval = 60.0 / 600  # ~0.1 seconds between requests
        elif provider == EmbeddingProvider.GEMINI:
            # Gemini: 100 requests/minute for experimental
            min_interval = 60.0 / 100  # 0.6 seconds between requests
        else:
            min_interval = delay
        
        # Ensure minimum interval
        time_since_last = current_time - rate_info["last_request"]
        if time_since_last < min_interval:
            sleep_time = min_interval - time_since_last
            logger.debug(f"Rate limiting {provider.value}: sleeping {sleep_time:.2f}s")
            await asyncio.sleep(sleep_time)
        
        # Update rate limiting state
        rate_info["last_request"] = time.time()
        rate_info["request_count"] += 1
    
    async def process_mixed_batch(
        self,
        texts: List[str],
        preferred_provider: EmbeddingProvider = EmbeddingProvider.JINA,
        fallback_provider: Optional[EmbeddingProvider] = None,
        config: Optional[BatchConfig] = None
    ) -> BatchResult:
        """
        Process texts with primary provider and fallback support
        
        Args:
            texts: List of texts to embed
            preferred_provider: Primary embedding provider
            fallback_provider: Fallback provider if primary fails
            config: Batch processing configuration
            
        Returns:
            BatchResult with embeddings and metadata
        """
        config = config or BatchConfig(provider=preferred_provider)
        
        # Try primary provider
        try:
            if preferred_provider == EmbeddingProvider.JINA:
                return await self.process_jina_batch(texts, config)
            elif preferred_provider == EmbeddingProvider.GEMINI:
                return await self.process_gemini_batch(texts, config=config)
        except Exception as e:
            logger.warning(f"Primary provider {preferred_provider.value} failed: {e}")
            
            # Try fallback if available
            if fallback_provider and fallback_provider != preferred_provider:
                try:
                    logger.info(f"Attempting fallback to {fallback_provider.value}")
                    config.provider = fallback_provider
                    
                    if fallback_provider == EmbeddingProvider.JINA:
                        return await self.process_jina_batch(texts, config)
                    elif fallback_provider == EmbeddingProvider.GEMINI:
                        return await self.process_gemini_batch(texts, config=config)
                except Exception as fallback_error:
                    logger.error(f"Fallback provider {fallback_provider.value} also failed: {fallback_error}")
            
            # Return failure result
            return BatchResult(
                success=False,
                total_items=len(texts),
                processed_items=0,
                failed_items=len(texts),
                embeddings=[],
                metadata={"provider": preferred_provider.value, "error": str(e)},
                processing_time=0.0,
                errors=[str(e)]
            )
    
    def prepare_export_data(
        self,
        result: BatchResult,
        texts: List[str],
        additional_metadata: Optional[Dict[str, Any]] = None
    ) -> ExportData:
        """
        Prepare data for export in optimized format
        
        Args:
            result: Batch processing result
            texts: Original texts
            additional_metadata: Additional metadata to include
            
        Returns:
            ExportData ready for serialization
        """
        metadata = result.metadata.copy()
        if additional_metadata:
            metadata.update(additional_metadata)
        
        return ExportData(
            embeddings=result.embeddings,
            texts=texts,
            metadata=metadata,
            provider=metadata.get("provider", "unknown"),
            model=metadata.get("model", "unknown"),
            dimensions=metadata.get("dimensions", 0),
            timestamp=time.time(),
            total_items=result.total_items
        )
    
    async def export_embeddings(
        self,
        export_data: ExportData,
        output_path: Union[str, Path],
        format: ExportFormat = ExportFormat.JSON,
        compress: bool = False
    ) -> bool:
        """
        Export embeddings in specified format with optimization
        
        Args:
            export_data: Data to export
            output_path: Output file path
            format: Export format
            compress: Whether to compress the output
            
        Returns:
            True if export successful, False otherwise
        """
        try:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"Exporting {export_data.total_items} embeddings to {output_path}")
            
            if format == ExportFormat.JSON:
                await self._export_json(export_data, output_path, compress)
            elif format == ExportFormat.NUMPY:
                await self._export_numpy(export_data, output_path, compress)
            elif format == ExportFormat.PICKLE:
                await self._export_pickle(export_data, output_path, compress)
            elif format == ExportFormat.CSV:
                await self._export_csv(export_data, output_path, compress)
            else:
                raise ValueError(f"Unsupported export format: {format}")
            
            logger.info(f"Successfully exported embeddings to {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"Export failed: {e}")
            return False
    
    async def _export_json(self, data: ExportData, path: Path, compress: bool):
        """Export as JSON format"""
        export_dict = {
            "metadata": data.metadata,
            "provider": data.provider,
            "model": data.model,
            "dimensions": data.dimensions,
            "timestamp": data.timestamp,
            "total_items": data.total_items,
            "data": [
                {
                    "text": text,
                    "embedding": embedding
                }
                for text, embedding in zip(data.texts, data.embeddings)
            ]
        }
        
        if compress:
            import gzip
            with gzip.open(f"{path}.gz", "wt", encoding="utf-8") as f:
                json.dump(export_dict, f, indent=2)
        else:
            with open(path, "w", encoding="utf-8") as f:
                json.dump(export_dict, f, indent=2)
    
    async def _export_numpy(self, data: ExportData, path: Path, compress: bool):
        """Export as NumPy format"""
        embeddings_array = np.array(data.embeddings)
        metadata_dict = asdict(data)
        metadata_dict.pop("embeddings")  # Remove embeddings from metadata
        
        if compress:
            np.savez_compressed(
                path,
                embeddings=embeddings_array,
                texts=np.array(data.texts),
                metadata=np.array([metadata_dict])
            )
        else:
            np.savez(
                path,
                embeddings=embeddings_array,
                texts=np.array(data.texts),
                metadata=np.array([metadata_dict])
            )
    
    async def _export_pickle(self, data: ExportData, path: Path, compress: bool):
        """Export as Pickle format"""
        if compress:
            import gzip
            with gzip.open(f"{path}.gz", "wb") as f:
                pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
        else:
            with open(path, "wb") as f:
                pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    async def _export_csv(self, data: ExportData, path: Path, compress: bool):
        """Export as CSV format (flattened embeddings)"""
        import csv
        
        # Determine if we need to open compressed or regular file
        if compress:
            import gzip
            file_context = gzip.open(f"{path}.gz", "wt", encoding="utf-8", newline="")
        else:
            file_context = open(path, "w", encoding="utf-8", newline="")
        
        with file_context as f:
            # Create header
            max_dims = max(len(emb) for emb in data.embeddings) if data.embeddings else 0
            header = ["text"] + [f"dim_{i}" for i in range(max_dims)]
            
            writer = csv.writer(f)
            writer.writerow(header)
            
            # Write data
            for text, embedding in zip(data.texts, data.embeddings):
                # Pad embedding to max dimensions if needed
                padded_embedding = embedding + [0.0] * (max_dims - len(embedding))
                writer.writerow([text] + padded_embedding)
    
    def get_memory_usage(self) -> Dict[str, float]:
        """
        Get current memory usage statistics
        
        Returns:
            Dictionary with memory usage in MB
        """
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        
        return {
            "rss_mb": memory_info.rss / 1024 / 1024,
            "vms_mb": memory_info.vms / 1024 / 1024,
            "percent": process.memory_percent()
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """
        Perform health check on all available services
        
        Returns:
            Health status for each provider
        """
        health_status = {
            "batch_service": True,
            "providers": {},
            "memory_usage": self.get_memory_usage()
        }
        
        # Check Jina service
        if self.jina_service:
            try:
                jina_healthy = await self.jina_service.test_connection()
                health_status["providers"]["jina"] = {
                    "available": True,
                    "healthy": jina_healthy
                }
            except Exception as e:
                health_status["providers"]["jina"] = {
                    "available": True,
                    "healthy": False,
                    "error": str(e)
                }
        else:
            health_status["providers"]["jina"] = {
                "available": False,
                "reason": "No API key configured"
            }
        
        # Check Gemini service
        if self.gemini_service:
            try:
                gemini_healthy = await self.gemini_service.test_connection()
                health_status["providers"]["gemini"] = {
                    "available": True,
                    "healthy": gemini_healthy
                }
            except Exception as e:
                health_status["providers"]["gemini"] = {
                    "available": True,
                    "healthy": False,
                    "error": str(e)
                }
        else:
            health_status["providers"]["gemini"] = {
                "available": False,
                "reason": "No API key configured"
            }
        
        return health_status


================================================
FILE: bulk_audio_processor.py
================================================
"""
Bulk Audio Processor Service

Provides batch processing capabilities for audio files, integrating with existing
audio preparation and separation services. Supports bulk transcription, vocal extraction,
and data export in multiple formats.
"""

import os
import json
import csv
import logging
import asyncio
import tempfile
import uuid
from typing import Dict, Any, List, Optional, Union
from pathlib import Path
from datetime import datetime
import concurrent.futures
from dataclasses import dataclass, asdict
import pandas as pd

from src.services.audio_preparation_service import audio_preparation_service
from src.services.audio_separation_service import audio_separation_service

logger = logging.getLogger(__name__)


@dataclass
class AudioProcessingResult:
    """Data class for storing individual audio processing results"""
    file_path: str
    file_name: str
    file_size: int
    duration: float
    transcription: str
    language: str
    segments: List[Dict[str, Any]]
    speaker_info: Dict[str, Any]
    metadata: Dict[str, Any]
    vocals_path: Optional[str] = None
    processing_time: float = 0.0
    error: Optional[str] = None
    success: bool = True


@dataclass
class BatchProcessingConfig:
    """Configuration for batch processing operations"""
    # Audio processing options
    use_whisper: bool = True
    segment_audio: bool = True
    max_segment_duration: int = 30
    clean_silence: bool = True
    separate_voices: bool = True
    identify_speakers: bool = True
    
    # Speaker diarization options
    min_speakers: int = 1
    max_speakers: int = 10
    min_speaker_duration: float = 1.0
    
    # Transcription options
    language: Optional[str] = None
    prompt: Optional[str] = None
    
    # Processing options
    max_workers: int = 4
    timeout_per_file: int = 300  # 5 minutes per file
    
    # Export options
    export_formats: List[str] = None
    export_directory: Optional[str] = None
    
    def __post_init__(self):
        if self.export_formats is None:
            self.export_formats = ["json", "csv"]


class BulkAudioProcessor:
    """Service for processing multiple audio files in batch operations"""
    
    def __init__(self):
        """Initialize the bulk audio processor"""
        self.temp_dir = tempfile.mkdtemp(prefix="bulk_audio_")
        self.results: List[AudioProcessingResult] = []
        self.current_batch_id = None
        self.processing_stats = {
            "total_files": 0,
            "processed_files": 0,
            "failed_files": 0,
            "total_duration": 0.0,
            "processing_time": 0.0,
            "start_time": None,
            "end_time": None
        }
        
        logger.info(f"Bulk Audio Processor initialized - temp dir: {self.temp_dir}")
    
    async def process_audio_batch(
        self,
        audio_files: List[str],
        config: Optional[BatchProcessingConfig] = None
    ) -> Dict[str, Any]:
        """
        Process multiple audio files in batch
        
        Args:
            audio_files: List of audio file paths to process
            config: Batch processing configuration
            
        Returns:
            Dictionary containing batch processing results and statistics
        """
        if config is None:
            config = BatchProcessingConfig()
        
        self.current_batch_id = str(uuid.uuid4())
        self.results = []
        self.processing_stats = {
            "total_files": len(audio_files),
            "processed_files": 0,
            "failed_files": 0,
            "total_duration": 0.0,
            "processing_time": 0.0,
            "start_time": datetime.now(),
            "end_time": None,
            "batch_id": self.current_batch_id
        }
        
        logger.info(f"Starting batch processing of {len(audio_files)} files")
        
        try:
            # Process files with concurrency control
            semaphore = asyncio.Semaphore(config.max_workers)
            
            async def process_single_file(file_path: str) -> AudioProcessingResult:
                async with semaphore:
                    return await self._process_single_audio_file(file_path, config)
            
            # Create tasks for all files
            tasks = [process_single_file(file_path) for file_path in audio_files]
            
            # Process with timeout
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Collect results and handle exceptions
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    error_result = AudioProcessingResult(
                        file_path=audio_files[i],
                        file_name=os.path.basename(audio_files[i]),
                        file_size=0,
                        duration=0.0,
                        transcription="",
                        language="unknown",
                        segments=[],
                        speaker_info={},
                        metadata={},
                        error=str(result),
                        success=False
                    )
                    self.results.append(error_result)
                    self.processing_stats["failed_files"] += 1
                else:
                    self.results.append(result)
                    if result.success:
                        self.processing_stats["processed_files"] += 1
                        self.processing_stats["total_duration"] += result.duration
                    else:
                        self.processing_stats["failed_files"] += 1
            
            self.processing_stats["end_time"] = datetime.now()
            self.processing_stats["processing_time"] = (
                self.processing_stats["end_time"] - self.processing_stats["start_time"]
            ).total_seconds()
            
            logger.info(f"Batch processing complete: {self.processing_stats['processed_files']} "
                       f"succeeded, {self.processing_stats['failed_files']} failed")
            
            # Export results if configured
            export_paths = {}
            if config.export_formats:
                export_paths = await self.export_transcriptions(
                    formats=config.export_formats,
                    export_directory=config.export_directory
                )
            
            return {
                "batch_id": self.current_batch_id,
                "statistics": self.processing_stats,
                "results": [asdict(result) for result in self.results],
                "export_paths": export_paths,
                "success": self.processing_stats["failed_files"] == 0
            }
            
        except Exception as e:
            logger.error(f"Error in batch processing: {str(e)}")
            self.processing_stats["end_time"] = datetime.now()
            raise
    
    async def extract_vocals_batch(
        self,
        audio_files: List[str],
        model_name: str = "htdemucs",
        max_workers: int = 2
    ) -> Dict[str, Any]:
        """
        Extract vocals from multiple audio files in batch
        
        Args:
            audio_files: List of audio file paths
            model_name: Demucs model to use
            max_workers: Maximum concurrent extractions
            
        Returns:
            Dictionary containing extraction results
        """
        logger.info(f"Starting batch vocal extraction for {len(audio_files)} files")
        
        results = []
        semaphore = asyncio.Semaphore(max_workers)
        
        async def extract_single_vocal(file_path: str) -> Dict[str, Any]:
            async with semaphore:
                try:
                    start_time = datetime.now()
                    vocals_path = await audio_separation_service.extract_vocals(
                        file_path, model_name=model_name
                    )
                    processing_time = (datetime.now() - start_time).total_seconds()
                    
                    return {
                        "file_path": file_path,
                        "file_name": os.path.basename(file_path),
                        "vocals_path": vocals_path,
                        "processing_time": processing_time,
                        "success": True,
                        "error": None
                    }
                except Exception as e:
                    return {
                        "file_path": file_path,
                        "file_name": os.path.basename(file_path),
                        "vocals_path": None,
                        "processing_time": 0,
                        "success": False,
                        "error": str(e)
                    }
        
        # Process all files
        tasks = [extract_single_vocal(file_path) for file_path in audio_files]
        results = await asyncio.gather(*tasks)
        
        # Compile statistics
        successful = sum(1 for r in results if r["success"])
        failed = len(results) - successful
        total_time = sum(r["processing_time"] for r in results)
        
        return {
            "total_files": len(audio_files),
            "successful_extractions": successful,
            "failed_extractions": failed,
            "total_processing_time": total_time,
            "results": results
        }
    
    async def transcribe_batch(
        self,
        audio_files: List[str],
        config: Optional[BatchProcessingConfig] = None
    ) -> Dict[str, Any]:
        """
        Transcribe multiple audio files in batch using Whisper
        
        Args:
            audio_files: List of audio file paths
            config: Batch processing configuration
            
        Returns:
            Dictionary containing transcription results
        """
        if config is None:
            config = BatchProcessingConfig()
        
        logger.info(f"Starting batch transcription for {len(audio_files)} files")
        
        results = []
        semaphore = asyncio.Semaphore(config.max_workers)
        
        async def transcribe_single_file(file_path: str) -> Dict[str, Any]:
            async with semaphore:
                try:
                    start_time = datetime.now()
                    
                    # Prepare audio for transcription
                    preparation_config = {
                        "use_whisper": config.use_whisper,
                        "segment_audio": config.segment_audio,
                        "max_segment_duration": config.max_segment_duration,
                        "transcribe": True,
                        "clean_silence": config.clean_silence,
                        "separate_voices": config.separate_voices,
                        "identify_speakers": config.identify_speakers,
                        "min_speakers": config.min_speakers,
                        "max_speakers": config.max_speakers,
                        "min_speaker_duration": config.min_speaker_duration,
                        "provider_specific": {
                            "language": config.language,
                            "prompt": config.prompt
                        }
                    }
                    
                    result = await audio_preparation_service.prepare_audio(
                        audio_path=file_path,
                        provider="transcription",
                        config=preparation_config
                    )
                    
                    processing_time = (datetime.now() - start_time).total_seconds()
                    
                    return {
                        "file_path": file_path,
                        "file_name": os.path.basename(file_path),
                        "transcription": result.get("transcription", ""),
                        "language": result.get("metadata", {}).get("language", "unknown"),
                        "segments": result.get("segments", []),
                        "speaker_info": result.get("diarization", {}),
                        "metadata": result.get("metadata", {}),
                        "processing_time": processing_time,
                        "success": True,
                        "error": None
                    }
                except Exception as e:
                    return {
                        "file_path": file_path,
                        "file_name": os.path.basename(file_path),
                        "transcription": "",
                        "language": "unknown",
                        "segments": [],
                        "speaker_info": {},
                        "metadata": {},
                        "processing_time": 0,
                        "success": False,
                        "error": str(e)
                    }
        
        # Process all files
        tasks = [transcribe_single_file(file_path) for file_path in audio_files]
        results = await asyncio.gather(*tasks)
        
        # Compile statistics
        successful = sum(1 for r in results if r["success"])
        failed = len(results) - successful
        total_time = sum(r["processing_time"] for r in results)
        
        return {
            "total_files": len(audio_files),
            "successful_transcriptions": successful,
            "failed_transcriptions": failed,
            "total_processing_time": total_time,
            "results": results
        }
    
    async def export_transcriptions(
        self,
        formats: List[str] = None,
        export_directory: Optional[str] = None
    ) -> Dict[str, str]:
        """
        Export transcription results in multiple formats
        
        Args:
            formats: List of export formats ('json', 'csv', 'txt', 'srt')
            export_directory: Directory to export files to
            
        Returns:
            Dictionary mapping format names to export file paths
        """
        if formats is None:
            formats = ["json", "csv"]
        
        if export_directory is None:
            export_directory = self.temp_dir
        
        os.makedirs(export_directory, exist_ok=True)
        
        export_paths = {}
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        batch_id = self.current_batch_id or "unknown"
        
        for format_name in formats:
            try:
                if format_name == "json":
                    export_paths["json"] = await self._export_json(
                        export_directory, timestamp, batch_id
                    )
                elif format_name == "csv":
                    export_paths["csv"] = await self._export_csv(
                        export_directory, timestamp, batch_id
                    )
                elif format_name == "txt":
                    export_paths["txt"] = await self._export_txt(
                        export_directory, timestamp, batch_id
                    )
                elif format_name == "srt":
                    export_paths["srt"] = await self._export_srt(
                        export_directory, timestamp, batch_id
                    )
                else:
                    logger.warning(f"Unsupported export format: {format_name}")
            except Exception as e:
                logger.error(f"Error exporting {format_name}: {str(e)}")
        
        return export_paths
    
    async def _process_single_audio_file(
        self,
        file_path: str,
        config: BatchProcessingConfig
    ) -> AudioProcessingResult:
        """Process a single audio file with full preparation and transcription"""
        try:
            start_time = datetime.now()
            
            # Get file info
            file_name = os.path.basename(file_path)
            file_size = os.path.getsize(file_path)
            
            # Prepare audio configuration
            preparation_config = {
                "use_whisper": config.use_whisper,
                "segment_audio": config.segment_audio,
                "max_segment_duration": config.max_segment_duration,
                "transcribe": True,
                "clean_silence": config.clean_silence,
                "separate_voices": config.separate_voices,
                "identify_speakers": config.identify_speakers,
                "min_speakers": config.min_speakers,
                "max_speakers": config.max_speakers,
                "min_speaker_duration": config.min_speaker_duration,
                "provider_specific": {
                    "language": config.language,
                    "prompt": config.prompt
                }
            }
            
            # Process audio
            result = await audio_preparation_service.prepare_audio(
                audio_path=file_path,
                provider="transcription",
                config=preparation_config
            )
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # Calculate duration from segments or metadata
            duration = 0.0
            segments = result.get("segments", [])
            if segments:
                duration = segments[-1].get("end", 0.0) if segments else 0.0
            else:
                duration = result.get("metadata", {}).get("total_duration", 0.0)
            
            # Build speaker info
            speaker_info = {}
            if result.get("diarization"):
                speaker_info = {
                    "total_speakers": result["diarization"].get("speakers", 1),
                    "speaker_labels": result["diarization"].get("speaker_labels", []),
                    "speaker_segments": result["diarization"].get("segments", [])
                }
            
            return AudioProcessingResult(
                file_path=file_path,
                file_name=file_name,
                file_size=file_size,
                duration=duration,
                transcription=result.get("transcription", ""),
                language=result.get("metadata", {}).get("language", "unknown"),
                segments=segments,
                speaker_info=speaker_info,
                metadata=result.get("metadata", {}),
                vocals_path=result.get("vocals_path"),
                processing_time=processing_time,
                success=True
            )
            
        except Exception as e:
            logger.error(f"Error processing {file_path}: {str(e)}")
            return AudioProcessingResult(
                file_path=file_path,
                file_name=os.path.basename(file_path),
                file_size=0,
                duration=0.0,
                transcription="",
                language="unknown",
                segments=[],
                speaker_info={},
                metadata={},
                error=str(e),
                success=False
            )
    
    async def _export_json(
        self,
        export_directory: str,
        timestamp: str,
        batch_id: str
    ) -> str:
        """Export results as JSON"""
        file_path = os.path.join(export_directory, f"transcriptions_{timestamp}_{batch_id}.json")
        
        export_data = {
            "batch_id": batch_id,
            "export_timestamp": timestamp,
            "statistics": self.processing_stats,
            "results": [asdict(result) for result in self.results]
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"Exported JSON results to: {file_path}")
        return file_path
    
    async def _export_csv(
        self,
        export_directory: str,
        timestamp: str,
        batch_id: str
    ) -> str:
        """Export results as CSV"""
        file_path = os.path.join(export_directory, f"transcriptions_{timestamp}_{batch_id}.csv")
        
        # Prepare data for CSV
        csv_data = []
        for result in self.results:
            csv_data.append({
                "file_name": result.file_name,
                "file_path": result.file_path,
                "file_size": result.file_size,
                "duration": result.duration,
                "transcription": result.transcription,
                "language": result.language,
                "total_segments": len(result.segments),
                "total_speakers": result.speaker_info.get("total_speakers", 1),
                "vocals_extracted": result.vocals_path is not None,
                "processing_time": result.processing_time,
                "success": result.success,
                "error": result.error or ""
            })
        
        # Write CSV
        if csv_data:
            df = pd.DataFrame(csv_data)
            df.to_csv(file_path, index=False, encoding='utf-8')
        
        logger.info(f"Exported CSV results to: {file_path}")
        return file_path
    
    async def _export_txt(
        self,
        export_directory: str,
        timestamp: str,
        batch_id: str
    ) -> str:
        """Export transcriptions as plain text"""
        file_path = os.path.join(export_directory, f"transcriptions_{timestamp}_{batch_id}.txt")
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"Batch Transcription Results\n")
            f.write(f"Batch ID: {batch_id}\n")
            f.write(f"Export Time: {timestamp}\n")
            f.write(f"Total Files: {len(self.results)}\n")
            f.write("=" * 50 + "\n\n")
            
            for i, result in enumerate(self.results, 1):
                f.write(f"File {i}: {result.file_name}\n")
                f.write(f"Duration: {result.duration:.2f}s\n")
                f.write(f"Language: {result.language}\n")
                if result.success:
                    f.write(f"Transcription:\n{result.transcription}\n")
                else:
                    f.write(f"Error: {result.error}\n")
                f.write("-" * 30 + "\n\n")
        
        logger.info(f"Exported TXT results to: {file_path}")
        return file_path
    
    async def _export_srt(
        self,
        export_directory: str,
        timestamp: str,
        batch_id: str
    ) -> str:
        """Export transcriptions as SRT subtitle files"""
        for result in self.results:
            if not result.success or not result.segments:
                continue
            
            file_name = os.path.splitext(result.file_name)[0]
            srt_path = os.path.join(export_directory, f"{file_name}_{timestamp}.srt")
            
            with open(srt_path, 'w', encoding='utf-8') as f:
                for i, segment in enumerate(result.segments, 1):
                    start_time = self._format_srt_time(segment.get("start", 0))
                    end_time = self._format_srt_time(segment.get("end", 0))
                    text = segment.get("text", "").strip()
                    
                    f.write(f"{i}\n")
                    f.write(f"{start_time} --> {end_time}\n")
                    f.write(f"{text}\n\n")
        
        # Return directory path since multiple files are created
        return export_directory
    
    def _format_srt_time(self, seconds: float) -> str:
        """Format time in SRT format (HH:MM:SS,mmm)"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds % 1) * 1000)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"
    
    def get_batch_statistics(self) -> Dict[str, Any]:
        """Get current batch processing statistics"""
        return self.processing_stats.copy()
    
    def get_results(self) -> List[AudioProcessingResult]:
        """Get current batch processing results"""
        return self.results.copy()
    
    def cleanup(self):
        """Clean up temporary files"""
        try:
            import shutil
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                logger.info(f"Cleaned up temp directory: {self.temp_dir}")
        except Exception as e:
            logger.warning(f"Error cleaning up temp files: {str(e)}")
    
    def __del__(self):
        """Cleanup on deletion"""
        self.cleanup()


# Global instance
bulk_audio_processor = BulkAudioProcessor()


================================================
FILE: bulk_job_manager.py
================================================
"""
Bulk Job Manager

Manages complex bulk operations with multiple stages, progress tracking,
and export functionality. Extends the existing Convex job system to handle
bulk processing workflows with detailed stage progression and cleanup.
"""

import os
import uuid
import logging
import tempfile
import shutil
import asyncio
from typing import Dict, Any, Optional, List, Callable, Union
from datetime import datetime, timedelta
from pathlib import Path
from enum import Enum
from dataclasses import dataclass, asdict
from convex import ConvexClient
import json
import zipfile
import csv
import time

logger = logging.getLogger(__name__)


class BulkJobStatus(Enum):
    """Bulk job status enumeration"""
    PENDING = "pending"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    EXPORTING = "exporting"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class BulkJobStage(Enum):
    """Bulk job processing stages"""
    INITIALIZATION = "initialization"
    VALIDATION = "validation"
    CONTENT_FETCH = "content_fetch"
    CONTENT_PROCESSING = "content_processing"
    AUDIO_EXTRACTION = "audio_extraction"
    TRANSCRIPTION = "transcription"
    VOICE_PREPARATION = "voice_preparation"
    EXPORT_PREPARATION = "export_preparation"
    EXPORT_GENERATION = "export_generation"
    CLEANUP = "cleanup"


@dataclass
class BulkJobStageInfo:
    """Information about a specific job stage"""
    stage: BulkJobStage
    status: str  # pending, processing, completed, failed, skipped
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    progress: float = 0.0  # 0.0 to 1.0
    items_total: int = 0
    items_completed: int = 0
    items_failed: int = 0
    error_message: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class BulkJobExportInfo:
    """Information about job export"""
    export_id: str
    format: str  # zip, csv, json
    file_path: Optional[str] = None
    file_size: Optional[int] = None
    download_url: Optional[str] = None
    expires_at: Optional[float] = None
    created_at: Optional[float] = None


class BulkJobManager:
    """
    Manages bulk operations with complex workflows and export capabilities.
    
    Features:
    - Multi-stage processing with detailed progress tracking
    - Export job management with multiple formats
    - Rate limiting integration
    - Resource cleanup and management
    - Convex webhook integration
    """
    
    def __init__(self):
        """Initialize bulk job manager"""
        self.convex_url = os.getenv("CONVEX_URL") or os.getenv("NEXT_PUBLIC_CONVEX_URL", "http://127.0.0.1:3210")
        self.convex_client = ConvexClient(self.convex_url)
        self.environment = os.getenv("ENVIRONMENT", "development")
        
        # Export configuration
        self.export_base_path = os.getenv("BULK_EXPORT_PATH", "/tmp/bulk_exports")
        self.export_retention_hours = int(os.getenv("BULK_EXPORT_RETENTION_HOURS", "24"))
        self.max_export_size_mb = int(os.getenv("BULK_MAX_EXPORT_SIZE_MB", "500"))
        
        # Rate limiting configuration
        self.rate_limit_window_minutes = int(os.getenv("BULK_RATE_LIMIT_WINDOW", "60"))
        self.rate_limit_max_jobs = int(os.getenv("BULK_RATE_LIMIT_MAX_JOBS", "5"))
        
        # Ensure export directory exists
        Path(self.export_base_path).mkdir(parents=True, exist_ok=True)
        
        # In-memory export storage for quick access
        self.exports: Dict[str, Dict[str, Any]] = {}
        
        # Local job storage as fallback when Convex is unavailable
        self.local_jobs: Dict[str, Dict[str, Any]] = {}
        
        logger.info(f"Bulk Job Manager initialized - Convex URL: {self.convex_url}")
        logger.info(f"Export path: {self.export_base_path}, retention: {self.export_retention_hours}h")
    
    async def create_bulk_job(
        self,
        job_type: str,
        user_id: str,
        job_data: Dict[str, Any],
        stages: List[BulkJobStage],
        estimated_duration_minutes: Optional[int] = None,
        priority: str = "normal",
        job_id: Optional[str] = None
    ) -> str:
        """
        Create a new bulk job with stage tracking
        
        Args:
            job_type: Type of bulk job (e.g., "bulk_tiktok_download", "bulk_transcription")
            user_id: User ID for rate limiting and ownership
            job_data: Job-specific data and configuration
            stages: List of processing stages for this job
            estimated_duration_minutes: Estimated processing time
            priority: Job priority (low, normal, high)
            
        Returns:
            Job ID
        """
        try:
            # Check rate limits
            await self._check_rate_limits(user_id)
            
            # Use provided job_id or generate new one
            if job_id is None:
                job_id = str(uuid.uuid4())
                logger.info(f"Generated new job ID: {job_id}")
            else:
                logger.info(f"Using provided job ID: {job_id}")
                # Validate job_id format
                if not job_id or not isinstance(job_id, str):
                    raise ValueError(f"Invalid job_id format: {job_id}")
            
            logger.info(f"Creating bulk job with ID: {job_id}")
            
            # Initialize stage information
            stage_info = {}
            for stage in stages:
                stage_data = BulkJobStageInfo(
                    stage=stage,
                    status="pending"
                )
                # Convert to dict and replace enum with string value
                stage_dict = asdict(stage_data)
                stage_dict["stage"] = stage.value  # Convert enum to string
                stage_info[stage.value] = stage_dict
            
            # Create job data
            bulk_job_data = {
                "jobId": job_id,
                "jobType": job_type,
                "userId": user_id,
                "status": BulkJobStatus.PENDING.value,
                "priority": priority,
                "createdAt": datetime.utcnow().timestamp() * 1000,
                "estimatedDurationMinutes": estimated_duration_minutes,
                "stages": stage_info,
                "currentStage": stages[0].value if stages else None,
                "progress": {
                    "overall": 0.0,
                    "currentStage": 0.0,
                    "itemsTotal": job_data.get("total_items", 0),
                    "itemsCompleted": 0,
                    "itemsFailed": 0
                },
                "jobData": job_data,
                "exports": {},
                "metadata": {
                    "environment": self.environment,
                    "totalStages": len(stages),
                    "stageNames": [stage.value for stage in stages]
                }
            }
            
            # Store job locally first
            self.local_jobs[job_id] = bulk_job_data.copy()
            
            # Try to create job in Convex
            result = await self._send_webhook("bulkJobs:create", bulk_job_data)
            
            if result is not None:
                logger.info(f"Successfully created bulk job in Convex: {job_id} ({job_type}) for user: {user_id}")
            else:
                logger.warning(f"Job {job_id} created locally but Convex integration failed - continuing with local storage")
            
            return job_id
            
        except Exception as e:
            logger.error(f"Error creating bulk job: {str(e)}")
            raise
    
    async def update_job_status(
        self,
        job_id: str,
        status: BulkJobStatus,
        updates: Optional[Dict[str, Any]] = None
    ):
        """
        Update bulk job status
        
        Args:
            job_id: Job ID to update
            status: New job status
            updates: Additional fields to update
        """
        try:
            logger.info(f"Updating job status for job ID: {job_id} to status: {status.value}")
            
            # Update local storage first
            if job_id in self.local_jobs:
                self.local_jobs[job_id]["status"] = status.value
                if updates:
                    self.local_jobs[job_id].update(updates)
            
            mutation_data = {
                "jobId": job_id,
                "status": status.value
            }
            
            # Add supported fields from updates to proper parameter names
            if updates:
                # Map common update fields to proper parameter names
                if "currentStage" in updates:
                    mutation_data["currentStage"] = updates["currentStage"]
                if "current_stage" in updates:
                    mutation_data["currentStage"] = updates["current_stage"]
                if "progress_percentage" in updates:
                    mutation_data["progress_percentage"] = updates["progress_percentage"]
                if "error" in updates:
                    mutation_data["error_message"] = updates["error"]
                if "error_message" in updates:
                    mutation_data["error_message"] = updates["error_message"]
                if "failedStage" in updates:
                    mutation_data["failedStage"] = updates["failedStage"]
                if "cancelledAt" in updates:
                    mutation_data["cancelledAt"] = updates["cancelledAt"]
                if "cancellationReason" in updates:
                    mutation_data["cancellationReason"] = updates["cancellationReason"]
                if "metadata" in updates:
                    mutation_data["metadata"] = updates["metadata"]
                
                # Add any remaining updates as a nested object
                remaining_updates = {k: v for k, v in updates.items() 
                                   if k not in ["currentStage", "current_stage", "progress_percentage", "error", "error_message", 
                                              "failedStage", "cancelledAt", "cancellationReason", "metadata"]}
                if remaining_updates:
                    mutation_data["updates"] = remaining_updates
            
            result = await self._send_webhook("bulkJobs:updateStatus", mutation_data)
            
            if result is not None:
                logger.info(f"Successfully updated bulk job {job_id} status to: {status.value} in Convex")
            else:
                logger.warning(f"Failed to update job {job_id} status in Convex, but continuing processing")
            
        except Exception as e:
            logger.error(f"Error updating bulk job status: {str(e)}")
            # Don't raise - allow processing to continue even if Convex updates fail
            logger.warning(f"Continuing processing despite Convex update failure for job {job_id}")
    
    async def update_stage_progress(
        self,
        job_id: str,
        stage: BulkJobStage,
        progress: float,
        items_completed: Optional[int] = None,
        items_failed: Optional[int] = None,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """
        Update progress for a specific stage
        
        Args:
            job_id: Job ID
            stage: Current stage
            progress: Stage progress (0.0 to 1.0)
            items_completed: Number of items completed
            items_failed: Number of items failed
            metadata: Additional stage metadata
        """
        try:
            stage_update = {
                "jobId": job_id,
                "stage": stage.value,
                "progress": min(max(progress, 0.0), 1.0),  # Clamp to 0.0-1.0
                "updatedAt": datetime.utcnow().timestamp() * 1000
            }
            
            if items_completed is not None:
                stage_update["itemsCompleted"] = items_completed
            
            if items_failed is not None:
                stage_update["itemsFailed"] = items_failed
            
            if metadata:
                stage_update["metadata"] = metadata
            
            await self._send_webhook("bulkJobs:updateStageProgress", stage_update)
            
            logger.debug(f"Updated stage {stage.value} progress for job {job_id}: {progress:.2%}")
            
        except Exception as e:
            logger.error(f"Error updating stage progress: {str(e)}")
            raise
    
    async def start_stage(
        self,
        job_id: str,
        stage: BulkJobStage,
        items_total: Optional[int] = None
    ):
        """
        Mark a stage as started
        
        Args:
            job_id: Job ID
            stage: Stage to start
            items_total: Total items to process in this stage
        """
        try:
            stage_update = {
                "jobId": job_id,
                "stage": stage.value,
                "status": "processing",
                "startTime": datetime.utcnow().timestamp() * 1000,
                "progress": 0.0
            }
            
            if items_total is not None:
                stage_update["itemsTotal"] = items_total
            
            await self._send_webhook("bulkJobs:startStage", stage_update)
            
            # Update job current stage
            await self.update_job_status(job_id, BulkJobStatus.PROCESSING, {
                "currentStage": stage.value
            })
            
            logger.info(f"Started stage {stage.value} for job {job_id}")
            
        except Exception as e:
            logger.error(f"Error starting stage: {str(e)}")
            raise
    
    async def complete_stage(
        self,
        job_id: str,
        stage: BulkJobStage,
        items_completed: Optional[int] = None,
        items_failed: Optional[int] = None,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """
        Mark a stage as completed
        
        Args:
            job_id: Job ID
            stage: Stage to complete
            items_completed: Number of items completed
            items_failed: Number of items failed
            metadata: Stage completion metadata
        """
        try:
            stage_update = {
                "jobId": job_id,
                "stage": stage.value,
                "status": "completed",
                "endTime": datetime.utcnow().timestamp() * 1000,
                "progress": 1.0
            }
            
            if items_completed is not None:
                stage_update["itemsCompleted"] = items_completed
            
            if items_failed is not None:
                stage_update["itemsFailed"] = items_failed
            
            if metadata:
                stage_update["metadata"] = metadata
            
            await self._send_webhook("bulkJobs:completeStage", stage_update)
            
            logger.info(f"Completed stage {stage.value} for job {job_id}")
            
        except Exception as e:
            logger.error(f"Error completing stage: {str(e)}")
            raise
    
    async def fail_stage(
        self,
        job_id: str,
        stage: BulkJobStage,
        error_message: str,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """
        Mark a stage as failed
        
        Args:
            job_id: Job ID
            stage: Stage that failed
            error_message: Error description
            metadata: Additional error metadata
        """
        try:
            stage_update = {
                "jobId": job_id,
                "stage": stage.value,
                "status": "failed",
                "endTime": datetime.utcnow().timestamp() * 1000,
                "errorMessage": error_message
            }
            
            if metadata:
                stage_update["metadata"] = metadata
            
            await self._send_webhook("bulkJobs:failStage", stage_update)
            
            # Mark entire job as failed
            await self.update_job_status(job_id, BulkJobStatus.FAILED, {
                "error": error_message,
                "failedStage": stage.value
            })
            
            logger.error(f"Failed stage {stage.value} for job {job_id}: {error_message}")
            
        except Exception as e:
            logger.error(f"Error failing stage: {str(e)}")
            raise
    
    async def create_export(
        self,
        job_id: str,
        export_format: str,
        data: Any,
        filename_prefix: str = "export",
        expires_in_hours: Optional[int] = None
    ) -> str:
        """
        Create an export file for a completed job
        
        Args:
            job_id: Job ID
            export_format: Export format (zip, csv, json)
            data: Data to export
            filename_prefix: Prefix for the export filename
            expires_in_hours: Custom expiration time
            
        Returns:
            Export ID
        """
        try:
            export_id = str(uuid.uuid4())
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            filename = f"{filename_prefix}_{timestamp}.{export_format}"
            
            # Create export directory for this job
            export_dir = Path(self.export_base_path) / job_id
            export_dir.mkdir(parents=True, exist_ok=True)
            
            export_path = export_dir / filename
            
            # Generate export file based on format
            if export_format == "json":
                await self._create_json_export(export_path, data)
            elif export_format == "csv":
                await self._create_csv_export(export_path, data)
            elif export_format == "zip":
                await self._create_zip_export(export_path, data)
            else:
                raise ValueError(f"Unsupported export format: {export_format}")
            
            # Check file size
            file_size = export_path.stat().st_size
            if file_size > self.max_export_size_mb * 1024 * 1024:
                export_path.unlink()
                raise ValueError(f"Export file too large: {file_size / 1024 / 1024:.2f}MB")
            
            # Calculate expiration
            expires_in = expires_in_hours or self.export_retention_hours
            expires_at = (datetime.utcnow() + timedelta(hours=expires_in)).timestamp() * 1000
            
            # Create export info
            export_info = BulkJobExportInfo(
                export_id=export_id,
                format=export_format,
                file_path=str(export_path),
                file_size=file_size,
                download_url=f"/api/bulk/exports/{export_id}/download",
                expires_at=expires_at,
                created_at=datetime.utcnow().timestamp() * 1000
            )
            
            # Update job with export info
            await self._send_webhook("bulkJobs:addExport", {
                "jobId": job_id,
                "exportId": export_id,
                "exportInfo": asdict(export_info)
            })
            
            logger.info(f"Created export {export_id} for job {job_id}: {filename} ({file_size} bytes)")
            return export_id
            
        except Exception as e:
            logger.error(f"Error creating export: {str(e)}")
            raise
    
    async def get_export_info(self, export_id: str) -> Optional[BulkJobExportInfo]:
        """
        Get export information by ID
        
        Args:
            export_id: Export ID
            
        Returns:
            Export information or None if not found
        """
        try:
            export_data = await self._query("bulkJobs:getExport", {"exportId": export_id})
            if export_data:
                return BulkJobExportInfo(**export_data)
            return None
        except Exception as e:
            logger.error(f"Error getting export info: {str(e)}")
            return None
    
    async def get_export_file_path(self, export_id: str) -> Optional[str]:
        """
        Get the file path for an export
        
        Args:
            export_id: Export ID
            
        Returns:
            File path or None if not found/expired
        """
        try:
            export_info = await self.get_export_info(export_id)
            if not export_info:
                return None
            
            # Check if export has expired
            if export_info.expires_at and export_info.expires_at < datetime.utcnow().timestamp() * 1000:
                await self.cleanup_export(export_id)
                return None
            
            # Check if file still exists
            if export_info.file_path and Path(export_info.file_path).exists():
                return export_info.file_path
            
            return None
            
        except Exception as e:
            logger.error(f"Error getting export file path: {str(e)}")
            return None
    
    async def cleanup_export(self, export_id: str):
        """
        Clean up an export file and remove from database
        
        Args:
            export_id: Export ID to clean up
        """
        try:
            export_info = await self.get_export_info(export_id)
            if export_info and export_info.file_path:
                file_path = Path(export_info.file_path)
                if file_path.exists():
                    file_path.unlink()
                    logger.info(f"Cleaned up export file: {export_info.file_path}")
                
                # Remove parent directory if empty
                try:
                    file_path.parent.rmdir()
                except OSError:
                    pass  # Directory not empty
            
            # Remove from database
            await self._send_webhook("bulkJobs:removeExport", {"exportId": export_id})
            
        except Exception as e:
            logger.error(f"Error cleaning up export: {str(e)}")
    
    async def cleanup_expired_exports(self):
        """Clean up all expired exports"""
        try:
            current_time = datetime.utcnow().timestamp() * 1000
            expired_exports = await self._query("bulkJobs:getExpiredExports", {
                "currentTime": current_time
            })
            
            for export_data in expired_exports or []:
                await self.cleanup_export(export_data["exportId"])
            
            logger.info(f"Cleaned up {len(expired_exports or [])} expired exports")
            
        except Exception as e:
            logger.error(f"Error cleaning up expired exports: {str(e)}")
    
    async def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """
        Get current job status and progress
        
        Args:
            job_id: Job ID to check
            
        Returns:
            Job data or None if not found
        """
        try:
            # Try Convex first
            job = await self._query("bulkJobs:getJob", {"jobId": job_id})
            if job:
                return job
        except Exception as e:
            logger.warning(f"Error getting job status from Convex: {str(e)}")
        
        # Fallback to local storage
        if job_id in self.local_jobs:
            logger.info(f"Retrieved job {job_id} from local storage")
            return self.local_jobs[job_id]
        
        logger.warning(f"Job {job_id} not found in Convex or local storage")
        return None
    
    async def get_user_jobs(
        self,
        user_id: str,
        status: Optional[str] = None,
        job_type: Optional[str] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Get jobs for a specific user
        
        Args:
            user_id: User ID
            status: Filter by status
            job_type: Filter by job type
            limit: Maximum number of jobs to return
            
        Returns:
            List of job data
        """
        try:
            query_args = {
                "userId": user_id,
                "limit": limit
            }
            
            if status:
                query_args["status"] = status
            
            if job_type:
                query_args["jobType"] = job_type
            
            jobs = await self._query("bulkJobs:getUserJobs", query_args)
            return jobs or []
            
        except Exception as e:
            logger.error(f"Error getting user jobs: {str(e)}")
            return []
    
    async def cancel_job(self, job_id: str, reason: str = "User cancelled"):
        """
        Cancel a running job
        
        Args:
            job_id: Job ID to cancel
            reason: Cancellation reason
        """
        try:
            await self.update_job_status(job_id, BulkJobStatus.CANCELLED, {
                "cancelledAt": datetime.utcnow().timestamp() * 1000,
                "cancellationReason": reason
            })
            
            logger.info(f"Cancelled job {job_id}: {reason}")
            
        except Exception as e:
            logger.error(f"Error cancelling job: {str(e)}")
            raise
    
    async def process_job_with_stages(
        self,
        job_id: str,
        stage_processors: Dict[BulkJobStage, Callable]
    ):
        """
        Process a job through multiple stages
        
        Args:
            job_id: Job ID to process
            stage_processors: Dictionary mapping stages to processor functions
        """
        try:
            job = await self.get_job_status(job_id)
            if not job:
                raise ValueError(f"Job {job_id} not found")
            
            stages = [BulkJobStage(stage) for stage in job.get("metadata", {}).get("stageNames", [])]
            
            for stage in stages:
                if stage not in stage_processors:
                    logger.warning(f"No processor for stage {stage.value}, skipping")
                    continue
                
                try:
                    await self.start_stage(job_id, stage)
                    
                    # Execute stage processor
                    await stage_processors[stage](job_id, job.get("jobData", {}))
                    
                    await self.complete_stage(job_id, stage)
                    
                except Exception as stage_error:
                    await self.fail_stage(job_id, stage, str(stage_error))
                    raise
            
            # Mark job as completed
            await self.update_job_status(job_id, BulkJobStatus.COMPLETED, {
                "completedAt": datetime.utcnow().timestamp() * 1000
            })
            
        except Exception as e:
            logger.error(f"Error processing job {job_id}: {str(e)}")
            raise
    
    async def _check_rate_limits(self, user_id: str):
        """Check if user has exceeded rate limits"""
        try:
            window_start = datetime.utcnow() - timedelta(minutes=self.rate_limit_window_minutes)
            window_start_ms = window_start.timestamp() * 1000
            
            recent_jobs = await self._query("bulkJobs:getUserRecentJobs", {
                "userId": user_id,
                "afterTime": window_start_ms
            })
            
            # If Convex function doesn't exist, skip rate limiting for now
            if recent_jobs is not None and len(recent_jobs) >= self.rate_limit_max_jobs:
                raise ValueError(f"Rate limit exceeded: {self.rate_limit_max_jobs} jobs per {self.rate_limit_window_minutes} minutes")
            
        except Exception as e:
            if "Rate limit exceeded" in str(e):
                raise
            logger.error(f"Error checking rate limits: {str(e)}")
    
    async def _create_json_export(self, export_path: Path, data: Any):
        """Create JSON export file"""
        with open(export_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    
    async def _create_csv_export(self, export_path: Path, data: Any):
        """Create CSV export file"""
        if not isinstance(data, list) or not data:
            raise ValueError("CSV export requires list of dictionaries")
        
        with open(export_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
    
    async def _create_zip_export(self, export_path: Path, data: Any):
        """Create ZIP export file"""
        with zipfile.ZipFile(export_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            if isinstance(data, dict):
                for filename, content in data.items():
                    if isinstance(content, (str, bytes)):
                        zf.writestr(filename, content)
                    else:
                        zf.writestr(filename, json.dumps(content, indent=2))
            else:
                zf.writestr("data.json", json.dumps(data, indent=2))
    
    async def _send_webhook(self, function_name: str, data: Dict[str, Any]):
        """Send webhook to Convex with fallback handling (supports both actions and mutations)"""
        try:
            # Use actions for complex operations that involve external APIs or long processing
            if function_name in ["bulkJobs:create", "bulkJobs:updateStatus"]:
                result = self.convex_client.action(function_name, data)
                logger.info(f"Convex action {function_name} completed successfully: {result}")
                return result
            else:
                # Use mutations for simple database operations
                result = self.convex_client.mutation(function_name, data)
                logger.info(f"Convex mutation {function_name} completed successfully: {result}")
                return result
        except Exception as e:
            if "Could not find public function" in str(e):
                logger.warning(f"Convex function {function_name} not found. Continuing without Convex integration.")
                # Don't raise error for missing functions - continue without Convex
                return None
            else:
                logger.error(f"Error sending webhook {function_name}: {str(e)}")
                # Log the data that failed to help debug
                logger.error(f"Failed data: {data}")
                raise
    
    async def _query(self, query_name: str, args: Dict[str, Any]) -> Any:
        """Query Convex with fallback handling"""
        try:
            return self.convex_client.query(query_name, args)
        except Exception as e:
            if "Could not find public function" in str(e):
                logger.warning(f"Convex function {query_name} not found. Returning None.")
                # Return None for missing functions instead of raising error
                return None
            else:
                logger.error(f"Error querying {query_name}: {str(e)}")
                raise

    # Additional methods for API compatibility
    def create_job(self, job_id: str, job_type: str, user_id: str, total_items: int, config: Dict[str, Any]):
        """Create a new bulk processing job (sync version)."""
        # Define default stages for bulk processing
        default_stages = [
            BulkJobStage.INITIALIZATION,
            BulkJobStage.CONTENT_FETCH,
            BulkJobStage.AUDIO_EXTRACTION,
            BulkJobStage.TRANSCRIPTION,
            BulkJobStage.EXPORT_PREPARATION
        ]
        
        # Create job data that includes the job_id
        job_data_dict = {
            "job_id": job_id,
            "total_items": total_items,
            "config": config
        }
        
        # Use asyncio.run for blocking execution in sync context
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If already in async context, create task
                task = asyncio.create_task(self.create_bulk_job(
                    job_type=job_type,
                    user_id=user_id,
                    job_data=job_data_dict,
                    stages=default_stages,
                    estimated_duration_minutes=total_items * 2,  # Estimate 2 minutes per item
                    job_id=job_id  # Pass the job_id to preserve it
                ))
                return job_id  # Return the job_id immediately
            else:
                # If not in async context, run synchronously
                return asyncio.run(self.create_bulk_job(
                    job_type=job_type,
                    user_id=user_id,
                    job_data=job_data_dict,
                    stages=default_stages,
                    estimated_duration_minutes=total_items * 2,
                    job_id=job_id  # Pass the job_id to preserve it
                ))
        except Exception as e:
            logger.error(f"Error creating job {job_id}: {e}")
            raise
    
    def update_job_progress(self, job_id: str, progress: float, stage: str, metadata: Dict[str, Any]):
        """Update job progress (sync version)."""
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # Convert stage string to enum value for consistency  
                stage_enum = BulkJobStage(stage) if stage in [s.value for s in BulkJobStage] else BulkJobStage.CONTENT_PROCESSING
                asyncio.create_task(self.update_job_status(
                    job_id=job_id,
                    status=BulkJobStatus.PROCESSING,
                    updates={
                        "progress_percentage": progress,
                        "currentStage": stage_enum.value,  # Convert enum to string
                        "metadata": self._filter_metadata_for_schema(metadata)
                    }
                ))
            else:
                # Convert stage string to enum value for consistency  
                stage_enum = BulkJobStage(stage) if stage in [s.value for s in BulkJobStage] else BulkJobStage.CONTENT_PROCESSING
                asyncio.run(self.update_job_status(
                    job_id=job_id,
                    status=BulkJobStatus.PROCESSING,
                    updates={
                        "progress_percentage": progress,
                        "currentStage": stage_enum.value,  # Convert enum to string
                        "metadata": self._filter_metadata_for_schema(metadata)
                    }
                ))
        except Exception as e:
            logger.error(f"Error updating job progress for {job_id}: {e}")
    
    def complete_job(self, job_id: str, result: Dict[str, Any]):
        """Complete a job (sync version)."""
        try:
            # Store result in local storage immediately
            if job_id in self.local_jobs:
                self.local_jobs[job_id]["result"] = result
                self.local_jobs[job_id]["status"] = BulkJobStatus.COMPLETED.value
                self.local_jobs[job_id]["progress_percentage"] = 100.0
                logger.info(f"Stored completion result for job {job_id} in local storage")
            
            # Also try to update Convex
            import asyncio
            loop = asyncio.get_event_loop()
            if loop.is_running():
                asyncio.create_task(self.update_job_status(
                    job_id=job_id,
                    status=BulkJobStatus.COMPLETED,
                    updates={
                        "progress_percentage": 100.0,
                        "result": result
                    }
                ))
            else:
                asyncio.run(self.update_job_status(
                    job_id=job_id,
                    status=BulkJobStatus.COMPLETED,
                    updates={
                        "progress_percentage": 100.0,
                        "result": result
                    }
                ))
        except Exception as e:
            logger.error(f"Error completing job {job_id}: {e}")
            # Even if Convex update fails, the job is marked complete locally
    
    def fail_job(self, job_id: str, error: str):
        """Fail a job (sync version)."""
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            if loop.is_running():
                asyncio.create_task(self.update_job_status(
                    job_id=job_id,
                    status=BulkJobStatus.FAILED,
                    updates={
                        "error_message": error
                    }
                ))
            else:
                asyncio.run(self.update_job_status(
                    job_id=job_id,
                    status=BulkJobStatus.FAILED,
                    updates={
                        "error_message": error
                    }
                ))
        except Exception as e:
            logger.error(f"Error failing job {job_id}: {e}")
    
    def store_export_info(self, export_id: str, export_info: Dict[str, Any]):
        """Store export information (sync version)."""
        self.exports[export_id] = export_info
        logger.info(f"Stored export info for {export_id} in local storage")
    
    def get_export_info(self, export_id: str) -> Optional[Dict[str, Any]]:
        """Get export information (sync version)."""
        export_info = self.exports.get(export_id)
        if export_info:
            logger.info(f"Retrieved export info for {export_id} from local storage")
        else:
            logger.warning(f"Export info for {export_id} not found in local storage")
        return export_info
    
    def _filter_metadata_for_schema(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        Filter metadata to only include fields allowed by the Convex schema.
        
        Args:
            metadata: Raw metadata dictionary
            
        Returns:
            Filtered metadata dictionary with only schema-allowed fields
        """
        if not metadata:
            return {}
            
        # Only include fields that are defined in the Convex schema
        allowed_fields = {
            "content_processed", "embeddings", "environment", "error", 
            "progress", "stage", "stageNames", "status", "totalStages"
        }
        
        filtered = {}
        for key, value in metadata.items():
            if key in allowed_fields:
                # Ensure stage is always a string, not an object
                if key == "stage" and isinstance(value, dict):
                    # If stage is an object, extract the stage string from it
                    filtered[key] = value.get("stage", str(value))
                elif key == "stage":
                    filtered[key] = str(value)
                else:
                    filtered[key] = value
        
        return filtered


# Singleton instance
bulk_job_manager = BulkJobManager()


================================================
FILE: BULK_JOB_MANAGER_USAGE.md
================================================
# Bulk Job Manager Usage Guide

The Bulk Job Manager provides a robust system for managing complex multi-stage bulk operations with progress tracking, export functionality, and resource management.

## Key Features

- **Multi-stage processing** with detailed progress tracking
- **Export management** with multiple formats (JSON, CSV, ZIP)
- **Rate limiting** integration for user protection
- **Resource cleanup** and temporary file management
- **Convex webhook integration** for real-time updates
- **Background task coordination** with existing systems

## Basic Usage

### 1. Create a Bulk Job

```python
from src.services.bulk_job_manager import bulk_job_manager, BulkJobStage

# Define the stages for your bulk operation
stages = [
    BulkJobStage.INITIALIZATION,
    BulkJobStage.CONTENT_FETCH,
    BulkJobStage.CONTENT_PROCESSING,
    BulkJobStage.EXPORT_PREPARATION,
    BulkJobStage.EXPORT_GENERATION
]

# Create the job
job_id = await bulk_job_manager.create_bulk_job(
    job_type="bulk_tiktok_download",
    user_id="user123",
    job_data={
        "username": "example_user",
        "video_count": 50,
        "total_items": 50,
        "export_format": "zip"
    },
    stages=stages,
    estimated_duration_minutes=15,
    priority="normal"
)
```

### 2. Process Through Stages

```python
# Start a stage
await bulk_job_manager.start_stage(job_id, BulkJobStage.CONTENT_FETCH, items_total=50)

# Update progress during processing
for i, item in enumerate(items_to_process):
    # Process item...
    progress = (i + 1) / len(items_to_process)
    await bulk_job_manager.update_stage_progress(
        job_id, 
        BulkJobStage.CONTENT_FETCH, 
        progress,
        items_completed=i + 1
    )

# Complete the stage
await bulk_job_manager.complete_stage(
    job_id, 
    BulkJobStage.CONTENT_FETCH,
    items_completed=len(items_to_process)
)
```

### 3. Handle Errors

```python
try:
    # Process stage...
    pass
except Exception as e:
    await bulk_job_manager.fail_stage(
        job_id, 
        BulkJobStage.CONTENT_FETCH, 
        str(e),
        metadata={"error_type": "network_error"}
    )
```

### 4. Create Exports

```python
# Export as JSON
export_id = await bulk_job_manager.create_export(
    job_id,
    export_format="json",
    data=processed_data,
    filename_prefix="tiktok_content"
)

# Export as ZIP with multiple files
export_data = {
    "metadata.json": metadata,
    "content.json": content_data,
    "transcripts.txt": transcripts
}
export_id = await bulk_job_manager.create_export(
    job_id,
    export_format="zip",
    data=export_data,
    filename_prefix="bulk_export"
)
```

## Integration Examples

### TikTok Bulk Download

```python
async def process_tiktok_bulk_download(job_id: str, job_data: Dict[str, Any]):
    """Example: Bulk TikTok video download with stages"""
    
    # Define stage processors
    stage_processors = {
        BulkJobStage.INITIALIZATION: initialize_tiktok_download,
        BulkJobStage.CONTENT_FETCH: fetch_tiktok_videos,
        BulkJobStage.CONTENT_PROCESSING: process_video_content,
        BulkJobStage.EXPORT_PREPARATION: prepare_export_data,
        BulkJobStage.EXPORT_GENERATION: generate_export_files
    }
    
    # Process through all stages
    await bulk_job_manager.process_job_with_stages(job_id, stage_processors)

async def fetch_tiktok_videos(job_id: str, job_data: Dict[str, Any]):
    """Fetch TikTok videos stage"""
    username = job_data["username"]
    video_count = job_data["video_count"]
    
    # Get TikTok service
    tiktok_service = get_tiktok_service()
    
    # Fetch videos with progress tracking
    videos = []
    for i in range(video_count):
        video = await tiktok_service.get_video(username, i)
        videos.append(video)
        
        # Update progress
        progress = (i + 1) / video_count
        await bulk_job_manager.update_stage_progress(
            job_id, 
            BulkJobStage.CONTENT_FETCH, 
            progress,
            items_completed=i + 1
        )
    
    # Store results in job data
    await bulk_job_manager.update_job_status(job_id, BulkJobStatus.PROCESSING, {
        "fetchedVideos": videos
    })
```

### Audio Transcription Bulk

```python
async def process_bulk_transcription(job_id: str, job_data: Dict[str, Any]):
    """Example: Bulk audio transcription"""
    
    files = job_data["audio_files"]
    
    # Initialize transcription stage
    await bulk_job_manager.start_stage(
        job_id, 
        BulkJobStage.TRANSCRIPTION, 
        items_total=len(files)
    )
    
    transcripts = []
    for i, file_path in enumerate(files):
        try:
            # Process file
            transcript = await transcribe_audio_file(file_path)
            transcripts.append({
                "file": file_path,
                "transcript": transcript,
                "status": "completed"
            })
            
            # Update progress
            progress = (i + 1) / len(files)
            await bulk_job_manager.update_stage_progress(
                job_id, 
                BulkJobStage.TRANSCRIPTION, 
                progress,
                items_completed=i + 1
            )
            
        except Exception as e:
            transcripts.append({
                "file": file_path,
                "error": str(e),
                "status": "failed"
            })
            
            await bulk_job_manager.update_stage_progress(
                job_id, 
                BulkJobStage.TRANSCRIPTION, 
                progress,
                items_completed=i + 1,
                items_failed=1
            )
    
    # Complete stage
    await bulk_job_manager.complete_stage(
        job_id, 
        BulkJobStage.TRANSCRIPTION,
        items_completed=len([t for t in transcripts if t["status"] == "completed"]),
        items_failed=len([t for t in transcripts if t["status"] == "failed"])
    )
    
    # Create export
    export_id = await bulk_job_manager.create_export(
        job_id,
        export_format="json",
        data=transcripts,
        filename_prefix="bulk_transcripts"
    )
```

## API Integration

### FastAPI Background Task

```python
from fastapi import BackgroundTasks
from src.services.bulk_job_manager import bulk_job_manager

@router.post("/bulk-process")
async def start_bulk_process(
    request: BulkProcessRequest,
    background_tasks: BackgroundTasks
):
    """Start a bulk processing job"""
    
    # Create job
    job_id = await bulk_job_manager.create_bulk_job(
        job_type=request.job_type,
        user_id=request.user_id,
        job_data=request.job_data,
        stages=request.stages
    )
    
    # Process in background
    background_tasks.add_task(process_bulk_job, job_id)
    
    return {"job_id": job_id, "status": "started"}

@router.get("/bulk-jobs/{job_id}/status")
async def get_job_status(job_id: str):
    """Get job status and progress"""
    job = await bulk_job_manager.get_job_status(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")
    return job

@router.get("/bulk-jobs/{job_id}/export/{export_id}")
async def download_export(job_id: str, export_id: str):
    """Download export file"""
    file_path = await bulk_job_manager.get_export_file_path(export_id)
    if not file_path:
        raise HTTPException(status_code=404, detail="Export not found or expired")
    
    return FileResponse(file_path)
```

## Configuration

### Environment Variables

```bash
# Export configuration
BULK_EXPORT_PATH=/tmp/bulk_exports          # Export files location
BULK_EXPORT_RETENTION_HOURS=24              # How long to keep exports
BULK_MAX_EXPORT_SIZE_MB=500                 # Maximum export file size

# Rate limiting
BULK_RATE_LIMIT_WINDOW=60                   # Rate limit window in minutes
BULK_RATE_LIMIT_MAX_JOBS=5                  # Max jobs per window per user

# Convex configuration
CONVEX_URL=https://your-convex-deployment.convex.cloud
NEXT_PUBLIC_CONVEX_URL=https://your-convex-deployment.convex.cloud
```

## Convex Schema Requirements

The bulk job manager requires these Convex mutations and queries:

### Mutations
- `bulkJobs:create` - Create new bulk job
- `bulkJobs:updateStatus` - Update job status
- `bulkJobs:updateStageProgress` - Update stage progress
- `bulkJobs:startStage` - Start a stage
- `bulkJobs:completeStage` - Complete a stage
- `bulkJobs:failStage` - Fail a stage
- `bulkJobs:addExport` - Add export to job
- `bulkJobs:removeExport` - Remove export

### Queries
- `bulkJobs:getJob` - Get job by ID
- `bulkJobs:getUserJobs` - Get user's jobs
- `bulkJobs:getUserRecentJobs` - Get recent jobs for rate limiting
- `bulkJobs:getExport` - Get export info
- `bulkJobs:getExpiredExports` - Get expired exports for cleanup

## Best Practices

1. **Always handle errors** at the stage level to prevent job failures
2. **Update progress frequently** for better user experience
3. **Use appropriate stage granularity** - not too fine, not too coarse
4. **Clean up resources** after processing
5. **Set realistic time estimates** for user expectations
6. **Use metadata** to store intermediate results and debug info
7. **Implement proper rate limiting** to protect system resources

## Error Handling

```python
try:
    # Stage processing
    await process_stage_data()
    await bulk_job_manager.complete_stage(job_id, stage)
except ValidationError as e:
    # Handle validation errors
    await bulk_job_manager.fail_stage(
        job_id, stage, f"Validation failed: {str(e)}"
    )
except NetworkError as e:
    # Handle network errors with retry info
    await bulk_job_manager.fail_stage(
        job_id, stage, str(e), 
        metadata={"retry_after": 300, "error_type": "network"}
    )
except Exception as e:
    # Handle unexpected errors
    await bulk_job_manager.fail_stage(
        job_id, stage, f"Unexpected error: {str(e)}"
    )
```

## Monitoring and Cleanup

```python
# Periodic cleanup of expired exports
async def cleanup_expired_exports():
    """Clean up expired exports (run periodically)"""
    await bulk_job_manager.cleanup_expired_exports()

# Job statistics for monitoring
async def get_job_statistics():
    """Get job processing statistics"""
    return await bulk_job_manager.get_job_stats()
```


================================================
FILE: bulk_processing_service.py
================================================
"""
Bulk Processing Service

A comprehensive orchestrator for processing large volumes of content through
the complete pipeline: Content → Audio Processing → Transcription → Embedding → Export.

This service coordinates all processing steps and manages the workflow state,
error handling, and resource cleanup for bulk content operations.
"""

import os
import json
import logging
import asyncio
import tempfile
import uuid
import threading
from typing import Dict, Any, List, Optional, Union, Tuple
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum
import aiofiles

from src.services.tiktok_service import get_tiktok_service
from src.services.audio_preparation_service import audio_preparation_service
from src.services.gemini.embeddings_service import GeminiEmbeddingsService
from src.services.jina.embeddings_service import JinaEmbeddingsService
from src.services.gemini.config import GeminiConfig
from src.services.jina.config import JinaConfig
from src.services.vector_db_connectors import (
    VectorDBType, VectorExportConfig, VectorRecord, 
    VectorDBConnectorFactory, VectorExportManager
)

logger = logging.getLogger(__name__)


class ProcessingStatus(Enum):
    """Processing status enumeration"""
    PENDING = "pending"
    INITIALIZING = "initializing"
    FETCHING_CONTENT = "fetching_content"
    PROCESSING_AUDIO = "processing_audio"
    TRANSCRIBING = "transcribing"
    EMBEDDING = "embedding"
    FINALIZING = "finalizing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class ContentType(Enum):
    """Supported content types"""
    TIKTOK = "tiktok"
    YOUTUBE = "youtube"
    TWITCH = "twitch"
    AUDIO_FILE = "audio_file"
    VIDEO_FILE = "video_file"


@dataclass
class ProcessingConfig:
    """Configuration for bulk processing operations"""
    
    # Content settings
    content_type: ContentType
    max_items: int = 25
    audio_format: str = "wav"
    
    # Audio processing settings
    enable_audio_processing: bool = True
    enable_transcription: bool = True
    enable_embedding: bool = True
    
    # Transcription settings
    whisper_model: str = "base"
    segment_audio: bool = True
    max_segment_duration: int = 30
    clean_silence: bool = True
    separate_voices: bool = True
    
    # Embedding settings
    embedding_provider: str = "jina"  # "jina" or "gemini"
    embedding_task_type: str = "RETRIEVAL_DOCUMENT"
    chunk_size: int = 1000
    chunk_overlap: int = 200
    
    # JINA V4 specific settings for transcript processing
    jina_v4_task: str = "retrieval.passage"  # Optimal for transcript content
    jina_v4_dimensions: int = 1024  # Embedding dimensions (128-2048)
    jina_v4_late_chunking: bool = True  # Enable late chunking for long transcripts
    jina_v4_multi_vector: bool = False  # Single vector embeddings
    jina_v4_optimize_for_rag: bool = True  # Optimize for RAG systems
    jina_v4_truncate_at_max: bool = True  # Safer for production
    
    # Export settings
    export_formats: List[str] = None
    include_metadata: bool = True
    include_analytics: bool = True
    
    # Processing settings
    max_concurrent_items: int = 3
    retry_attempts: int = 3
    timeout_seconds: int = 300
    
    def __post_init__(self):
        if self.export_formats is None:
            self.export_formats = ["json", "csv", "jsonl"]


@dataclass
class ProcessingResult:
    """Result of processing a single content item"""
    
    item_id: str
    content_type: ContentType
    status: ProcessingStatus
    
    # Original content metadata
    original_metadata: Dict[str, Any] = None
    
    # Audio processing results
    audio_path: Optional[str] = None
    audio_duration: Optional[float] = None
    audio_format: Optional[str] = None
    vocals_extracted: bool = False
    
    # Transcription results
    transcription: Optional[str] = None
    segments: List[Dict[str, Any]] = None
    language: Optional[str] = None
    
    # Embedding results
    embeddings: List[List[float]] = None
    chunks: List[Dict[str, Any]] = None
    
    # V4 Embedding metadata
    embedding_provider: Optional[str] = None
    embedding_dimensions: Optional[int] = None
    processing_method: Optional[str] = None  # 'v4_transcript', 'late_chunking', 'traditional_chunking'
    token_count: Optional[int] = None
    v4_optimized: bool = False
    v4_task_type: Optional[str] = None
    
    # Processing metadata
    processing_time: float = 0.0
    error_message: Optional[str] = None
    retry_count: int = 0
    
    def __post_init__(self):
        if self.segments is None:
            self.segments = []
        if self.embeddings is None:
            self.embeddings = []
        if self.chunks is None:
            self.chunks = []


@dataclass
class BulkProcessingState:
    """State management for bulk processing operations"""
    
    session_id: str
    status: ProcessingStatus
    config: ProcessingConfig
    
    # Progress tracking
    total_items: int = 0
    completed_items: int = 0
    failed_items: int = 0
    
    # Results storage
    results: List[ProcessingResult] = None
    
    # Real-time progress tracking (thread-safe)
    _processing_results: List[ProcessingResult] = None
    _embeddings_count: int = 0
    _lock: threading.Lock = None
    
    # Timing
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    
    # Error handling
    errors: List[Dict[str, Any]] = None
    
    def __post_init__(self):
        if self.results is None:
            self.results = []
        if self.errors is None:
            self.errors = []
        if self._processing_results is None:
            self._processing_results = []
        if self._lock is None:
            self._lock = threading.Lock()
    
    def add_completed_item(self, result: ProcessingResult):
        """Thread-safe method to add a completed processing result"""
        with self._lock:
            self._processing_results.append(result)
            self.completed_items += 1
            # Count embeddings if they exist - embeddings is List[List[float]]
            if hasattr(result, 'embeddings') and result.embeddings:
                if isinstance(result.embeddings, list):
                    # Each item in embeddings list is a vector (List[float])
                    self._embeddings_count += len(result.embeddings)
                else:
                    self._embeddings_count += 1
    
    def get_real_time_stats(self) -> Dict[str, int]:
        """Get real-time statistics in a thread-safe way"""
        with self._lock:
            return {
                "completed_items": self.completed_items,
                "embeddings_count": self._embeddings_count,
                "failed_items": self.failed_items
            }
    
    @property
    def progress_percentage(self) -> float:
        """Calculate completion percentage"""
        if self.total_items == 0:
            return 0.0
        return (self.completed_items / self.total_items) * 100.0
    
    @property
    def processing_time(self) -> float:
        """Calculate total processing time in seconds"""
        if self.start_time is None:
            return 0.0
        end = self.end_time or datetime.now()
        return (end - self.start_time).total_seconds()


class BulkProcessingService:
    """
    Master orchestrator for bulk content processing.
    
    Coordinates the complete workflow:
    1. Content ingestion from various sources
    2. Audio extraction and preparation
    3. Transcription with Whisper
    4. Embedding generation
    5. Export preparation in multiple formats
    """
    
    def __init__(self):
        """Initialize the bulk processing service"""
        self.temp_dir = tempfile.mkdtemp(prefix="bulk_processing_")
        self.active_sessions: Dict[str, BulkProcessingState] = {}
        
        # Initialize service clients
        self.tiktok_service = get_tiktok_service()
        self.audio_service = audio_preparation_service
        
        # Initialize embedding services
        self.gemini_service = None
        self.jina_service = None
        
        logger.info(f"Bulk Processing Service initialized - temp_dir: {self.temp_dir}")
    
    def _get_embedding_service(self, provider: str):
        """Get or create embedding service instance"""
        # Normalize provider names - map jina-v4 to jina
        normalized_provider = provider
        if provider == "jina-v4":
            normalized_provider = "jina"
            logger.info(f"Mapped embedding provider '{provider}' to '{normalized_provider}'")
        
        if normalized_provider == "gemini":
            if self.gemini_service is None:
                try:
                    self.gemini_service = GeminiEmbeddingsService(GeminiConfig())
                except Exception as e:
                    logger.error(f"Failed to initialize Gemini service: {e}")
                    raise
            return self.gemini_service
        elif normalized_provider == "jina":
            if self.jina_service is None:
                try:
                    self.jina_service = JinaEmbeddingsService(JinaConfig())
                except Exception as e:
                    logger.error(f"Failed to initialize Jina service: {e}")
                    raise
            return self.jina_service
        else:
            raise ValueError(f"Unsupported embedding provider: {provider}")
    
    async def process_bulk_content_original(
        self,
        content_source: Union[str, List[str]],
        config: ProcessingConfig
    ) -> str:
        """
        Original processing method for bulk content (kept for backward compatibility).
        
        Args:
            content_source: Username, URL, or list of content identifiers
            config: Processing configuration
            
        Returns:
            Session ID for tracking progress
        """
        session_id = str(uuid.uuid4())
        state = BulkProcessingState(
            session_id=session_id,
            status=ProcessingStatus.INITIALIZING,
            config=config,
            start_time=datetime.now()
        )
        
        self.active_sessions[session_id] = state
        logger.info(f"Starting bulk processing session: {session_id}")
        
        try:
            # Step 1: Content ingestion
            await self._update_status(session_id, ProcessingStatus.FETCHING_CONTENT)
            content_items = await self._ingest_content(content_source, config)
            
            state.total_items = len(content_items)
            logger.info(f"Ingested {len(content_items)} content items")
            
            # Step 2: Process items with concurrency control
            await self._process_content_items(session_id, content_items)
            
            # Step 3: Finalize results
            await self._update_status(session_id, ProcessingStatus.FINALIZING)
            await self._finalize_processing(session_id)
            
            await self._update_status(session_id, ProcessingStatus.COMPLETED)
            state.end_time = datetime.now()
            
            logger.info(f"Bulk processing completed for session {session_id}")
            return session_id
            
        except Exception as e:
            logger.error(f"Bulk processing failed for session {session_id}: {e}")
            await self._update_status(session_id, ProcessingStatus.FAILED)
            state.errors.append({
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "stage": "main_processing"
            })
            state.end_time = datetime.now()
            raise
    
    async def process_bulk_content(
        self,
        config: Dict[str, Any],
        job_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        Process bulk content with new API signature for job manager integration.
        
        Args:
            config: Processing configuration dictionary
            job_id: Job ID for tracking
            progress_callback: Callback function for progress updates
            
        Returns:
            Processing results
        """
        try:
            logger.info(f"Starting bulk processing for job {job_id}")
            
            # Extract content selection from config
            selected_content = config.get("selected_content", [])
            platform = config.get("platform", "tiktok")
            
            # Build processing config
            content_type = ContentType.TIKTOK if platform == "tiktok" else ContentType.AUDIO_FILE
            
            # Extract embedding model config
            embedding_model = config.get("embedding_model", {})
            settings = config.get("settings", {})
            
            processing_config = ProcessingConfig(
                content_type=content_type,
                max_items=len(selected_content),
                max_concurrent_items=3,  # Reasonable default
                separate_voices=True,
                embedding_provider=embedding_model.get("id", "jina-v4"),
                chunk_size=settings.get("chunkSize", 1024),
                chunk_overlap=settings.get("chunkOverlap", 100),
                # JINA V4 specific parameters from API request
                jina_v4_task=embedding_model.get("jina_v4_task", "retrieval.passage"),
                jina_v4_dimensions=embedding_model.get("jina_v4_dimensions", 1024),
                jina_v4_late_chunking=embedding_model.get("jina_v4_late_chunking", True),
                jina_v4_multi_vector=embedding_model.get("jina_v4_multi_vector", False),
                jina_v4_optimize_for_rag=embedding_model.get("jina_v4_optimize_for_rag", True),
                jina_v4_truncate_at_max=embedding_model.get("jina_v4_truncate_at_max", True)
            )
            
            # Start processing session
            session_id = await self.process_bulk_content_legacy(selected_content, processing_config, progress_callback)
            
            # Wait for completion and return results
            state = self.active_sessions.get(session_id)
            if state:
                # Return formatted results
                return {
                    "session_id": session_id,
                    "status": state.status.value,
                    "total_items": state.total_items,
                    "completed_items": state.completed_items,
                    "failed_items": state.failed_items,
                    "results": [self._serialize_processing_result(result) for result in state.results if isinstance(result, ProcessingResult)] if state.results else [],
                    "processing_time": state.processing_time,
                    "progress": state.progress_percentage
                }
            else:
                raise Exception("Processing session not found")
                
        except Exception as e:
            logger.error(f"Error in process_bulk_content for job {job_id}: {e}")
            if progress_callback:
                try:
                    await progress_callback({
                        "progress": 0,
                        "stage": "failed",
                        "status": "failed",
                        "error": str(e)
                    })
                except Exception as callback_error:
                    logger.warning(f"Progress callback failed during error reporting: {callback_error}")
            raise
    
    async def process_bulk_content_legacy(
        self,
        content_source: Union[str, List[str]], 
        config: ProcessingConfig,
        progress_callback: Optional[callable] = None
    ) -> str:
        """
        Legacy processing method (renamed from original process_bulk_content).
        
        Args:
            content_source: Username, URL, or list of content identifiers
            config: Processing configuration
            progress_callback: Optional callback for progress updates
            
        Returns:
            Session ID for tracking progress
        """
        session_id = str(uuid.uuid4())
        state = BulkProcessingState(
            session_id=session_id,
            status=ProcessingStatus.INITIALIZING,
            config=config,
            start_time=datetime.now()
        )
        
        self.active_sessions[session_id] = state
        logger.info(f"Starting bulk processing session: {session_id}")
        
        try:
            # Progress callback support
            async def send_progress(stage: str, progress: float = 0.0):
                if progress_callback:
                    # Ensure stage is always a string
                    stage_str = str(stage) if stage is not None else "Processing..."
                    real_time_stats = state.get_real_time_stats()
                    await progress_callback({
                        "progress": progress,
                        "stage": stage_str,
                        "status": "processing",
                        "content_processed": real_time_stats["completed_items"],
                        "embeddings": real_time_stats["embeddings_count"]
                    })
            
            # Step 1: Content ingestion
            await self._update_status(session_id, ProcessingStatus.FETCHING_CONTENT)
            await send_progress("Fetching content", 10)
            
            content_items = await self._ingest_content(content_source, config)
            
            state.total_items = len(content_items)
            logger.info(f"Ingested {len(content_items)} content items")
            await send_progress("Content ingested", 20)
            
            # Step 2: Process items with concurrency control
            await send_progress("Processing content", 30)
            await self._process_content_items_with_progress(session_id, content_items, send_progress)
            
            # Step 3: Finalize results
            await self._update_status(session_id, ProcessingStatus.FINALIZING)
            await send_progress("Finalizing", 90)
            await self._finalize_processing(session_id)
            
            await self._update_status(session_id, ProcessingStatus.COMPLETED)
            await send_progress("Completed", 100)
            state.end_time = datetime.now()
            
            logger.info(f"Bulk processing completed for session {session_id}")
            return session_id
            
        except Exception as e:
            logger.error(f"Bulk processing failed for session {session_id}: {e}")
            await self._update_status(session_id, ProcessingStatus.FAILED)
            if progress_callback:
                await progress_callback({
                    "progress": 0,
                    "stage": "failed",
                    "status": "failed",
                    "error": str(e)
                })
            raise
    
    async def _process_content_items_with_progress(
        self,
        session_id: str,
        content_items: List[Dict[str, Any]],
        progress_callback: callable
    ) -> None:
        """
        Process content items with progress updates.
        """
        state = self.active_sessions[session_id]
        
        # Create semaphore for concurrency control
        semaphore = asyncio.Semaphore(state.config.max_concurrent_items)
        
        async def process_single_item_with_progress(item: Dict[str, Any], index: int) -> ProcessingResult:
            async with semaphore:
                try:
                    result = await self._process_single_content_item(session_id, item)
                    
                    # Add completed item to real-time tracking
                    if result and not isinstance(result, Exception):
                        state.add_completed_item(result)
                    else:
                        # Handle failed item
                        with state._lock:
                            state.failed_items += 1
                except Exception as e:
                    # Handle processing exception
                    with state._lock:
                        state.failed_items += 1
                    logger.error(f"Error processing item {index + 1}: {e}")
                    result = e
                
                # Update progress with error handling
                try:
                    progress = 30 + (60 * (index + 1) / len(content_items))
                    real_time_stats = state.get_real_time_stats()
                    
                    await progress_callback({
                        "progress": progress,
                        "stage": f"Processing item {index + 1}/{len(content_items)}",
                        "status": "processing",
                        "content_processed": index + 1,
                        "embeddings": real_time_stats["embeddings_count"],
                        "failed_items": real_time_stats["failed_items"],
                        "completed_items": real_time_stats["completed_items"]
                    })
                except Exception as callback_error:
                    logger.warning(f"Progress callback failed: {callback_error}")
                    # Continue processing even if callback fails
                
                return result
        
        # Process all items with progress tracking
        tasks = [
            process_single_item_with_progress(item, i) 
            for i, item in enumerate(content_items)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results - note: completed_items and failed_items are already tracked in real-time
        for result in results:
            if isinstance(result, Exception):
                state.errors.append({
                    "error": str(result),
                    "timestamp": datetime.now().isoformat()
                })
            else:
                state.results.append(result)
        
        # Copy real-time results to final results for consistency
        with state._lock:
            # Ensure final state matches real-time tracking
            state.results.extend([r for r in state._processing_results if r not in state.results])
    
    async def _ingest_content(
        self,
        content_source: Union[str, List[str]],
        config: ProcessingConfig
    ) -> List[Dict[str, Any]]:
        """
        Ingest content from various sources.
        
        Args:
            content_source: Content source identifier
            config: Processing configuration
            
        Returns:
            List of content items to process
        """
        content_items = []
        
        if config.content_type == ContentType.TIKTOK:
            if isinstance(content_source, str):
                # Single TikTok user
                user_videos = await self.tiktok_service.get_user_videos(
                    content_source, 
                    count=config.max_items
                )
                
                for video in user_videos.get("videos", []):
                    content_items.append({
                        "id": video["videoId"],
                        "type": "tiktok_video",
                        "source": content_source,
                        "metadata": video,
                        "url": video.get("playAddr", "")
                    })
            else:
                # Multiple sources or video IDs
                for source in content_source[:config.max_items]:
                    if source.startswith("http"):
                        # Direct video URL
                        video_id = self._extract_video_id_from_url(source)
                        if video_id:
                            video_info = await self.tiktok_service.get_video_info(video_id)
                            content_items.append({
                                "id": video_id,
                                "type": "tiktok_video",
                                "source": source,
                                "metadata": video_info,
                                "url": source
                            })
                    elif source.isdigit() and len(source) > 10:
                        # This looks like a TikTok video ID (long numeric string)
                        # Process it directly instead of trying to fetch from user
                        logger.info(f"Processing pre-selected TikTok video ID: {source}")
                        try:
                            video_info = await self.tiktok_service.get_video_info(source)
                            content_items.append({
                                "id": source,
                                "type": "tiktok_video",
                                "source": f"video_{source}",
                                "metadata": video_info or {"videoId": source},
                                "url": f"https://www.tiktok.com/@unknown/video/{source}"
                            })
                        except Exception as e:
                            logger.warning(f"Could not get video info for {source}, using basic metadata: {e}")
                            # Still add the item with basic metadata so processing can continue
                            content_items.append({
                                "id": source,
                                "type": "tiktok_video",
                                "source": f"video_{source}",
                                "metadata": {"videoId": source, "title": f"TikTok Video {source}"},
                                "url": f"https://www.tiktok.com/@unknown/video/{source}"
                            })
                    else:
                        # Assume it's a username
                        user_videos = await self.tiktok_service.get_user_videos(
                            source, 
                            count=min(config.max_items // len(content_source), 10)
                        )
                        
                        for video in user_videos.get("videos", []):
                            content_items.append({
                                "id": video["videoId"],
                                "type": "tiktok_video",
                                "source": source,
                                "metadata": video,
                                "url": video.get("playAddr", "")
                            })
        
        elif config.content_type == ContentType.AUDIO_FILE:
            # Direct audio file processing
            if isinstance(content_source, str):
                content_items.append({
                    "id": str(uuid.uuid4()),
                    "type": "audio_file",
                    "source": content_source,
                    "metadata": {"file_path": content_source},
                    "url": content_source
                })
            else:
                for file_path in content_source[:config.max_items]:
                    content_items.append({
                        "id": str(uuid.uuid4()),
                        "type": "audio_file",
                        "source": file_path,
                        "metadata": {"file_path": file_path},
                        "url": file_path
                    })
        
        else:
            raise ValueError(f"Unsupported content type: {config.content_type}")
        
        return content_items
    
    async def _process_content_items(
        self,
        session_id: str,
        content_items: List[Dict[str, Any]]
    ) -> None:
        """
        Process content items with controlled concurrency.
        
        Args:
            session_id: Processing session ID
            content_items: List of content items to process
        """
        state = self.active_sessions[session_id]
        
        # Create semaphore for concurrency control
        semaphore = asyncio.Semaphore(state.config.max_concurrent_items)
        
        async def process_single_item(item: Dict[str, Any]) -> ProcessingResult:
            async with semaphore:
                return await self._process_single_content_item(session_id, item)
        
        # Process items concurrently
        tasks = [process_single_item(item) for item in content_items]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Update state with results
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Processing error: {result}")
                state.failed_items += 1
                state.errors.append({
                    "error": str(result),
                    "timestamp": datetime.now().isoformat(),
                    "stage": "item_processing"
                })
            else:
                state.results.append(result)
                if result.status == ProcessingStatus.COMPLETED:
                    state.completed_items += 1
                else:
                    state.failed_items += 1
    
    async def _process_single_content_item(
        self,
        session_id: str,
        item: Dict[str, Any]
    ) -> ProcessingResult:
        """
        Process a single content item through the complete pipeline.
        
        Args:
            session_id: Processing session ID
            item: Content item to process
            
        Returns:
            ProcessingResult with processing outcomes
        """
        state = self.active_sessions[session_id]
        config = state.config
        
        result = ProcessingResult(
            item_id=item["id"],
            content_type=config.content_type,
            status=ProcessingStatus.PENDING,
            original_metadata=item.get("metadata", {})
        )
        
        start_time = datetime.now()
        
        try:
            # Step 1: Audio extraction/preparation
            if config.enable_audio_processing:
                await self._update_status(session_id, ProcessingStatus.PROCESSING_AUDIO)
                audio_result = await self._process_audio(item, config)
                
                result.audio_path = audio_result.get("audio_path")
                result.audio_duration = audio_result.get("duration")
                result.audio_format = audio_result.get("format")
                result.vocals_extracted = audio_result.get("vocals_extracted", False)
            
            # Step 2: Transcription
            if config.enable_transcription and result.audio_path:
                await self._update_status(session_id, ProcessingStatus.TRANSCRIBING)
                transcription_result = await self._process_transcription(
                    result.audio_path, 
                    config
                )
                
                result.transcription = transcription_result.get("transcription")
                result.segments = transcription_result.get("segments", [])
                result.language = transcription_result.get("language")
            
            # Step 3: Embedding generation
            if config.enable_embedding and result.transcription:
                await self._update_status(session_id, ProcessingStatus.EMBEDDING)
                embedding_result = await self._process_embeddings(
                    result.transcription,
                    config
                )
                
                result.embeddings = embedding_result.get("embeddings", [])
                result.chunks = embedding_result.get("chunks", [])
                
                # Populate V4 metadata
                result.embedding_provider = embedding_result.get("provider", config.embedding_provider)
                result.processing_method = embedding_result.get("processing_method", "unknown")
                result.token_count = embedding_result.get("token_count")
                result.v4_optimized = embedding_result.get("v4_optimized", False)
                result.embedding_dimensions = embedding_result.get("dimensions")
                if config.embedding_provider == "jina":
                    result.v4_task_type = config.jina_v4_task
            
            result.status = ProcessingStatus.COMPLETED
            result.processing_time = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"Successfully processed item {item['id']}")
            return result
            
        except Exception as e:
            logger.error(f"Error processing item {item['id']}: {e}")
            result.status = ProcessingStatus.FAILED
            result.error_message = str(e)
            result.processing_time = (datetime.now() - start_time).total_seconds()
            return result
    
    async def _process_audio(
        self,
        item: Dict[str, Any],
        config: ProcessingConfig
    ) -> Dict[str, Any]:
        """
        Process audio for a content item.
        
        Args:
            item: Content item
            config: Processing configuration
            
        Returns:
            Audio processing results
        """
        if item["type"] == "tiktok_video":
            # Download TikTok video audio
            video_id = item["id"]
            audio_bytes = await self.tiktok_service.download_audio_bytes(
                video_id, 
                format=config.audio_format
            )
            
            # Save to temporary file
            audio_path = os.path.join(self.temp_dir, f"{video_id}.{config.audio_format}")
            async with aiofiles.open(audio_path, 'wb') as f:
                await f.write(audio_bytes)
            
            return {
                "audio_path": audio_path,
                "format": config.audio_format,
                "duration": item.get("metadata", {}).get("duration", 0),
                "vocals_extracted": False
            }
        
        elif item["type"] == "audio_file":
            # Use existing audio file
            return {
                "audio_path": item["url"],
                "format": Path(item["url"]).suffix.lstrip('.'),
                "duration": 0,  # TODO: Extract duration
                "vocals_extracted": False
            }
        
        else:
            raise ValueError(f"Unsupported item type for audio processing: {item['type']}")
    
    async def _process_transcription(
        self,
        audio_path: str,
        config: ProcessingConfig
    ) -> Dict[str, Any]:
        """
        Process transcription for audio content.
        
        Args:
            audio_path: Path to audio file
            config: Processing configuration
            
        Returns:
            Transcription results
        """
        prep_config = {
            "use_whisper": True,
            "segment_audio": config.segment_audio,
            "max_segment_duration": config.max_segment_duration,
            "transcribe": True,
            "clean_silence": config.clean_silence,
            "separate_voices": config.separate_voices,
            "provider_specific": {
                "whisper_model": config.whisper_model
            }
        }
        
        result = await self.audio_service.prepare_audio(
            audio_path,
            provider="transcription",
            config=prep_config
        )
        
        return {
            "transcription": result.get("transcription"),
            "segments": result.get("segments", []),
            "language": result.get("metadata", {}).get("language"),
            "prepared_audio_path": result.get("prepared_audio_path")
        }
    
    async def _process_embeddings(
        self,
        text: str,
        config: ProcessingConfig
    ) -> Dict[str, Any]:
        """
        Process embedding generation for text content using optimized V4 transcript processing.
        
        Args:
            text: Text content to embed (transcript)
            config: Processing configuration
            
        Returns:
            Embedding results with V4 optimizations
        """
        embedding_service = self._get_embedding_service(config.embedding_provider)
        
        # Use V4 transcript-optimized embedding for JINA (supports both "jina" and "jina-v4")
        if config.embedding_provider in ["jina", "jina-v4"] and hasattr(embedding_service, 'embed_transcripts'):
            logger.info(f"Using JINA V4 transcript-optimized embedding with task: {config.jina_v4_task}")
            
            # Create V4 transcript configuration
            from src.services.jina.models import TranscriptEmbeddingConfig
            transcript_config = TranscriptEmbeddingConfig(
                task=config.jina_v4_task,
                dimensions=config.jina_v4_dimensions,
                late_chunking=config.jina_v4_late_chunking,
                chunk_size=config.chunk_size if not config.jina_v4_late_chunking else None,
                chunk_overlap=config.chunk_overlap if not config.jina_v4_late_chunking else None,
                multi_vector=config.jina_v4_multi_vector,
                optimize_for_rag=config.jina_v4_optimize_for_rag
            )
            
            # Process with V4 transcript method
            v4_results = await embedding_service.embed_transcripts([text], transcript_config)
            
            if v4_results:
                result = v4_results[0]
                return {
                    "embeddings": result["embeddings"],
                    "chunks": result.get("chunk_metadata", []),
                    "provider": config.embedding_provider,
                    "processing_method": result.get("processing_method", "v4_transcript"),
                    "token_count": result.get("token_count", 0),
                    "dimensions": config.jina_v4_dimensions,
                    "v4_optimized": True
                }
        
        # Fallback to traditional chunking for other providers or legacy mode
        logger.info(f"Using traditional chunking for provider: {config.embedding_provider}")
        chunks = await embedding_service.chunk_and_embed(
            text,
            chunk_size=config.chunk_size,
            overlap=config.chunk_overlap
        )
        
        # Extract embeddings
        embeddings = [chunk["embedding"] for chunk in chunks]
        
        return {
            "embeddings": embeddings,
            "chunks": chunks,
            "provider": config.embedding_provider,
            "processing_method": "traditional_chunking",
            "v4_optimized": False
        }
    
    async def _finalize_processing(self, session_id: str) -> None:
        """
        Finalize processing and prepare export data.
        
        Args:
            session_id: Processing session ID
        """
        state = self.active_sessions[session_id]
        
        # Generate summary statistics
        successful_results = [r for r in state.results if r.status == ProcessingStatus.COMPLETED]
        
        summary = {
            "session_id": session_id,
            "total_items": state.total_items,
            "successful_items": len(successful_results),
            "failed_items": state.failed_items,
            "processing_time": state.processing_time,
            "average_processing_time": sum(r.processing_time for r in successful_results) / len(successful_results) if successful_results else 0,
            "total_transcription_length": sum(len(r.transcription or "") for r in successful_results),
            "total_embeddings": sum(len(r.embeddings) for r in successful_results),
            "languages_detected": list(set(r.language for r in successful_results if r.language))
        }
        
        # Store summary in state
        state.results.append(summary)
        
        logger.info(f"Processing summary: {summary}")
    
    async def prepare_export_data(
        self,
        session_id: str,
        export_format: str = "json"
    ) -> Dict[str, Any]:
        """
        Prepare processed data for export in specified format.
        
        Args:
            session_id: Processing session ID
            export_format: Export format (json, csv, jsonl)
            
        Returns:
            Export data structure
        """
        if session_id not in self.active_sessions:
            raise ValueError(f"Session {session_id} not found")
        
        state = self.active_sessions[session_id]
        
        if export_format == "json":
            return await self._prepare_json_export(state)
        elif export_format == "csv":
            return await self._prepare_csv_export(state)
        elif export_format == "jsonl":
            return await self._prepare_jsonl_export(state)
        else:
            raise ValueError(f"Unsupported export format: {export_format}")
    
    async def _prepare_json_export(self, state: BulkProcessingState) -> Dict[str, Any]:
        """Prepare JSON export format"""
        return {
            "session_metadata": {
                "session_id": state.session_id,
                "status": state.status.value,
                "config": asdict(state.config),
                "start_time": state.start_time.isoformat() if state.start_time else None,
                "end_time": state.end_time.isoformat() if state.end_time else None,
                "processing_time": state.processing_time,
                "progress": state.progress_percentage
            },
            "results": [asdict(result) for result in state.results if isinstance(result, ProcessingResult)],
            "errors": state.errors,
            "summary": state.results[-1] if state.results and isinstance(state.results[-1], dict) else {}
        }
    
    async def _prepare_csv_export(self, state: BulkProcessingState) -> Dict[str, Any]:
        """Prepare CSV export format"""
        # Flatten results for CSV
        csv_rows = []
        for result in state.results:
            if isinstance(result, ProcessingResult):
                row = {
                    "item_id": result.item_id,
                    "content_type": result.content_type.value,
                    "status": result.status.value,
                    "transcription": result.transcription,
                    "language": result.language,
                    "audio_duration": result.audio_duration,
                    "processing_time": result.processing_time,
                    "error_message": result.error_message,
                    "segments_count": len(result.segments),
                    "embeddings_count": len(result.embeddings),
                    "vocals_extracted": result.vocals_extracted
                }
                csv_rows.append(row)
        
        return {
            "format": "csv",
            "headers": list(csv_rows[0].keys()) if csv_rows else [],
            "rows": csv_rows
        }
    
    async def _prepare_jsonl_export(self, state: BulkProcessingState) -> Dict[str, Any]:
        """Prepare JSONL export format"""
        jsonl_lines = []
        for result in state.results:
            if isinstance(result, ProcessingResult):
                jsonl_lines.append(asdict(result))
        
        return {
            "format": "jsonl",
            "lines": jsonl_lines
        }
    
    def validate_processing_config(self, config: ProcessingConfig) -> Dict[str, Any]:
        """
        Validate processing configuration.
        
        Args:
            config: Processing configuration to validate
            
        Returns:
            Validation results
        """
        errors = []
        warnings = []
        
        # Check required fields
        if not config.content_type:
            errors.append("Content type is required")
        
        if config.max_items <= 0:
            errors.append("Max items must be positive")
        
        if config.max_items > 50:
            warnings.append("Processing more than 50 items may take significant time")
        
        # Check embedding provider
        if config.enable_embedding:
            if config.embedding_provider not in ["jina", "gemini"]:
                errors.append("Embedding provider must be 'jina' or 'gemini'")
            
            # Validate JINA V4 specific settings
            if config.embedding_provider == "jina":
                if config.jina_v4_dimensions not in [128, 256, 512, 1024, 2048]:
                    errors.append("JINA V4 dimensions must be one of: 128, 256, 512, 1024, 2048")
                
                valid_tasks = ["retrieval.passage", "retrieval.query", "text-matching", "code.query", "code.passage"]
                if config.jina_v4_task not in valid_tasks:
                    errors.append(f"JINA V4 task must be one of: {valid_tasks}")
                
                if config.jina_v4_late_chunking and config.chunk_size > 32000:
                    warnings.append("Large chunk sizes with late chunking may cause performance issues")
                
                if not config.jina_v4_optimize_for_rag:
                    warnings.append("Consider enabling RAG optimization for better transcript search performance")
        
        # Check export formats
        valid_formats = ["json", "csv", "jsonl"]
        for fmt in config.export_formats:
            if fmt not in valid_formats:
                errors.append(f"Invalid export format: {fmt}")
        
        return {
            "valid": len(errors) == 0,
            "errors": errors,
            "warnings": warnings
        }
    
    async def get_processing_status(self, session_id: str) -> Dict[str, Any]:
        """
        Get current processing status for a session.
        
        Args:
            session_id: Processing session ID
            
        Returns:
            Status information
        """
        if session_id not in self.active_sessions:
            raise ValueError(f"Session {session_id} not found")
        
        state = self.active_sessions[session_id]
        
        return {
            "session_id": session_id,
            "status": state.status.value,
            "progress_percentage": state.progress_percentage,
            "total_items": state.total_items,
            "completed_items": state.completed_items,
            "failed_items": state.failed_items,
            "processing_time": state.processing_time,
            "errors": state.errors[-5:] if state.errors else []  # Last 5 errors
        }
    
    async def cancel_processing(self, session_id: str) -> bool:
        """
        Cancel an active processing session.
        
        Args:
            session_id: Processing session ID
            
        Returns:
            True if cancelled successfully
        """
        if session_id not in self.active_sessions:
            return False
        
        state = self.active_sessions[session_id]
        state.status = ProcessingStatus.CANCELLED
        state.end_time = datetime.now()
        
        logger.info(f"Cancelled processing session: {session_id}")
        return True
    
    async def cleanup_temporary_files(self, session_id: Optional[str] = None) -> None:
        """
        Clean up temporary files for a session or all sessions.
        
        Args:
            session_id: Specific session to clean up (optional)
        """
        try:
            if session_id and session_id in self.active_sessions:
                state = self.active_sessions[session_id]
                
                # Clean up result files
                for result in state.results:
                    if isinstance(result, ProcessingResult) and result.audio_path:
                        if os.path.exists(result.audio_path):
                            os.unlink(result.audio_path)
                            logger.debug(f"Cleaned up audio file: {result.audio_path}")
                
                # Remove session
                del self.active_sessions[session_id]
                logger.info(f"Cleaned up session: {session_id}")
            
            else:
                # Clean up all temporary files
                import shutil
                if os.path.exists(self.temp_dir):
                    shutil.rmtree(self.temp_dir)
                    self.temp_dir = tempfile.mkdtemp(prefix="bulk_processing_")
                    logger.info("Cleaned up all temporary files")
                
                # Clear all sessions
                self.active_sessions.clear()
                
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
    
    async def _update_status(self, session_id: str, status: ProcessingStatus) -> None:
        """Update processing status for a session"""
        if session_id in self.active_sessions:
            self.active_sessions[session_id].status = status
            logger.debug(f"Session {session_id} status updated to: {status.value}")
    
    def _extract_video_id_from_url(self, url: str) -> Optional[str]:
        """Extract video ID from TikTok URL"""
        import re
        
        # TikTok video URL patterns
        patterns = [
            r'tiktok\.com/.*?/video/(\d+)',
            r'tiktok\.com/@[^/]+/video/(\d+)',
            r'vm\.tiktok\.com/([A-Za-z0-9]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        return None
    
    async def export_data(
        self,
        job_result: Dict[str, Any],
        format: str,
        export_id: str,
        vector_db_type: Optional[str] = None
    ) -> str:
        """
        Export processing results to specified format.
        
        Args:
            job_result: Processing results from completed job
            format: Export format (json, csv, parquet, vector)
            export_id: Export identifier
            vector_db_type: Vector database type for vector exports (pinecone, chromadb, weaviate)
            
        Returns:
            Path to exported file
        """
        try:
            # Create export directory
            export_dir = Path(self.temp_dir) / "exports"
            export_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if format == "json":
                export_path = export_dir / f"{export_id}_{timestamp}.json"
                async with aiofiles.open(export_path, 'w') as f:
                    await f.write(json.dumps(job_result, indent=2))
                    
            elif format == "csv":
                export_path = export_dir / f"{export_id}_{timestamp}.csv"
                
                # Convert results to CSV format
                if "results" in job_result and job_result["results"]:
                    import csv
                    async with aiofiles.open(export_path, 'w', newline='') as f:
                        writer = csv.DictWriter(f, fieldnames=job_result["results"][0].keys())
                        await f.write(','.join(writer.fieldnames) + '\n')
                        for result in job_result["results"]:
                            await f.write(','.join(str(result.get(field, '')) for field in writer.fieldnames) + '\n')
                else:
                    # Empty results
                    async with aiofiles.open(export_path, 'w') as f:
                        await f.write("No results to export\n")
                        
            elif format == "parquet":
                export_path = export_dir / f"{export_id}_{timestamp}.parquet"
                
                # For now, save as JSON (could implement proper Parquet later)
                async with aiofiles.open(export_path, 'w') as f:
                    await f.write(json.dumps(job_result, indent=2))
                    
            elif format == "vector":
                # Vector database export with import scripts and configurations
                return await self._export_vector_database_format(job_result, export_id, vector_db_type, export_dir, timestamp)
                    
            else:
                raise ValueError(f"Unsupported export format: {format}")
            
            logger.info(f"Exported data to {export_path} in {format} format")
            return str(export_path)
            
        except Exception as e:
            logger.error(f"Error exporting data: {e}")
            raise

    async def _export_vector_database_format(
        self,
        job_result: Dict[str, Any],
        export_id: str,
        vector_db_type: Optional[str],
        export_dir: Path,
        timestamp: str
    ) -> str:
        """Export data in vector database specific format with import scripts."""
        
        try:
            # Convert job results to VectorRecord format
            vector_records = []
            
            if "results" in job_result and job_result["results"]:
                for i, result in enumerate(job_result["results"]):
                    if "embeddings" in result and result["embeddings"]:
                        # Handle both single embeddings and list of embeddings
                        embeddings_data = result["embeddings"]
                        if isinstance(embeddings_data, list) and len(embeddings_data) > 0:
                            # Take the first embedding if it's a list of embeddings
                            if isinstance(embeddings_data[0], list):
                                embedding_vector = embeddings_data[0]
                            else:
                                embedding_vector = embeddings_data
                        else:
                            continue  # Skip if no valid embeddings
                        
                        # Create vector record
                        vector_record = VectorRecord(
                            id=result.get("item_id", f"{export_id}_{i}"),
                            vector=embedding_vector,
                            metadata={
                                "text": result.get("transcription", ""),
                                "content_type": result.get("content_type", ""),
                                "processing_time": result.get("processing_time", 0),
                                "language": result.get("language", "en"),
                                "audio_duration": result.get("audio_duration", 0),
                                "token_count": result.get("token_count", 0),
                                "source": result.get("original_metadata", {}).get("title", ""),
                                "embedding_provider": result.get("embedding_provider", ""),
                                "embedding_dimensions": result.get("embedding_dimensions", len(embedding_vector) if embedding_vector else 0)
                            },
                            namespace=vector_db_type or "default",
                            timestamp=datetime.now()
                        )
                        vector_records.append(vector_record)
            
            if not vector_records:
                # Fallback: create simple vector export as JSONL
                export_path = export_dir / f"{export_id}_{timestamp}.jsonl"
                async with aiofiles.open(export_path, 'w') as f:
                    await f.write('{"error": "No valid embeddings found for vector export"}\n')
                return str(export_path)
            
            # Determine vector database type
            if vector_db_type:
                try:
                    db_type = VectorDBType(vector_db_type.lower())
                except ValueError:
                    logger.warning(f"Unknown vector DB type: {vector_db_type}, using generic export")
                    db_type = None
            else:
                db_type = None
            
            if db_type:
                # Use vector database specific export
                db_export_dir = export_dir / f"{export_id}_{db_type.value}_{timestamp}"
                db_export_dir.mkdir(exist_ok=True)
                
                # Configure export
                config = VectorExportConfig(
                    output_directory=str(db_export_dir),
                    batch_size=1000,
                    include_metadata=True,
                    generate_import_script=True,
                    validate_schema=True
                )
                
                # Create connector and export
                connector = VectorDBConnectorFactory.create_connector(db_type, config)
                export_info = connector.export_vectors(vector_records)
                
                # Create a summary file
                summary_path = db_export_dir / "export_summary.json"
                async with aiofiles.open(summary_path, 'w') as f:
                    await f.write(json.dumps({
                        "export_info": export_info,
                        "job_metadata": {
                            "export_id": export_id,
                            "timestamp": timestamp,
                            "total_vectors": len(vector_records),
                            "vector_database": db_type.value,
                            "source": "diala-voice-agent"
                        }
                    }, indent=2, default=str))
                
                # Create a zip file containing all exports
                zip_path = export_dir / f"{export_id}_{db_type.value}_{timestamp}.zip"
                import zipfile
                with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for file_path in db_export_dir.rglob('*'):
                        if file_path.is_file():
                            arcname = file_path.relative_to(db_export_dir)
                            zipf.write(file_path, arcname)
                
                logger.info(f"Vector database export completed: {len(vector_records)} vectors for {db_type.value}")
                return str(zip_path)
            
            else:
                # Generic vector export as JSONL
                export_path = export_dir / f"{export_id}_{timestamp}.jsonl"
                async with aiofiles.open(export_path, 'w') as f:
                    for record in vector_records:
                        vector_data = {
                            "id": record.id,
                            "text": record.metadata.get("text", ""),
                            "vector": record.vector,
                            "metadata": record.metadata
                        }
                        await f.write(json.dumps(vector_data) + '\n')
                
                logger.info(f"Generic vector export completed: {len(vector_records)} vectors")
                return str(export_path)
                
        except Exception as e:
            logger.error(f"Error in vector database export: {e}")
            # Fallback to simple JSONL export
            export_path = export_dir / f"{export_id}_{timestamp}_fallback.jsonl"
            async with aiofiles.open(export_path, 'w') as f:
                await f.write(f'{{"error": "Vector export failed: {str(e)}"}}\n')
            return str(export_path)
    
    def _serialize_processing_result(self, result: ProcessingResult) -> Dict[str, Any]:
        """
        Serialize ProcessingResult to a dictionary, converting enums to strings for Convex compatibility.
        
        Args:
            result: ProcessingResult instance
            
        Returns:
            Dictionary with enum values converted to strings
        """
        # Convert to dict first
        result_dict = asdict(result)
        
        # Convert enums to strings
        if 'content_type' in result_dict and hasattr(result_dict['content_type'], 'value'):
            result_dict['content_type'] = result_dict['content_type'].value
        elif 'content_type' in result_dict:
            # Handle case where it's already a string or other serializable type
            result_dict['content_type'] = str(result_dict['content_type'])
            
        if 'status' in result_dict and hasattr(result_dict['status'], 'value'):
            result_dict['status'] = result_dict['status'].value
        elif 'status' in result_dict:
            result_dict['status'] = str(result_dict['status'])
        
        return result_dict
    
    def __del__(self):
        """Cleanup on deletion"""
        try:
            import shutil
            if hasattr(self, 'temp_dir') and os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
        except Exception:
            pass


# Global service instance
bulk_processing_service = BulkProcessingService()


def get_bulk_processing_service() -> BulkProcessingService:
    """Get the global bulk processing service instance."""
    return bulk_processing_service


================================================
FILE: bulk_workflow_orchestrator.py
================================================
"""
Bulk Processing Workflow Orchestrator

Manages the complete pipeline for bulk processing of TikTok content:
TikTok content → Audio processing → Transcription → Embedding → Export

This orchestrator coordinates multiple stages, handles data flow, validates quality,
and prepares export-ready data in formats suitable for vector database import.
"""

import os
import json
import logging
import asyncio
import tempfile
import uuid
import time
from typing import Dict, List, Any, Optional, Union, Callable
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum
import traceback

# Import existing services
from .tiktok_service import TikTokService, get_tiktok_service
from .audio_preparation_service import AudioPreparationService, audio_preparation_service
from .jina.embeddings_service import JinaEmbeddingsService
from .gemini.embeddings_service import GeminiEmbeddingsService

logger = logging.getLogger(__name__)


class WorkflowStage(Enum):
    """Workflow stage enumeration"""
    CONTENT_INGESTION = "content_ingestion"
    AUDIO_EXTRACTION = "audio_extraction"
    AUDIO_CLEANUP = "audio_cleanup"
    TRANSCRIPTION = "transcription"
    EMBEDDING_GENERATION = "embedding_generation"
    EXPORT_PREPARATION = "export_preparation"
    COMPLETED = "completed"
    FAILED = "failed"


class ExportFormat(Enum):
    """Export format enumeration"""
    PINECONE = "pinecone"
    WEAVIATE = "weaviate"
    CHROMA = "chroma"
    JSONL = "jsonl"
    CSV = "csv"
    PARQUET = "parquet"


class ProcessingStatus(Enum):
    """Processing status enumeration"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"


@dataclass
class ContentItem:
    """Represents a single content item in the pipeline"""
    id: str
    video_id: str
    username: str
    title: str
    description: str
    duration: float
    thumbnail_url: str
    created_time: int
    status: ProcessingStatus = ProcessingStatus.PENDING
    stage: WorkflowStage = WorkflowStage.CONTENT_INGESTION
    audio_path: Optional[str] = None
    cleaned_audio_path: Optional[str] = None
    transcription: Optional[str] = None
    embeddings: Optional[List[float]] = None
    metadata: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    processing_time: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        data = asdict(self)
        # Convert enums to strings
        data['status'] = self.status.value
        data['stage'] = self.stage.value
        return data
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ContentItem':
        """Create from dictionary"""
        # Convert string enums back to enums
        if 'status' in data:
            data['status'] = ProcessingStatus(data['status'])
        if 'stage' in data:
            data['stage'] = WorkflowStage(data['stage'])
        return cls(**data)


@dataclass
class WorkflowConfig:
    """Configuration for the bulk processing workflow"""
    # Content ingestion settings
    max_videos_per_user: int = 25
    video_duration_limit: int = 180  # 3 minutes max
    
    # Audio processing settings
    audio_format: str = "wav"
    sample_rate: int = 24000
    channels: int = 1
    clean_silence: bool = True
    separate_vocals: bool = True
    
    # Transcription settings
    whisper_model: str = "base"
    language: Optional[str] = None
    segment_audio: bool = True
    max_segment_duration: int = 30
    
    # Embedding settings
    embedding_provider: str = "jina"  # "jina" or "gemini"
    embedding_dimensions: Optional[int] = None
    chunk_size: int = 1000
    chunk_overlap: int = 200
    
    # Export settings
    export_formats: List[ExportFormat] = None
    include_metadata: bool = True
    include_audio_features: bool = False
    
    # Quality validation settings
    min_transcription_length: int = 10
    max_transcription_length: int = 10000
    min_audio_duration: float = 1.0
    max_processing_retries: int = 3
    
    # Performance settings
    max_concurrent_items: int = 5
    batch_size: int = 10
    timeout_per_item: int = 300  # 5 minutes
    
    def __post_init__(self):
        if self.export_formats is None:
            self.export_formats = [ExportFormat.JSONL, ExportFormat.PINECONE]


@dataclass
class WorkflowProgress:
    """Represents the current progress of the workflow"""
    total_items: int
    completed_items: int
    failed_items: int
    current_stage: WorkflowStage
    stage_progress: float
    overall_progress: float
    estimated_time_remaining: Optional[float] = None
    processing_rate: Optional[float] = None  # items per minute
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        data = asdict(self)
        data['currentStage'] = self.current_stage.value
        # Remove the original field to avoid conflicts
        if 'current_stage' in data:
            del data['current_stage']
        return data


class BulkWorkflowOrchestrator:
    """
    Orchestrates the complete bulk processing pipeline for TikTok content.
    
    Manages workflow stages, data flow, quality validation, and export preparation.
    """
    
    def __init__(self, config: Optional[WorkflowConfig] = None):
        """Initialize the orchestrator"""
        self.config = config or WorkflowConfig()
        self.tiktok_service = get_tiktok_service()
        self.audio_service = audio_preparation_service
        
        # Initialize embedding service based on config
        if self.config.embedding_provider == "gemini":
            try:
                self.embedding_service = GeminiEmbeddingsService()
                logger.info("Initialized Gemini embeddings service")
            except Exception as e:
                logger.warning(f"Failed to initialize Gemini service: {e}, falling back to Jina")
                self.embedding_service = JinaEmbeddingsService()
        else:
            self.embedding_service = JinaEmbeddingsService()
            
        # Workflow state
        self.items: List[ContentItem] = []
        self.progress: Optional[WorkflowProgress] = None
        self.is_running = False
        self.should_stop = False
        self.temp_dir = tempfile.mkdtemp(prefix="bulk_workflow_")
        self.start_time: Optional[float] = None
        
        # Concurrency control
        self.semaphore = asyncio.Semaphore(self.config.max_concurrent_items)
        
        logger.info(f"BulkWorkflowOrchestrator initialized with {self.config.embedding_provider} embeddings")
    
    async def process_user_content(
        self,
        username: str,
        progress_callback: Optional[Callable[[WorkflowProgress], None]] = None
    ) -> Dict[str, Any]:
        """
        Process all content for a given TikTok user through the complete pipeline.
        
        Args:
            username: TikTok username to process
            progress_callback: Optional callback for progress updates
            
        Returns:
            Dictionary containing processing results and export data
        """
        if self.is_running:
            raise ValueError("Workflow is already running")
        
        self.is_running = True
        self.should_stop = False
        self.start_time = time.time()
        
        try:
            logger.info(f"Starting bulk processing for user: @{username}")
            
            # Stage 1: Content Ingestion
            await self._stage_content_ingestion(username, progress_callback)
            
            if self.should_stop:
                return self._create_result_summary("cancelled")
            
            # Stage 2: Audio Extraction
            await self._stage_audio_extraction(progress_callback)
            
            if self.should_stop:
                return self._create_result_summary("cancelled")
            
            # Stage 3: Audio Cleanup
            await self._stage_audio_cleanup(progress_callback)
            
            if self.should_stop:
                return self._create_result_summary("cancelled")
            
            # Stage 4: Transcription
            await self._stage_transcription(progress_callback)
            
            if self.should_stop:
                return self._create_result_summary("cancelled")
            
            # Stage 5: Embedding Generation
            await self._stage_embedding_generation(progress_callback)
            
            if self.should_stop:
                return self._create_result_summary("cancelled")
            
            # Stage 6: Export Preparation
            export_data = await self._stage_export_preparation(progress_callback)
            
            # Create final result
            result = self._create_result_summary("completed")
            result["export_data"] = export_data
            
            logger.info(f"Bulk processing completed for @{username}")
            return result
            
        except Exception as e:
            logger.error(f"Workflow failed for @{username}: {str(e)}")
            logger.error(traceback.format_exc())
            return self._create_result_summary("failed", str(e))
        finally:
            self.is_running = False
    
    async def _stage_content_ingestion(
        self,
        username: str,
        progress_callback: Optional[Callable] = None
    ):
        """Stage 1: Ingest TikTok content for the user"""
        logger.info(f"Stage 1: Content ingestion for @{username}")
        
        try:
            # Fetch user videos
            videos_data = await self.tiktok_service.get_user_videos(
                username,
                count=self.config.max_videos_per_user
            )
            
            videos = videos_data.get("videos", [])
            logger.info(f"Found {len(videos)} videos for @{username}")
            
            # Convert to ContentItem objects
            self.items = []
            for video in videos:
                # Skip videos that are too long
                duration = video.get("duration", 0)
                if duration > self.config.video_duration_limit:
                    logger.warning(f"Skipping video {video.get('videoId')} - too long: {duration}s")
                    continue
                
                item = ContentItem(
                    id=str(uuid.uuid4()),
                    video_id=video.get("videoId", ""),
                    username=username,
                    title=video.get("title", ""),
                    description=video.get("title", ""),  # TikTok uses title as description
                    duration=duration,
                    thumbnail_url=video.get("thumbnail", ""),
                    created_time=video.get("createTime", 0),
                    metadata={
                        "stats": video.get("stats", {}),
                        "hashtags": video.get("hashtags", []),
                        "music": video.get("music", {}),
                        "original_video_data": video
                    }
                )
                self.items.append(item)
            
            # Initialize progress tracking
            self.progress = WorkflowProgress(
                total_items=len(self.items),
                completed_items=0,
                failed_items=0,
                current_stage=WorkflowStage.CONTENT_INGESTION,
                stage_progress=100.0,
                overall_progress=100.0 / 7  # 7 stages total
            )
            
            if progress_callback:
                progress_callback(self.progress)
            
            logger.info(f"Content ingestion completed: {len(self.items)} items")
            
        except Exception as e:
            logger.error(f"Content ingestion failed: {str(e)}")
            raise
    
    async def _stage_audio_extraction(self, progress_callback: Optional[Callable] = None):
        """Stage 2: Extract audio from TikTok videos"""
        logger.info("Stage 2: Audio extraction")
        
        self.progress.current_stage = WorkflowStage.AUDIO_EXTRACTION
        self.progress.stage_progress = 0.0
        
        async def extract_audio(item: ContentItem) -> ContentItem:
            """Extract audio for a single item"""
            async with self.semaphore:
                start_time = time.time()
                
                try:
                    # Download audio from TikTok
                    logger.info(f"Extracting audio for video {item.video_id}")
                    
                    audio_bytes = await self.tiktok_service.download_audio_bytes(
                        item.video_id,
                        format=self.config.audio_format
                    )
                    
                    # Save audio to temporary file
                    audio_filename = f"{item.id}_audio.{self.config.audio_format}"
                    audio_path = os.path.join(self.temp_dir, audio_filename)
                    
                    with open(audio_path, 'wb') as f:
                        f.write(audio_bytes)
                    
                    item.audio_path = audio_path
                    item.stage = WorkflowStage.AUDIO_EXTRACTION
                    item.status = ProcessingStatus.COMPLETED
                    item.processing_time += time.time() - start_time
                    
                    logger.info(f"Audio extracted for {item.video_id}: {len(audio_bytes)} bytes")
                    
                except Exception as e:
                    logger.error(f"Audio extraction failed for {item.video_id}: {str(e)}")
                    item.status = ProcessingStatus.FAILED
                    item.error_message = str(e)
                    item.processing_time += time.time() - start_time
                
                return item
        
        # Process items concurrently
        tasks = [extract_audio(item) for item in self.items if item.status != ProcessingStatus.FAILED]
        
        completed = 0
        for task in asyncio.as_completed(tasks):
            if self.should_stop:
                break
                
            await task
            completed += 1
            
            # Update progress
            self.progress.stage_progress = (completed / len(tasks)) * 100
            self.progress.overall_progress = (2 * 100 + self.progress.stage_progress) / 7
            
            if progress_callback:
                progress_callback(self.progress)
        
        # Update completed/failed counts
        self.progress.completed_items = sum(1 for item in self.items if item.status == ProcessingStatus.COMPLETED)
        self.progress.failed_items = sum(1 for item in self.items if item.status == ProcessingStatus.FAILED)
        
        logger.info(f"Audio extraction completed: {self.progress.completed_items}/{len(self.items)} successful")
    
    async def _stage_audio_cleanup(self, progress_callback: Optional[Callable] = None):
        """Stage 3: Clean and prepare audio files"""
        logger.info("Stage 3: Audio cleanup")
        
        self.progress.current_stage = WorkflowStage.AUDIO_CLEANUP
        self.progress.stage_progress = 0.0
        
        async def cleanup_audio(item: ContentItem) -> ContentItem:
            """Clean audio for a single item"""
            async with self.semaphore:
                start_time = time.time()
                
                try:
                    if not item.audio_path or item.status == ProcessingStatus.FAILED:
                        item.status = ProcessingStatus.SKIPPED
                        return item
                    
                    logger.info(f"Cleaning audio for video {item.video_id}")
                    
                    # Prepare audio with cleanup configuration
                    config = {
                        "use_whisper": False,  # Don't transcribe yet
                        "segment_audio": False,  # Don't segment yet
                        "clean_silence": self.config.clean_silence,
                        "separate_voices": self.config.separate_vocals,
                        "provider_specific": {
                            "sample_rate": self.config.sample_rate,
                            "channels": self.config.channels
                        }
                    }
                    
                    result = await self.audio_service.prepare_audio(
                        item.audio_path,
                        provider="transcription",
                        config=config
                    )
                    
                    item.cleaned_audio_path = result["prepared_audio_path"]
                    
                    # Update metadata with audio preparation info
                    if not item.metadata:
                        item.metadata = {}
                    item.metadata.update({
                        "audio_cleanup": result.get("metadata", {}),
                        "vocals_extracted": result.get("metadata", {}).get("vocals_extracted", False),
                        "silence_removed": result.get("metadata", {}).get("silence_removed", False)
                    })
                    
                    item.stage = WorkflowStage.AUDIO_CLEANUP
                    item.status = ProcessingStatus.COMPLETED
                    item.processing_time += time.time() - start_time
                    
                    logger.info(f"Audio cleaned for {item.video_id}")
                    
                except Exception as e:
                    logger.error(f"Audio cleanup failed for {item.video_id}: {str(e)}")
                    item.status = ProcessingStatus.FAILED
                    item.error_message = str(e)
                    item.processing_time += time.time() - start_time
                
                return item
        
        # Process items that have audio
        items_to_process = [item for item in self.items if item.audio_path and item.status != ProcessingStatus.FAILED]
        tasks = [cleanup_audio(item) for item in items_to_process]
        
        completed = 0
        for task in asyncio.as_completed(tasks):
            if self.should_stop:
                break
                
            await task
            completed += 1
            
            # Update progress
            self.progress.stage_progress = (completed / len(tasks)) * 100 if tasks else 100
            self.progress.overall_progress = (3 * 100 + self.progress.stage_progress) / 7
            
            if progress_callback:
                progress_callback(self.progress)
        
        logger.info(f"Audio cleanup completed: {completed}/{len(items_to_process)} processed")
    
    async def _stage_transcription(self, progress_callback: Optional[Callable] = None):
        """Stage 4: Transcribe audio content"""
        logger.info("Stage 4: Transcription")
        
        self.progress.current_stage = WorkflowStage.TRANSCRIPTION
        self.progress.stage_progress = 0.0
        
        async def transcribe_audio(item: ContentItem) -> ContentItem:
            """Transcribe audio for a single item"""
            async with self.semaphore:
                start_time = time.time()
                
                try:
                    if not item.cleaned_audio_path or item.status == ProcessingStatus.FAILED:
                        item.status = ProcessingStatus.SKIPPED
                        return item
                    
                    logger.info(f"Transcribing audio for video {item.video_id}")
                    
                    # Transcription configuration
                    config = {
                        "use_whisper": True,
                        "segment_audio": self.config.segment_audio,
                        "max_segment_duration": self.config.max_segment_duration,
                        "transcribe": True,
                        "clean_silence": False,  # Already cleaned
                        "separate_voices": False,  # Already separated
                        "provider_specific": {
                            "language": self.config.language,
                            "model_size": self.config.whisper_model
                        }
                    }
                    
                    result = await self.audio_service.prepare_audio(
                        item.cleaned_audio_path,
                        provider="transcription",
                        config=config
                    )
                    
                    transcription = result.get("transcription", "").strip()
                    
                    # Validate transcription quality
                    if len(transcription) < self.config.min_transcription_length:
                        raise ValueError(f"Transcription too short: {len(transcription)} characters")
                    
                    if len(transcription) > self.config.max_transcription_length:
                        logger.warning(f"Transcription very long: {len(transcription)} characters")
                        # Truncate but don't fail
                        transcription = transcription[:self.config.max_transcription_length]
                    
                    item.transcription = transcription
                    
                    # Update metadata with transcription info
                    if not item.metadata:
                        item.metadata = {}
                    item.metadata.update({
                        "transcription_metadata": result.get("metadata", {}),
                        "language": result.get("metadata", {}).get("language", "unknown"),
                        "segments": result.get("segments", []),
                        "transcription_length": len(transcription),
                        "word_count": len(transcription.split()) if transcription else 0
                    })
                    
                    item.stage = WorkflowStage.TRANSCRIPTION
                    item.status = ProcessingStatus.COMPLETED
                    item.processing_time += time.time() - start_time
                    
                    logger.info(f"Transcription completed for {item.video_id}: {len(transcription)} chars")
                    
                except Exception as e:
                    logger.error(f"Transcription failed for {item.video_id}: {str(e)}")
                    item.status = ProcessingStatus.FAILED
                    item.error_message = str(e)
                    item.processing_time += time.time() - start_time
                
                return item
        
        # Process items that have cleaned audio
        items_to_process = [
            item for item in self.items 
            if item.cleaned_audio_path and item.status != ProcessingStatus.FAILED
        ]
        
        tasks = [transcribe_audio(item) for item in items_to_process]
        
        completed = 0
        for task in asyncio.as_completed(tasks):
            if self.should_stop:
                break
                
            await task
            completed += 1
            
            # Update progress
            self.progress.stage_progress = (completed / len(tasks)) * 100 if tasks else 100
            self.progress.overall_progress = (4 * 100 + self.progress.stage_progress) / 7
            
            if progress_callback:
                progress_callback(self.progress)
        
        logger.info(f"Transcription completed: {completed}/{len(items_to_process)} processed")
    
    async def _stage_embedding_generation(self, progress_callback: Optional[Callable] = None):
        """Stage 5: Generate embeddings for transcriptions"""
        logger.info("Stage 5: Embedding generation")
        
        self.progress.current_stage = WorkflowStage.EMBEDDING_GENERATION
        self.progress.stage_progress = 0.0
        
        # Collect all transcriptions for batch processing
        items_with_transcriptions = [
            item for item in self.items 
            if item.transcription and item.status != ProcessingStatus.FAILED
        ]
        
        if not items_with_transcriptions:
            logger.warning("No items with transcriptions found for embedding generation")
            self.progress.stage_progress = 100.0
            self.progress.overall_progress = (5 * 100) / 7
            if progress_callback:
                progress_callback(self.progress)
            return
        
        try:
            # Process in batches
            batch_size = self.config.batch_size
            total_batches = (len(items_with_transcriptions) + batch_size - 1) // batch_size
            
            for batch_idx in range(0, len(items_with_transcriptions), batch_size):
                if self.should_stop:
                    break
                
                batch_items = items_with_transcriptions[batch_idx:batch_idx + batch_size]
                texts = [item.transcription for item in batch_items]
                
                logger.info(f"Generating embeddings for batch {batch_idx // batch_size + 1}/{total_batches}")
                
                # Generate embeddings based on provider
                if self.config.embedding_provider == "gemini":
                    embeddings = await self.embedding_service.embed_documents(
                        texts,
                        output_dimensionality=self.config.embedding_dimensions
                    )
                else:  # Jina
                    embeddings = await self.embedding_service.embed_documents(texts)
                
                # Assign embeddings to items
                for item, embedding in zip(batch_items, embeddings):
                    item.embeddings = embedding
                    item.stage = WorkflowStage.EMBEDDING_GENERATION
                    item.status = ProcessingStatus.COMPLETED
                    
                    # Update metadata
                    if not item.metadata:
                        item.metadata = {}
                    item.metadata.update({
                        "embedding_provider": self.config.embedding_provider,
                        "embedding_dimensions": len(embedding),
                        "embedding_model": getattr(self.embedding_service, 'model_name', 'unknown')
                    })
                
                # Update progress
                completed_batches = (batch_idx // batch_size) + 1
                self.progress.stage_progress = (completed_batches / total_batches) * 100
                self.progress.overall_progress = (5 * 100 + self.progress.stage_progress) / 7
                
                if progress_callback:
                    progress_callback(self.progress)
                
                # Small delay between batches to avoid rate limiting
                if batch_idx + batch_size < len(items_with_transcriptions):
                    await asyncio.sleep(0.5)
            
            successful_embeddings = sum(1 for item in items_with_transcriptions if item.embeddings)
            logger.info(f"Embedding generation completed: {successful_embeddings}/{len(items_with_transcriptions)} successful")
            
        except Exception as e:
            logger.error(f"Embedding generation failed: {str(e)}")
            # Mark all remaining items as failed
            for item in items_with_transcriptions:
                if not item.embeddings:
                    item.status = ProcessingStatus.FAILED
                    item.error_message = f"Embedding generation failed: {str(e)}"
            raise
    
    async def _stage_export_preparation(self, progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """Stage 6: Prepare data for export in various formats"""
        logger.info("Stage 6: Export preparation")
        
        self.progress.current_stage = WorkflowStage.EXPORT_PREPARATION
        self.progress.stage_progress = 0.0
        
        # Filter successfully processed items
        successful_items = [
            item for item in self.items 
            if item.embeddings and item.transcription and item.status == ProcessingStatus.COMPLETED
        ]
        
        if not successful_items:
            logger.warning("No successfully processed items found for export")
            return {"formats": {}, "summary": {"total_items": 0}}
        
        logger.info(f"Preparing export for {len(successful_items)} successfully processed items")
        
        export_data = {"formats": {}, "summary": {}}
        
        try:
            # Generate export data for each requested format
            for i, export_format in enumerate(self.config.export_formats):
                logger.info(f"Preparing {export_format.value} export format")
                
                if export_format == ExportFormat.PINECONE:
                    export_data["formats"]["pinecone"] = self._prepare_pinecone_format(successful_items)
                elif export_format == ExportFormat.WEAVIATE:
                    export_data["formats"]["weaviate"] = self._prepare_weaviate_format(successful_items)
                elif export_format == ExportFormat.CHROMA:
                    export_data["formats"]["chroma"] = self._prepare_chroma_format(successful_items)
                elif export_format == ExportFormat.JSONL:
                    export_data["formats"]["jsonl"] = self._prepare_jsonl_format(successful_items)
                elif export_format == ExportFormat.CSV:
                    export_data["formats"]["csv"] = self._prepare_csv_format(successful_items)
                elif export_format == ExportFormat.PARQUET:
                    export_data["formats"]["parquet"] = self._prepare_parquet_format(successful_items)
                
                # Update progress
                self.progress.stage_progress = ((i + 1) / len(self.config.export_formats)) * 100
                self.progress.overall_progress = (6 * 100 + self.progress.stage_progress) / 7
                
                if progress_callback:
                    progress_callback(self.progress)
            
            # Add summary information
            export_data["summary"] = {
                "total_items": len(successful_items),
                "embedding_provider": self.config.embedding_provider,
                "embedding_dimensions": len(successful_items[0].embeddings) if successful_items else 0,
                "export_formats": [fmt.value for fmt in self.config.export_formats],
                "processing_time": time.time() - self.start_time if self.start_time else 0,
                "username": successful_items[0].username if successful_items else "",
                "generated_at": time.time()
            }
            
            logger.info("Export preparation completed")
            return export_data
            
        except Exception as e:
            logger.error(f"Export preparation failed: {str(e)}")
            raise
    
    def _prepare_pinecone_format(self, items: List[ContentItem]) -> Dict[str, Any]:
        """Prepare data in Pinecone format"""
        vectors = []
        
        for item in items:
            vector_data = {
                "id": f"{item.username}_{item.video_id}",
                "values": item.embeddings,
                "metadata": {
                    "username": item.username,
                    "video_id": item.video_id,
                    "title": item.title,
                    "description": item.description,
                    "transcription": item.transcription,
                    "duration": item.duration,
                    "created_time": item.created_time,
                    "thumbnail_url": item.thumbnail_url
                }
            }
            
            # Add optional metadata
            if self.config.include_metadata and item.metadata:
                # Filter out large nested objects for Pinecone metadata limits
                filtered_metadata = {}
                for key, value in item.metadata.items():
                    if key in ["stats", "language", "word_count", "transcription_length"]:
                        if isinstance(value, (str, int, float, bool)):
                            filtered_metadata[key] = value
                        elif isinstance(value, dict) and key == "stats":
                            # Include basic stats
                            stats = value
                            filtered_metadata.update({
                                "views": stats.get("views", 0),
                                "likes": stats.get("likes", 0),
                                "comments": stats.get("comments", 0),
                                "shares": stats.get("shares", 0)
                            })
                
                vector_data["metadata"].update(filtered_metadata)
            
            vectors.append(vector_data)
        
        return {
            "vectors": vectors,
            "namespace": f"tiktok_{items[0].username}",
            "dimension": len(items[0].embeddings)
        }
    
    def _prepare_weaviate_format(self, items: List[ContentItem]) -> Dict[str, Any]:
        """Prepare data in Weaviate format"""
        objects = []
        
        for item in items:
            obj = {
                "class": "TikTokContent",
                "id": f"{item.username}_{item.video_id}",
                "properties": {
                    "username": item.username,
                    "videoId": item.video_id,
                    "title": item.title,
                    "description": item.description,
                    "transcription": item.transcription,
                    "duration": item.duration,
                    "createdTime": item.created_time,
                    "thumbnailUrl": item.thumbnail_url
                },
                "vector": item.embeddings
            }
            
            # Add metadata
            if self.config.include_metadata and item.metadata:
                if "stats" in item.metadata:
                    stats = item.metadata["stats"]
                    obj["properties"].update({
                        "views": stats.get("views", 0),
                        "likes": stats.get("likes", 0),
                        "comments": stats.get("comments", 0),
                        "shares": stats.get("shares", 0)
                    })
                
                obj["properties"]["language"] = item.metadata.get("language", "unknown")
                obj["properties"]["wordCount"] = item.metadata.get("word_count", 0)
            
            objects.append(obj)
        
        return {
            "objects": objects,
            "class_name": "TikTokContent"
        }
    
    def _prepare_chroma_format(self, items: List[ContentItem]) -> Dict[str, Any]:
        """Prepare data in Chroma format"""
        ids = []
        embeddings = []
        metadatas = []
        documents = []
        
        for item in items:
            ids.append(f"{item.username}_{item.video_id}")
            embeddings.append(item.embeddings)
            documents.append(item.transcription)
            
            metadata = {
                "username": item.username,
                "video_id": item.video_id,
                "title": item.title,
                "description": item.description,
                "duration": item.duration,
                "created_time": item.created_time,
                "thumbnail_url": item.thumbnail_url
            }
            
            # Add stats if available
            if self.config.include_metadata and item.metadata and "stats" in item.metadata:
                stats = item.metadata["stats"]
                metadata.update({
                    "views": stats.get("views", 0),
                    "likes": stats.get("likes", 0),
                    "comments": stats.get("comments", 0),
                    "shares": stats.get("shares", 0)
                })
            
            metadatas.append(metadata)
        
        return {
            "ids": ids,
            "embeddings": embeddings,
            "metadatas": metadatas,
            "documents": documents,
            "collection_name": f"tiktok_{items[0].username}"
        }
    
    def _prepare_jsonl_format(self, items: List[ContentItem]) -> Dict[str, Any]:
        """Prepare data in JSONL format"""
        records = []
        
        for item in items:
            record = {
                "id": f"{item.username}_{item.video_id}",
                "username": item.username,
                "video_id": item.video_id,
                "title": item.title,
                "description": item.description,
                "transcription": item.transcription,
                "duration": item.duration,
                "created_time": item.created_time,
                "thumbnail_url": item.thumbnail_url,
                "embeddings": item.embeddings
            }
            
            # Add metadata
            if self.config.include_metadata and item.metadata:
                record["metadata"] = item.metadata
            
            records.append(record)
        
        return {
            "records": records,
            "format": "jsonl"
        }
    
    def _prepare_csv_format(self, items: List[ContentItem]) -> Dict[str, Any]:
        """Prepare data in CSV format (without embeddings)"""
        rows = []
        
        for item in items:
            row = {
                "id": f"{item.username}_{item.video_id}",
                "username": item.username,
                "video_id": item.video_id,
                "title": item.title,
                "description": item.description,
                "transcription": item.transcription,
                "duration": item.duration,
                "created_time": item.created_time,
                "thumbnail_url": item.thumbnail_url,
                "embedding_dimensions": len(item.embeddings)
            }
            
            # Add basic stats
            if self.config.include_metadata and item.metadata and "stats" in item.metadata:
                stats = item.metadata["stats"]
                row.update({
                    "views": stats.get("views", 0),
                    "likes": stats.get("likes", 0),
                    "comments": stats.get("comments", 0),
                    "shares": stats.get("shares", 0)
                })
            
            rows.append(row)
        
        return {
            "rows": rows,
            "format": "csv"
        }
    
    def _prepare_parquet_format(self, items: List[ContentItem]) -> Dict[str, Any]:
        """Prepare data in Parquet format"""
        # Similar to CSV but more structured
        return self._prepare_csv_format(items)
    
    def _create_result_summary(self, status: str, error_message: Optional[str] = None) -> Dict[str, Any]:
        """Create a summary of the processing results"""
        total_items = len(self.items)
        completed_items = sum(1 for item in self.items if item.status == ProcessingStatus.COMPLETED)
        failed_items = sum(1 for item in self.items if item.status == ProcessingStatus.FAILED)
        skipped_items = sum(1 for item in self.items if item.status == ProcessingStatus.SKIPPED)
        
        # Calculate processing time
        processing_time = time.time() - self.start_time if self.start_time else 0
        
        # Stage completion statistics
        stage_stats = {}
        for stage in WorkflowStage:
            stage_stats[stage.value] = sum(1 for item in self.items if item.stage == stage)
        
        summary = {
            "status": status,
            "processing_time": processing_time,
            "total_items": total_items,
            "completed_items": completed_items,
            "failed_items": failed_items,
            "skipped_items": skipped_items,
            "success_rate": (completed_items / total_items * 100) if total_items > 0 else 0,
            "stage_statistics": stage_stats,
            "configuration": {
                "embedding_provider": self.config.embedding_provider,
                "max_videos": self.config.max_videos_per_user,
                "export_formats": [fmt.value for fmt in self.config.export_formats],
                "whisper_model": self.config.whisper_model
            }
        }
        
        if error_message:
            summary["error_message"] = error_message
        
        # Add item details for failed items
        if failed_items > 0:
            summary["failed_items_details"] = [
                {
                    "video_id": item.video_id,
                    "stage": item.stage.value,
                    "error": item.error_message
                }
                for item in self.items if item.status == ProcessingStatus.FAILED
            ]
        
        return summary
    
    def stop_processing(self):
        """Signal the workflow to stop processing"""
        logger.info("Stop signal received - workflow will halt after current operations")
        self.should_stop = True
    
    def cleanup(self):
        """Clean up temporary files and resources"""
        try:
            if os.path.exists(self.temp_dir):
                import shutil
                shutil.rmtree(self.temp_dir)
                logger.info(f"Cleaned up temporary directory: {self.temp_dir}")
        except Exception as e:
            logger.warning(f"Error cleaning up temporary files: {str(e)}")
    
    def __del__(self):
        """Cleanup on deletion"""
        self.cleanup()


# Factory function for easy instantiation
def create_bulk_orchestrator(config: Optional[WorkflowConfig] = None) -> BulkWorkflowOrchestrator:
    """
    Create a new bulk workflow orchestrator instance.
    
    Args:
        config: Optional workflow configuration
        
    Returns:
        Configured BulkWorkflowOrchestrator instance
    """
    return BulkWorkflowOrchestrator(config)


# Example usage configuration presets
class ConfigPresets:
    """Pre-configured workflow settings for common use cases"""
    
    @staticmethod
    def quick_processing() -> WorkflowConfig:
        """Configuration for quick processing with basic features"""
        return WorkflowConfig(
            max_videos_per_user=10,
            clean_silence=False,
            separate_vocals=False,
            whisper_model="tiny",
            embedding_provider="jina",
            export_formats=[ExportFormat.JSONL],
            max_concurrent_items=3,
            batch_size=5
        )
    
    @staticmethod
    def high_quality() -> WorkflowConfig:
        """Configuration for high-quality processing"""
        return WorkflowConfig(
            max_videos_per_user=25,
            clean_silence=True,
            separate_vocals=True,
            whisper_model="base",
            embedding_provider="gemini",
            export_formats=[ExportFormat.PINECONE, ExportFormat.WEAVIATE, ExportFormat.JSONL],
            max_concurrent_items=5,
            batch_size=10,
            include_metadata=True
        )
    
    @staticmethod
    def production_ready() -> WorkflowConfig:
        """Configuration optimized for production use"""
        return WorkflowConfig(
            max_videos_per_user=20,
            clean_silence=True,
            separate_vocals=True,
            whisper_model="base",
            embedding_provider="jina",  # More stable than experimental Gemini
            export_formats=[ExportFormat.PINECONE, ExportFormat.JSONL],
            max_concurrent_items=3,  # Conservative for stability
            batch_size=8,
            include_metadata=True,
            timeout_per_item=600,  # 10 minutes
            max_processing_retries=2
        )


================================================
FILE: chatterbox_client.py
================================================
"""
Chatterbox TTS client service for interacting with the containerized API
"""
import os
import asyncio
import aiohttp
import tempfile
from typing import Optional, Dict, Any, List
import logging
from io import BytesIO
import tempfile

logger = logging.getLogger(__name__)

class ChatterboxClient:
    """Client for interacting with the Chatterbox TTS API"""
    
    def __init__(self, base_url: Optional[str] = None):
        """
        Initialize the Chatterbox client
        
        Args:
            base_url: Base URL for the Chatterbox API. 
                     Defaults to environment variable or localhost
        """
        self.base_url = base_url or os.getenv("CHATTERBOX_API_URL", "http://localhost:8001")
        self.session = None
        
        # Determine mode based on configuration
        self.mode = os.getenv("CHATTERBOX_MODE", "local")
        
        # Import local service if in local mode
        if self.mode == "local":
            try:
                from src.services.chatterbox_service import ChatterboxService
                self.local_service = ChatterboxService()
                logger.info("ChatterboxClient initialized in LOCAL mode")
            except ImportError as e:
                logger.warning(f"Failed to import ChatterboxService, falling back to remote mode: {e}")
                self.mode = "remote"
                self.local_service = None
        else:
            self.local_service = None
            logger.info(f"ChatterboxClient initialized in REMOTE mode: {self.base_url}")
        
    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
            
    async def health_check(self) -> Dict[str, Any]:
        """
        Check if the Chatterbox service is healthy
        
        Returns:
            Dict with health status information
        """
        if self.mode == "local" and self.local_service:
            return await self.local_service.get_health_info()
        
        try:
            if not self.session:
                self.session = aiohttp.ClientSession()
            async with self.session.get(f"{self.base_url}/health") as response:
                response.raise_for_status()
                return await response.json()
        except Exception as e:
            logger.error(f"Health check failed: {str(e)}")
            return {"status": "unhealthy", "error": str(e)}
            
    async def list_voices(self) -> List[Dict[str, str]]:
        """
        Get list of available voices
        
        Returns:
            List of voice information dictionaries
        """
        if self.mode == "local" and self.local_service:
            # Return default voices for local mode
            return [
                {
                    "id": "default",
                    "name": "Default Voice",
                    "description": "Default Chatterbox TTS voice"
                },
                {
                    "id": "custom",
                    "name": "Custom Voice",
                    "description": "Upload audio for voice cloning"
                }
            ]
        
        try:
            if not self.session:
                self.session = aiohttp.ClientSession()
            async with self.session.get(f"{self.base_url}/voices") as response:
                response.raise_for_status()
                return await response.json()
        except Exception as e:
            logger.error(f"Failed to list voices: {str(e)}")
            raise
            
    async def generate_speech(
        self,
        text: str,
        voice_id: str = "default",
        chunk_size: int = 2048,
        exaggeration: float = 1.0,
        cfg_weight: float = 1.7
    ) -> bytes:
        """
        Generate speech from text
        
        Args:
            text: Text to convert to speech
            voice_id: ID of the voice to use
            chunk_size: Size of audio chunks for generation
            exaggeration: Voice exaggeration factor
            cfg_weight: Configuration weight for generation
            
        Returns:
            Audio data as bytes
        """
        if self.mode == "local" and self.local_service:
            # Use local service
            try:
                audio_path = await self.local_service.generate_speech(
                    text=text,
                    voice_id=voice_id,
                    chunk_size=chunk_size,
                    exaggeration=exaggeration,
                    cfg_weight=cfg_weight
                )
                
                # Read the generated file
                with open(audio_path, 'rb') as f:
                    audio_data = f.read()
                
                # Cleanup
                os.unlink(audio_path)
                
                return audio_data
            except Exception as e:
                logger.error(f"Failed to generate speech locally: {str(e)}")
                raise
        
        # Remote mode
        try:
            payload = {
                "text": text,
                "voice_id": voice_id,
                "chunk_size": chunk_size,
                "exaggeration": exaggeration,
                "cfg_weight": cfg_weight
            }
            
            if not self.session:
                self.session = aiohttp.ClientSession()
            
            async with self.session.post(
                f"{self.base_url}/generate",
                json=payload
            ) as response:
                response.raise_for_status()
                return await response.read()
                
        except Exception as e:
            logger.error(f"Failed to generate speech: {str(e)}")
            raise
            
    async def generate_with_voice_cloning(
        self,
        text: str,
        voice_audio_path: Optional[str] = None,
        voice_audio_data: Optional[bytes] = None,
        voice_filename: str = "voice.mp3",
        chunk_size: int = 2048,
        exaggeration: float = 1.0,
        cfg_weight: float = 1.7
    ) -> bytes:
        """
        Generate speech with voice cloning from audio file or data
        
        Args:
            text: Text to convert to speech
            voice_audio_path: Path to audio file for voice cloning
            voice_audio_data: Audio data as bytes (alternative to path)
            voice_filename: Filename for the audio data
            chunk_size: Size of audio chunks for generation
            exaggeration: Voice exaggeration factor
            cfg_weight: Configuration weight for generation
            
        Returns:
            Audio data as bytes
        """
        try:
            # Validate input
            if voice_audio_path is None and voice_audio_data is None:
                raise ValueError("Either voice_audio_path or voice_audio_data must be provided")
            
            if self.mode == "local" and self.local_service:
                # Use local service
                try:
                    # If we have data but no path, write to temp file
                    if voice_audio_data and not voice_audio_path:
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_audio:
                            tmp_audio.write(voice_audio_data)
                            tmp_audio.flush()
                            temp_path = tmp_audio.name
                    else:
                        temp_path = voice_audio_path
                    
                    # Generate with voice cloning
                    audio_path = await self.local_service.generate_with_voice_cloning(
                        text=text,
                        audio_prompt_path=temp_path,
                        chunk_size=chunk_size,
                        exaggeration=exaggeration,
                        cfg_weight=cfg_weight
                    )
                    
                    # Read the generated file
                    with open(audio_path, 'rb') as f:
                        audio_data = f.read()
                    
                    # Cleanup
                    os.unlink(audio_path)
                    if voice_audio_data and not voice_audio_path:
                        os.unlink(temp_path)
                    
                    return audio_data
                except Exception as e:
                    logger.error(f"Failed to generate speech with voice cloning locally: {str(e)}")
                    raise
            
            # Remote mode
            # Create session if not exists
            if not self.session:
                self.session = aiohttp.ClientSession()
            
            # Prepare multipart form data
            data = aiohttp.FormData()
            data.add_field('text', text)
            data.add_field('chunk_size', str(chunk_size))
            data.add_field('exaggeration', str(exaggeration))
            data.add_field('cfg_weight', str(cfg_weight))
            
            # Add audio file or data
            if voice_audio_path:
                with open(voice_audio_path, 'rb') as f:
                    audio_content = f.read()
                    data.add_field(
                        'voice_audio',
                        audio_content,
                        filename=os.path.basename(voice_audio_path),
                        content_type='audio/mpeg'
                    )
            else:
                # Use provided audio data
                data.add_field(
                    'voice_audio',
                    voice_audio_data,
                    filename=voice_filename,
                    content_type='audio/mpeg'
                )
            
            async with self.session.post(
                f"{self.base_url}/generate_with_voice",
                data=data,
                timeout=aiohttp.ClientTimeout(total=120)  # 2 minute timeout for voice cloning
            ) as response:
                response.raise_for_status()
                return await response.read()
                    
        except Exception as e:
            logger.error(f"Failed to generate speech with voice cloning: {str(e)}")
            raise
            
    async def generate_speech_stream(
        self,
        text: str,
        voice_id: str = "default",
        chunk_size: int = 2048,
        exaggeration: float = 1.0,
        cfg_weight: float = 1.7
    ):
        """
        Generate speech from text with streaming response
        
        Args:
            text: Text to convert to speech
            voice_id: ID of the voice to use
            chunk_size: Size of audio chunks for generation
            exaggeration: Voice exaggeration factor
            cfg_weight: Configuration weight for generation
            
        Yields:
            Audio data chunks as bytes
        """
        try:
            # Create session if not exists
            if not self.session:
                self.session = aiohttp.ClientSession()
                
            payload = {
                "text": text,
                "voice_id": voice_id,
                "chunk_size": chunk_size,
                "exaggeration": exaggeration,
                "cfg_weight": cfg_weight
            }
            
            async with self.session.post(
                f"{self.base_url}/generate_stream",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=300)  # 5 minute timeout for streaming
            ) as response:
                response.raise_for_status()
                
                # Stream the response chunks
                async for chunk in response.content.iter_chunked(8192):
                    if chunk:
                        yield chunk
                        
        except aiohttp.ClientResponseError as e:
            if e.status == 501:
                logger.warning("Streaming not implemented, falling back to non-streaming")
                # Fall back to non-streaming
                audio_data = await self.generate_speech(
                    text, voice_id, chunk_size, exaggeration, cfg_weight
                )
                yield audio_data
            else:
                logger.error(f"Failed to generate streaming speech: {str(e)}")
                raise
        except Exception as e:
            logger.error(f"Failed to generate streaming speech: {str(e)}")
            raise

    async def save_audio(self, audio_data: bytes, output_path: str) -> str:
        """
        Save audio data to file
        
        Args:
            audio_data: Audio data as bytes
            output_path: Path to save the audio file
            
        Returns:
            Path to saved file
        """
        try:
            with open(output_path, 'wb') as f:
                f.write(audio_data)
            return output_path
        except Exception as e:
            logger.error(f"Failed to save audio: {str(e)}")
            raise


# Synchronous wrapper for compatibility
class ChatterboxClientSync:
    """Synchronous wrapper for ChatterboxClient"""
    
    def __init__(self, base_url: Optional[str] = None):
        self.base_url = base_url
        
    def generate_speech(self, text: str, **kwargs) -> bytes:
        """Synchronous speech generation"""
        async def _generate():
            async with ChatterboxClient(self.base_url) as client:
                return await client.generate_speech(text, **kwargs)
        
        return asyncio.run(_generate())
        
    def generate_with_voice_cloning(
        self, 
        text: str, 
        voice_audio_path: str, 
        **kwargs
    ) -> bytes:
        """Synchronous speech generation with voice cloning"""
        async def _generate():
            async with ChatterboxClient(self.base_url) as client:
                return await client.generate_with_voice_cloning(
                    text, voice_audio_path, **kwargs
                )
        
        return asyncio.run(_generate())
        
    def health_check(self) -> Dict[str, Any]:
        """Synchronous health check"""
        async def _check():
            async with ChatterboxClient(self.base_url) as client:
                return await client.health_check()
        
        return asyncio.run(_check())


================================================
FILE: chatterbox_service.py
================================================
"""
Chatterbox TTS Service

Singleton service for managing the Chatterbox TTS model and providing
TTS operations within the main application process.
"""

import os
import torch
import tempfile
import subprocess
import asyncio
from typing import Optional, Dict, Any, Callable
import logging
import threading
import time
from pathlib import Path

# Configure logging
logger = logging.getLogger(__name__)

# Enable TunableOp for optimal kernel selection
os.environ["PYTORCH_TUNABLEOP_ENABLED"] = "1"


class ChatterboxService:
    """Singleton service for Chatterbox TTS operations"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """Initialize the service (only runs once due to singleton)"""
        if self._initialized:
            return
            
        self.model = None
        self.device = None
        self.backend = None
        self.model_loaded = False
        self.loading_error = None
        self.loading_progress = 0
        self.loading_stage = ""
        self.download_progress = {}
        
        # Configuration
        self.preload_model = os.getenv("CHATTERBOX_PRELOAD_MODEL", "false").lower() == "true"
        
        # Check for Flash Attention 2 support
        try:
            import flash_attn
            self.flash_attention_available = True
            logger.info("Flash Attention 2 is available")
        except ImportError:
            self.flash_attention_available = False
            logger.warning("Flash Attention 2 not available")
        
        # Detect GPU backend
        self._detect_gpu_backend()
        
        # Optionally preload model
        if self.preload_model:
            try:
                self._load_model()
            except Exception as e:
                logger.error(f"Failed to preload model: {str(e)}")
                self.loading_error = str(e)
        
        self._initialized = True
    
    def _detect_gpu_backend(self):
        """Detect whether we're using ROCm or CUDA"""
        if torch.cuda.is_available():
            if hasattr(torch.version, 'hip') and torch.version.hip is not None:
                self.backend = "rocm"
            else:
                self.backend = "cuda"
            self.device = "cuda"
        else:
            self.backend = "cpu"
            self.device = "cpu"
        
        logger.info(f"Detected GPU backend: {self.backend}")
        
        # Log GPU information if available
        if self.device == "cuda":
            props = torch.cuda.get_device_properties(0)
            logger.info(f"GPU detected: {props.name}")
            logger.info(f"GPU memory: {props.total_memory / 1024**3:.2f} GB")
    
    def _download_with_progress(self, repo_id: str, filename: str, max_retries: int = 3) -> str:
        """Download model file with progress tracking and retries"""
        from huggingface_hub import hf_hub_download
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        # Setup retry strategy
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=2,  # 2, 4, 8 seconds
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        
        # Create session with retry
        session = requests.Session()
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # Set longer timeout
        session.timeout = 300  # 5 minutes
        
        attempt = 0
        last_error = None
        
        while attempt < max_retries:
            try:
                attempt += 1
                logger.info(f"Downloading {filename} (attempt {attempt}/{max_retries})")
                
                # Update progress
                self.download_progress[filename] = {
                    "status": "downloading",
                    "progress": 0,
                    "attempt": attempt
                }
                
                # Try to download with progress callback
                def progress_callback(block_num, block_size, total_size):
                    if total_size > 0:
                        progress = min(100, (block_num * block_size) / total_size * 100)
                        self.download_progress[filename]["progress"] = progress
                        if progress % 10 == 0:  # Log every 10%
                            logger.info(f"{filename}: {progress:.1f}% downloaded")
                
                # Download with huggingface_hub (it handles caching)
                local_path = hf_hub_download(
                    repo_id=repo_id,
                    filename=filename,
                    local_dir_use_symlinks=False,
                    resume_download=True,  # Resume partial downloads
                )
                
                self.download_progress[filename] = {
                    "status": "completed",
                    "progress": 100,
                    "path": local_path
                }
                
                logger.info(f"Successfully downloaded {filename}")
                return local_path
                
            except Exception as e:
                last_error = e
                logger.warning(f"Download attempt {attempt} failed for {filename}: {str(e)}")
                
                self.download_progress[filename] = {
                    "status": "failed",
                    "progress": 0,
                    "error": str(e),
                    "attempt": attempt
                }
                
                if attempt < max_retries:
                    wait_time = 2 ** attempt  # Exponential backoff
                    logger.info(f"Waiting {wait_time} seconds before retry...")
                    time.sleep(wait_time)
                
        # All retries exhausted
        raise Exception(f"Failed to download {filename} after {max_retries} attempts. Last error: {last_error}")
    
    def _load_model(self, progress_callback: Optional[Callable[[str, float], None]] = None):
        """Load the Chatterbox TTS model with progress tracking"""
        if self.model_loaded:
            return
        
        try:
            self.loading_stage = "Initializing"
            self.loading_progress = 0
            logger.info("Loading Chatterbox TTS model...")
            
            if progress_callback:
                progress_callback("Initializing", 0)
            
            # Import here to avoid loading at module level
            from chatterbox.tts import ChatterboxTTS
            
            # Download model files with retry logic
            self.loading_stage = "Downloading model files"
            self.loading_progress = 10
            
            if progress_callback:
                progress_callback("Downloading model files", 10)
            
            # Model files to download
            model_files = ["ve.pt", "t3_cfg.pt", "s3gen.pt", "tokenizer.json", "conds.pt"]
            repo_id = "ResembleAI/chatterbox"
            
            downloaded_files = {}
            for i, filename in enumerate(model_files):
                self.loading_stage = f"Downloading {filename}"
                progress = 10 + (i * 15)  # 10-85% for downloads
                self.loading_progress = progress
                
                if progress_callback:
                    progress_callback(self.loading_stage, progress)
                
                try:
                    local_path = self._download_with_progress(repo_id, filename)
                    downloaded_files[filename] = local_path
                except Exception as e:
                    logger.error(f"Failed to download {filename}: {str(e)}")
                    raise
            
            # Load model from downloaded files
            self.loading_stage = "Loading model into memory"
            self.loading_progress = 85
            
            if progress_callback:
                progress_callback("Loading model into memory", 85)
            
            # Get directory of first downloaded file
            model_dir = Path(downloaded_files["ve.pt"]).parent
            
            # Load model using the local files
            self.model = ChatterboxTTS.from_local(model_dir, device=self.device)
            
            self.loading_stage = "Model ready"
            self.loading_progress = 100
            self.model_loaded = True
            self.loading_error = None
            
            if progress_callback:
                progress_callback("Model ready", 100)
            
            logger.info("Model loaded successfully!")
            
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            self.loading_error = str(e)
            self.loading_stage = "Failed"
            
            if progress_callback:
                progress_callback(f"Failed: {str(e)}", self.loading_progress)
            
            raise
    
    async def ensure_model_loaded(self, progress_callback: Optional[Callable[[str, float], None]] = None):
        """Ensure model is loaded before using it"""
        if not self.model_loaded:
            # Run model loading in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._load_model, progress_callback)
    
    async def get_health_info(self) -> Dict[str, Any]:
        """Get health and status information"""
        return {
            "status": "healthy" if self.model_loaded or not self.loading_error else "degraded",
            "model_loaded": self.model_loaded,
            "loading_error": self.loading_error,
            "loading_progress": self.loading_progress,
            "loading_stage": self.loading_stage,
            "device": self.device,
            "gpu_available": torch.cuda.is_available(),
            "gpu_backend": self.backend,
            "flash_attention": self.flash_attention_available,
            "download_status": self.download_progress
        }
    
    async def get_gpu_metrics(self) -> Dict[str, Any]:
        """Get GPU utilization metrics"""
        metrics = {
            "gpu_available": torch.cuda.is_available(),
            "gpu_backend": self.backend,
            "device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
        
        if torch.cuda.is_available():
            metrics.update({
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_used_gb": torch.cuda.memory_allocated() / 1024**3,
                "gpu_memory_cached_gb": torch.cuda.memory_reserved() / 1024**3,
                "gpu_memory_total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3,
                "gpu_utilization": torch.cuda.utilization() if hasattr(torch.cuda, 'utilization') else "N/A"
            })
            
            # Backend-specific metrics
            if self.backend == "rocm":
                metrics["rocm_version"] = torch.version.hip if hasattr(torch.version, 'hip') else "N/A"
                # Try to get temperature from rocm-smi
                try:
                    result = subprocess.run(['rocm-smi', '--showtemp'], capture_output=True, text=True)
                    if result.returncode == 0:
                        metrics["gpu_temperature"] = "See rocm-smi output"
                except:
                    pass
            elif self.backend == "cuda":
                metrics["cuda_version"] = torch.version.cuda
                metrics["cudnn_version"] = torch.backends.cudnn.version()
                # Try to get temperature from nvidia-smi
                try:
                    result = subprocess.run(['nvidia-smi', '--query-gpu=temperature.gpu', '--format=csv,noheader,nounits'], 
                                          capture_output=True, text=True)
                    if result.returncode == 0:
                        metrics["gpu_temperature_c"] = int(result.stdout.strip())
                except:
                    pass
        
        return metrics
    
    async def generate_speech(
        self,
        text: str,
        voice_id: str = "default",
        chunk_size: int = 2048,
        exaggeration: float = 1.0,
        cfg_weight: float = 1.7
    ) -> str:
        """
        Generate speech from text
        
        Returns:
            Path to generated audio file
        """
        # Ensure model is loaded
        await self.ensure_model_loaded()
        
        # Generate in thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            self._generate_speech_sync,
            text, voice_id, chunk_size, exaggeration, cfg_weight
        )
        
        return result
    
    def _generate_speech_sync(
        self,
        text: str,
        voice_id: str,
        chunk_size: int,
        exaggeration: float,
        cfg_weight: float
    ) -> str:
        """Synchronous speech generation"""
        # For default voice, generate without audio prompt
        if voice_id == "default":
            # Note: chunk_size is not used by ChatterboxTTS.generate()
            audio = self.model.generate(
                text=text,
                exaggeration=exaggeration,
                cfg_weight=cfg_weight
            )
        else:
            # For custom voice, would need audio_prompt_path
            raise ValueError("Custom voice requires audio file upload")
        
        # Save to temporary file
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            # Save audio tensor to file
            import torchaudio
            torchaudio.save(tmp_file.name, audio.cpu(), 24000)  # Assuming 24kHz sample rate
            return tmp_file.name
    
    async def generate_with_voice_cloning(
        self,
        text: str,
        audio_prompt_path: str,
        chunk_size: int = 2048,
        exaggeration: float = 1.0,
        cfg_weight: float = 1.7,
        progress_callback: Optional[Callable[[str, float], None]] = None
    ) -> str:
        """
        Generate speech with voice cloning
        
        Returns:
            Path to generated audio file
        """
        # Ensure model is loaded with progress tracking
        await self.ensure_model_loaded(progress_callback)
        
        # Generate in thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            self._generate_with_voice_cloning_sync,
            text, audio_prompt_path, chunk_size, exaggeration, cfg_weight
        )
        
        return result
    
    def _generate_with_voice_cloning_sync(
        self,
        text: str,
        audio_prompt_path: str,
        chunk_size: int,
        exaggeration: float,
        cfg_weight: float
    ) -> str:
        """Synchronous speech generation with voice cloning"""
        # Generate audio with voice cloning
        # Note: chunk_size is not used by ChatterboxTTS.generate()
        audio = self.model.generate(
            text=text,
            audio_prompt_path=audio_prompt_path,
            exaggeration=exaggeration,
            cfg_weight=cfg_weight
        )
        
        # Save to temporary file
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            import torchaudio
            torchaudio.save(tmp_file.name, audio.cpu(), 24000)
            return tmp_file.name


================================================
FILE: comprehensive_audio_service.py
================================================
"""
Comprehensive Audio Processing Service
Based on the proven logic from tests/stream_simulation.py that actually works
"""

import os
import sys
import logging
import json
import numpy as np
import asyncio
import tempfile
import soundfile as sf
import textwrap
from pathlib import Path
from scipy.spatial.distance import cosine
from typing import Dict, Any, Optional, List
from datetime import datetime

# Import our working services
from src.services.realtime_analysis_service import get_realtime_analysis_service
from src.services.audio_separation_service import audio_separation_service

# Import LangExtract for advanced analysis
try:
    import langextract as lx
    LANGEXTRACT_AVAILABLE = True
except ImportError:
    logger.warning("LangExtract not available. Advanced sentiment analysis will be disabled.")
    LANGEXTRACT_AVAILABLE = False

logger = logging.getLogger(__name__)

# --- START: LangExtract Integration (copied from stream_simulation.py) ---
class LangExtractStreamAnalyzer:
    """
    Uses LangExtract to extract structured information from transcribed text
    in streaming audio, providing detailed sentiment and contextual analysis.
    """
    
    def __init__(self):
        self.prompt = textwrap.dedent("""
            Extract comprehensive information from the transcribed text including:
            1. Speaker emotional states (happy, sad, angry, neutral, anxious, excited, frustrated)
            2. Sentiment analysis (positive, negative, neutral, mixed)
            3. Conversation topics and themes
            4. Speaker engagement level (high, medium, low)
            5. Key phrases or important statements
            6. Turn-taking indicators
            
            Provide exact text extractions with contextual attributes.
        """)
        
        self.examples = [
            lx.data.ExampleData(
                text="I'm really disappointed with this service, it's been terrible.",
                extractions=[
                    lx.data.Extraction(
                        extraction_class="emotion",
                        extraction_text="disappointed",
                        attributes={"type": "negative", "intensity": "high", "context": "service quality"}
                    ),
                    lx.data.Extraction(
                        extraction_class="sentiment",
                        extraction_text="negative",
                        attributes={"confidence": "high", "trigger": "terrible service"}
                    ),
                    lx.data.Extraction(
                        extraction_class="topic",
                        extraction_text="service quality",
                        attributes={"category": "customer_service", "sentiment": "negative"}
                    )
                ]
            ),
            lx.data.ExampleData(
                text="That's wonderful news! I'm so happy to hear about your promotion.",
                extractions=[
                    lx.data.Extraction(
                        extraction_class="emotion",
                        extraction_text="happy",
                        attributes={"type": "positive", "intensity": "high", "context": "promotion news"}
                    ),
                    lx.data.Extraction(
                        extraction_class="engagement",
                        extraction_text="high",
                        attributes={"type": "enthusiastic", "response": "excitement"}
                    )
                ]
            )
        ]
    
    def analyze_transcription(self, text: str) -> dict:
        """Analyze transcribed text using LangExtract; falls back to local heuristics if no API key."""
        if not text or not text.strip():
            return {"error": "Empty transcription"}

        # Fallback analyzer for offline/no-API mode
        def _fallback(text_in: str) -> dict:
            lower = text_in.lower()
            # Expanded keyword detection
            pos_words = {"great", "amazing", "happy", "wonderful", "excited", "love", "good", "excellent", "fantastic", "awesome", "pleased"}
            neg_words = {"angry", "frustrated", "terrible", "bad", "awful", "hate", "disappointed", "sad", "upset", "annoyed", "furious"}
            question_words = {"what", "how", "when", "where", "why", "who", "can", "could", "would", "should"}
            business_words = {"company", "business", "service", "product", "customer", "client", "sales", "marketing"}
            
            emo_map = {
                "happy": ["happy", "excited", "joy", "glad", "pleased", "thrilled"],
                "angry": ["angry", "mad", "furious", "upset", "annoyed", "frustrated"],
                "sad": ["sad", "down", "unhappy", "disappointed", "depressed"],
                "neutral": ["ok", "fine", "alright"],
                "curious": ["interested", "wondering", "curious"],
            }
            
            sentiments = []
            emotions = []
            topics = []
            engagement = []
            
            # Sentiment detection
            if any(w in lower for w in pos_words):
                sentiments.append({"class": "sentiment", "text": "positive", "attributes": {"confidence": "medium", "trigger": "positive_keywords"}})
            elif any(w in lower for w in neg_words):
                sentiments.append({"class": "sentiment", "text": "negative", "attributes": {"confidence": "medium", "trigger": "negative_keywords"}})
            else:
                sentiments.append({"class": "sentiment", "text": "neutral", "attributes": {"confidence": "low", "reason": "no_clear_indicators"}})
            
            # Emotion detection
            for label, keys in emo_map.items():
                if any(k in lower for k in keys):
                    emotions.append({"class": "emotion", "text": label, "attributes": {"detected": True, "confidence": "medium"}})
            
            # Topic detection
            if any(w in lower for w in business_words):
                topics.append({"class": "topic", "text": "business_conversation", "attributes": {"category": "professional"}})
            if any(w in lower for w in question_words):
                topics.append({"class": "topic", "text": "inquiry", "attributes": {"type": "question"}})
            
            # Engagement detection
            if len([w for w in question_words if w in lower]) > 0:
                engagement.append({"class": "engagement", "text": "inquisitive", "attributes": {"type": "asking_questions"}})
            if len(text_in.split()) > 10:
                engagement.append({"class": "engagement", "text": "high", "attributes": {"reason": "lengthy_response"}})
            elif len(text_in.split()) < 3:
                engagement.append({"class": "engagement", "text": "low", "attributes": {"reason": "brief_response"}})
            else:
                engagement.append({"class": "engagement", "text": "medium", "attributes": {"reason": "moderate_length"}})
            
            analysis = {
                "emotions": emotions,
                "sentiments": sentiments,
                "topics": topics,
                "engagement": engagement,
                "key_phrases": [{"class": "key_phrases", "text": phrase, "attributes": {"importance": "medium"}} for phrase in text_in.split('.') if len(phrase.strip()) > 5][:3],
                "raw_extractions": emotions + sentiments + topics + engagement,
                "note": "enhanced_offline_fallback"
            }
            return analysis

        # If no API key, use fallback
        if not LANGEXTRACT_AVAILABLE:
            return _fallback(text)
            
        api_key = os.getenv("LANGEXTRACT_API_KEY")
        if not api_key:
            return _fallback(text)

        try:
            result = lx.extract(
                text_or_documents=text,
                prompt_description=self.prompt,
                examples=self.examples,
                model_id="gemini-2.5-flash",
                extraction_passes=2,
                max_workers=3,
                api_key=api_key
            )

            analysis = {
                "emotions": [],
                "sentiments": [],
                "topics": [],
                "engagement": [],
                "key_phrases": [],
                "raw_extractions": []
            }

            for extraction in result.extractions:
                extraction_data = {
                    "class": extraction.extraction_class,
                    "text": extraction.extraction_text,
                    "attributes": extraction.attributes or {}
                }
                analysis["raw_extractions"].append(extraction_data)

                if extraction.extraction_class == "emotion":
                    analysis["emotions"].append(extraction_data)
                elif extraction.extraction_class == "sentiment":
                    analysis["sentiments"].append(extraction_data)
                elif extraction.extraction_class == "topic":
                    analysis["topics"].append(extraction_data)
                elif extraction.extraction_class == "engagement":
                    analysis["engagement"].append(extraction_data)
                elif extraction.extraction_class == "key_phrases":
                    analysis["key_phrases"].append(extraction_data)

            return analysis

        except Exception as e:
            logging.error(f"LangExtract analysis failed: {e}")
            return _fallback(text)

# Initialize LangExtract analyzer
langextract_analyzer = LangExtractStreamAnalyzer()
# --- END: LangExtract Integration ---

# --- Emotion2Vec Integration (copied from stream_simulation.py) ---
EMOTION2VEC_AVAILABLE = False
emotion2vec_model = None

def _load_emotion2vec_if_needed():
    global EMOTION2VEC_AVAILABLE, emotion2vec_model
    if EMOTION2VEC_AVAILABLE and emotion2vec_model is not None:
        return
    try:
        from funasr import AutoModel as FunASRAutoModel
        # Allow override via env
        override_model_id = os.getenv("EMOTION2VEC_MODEL_ID")
        candidate_models = [
            override_model_id.strip() if override_model_id else None,
            "iic/emotion2vec_plus_large",
            "iic/emotion2vec_base",
        ]
        candidate_models = [m for m in candidate_models if m]

        # Prefer local cache if available
        def _local_candidate_for(hf_id: str) -> str | None:
            owner, name = hf_id.split("/")
            # standard HF cache layout
            local_dir = Path.home() / ".cache" / "huggingface" / "hub" / f"models--{owner}--{name}"
            return str(local_dir) if local_dir.exists() else None

        # Temporarily disable global HF Hub token to avoid 401 on public models
        prev_tokens = {
            "HUGGINGFACE_HUB_TOKEN": os.environ.get("HUGGINGFACE_HUB_TOKEN"),
            "HUGGINGFACE_TOKEN": os.environ.get("HUGGINGFACE_TOKEN"),
            "HF_TOKEN": os.environ.get("HF_TOKEN"),
        }
        try:
            # Clear all token env vars to prevent invalid-token 401 on public models
            for k in list(prev_tokens.keys()):
                if prev_tokens[k] is not None:
                    os.environ[k] = ""
            last_err = None
            for mid in candidate_models:
                try:
                    local_dir = _local_candidate_for(mid)
                    model_arg = local_dir if local_dir else mid
                    emotion2vec_model = FunASRAutoModel(
                        model=model_arg,
                        hub="hf",
                        disable_update=True,
                        device="cpu",  # Use CPU to avoid CUDA memory conflicts
                    )
                    logger.info(f"emotion2vec model loaded: {model_arg}")
                    break
                except Exception as inner_e:
                    last_err = inner_e
                    emotion2vec_model = None
            if emotion2vec_model is None and last_err:
                raise last_err
        finally:
            # Restore token environment variable
            for k, v in prev_tokens.items():
                if v is not None:
                    os.environ[k] = v
        EMOTION2VEC_AVAILABLE = True
        logger.info("emotion2vec model loaded successfully")
    except Exception as e:
        EMOTION2VEC_AVAILABLE = False
        emotion2vec_model = None
        logger.warning(f"Could not load emotion2vec model: {e}")

def _compute_emotion2vec_head_tokens(int16_audio: np.ndarray, head: int = 8) -> dict:
    """Compute emotion2vec embedding, label, and return a small head of the embedding as tokens."""
    if int16_audio.size == 0:
        return {"tokens": [], "label": None, "scores": None}
    if not EMOTION2VEC_AVAILABLE or emotion2vec_model is None:
        _load_emotion2vec_if_needed()
        if not EMOTION2VEC_AVAILABLE or emotion2vec_model is None:
            return {"tokens": [], "label": None, "scores": None}
    try:
        audio_float = int16_audio.astype(np.float32) / 32768.0
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as tmp_file:
            sf.write(tmp_file.name, audio_float, SAMPLE_RATE, subtype='PCM_16')
            rec = emotion2vec_model.generate(
                tmp_file.name,
                granularity="utterance",
                extract_embedding=True,
            )
        if not rec or "feats" not in rec[0]:
            return {"tokens": [], "label": None, "scores": None}
        emb = rec[0]["feats"].astype(np.float32)
        token_head = emb[:head].tolist()
        return {"tokens": token_head, "label": rec[0].get("labels"), "scores": rec[0].get("scores")}
    except Exception as e:
        logger.warning(f"emotion2vec failed: {e}")
        return {"tokens": [], "label": None, "scores": None}
# --- END: Emotion2Vec Integration ---

# Import dependencies with fallbacks
try:
    import torch
    import torchaudio
    # Enable better CUDA memory management
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    from speechbrain.pretrained import SepformerSeparation as separator_model
    from speechbrain.pretrained import EncoderClassifier as speaker_id_model
    SPEECHBRAIN_AVAILABLE = True
except ImportError:
    logger.warning("Could not import SpeechBrain. Speaker separation/ID will be unavailable.")
    SPEECHBRAIN_AVAILABLE = False
    
try:
    if 'torch' not in sys.modules:
        import torch
    torch.set_num_threads(1)
    vad_model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False, trust_repo=True)
    (get_speech_timestamps, _, read_audio, _, _) = utils
    SILERO_AVAILABLE = True
    logger.info("Silero VAD loaded successfully")
except Exception as e:
    logger.warning(f"Could not load Silero VAD: {e}. VAD segmentation will be unavailable.")
    SILERO_AVAILABLE = False

SAMPLE_RATE = 16000

class StatefulSpeakerIdentifier:
    """
    Stateful speaker identification that maintains speaker profiles across audio processing.
    This is the proven approach from tests/stream_simulation.py
    """
    def __init__(self, cache_dir="speaker_id_cache", similarity_threshold=0.60):
        if not SPEECHBRAIN_AVAILABLE:
            raise ImportError("SpeechBrain is not installed.")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"SpeakerIdentifier using device: {self.device}")
        
        self.embedding_model = speaker_id_model.from_hparams(
            source="speechbrain/spkrec-ecapa-voxceleb",
            savedir=Path(cache_dir) / "spkrec-ecapa-voxceleb",
            run_opts={"device": self.device}
        )
        self.similarity_threshold = similarity_threshold
        
        # Speaker profiles store centroid and count for updating
        self.speaker_profiles = {}  # Format: { 'Speaker 1': {'centroid': vector, 'count': N}, ... }
        self.next_speaker_id = 1
        logger.info(f"StatefulSpeakerIdentifier initialized with threshold={self.similarity_threshold}.")

    def _get_embedding(self, audio_chunk_np: np.ndarray):
        """Generates a voice embedding for an audio chunk."""
        if audio_chunk_np.dtype != np.float32:
            audio_chunk_np = audio_chunk_np.astype(np.float32) / 32768.0
        
        audio_tensor = torch.from_numpy(audio_chunk_np).to(self.device)
        with torch.no_grad():
            embedding = self.embedding_model.encode_batch(audio_tensor.unsqueeze(0))
            return embedding.squeeze(0).squeeze(0).cpu().numpy()

    def identify_speaker(self, audio_chunk_np: np.ndarray):
        """
        Identifies speaker and updates centroids for stateful identification.
        This is the core logic that works in the test.
        """
        if len(audio_chunk_np) < SAMPLE_RATE * 0.25:
            return "Unknown", 0.0
            
        current_embedding = self._get_embedding(audio_chunk_np)

        # Handle the very first speaker
        if not self.speaker_profiles:
            new_speaker_name = f"Speaker {self.next_speaker_id}"
            self.speaker_profiles[new_speaker_name] = {'centroid': current_embedding, 'count': 1}
            self.next_speaker_id += 1
            logger.info(f"Enrolled first speaker: {new_speaker_name}")
            return new_speaker_name, 1.0

        # Compare with existing speakers
        best_match_speaker = "Unknown"
        highest_similarity = -1

        for name, profile in self.speaker_profiles.items():
            similarity = 1 - cosine(current_embedding, profile['centroid'])
            if similarity > highest_similarity:
                highest_similarity = similarity
                best_match_speaker = name

        # Update or create speaker profile
        if highest_similarity >= self.similarity_threshold:
            # Update existing speaker centroid
            profile_to_update = self.speaker_profiles[best_match_speaker]
            old_centroid = profile_to_update['centroid']
            old_count = profile_to_update['count']
            
            # Calculate new running average for the centroid
            new_centroid = (old_centroid * old_count + current_embedding) / (old_count + 1)
            
            # Update the stored profile
            self.speaker_profiles[best_match_speaker]['centroid'] = new_centroid
            self.speaker_profiles[best_match_speaker]['count'] += 1
            
            logger.info(f"Matched existing speaker: {best_match_speaker}. Updating profile (count: {old_count + 1}).")
            return best_match_speaker, highest_similarity
        else:
            # Create new speaker
            new_speaker_name = f"Speaker {self.next_speaker_id}"
            self.speaker_profiles[new_speaker_name] = {'centroid': current_embedding, 'count': 1}
            self.next_speaker_id += 1
            logger.info(f"Enrolled new speaker: {new_speaker_name} (highest similarity to others: {highest_similarity:.2f})")
            return new_speaker_name, highest_similarity


def get_vad_segments(audio_data, min_speech_duration=0.5, min_silence_duration=0.25):
    """Extract speech segments using Silero VAD"""
    if not SILERO_AVAILABLE:
        # Fallback: treat entire audio as one segment
        return [{'audio': audio_data, 'start_time': 0, 'end_time': len(audio_data) / SAMPLE_RATE}]
    
    audio_float = audio_data.astype(np.float32) / 32768.0
    speech_timestamps = get_speech_timestamps(
        torch.from_numpy(audio_float), 
        vad_model, 
        sampling_rate=SAMPLE_RATE,
        min_speech_duration_ms=int(min_speech_duration*1000),
        min_silence_duration_ms=int(min_silence_duration*1000)
    )
    return [{'audio': audio_data[s['start']:s['end']], 'start_time': s['start']/SAMPLE_RATE, 'end_time': s['end']/SAMPLE_RATE} for s in speech_timestamps]


async def _segments_from_pyannote_diarization(int16_audio: np.ndarray) -> list[dict]:
    """Build segments using pyannote diarization over the given audio."""
    import torch as _torch
    float_audio = int16_audio.astype(np.float32) / 32768.0
    waveform = _torch.from_numpy(float_audio).unsqueeze(0)
    diarization_result = await audio_separation_service.diarize_from_waveform(
        waveform=waveform, sample_rate=SAMPLE_RATE, min_duration=0.5
    )
    segments: list[dict] = []
    for seg in diarization_result.get("segments", []):
        start_idx = int(seg["start"] * SAMPLE_RATE)
        end_idx = int(seg["end"] * SAMPLE_RATE)
        segments.append({
            'audio': (int16_audio[start_idx:end_idx] if end_idx > start_idx else np.array([], dtype=np.int16)),
            'start_time': float(seg['start']),
            'end_time': float(seg['end']),
            'speaker': seg.get('speaker', 'SPEAKER_00')
        })
    return segments


class ComprehensiveAudioService:
    """
    Comprehensive audio processing service that uses the proven approach from stream_simulation.py
    """
    
    def __init__(self):
        self.speaker_identifier = None
        
    async def process_audio_comprehensive(
        self,
        audio_path: str,
        separate_speakers: bool = True,
        use_pyannote: bool = True,
        max_seconds: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        Process audio using the proven comprehensive approach that actually works.
        Based on tests/stream_simulation.py logic.
        """
        logger.info("=== COMPREHENSIVE AUDIO PROCESSING START ===")
        
        # Initialize speaker identifier if needed
        if separate_speakers and SPEECHBRAIN_AVAILABLE:
            try:
                self.speaker_identifier = StatefulSpeakerIdentifier()
            except Exception as e:
                logger.warning(f"Could not initialize speaker identifier: {e}")
                separate_speakers = False
        
        # Get realtime analysis service
        service = await get_realtime_analysis_service()
        
        # Load audio
        try:
            import librosa
            audio, _ = librosa.load(audio_path, sr=SAMPLE_RATE, mono=True)
            if max_seconds is not None and max_seconds > 0:
                max_samples = int(max_seconds * SAMPLE_RATE)
                audio = audio[:max_samples]
            test_audio = (audio * 32767).astype(np.int16)
        except Exception as e:
            logger.error(f"Failed to load audio: {e}")
            return {"error": f"Failed to load audio: {e}"}
        
        if test_audio is None or len(test_audio) == 0:
            return {"error": "No audio data"}
        
        # Get segments using the proven approach
        if use_pyannote:
            segments = await _segments_from_pyannote_diarization(test_audio)
        else:
            segments = get_vad_segments(test_audio)
        
        if not segments:
            logger.warning("No speech segments detected.")
            return {"error": "No speech segments detected"}
        
        # Process each segment
        results = []
        full_transcript = ""
        
        for i, segment in enumerate(segments, 1):
            original_segment_audio = segment['audio']
            logger.info(f"Processing segment {i}/{len(segments)} (Time: {segment['start_time']:.2f}s - {segment['end_time']:.2f}s)")
            
            if len(original_segment_audio) == 0:
                continue
            
            try:
                # Process with realtime analysis service
                result = await service.process_sentiment_chunk(original_segment_audio.tobytes())
                transcription = result.get('text', '').strip()
                
                if not transcription or transcription in ["[NO SPEECH DETECTED]", "[ASR FAILED]"]:
                    logger.warning(f"Segment {i}: No speech detected. Skipping.")
                    continue
                
                # Speaker identification if enabled
                speaker_label = segment.get('speaker', 'Unknown')
                speaker_similarity = 0.0
                
                if separate_speakers and self.speaker_identifier:
                    try:
                        # Clear CUDA cache before speaker identification to avoid memory issues
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        speaker_label, speaker_similarity = self.speaker_identifier.identify_speaker(original_segment_audio)
                    except Exception as e:
                        logger.warning(f"Speaker identification failed for segment {i}: {e}")
                        # Fallback to pyannote speaker if available
                        speaker_label = segment.get('speaker', 'Unknown')
                
                # 🔥 LangExtract analysis (uses env var LANGEXTRACT_API_KEY if set)
                langextract_analysis = langextract_analyzer.analyze_transcription(transcription)
                
                # Create concise summary like in the working test
                le_summary = {
                    "emotions": [e.get("text", "") for e in langextract_analysis.get("emotions", [])],
                    "sentiments": [s.get("text", "") for s in langextract_analysis.get("sentiments", [])],
                    "topics": [t.get("text", "") for t in langextract_analysis.get("topics", [])],
                    "engagement": [eng.get("text", "") for eng in langextract_analysis.get("engagement", [])]
                }
                logger.info(f"LangExtract Summary: {json.dumps(le_summary, indent=2)}")
                
                # 🎭 Emotion2Vec analysis with CUDA memory management
                try:
                    # Clear CUDA cache before emotion2vec
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    emotion2vec_result = _compute_emotion2vec_head_tokens(original_segment_audio)
                except Exception as e:
                    logger.warning(f"Emotion2Vec failed for segment {i}: {e}")
                    emotion2vec_result = {"tokens": [], "label": None, "scores": None}
                
                segment_result = {
                    "segment_id": i,
                    "start_time": segment['start_time'],
                    "end_time": segment['end_time'],
                    "duration": segment['end_time'] - segment['start_time'],
                    "speaker": speaker_label,
                    "speaker_similarity": speaker_similarity,
                    "text": transcription,
                    "sentiment": result.get('sentiment', 'neutral'),
                    "tokens": result.get('tokens', []),
                    "langextract_analysis": langextract_analysis,  # 🔥 Advanced LLM-based analysis
                    "emotion2vec": emotion2vec_result  # 🎭 AI emotion detection
                }
                
                results.append(segment_result)
                full_transcript += f"{transcription} "
                
                # Concise segment logging
                logger.info(f"Segment {i} - Speaker: {speaker_label}, Text: {transcription[:50]}...")
                if emotion2vec_result.get("label"):
                    logger.info(f"  Emotion2Vec: {emotion2vec_result['label']}")
                
                # Only log if there are meaningful results (not just neutral)
                if le_summary.get("emotions") and any(e != "neutral" for e in le_summary["emotions"]):
                    logger.info(f"  Emotions: {', '.join(le_summary['emotions'])}")
                if le_summary.get("topics"):
                    logger.info(f"  Topics: {', '.join(le_summary['topics'])}")
                if le_summary.get("engagement") and any(e in ["high", "low"] for e in le_summary["engagement"]):
                    logger.info(f"  Engagement: {', '.join(le_summary['engagement'])}")
                
            except Exception as e:
                logger.error(f"Error processing segment {i}: {e}")
                continue
        
        logger.info("=== COMPREHENSIVE AUDIO PROCESSING COMPLETE ===")
        
        # Generate Speakers Page Summary (like frontend display)
        if results:
            logger.info("\n🎭 === SPEAKERS PAGE PREVIEW ===")
            speakers_summary = {}
            
            # Group segments by speaker
            for segment in results:
                speaker = segment["speaker"]
                if speaker not in speakers_summary:
                    speakers_summary[speaker] = {
                        "segments": [],
                        "total_time": 0,
                        "emotions": [],
                        "topics": [],
                        "sentiment_counts": {"positive": 0, "negative": 0, "neutral": 0}
                    }
                
                speakers_summary[speaker]["segments"].append(segment)
                speakers_summary[speaker]["total_time"] += segment.get("duration", 0)
                
                # Collect analysis data
                if segment.get("langextract_analysis"):
                    le = segment["langextract_analysis"]
                    speakers_summary[speaker]["emotions"].extend([e.get("text", "") for e in le.get("emotions", [])])
                    speakers_summary[speaker]["topics"].extend([t.get("text", "") for t in le.get("topics", [])])
                
                # Count sentiments
                sentiment = segment.get("sentiment", "neutral")
                if sentiment in speakers_summary[speaker]["sentiment_counts"]:
                    speakers_summary[speaker]["sentiment_counts"][sentiment] += 1
            
            # Log speaker summaries
            for speaker, data in speakers_summary.items():
                logger.info(f"\n📋 {speaker}:")
                logger.info(f"  • Segments: {len(data['segments'])}")
                logger.info(f"  • Speaking time: {data['total_time']:.1f}s")
                
                # Top emotions (non-neutral)
                emotions = [e for e in data['emotions'] if e and e != 'neutral']
                if emotions:
                    emotion_counts = {}
                    for e in emotions:
                        emotion_counts[e] = emotion_counts.get(e, 0) + 1
                    top_emotions = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)[:3]
                    logger.info(f"  • Top emotions: {', '.join([f'{e}({c})' for e, c in top_emotions])}")
                
                # Top topics
                topics = [t for t in data['topics'] if t]
                if topics:
                    topic_counts = {}
                    for t in topics:
                        topic_counts[t] = topic_counts.get(t, 0) + 1
                    top_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:2]
                    logger.info(f"  • Key topics: {', '.join([f'{t}({c})' for t, c in top_topics])}")
                
                # Sentiment distribution
                sentiments = data['sentiment_counts']
                total_segments = sum(sentiments.values())
                if total_segments > 0:
                    sentiment_pct = {k: (v/total_segments)*100 for k, v in sentiments.items() if v > 0}
                    sentiment_str = ', '.join([f'{k}: {v:.0f}%' for k, v in sentiment_pct.items()])
                    logger.info(f"  • Sentiment: {sentiment_str}")
                
                # Sample quotes
                sample_quotes = [s["text"][:60] + "..." if len(s["text"]) > 60 else s["text"] 
                               for s in data["segments"][:2] if s.get("text")]
                if sample_quotes:
                    logger.info(f"  • Sample quotes:")
                    for quote in sample_quotes:
                        logger.info(f"    - \"{quote}\"")
            
            logger.info("\n🎭 === END SPEAKERS PAGE PREVIEW ===\n")
        
        return {
            "transcript": full_transcript.strip(),
            "language": "en",
            "total_segments": len(results),
            "total_duration": segments[-1]['end_time'] if segments else 0,
            "segments": results,
            "speakers": list(set(r["speaker"] for r in results)),
            "processing_approach": "comprehensive_stateful"
        }


# Global instance
comprehensive_audio_service = ComprehensiveAudioService()


================================================
FILE: content_chunking_service.py
================================================
"""
Content Chunking Service

Provides text segmentation and chunking capabilities for preparing content 
for vector database storage and retrieval. Supports configurable chunk sizes,
overlap settings, metadata preservation, and boundary detection.
"""

import os
import uuid
import logging
import re
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
import tiktoken
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

logger = logging.getLogger(__name__)


class ChunkBoundary(Enum):
    """Chunking boundary types"""
    SENTENCE = "sentence"
    PARAGRAPH = "paragraph"
    WORD = "word"
    TOKEN = "token"


class ChunkQuality(Enum):
    """Chunk quality indicators"""
    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"


@dataclass
class ChunkingConfig:
    """Configuration for content chunking"""
    chunk_size: int = 1024  # Target chunk size in tokens
    overlap: int = 200  # Overlap between chunks in tokens
    min_chunk_size: int = 100  # Minimum chunk size in tokens
    max_chunk_size: int = 4096  # Maximum chunk size in tokens
    boundary_type: ChunkBoundary = ChunkBoundary.SENTENCE
    preserve_metadata: bool = True
    sentence_aware: bool = True
    paragraph_aware: bool = True
    remove_empty_chunks: bool = True
    quality_threshold: ChunkQuality = ChunkQuality.FAIR
    encoding_model: str = "cl100k_base"  # GPT-3.5/GPT-4 tokenizer


@dataclass
class ChunkMetadata:
    """Metadata for a content chunk"""
    chunk_id: str
    source_id: str
    chunk_index: int
    total_chunks: int
    start_position: int
    end_position: int
    token_count: int
    word_count: int
    sentence_count: int
    paragraph_count: int
    quality_score: float
    quality_level: ChunkQuality
    boundary_type: ChunkBoundary
    overlap_with_previous: int
    overlap_with_next: int
    created_at: datetime
    source_metadata: Dict[str, Any]


@dataclass
class ContentChunk:
    """A chunk of content with metadata"""
    text: str
    metadata: ChunkMetadata
    relationships: Dict[str, str]  # Links to related chunks
    embeddings: Optional[List[float]] = None


@dataclass
class ChunkingResult:
    """Result of content chunking operation"""
    chunks: List[ContentChunk]
    total_chunks: int
    total_tokens: int
    avg_chunk_size: float
    quality_distribution: Dict[ChunkQuality, int]
    processing_time: float
    config: ChunkingConfig


class ContentChunkingService:
    """Service for chunking text content for vector database storage"""
    
    def __init__(self, config: Optional[ChunkingConfig] = None):
        """Initialize the content chunking service"""
        self.config = config or ChunkingConfig()
        self.encoder = tiktoken.get_encoding(self.config.encoding_model)
        
        # Initialize NLTK if not already done
        self._ensure_nltk_data()
        
        logger.info(f"Content Chunking Service initialized with config: {self.config}")
    
    def _ensure_nltk_data(self):
        """Ensure required NLTK data is downloaded"""
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            logger.info("Downloading NLTK punkt tokenizer...")
            nltk.download('punkt', quiet=True)
        
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            logger.info("Downloading NLTK stopwords...")
            nltk.download('stopwords', quiet=True)
    
    def chunk_transcript(
        self,
        transcript: str,
        source_metadata: Dict[str, Any],
        config: Optional[ChunkingConfig] = None
    ) -> ChunkingResult:
        """
        Chunk a transcript with speaker-aware segmentation
        
        Args:
            transcript: The transcript text to chunk
            source_metadata: Metadata about the source content
            config: Optional chunking configuration
            
        Returns:
            ChunkingResult with chunked content and metadata
        """
        start_time = datetime.now()
        config = config or self.config
        
        try:
            # Preprocess transcript to handle speaker labels
            processed_text = self._preprocess_transcript(transcript)
            
            # Perform chunking
            chunks = self._chunk_text(processed_text, source_metadata, config)
            
            # Calculate result metrics
            total_tokens = sum(chunk.metadata.token_count for chunk in chunks)
            avg_chunk_size = total_tokens / len(chunks) if chunks else 0
            
            # Quality distribution
            quality_dist = {quality: 0 for quality in ChunkQuality}
            for chunk in chunks:
                quality_dist[chunk.metadata.quality_level] += 1
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return ChunkingResult(
                chunks=chunks,
                total_chunks=len(chunks),
                total_tokens=total_tokens,
                avg_chunk_size=avg_chunk_size,
                quality_distribution=quality_dist,
                processing_time=processing_time,
                config=config
            )
            
        except Exception as e:
            logger.error(f"Error chunking transcript: {str(e)}")
            raise
    
    def chunk_with_overlap(
        self,
        text: str,
        source_metadata: Dict[str, Any],
        chunk_size: int,
        overlap: int,
        config: Optional[ChunkingConfig] = None
    ) -> ChunkingResult:
        """
        Chunk text with configurable overlap
        
        Args:
            text: Text to chunk
            source_metadata: Source metadata
            chunk_size: Target chunk size in tokens
            overlap: Overlap between chunks in tokens
            config: Optional chunking configuration
            
        Returns:
            ChunkingResult with chunked content
        """
        start_time = datetime.now()
        config = config or self.config
        
        # Override config with provided parameters
        config.chunk_size = chunk_size
        config.overlap = overlap
        
        try:
            chunks = self._chunk_text(text, source_metadata, config)
            
            # Calculate metrics
            total_tokens = sum(chunk.metadata.token_count for chunk in chunks)
            avg_chunk_size = total_tokens / len(chunks) if chunks else 0
            
            quality_dist = {quality: 0 for quality in ChunkQuality}
            for chunk in chunks:
                quality_dist[chunk.metadata.quality_level] += 1
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return ChunkingResult(
                chunks=chunks,
                total_chunks=len(chunks),
                total_tokens=total_tokens,
                avg_chunk_size=avg_chunk_size,
                quality_distribution=quality_dist,
                processing_time=processing_time,
                config=config
            )
            
        except Exception as e:
            logger.error(f"Error chunking with overlap: {str(e)}")
            raise
    
    def preserve_metadata(
        self,
        chunks: List[ContentChunk],
        additional_metadata: Dict[str, Any]
    ) -> List[ContentChunk]:
        """
        Preserve and enhance metadata for chunks
        
        Args:
            chunks: List of content chunks
            additional_metadata: Additional metadata to preserve
            
        Returns:
            Updated chunks with preserved metadata
        """
        try:
            for chunk in chunks:
                # Merge additional metadata
                chunk.metadata.source_metadata.update(additional_metadata)
                
                # Add preservation timestamp
                chunk.metadata.source_metadata['metadata_preserved_at'] = datetime.now().isoformat()
                
                # Add chunk-specific metadata
                chunk.metadata.source_metadata['chunk_version'] = '1.0'
                chunk.metadata.source_metadata['chunking_service'] = 'ContentChunkingService'
            
            return chunks
            
        except Exception as e:
            logger.error(f"Error preserving metadata: {str(e)}")
            raise
    
    def create_chunk_index(
        self,
        chunks: List[ContentChunk],
        index_type: str = "sequential"
    ) -> Dict[str, Any]:
        """
        Create indexing information for chunks
        
        Args:
            chunks: List of content chunks
            index_type: Type of index to create
            
        Returns:
            Dictionary containing indexing information
        """
        try:
            index = {
                "index_type": index_type,
                "created_at": datetime.now().isoformat(),
                "total_chunks": len(chunks),
                "index_version": "1.0"
            }
            
            if index_type == "sequential":
                index["chunk_map"] = {
                    chunk.metadata.chunk_id: {
                        "index": chunk.metadata.chunk_index,
                        "start_position": chunk.metadata.start_position,
                        "end_position": chunk.metadata.end_position,
                        "token_count": chunk.metadata.token_count,
                        "quality": chunk.metadata.quality_level.value
                    }
                    for chunk in chunks
                }
            
            elif index_type == "hierarchical":
                # Group chunks by quality and size
                index["quality_groups"] = {}
                for quality in ChunkQuality:
                    quality_chunks = [c for c in chunks if c.metadata.quality_level == quality]
                    if quality_chunks:
                        index["quality_groups"][quality.value] = {
                            "count": len(quality_chunks),
                            "avg_size": sum(c.metadata.token_count for c in quality_chunks) / len(quality_chunks),
                            "chunk_ids": [c.metadata.chunk_id for c in quality_chunks]
                        }
            
            elif index_type == "similarity":
                # Create similarity-based relationships
                index["relationships"] = {}
                for chunk in chunks:
                    relationships = []
                    
                    # Add previous and next chunks
                    if chunk.metadata.chunk_index > 0:
                        prev_chunk = chunks[chunk.metadata.chunk_index - 1]
                        relationships.append({
                            "type": "previous",
                            "chunk_id": prev_chunk.metadata.chunk_id,
                            "overlap": chunk.metadata.overlap_with_previous
                        })
                    
                    if chunk.metadata.chunk_index < len(chunks) - 1:
                        next_chunk = chunks[chunk.metadata.chunk_index + 1]
                        relationships.append({
                            "type": "next",
                            "chunk_id": next_chunk.metadata.chunk_id,
                            "overlap": chunk.metadata.overlap_with_next
                        })
                    
                    index["relationships"][chunk.metadata.chunk_id] = relationships
            
            return index
            
        except Exception as e:
            logger.error(f"Error creating chunk index: {str(e)}")
            raise
    
    def structure_for_export(
        self,
        chunks: List[ContentChunk],
        export_format: str = "vector_db"
    ) -> Dict[str, Any]:
        """
        Structure chunks for vector database export
        
        Args:
            chunks: List of content chunks
            export_format: Format for export (vector_db, json, etc.)
            
        Returns:
            Structured data ready for export
        """
        try:
            if export_format == "vector_db":
                return {
                    "documents": [
                        {
                            "id": chunk.metadata.chunk_id,
                            "text": chunk.text,
                            "metadata": {
                                "source_id": chunk.metadata.source_id,
                                "chunk_index": chunk.metadata.chunk_index,
                                "token_count": chunk.metadata.token_count,
                                "quality_score": chunk.metadata.quality_score,
                                "quality_level": chunk.metadata.quality_level.value,
                                "created_at": chunk.metadata.created_at.isoformat(),
                                **chunk.metadata.source_metadata
                            },
                            "embeddings": chunk.embeddings
                        }
                        for chunk in chunks
                    ],
                    "export_metadata": {
                        "total_documents": len(chunks),
                        "export_format": export_format,
                        "exported_at": datetime.now().isoformat(),
                        "service_version": "1.0"
                    }
                }
            
            elif export_format == "json":
                return {
                    "chunks": [
                        {
                            "chunk_id": chunk.metadata.chunk_id,
                            "text": chunk.text,
                            "metadata": {
                                "source_id": chunk.metadata.source_id,
                                "chunk_index": chunk.metadata.chunk_index,
                                "total_chunks": chunk.metadata.total_chunks,
                                "positions": {
                                    "start": chunk.metadata.start_position,
                                    "end": chunk.metadata.end_position
                                },
                                "counts": {
                                    "tokens": chunk.metadata.token_count,
                                    "words": chunk.metadata.word_count,
                                    "sentences": chunk.metadata.sentence_count,
                                    "paragraphs": chunk.metadata.paragraph_count
                                },
                                "quality": {
                                    "score": chunk.metadata.quality_score,
                                    "level": chunk.metadata.quality_level.value
                                },
                                "overlap": {
                                    "previous": chunk.metadata.overlap_with_previous,
                                    "next": chunk.metadata.overlap_with_next
                                },
                                "created_at": chunk.metadata.created_at.isoformat(),
                                "source_metadata": chunk.metadata.source_metadata
                            },
                            "relationships": chunk.relationships,
                            "embeddings": chunk.embeddings
                        }
                        for chunk in chunks
                    ]
                }
            
            else:
                raise ValueError(f"Unsupported export format: {export_format}")
            
        except Exception as e:
            logger.error(f"Error structuring for export: {str(e)}")
            raise
    
    def validate_chunk_quality(
        self,
        chunks: List[ContentChunk],
        min_quality: ChunkQuality = ChunkQuality.FAIR
    ) -> Dict[str, Any]:
        """
        Validate chunk quality and provide recommendations
        
        Args:
            chunks: List of content chunks to validate
            min_quality: Minimum acceptable quality level
            
        Returns:
            Validation results and recommendations
        """
        try:
            validation_results = {
                "total_chunks": len(chunks),
                "quality_distribution": {quality.value: 0 for quality in ChunkQuality},
                "passed_validation": 0,
                "failed_validation": 0,
                "recommendations": [],
                "detailed_results": []
            }
            
            for chunk in chunks:
                quality = chunk.metadata.quality_level
                validation_results["quality_distribution"][quality.value] += 1
                
                # Check if chunk meets minimum quality
                quality_levels = [ChunkQuality.POOR, ChunkQuality.FAIR, ChunkQuality.GOOD, ChunkQuality.EXCELLENT]
                meets_threshold = quality_levels.index(quality) >= quality_levels.index(min_quality)
                
                if meets_threshold:
                    validation_results["passed_validation"] += 1
                else:
                    validation_results["failed_validation"] += 1
                
                # Detailed validation for each chunk
                chunk_validation = {
                    "chunk_id": chunk.metadata.chunk_id,
                    "quality_level": quality.value,
                    "quality_score": chunk.metadata.quality_score,
                    "meets_threshold": meets_threshold,
                    "issues": []
                }
                
                # Check for specific issues
                if chunk.metadata.token_count < self.config.min_chunk_size:
                    chunk_validation["issues"].append("Below minimum size")
                
                if chunk.metadata.token_count > self.config.max_chunk_size:
                    chunk_validation["issues"].append("Above maximum size")
                
                if chunk.metadata.sentence_count == 0:
                    chunk_validation["issues"].append("No complete sentences")
                
                if len(chunk.text.strip()) == 0:
                    chunk_validation["issues"].append("Empty or whitespace-only content")
                
                validation_results["detailed_results"].append(chunk_validation)
            
            # Generate recommendations
            if validation_results["failed_validation"] > 0:
                validation_results["recommendations"].append(
                    f"Consider adjusting chunk size or overlap settings. "
                    f"{validation_results['failed_validation']} chunks failed validation."
                )
            
            if validation_results["quality_distribution"]["poor"] > len(chunks) * 0.3:
                validation_results["recommendations"].append(
                    "High percentage of poor quality chunks. Consider preprocessing the source text."
                )
            
            return validation_results
            
        except Exception as e:
            logger.error(f"Error validating chunk quality: {str(e)}")
            raise
    
    def _preprocess_transcript(self, transcript: str) -> str:
        """Preprocess transcript to handle speaker labels and formatting"""
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', transcript.strip())
        
        # Handle speaker labels (e.g., "SPEAKER_01: Hello there")
        # Keep speaker labels as they provide context
        text = re.sub(r'SPEAKER_(\d+):\s*', r'Speaker \1: ', text)
        
        # Handle timestamps if present
        text = re.sub(r'\[\d{2}:\d{2}:\d{2}\]', '', text)
        
        # Clean up punctuation
        text = re.sub(r'\.{2,}', '.', text)
        text = re.sub(r'\,{2,}', ',', text)
        
        return text
    
    def _chunk_text(
        self,
        text: str,
        source_metadata: Dict[str, Any],
        config: ChunkingConfig
    ) -> List[ContentChunk]:
        """Main text chunking logic"""
        chunks = []
        
        if config.boundary_type == ChunkBoundary.SENTENCE:
            chunks = self._chunk_by_sentences(text, source_metadata, config)
        elif config.boundary_type == ChunkBoundary.PARAGRAPH:
            chunks = self._chunk_by_paragraphs(text, source_metadata, config)
        elif config.boundary_type == ChunkBoundary.WORD:
            chunks = self._chunk_by_words(text, source_metadata, config)
        else:  # TOKEN
            chunks = self._chunk_by_tokens(text, source_metadata, config)
        
        # Remove empty chunks if configured
        if config.remove_empty_chunks:
            chunks = [chunk for chunk in chunks if chunk.text.strip()]
        
        # Update relationships between chunks
        self._update_chunk_relationships(chunks)
        
        return chunks
    
    def _chunk_by_sentences(
        self,
        text: str,
        source_metadata: Dict[str, Any],
        config: ChunkingConfig
    ) -> List[ContentChunk]:
        """Chunk text by sentences while respecting token limits"""
        sentences = sent_tokenize(text)
        chunks = []
        current_chunk = []
        current_tokens = 0
        start_pos = 0
        
        source_id = source_metadata.get('source_id', str(uuid.uuid4()))
        
        for sentence in sentences:
            sentence_tokens = len(self.encoder.encode(sentence))
            
            # If adding this sentence would exceed the limit, finalize current chunk
            if current_tokens + sentence_tokens > config.chunk_size and current_chunk:
                chunk_text = ' '.join(current_chunk)
                chunk = self._create_chunk(
                    chunk_text, len(chunks), source_id, source_metadata, config, start_pos
                )
                chunks.append(chunk)
                
                # Handle overlap
                if config.overlap > 0:
                    overlap_text = self._get_overlap_text(chunk_text, config.overlap)
                    current_chunk = [overlap_text] if overlap_text else []
                    current_tokens = len(self.encoder.encode(overlap_text)) if overlap_text else 0
                else:
                    current_chunk = []
                    current_tokens = 0
                
                start_pos = len(text) - len(' '.join(sentences[sentences.index(sentence):]))
            
            current_chunk.append(sentence)
            current_tokens += sentence_tokens
        
        # Handle remaining chunk
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            chunk = self._create_chunk(
                chunk_text, len(chunks), source_id, source_metadata, config, start_pos
            )
            chunks.append(chunk)
        
        return chunks
    
    def _chunk_by_paragraphs(
        self,
        text: str,
        source_metadata: Dict[str, Any],
        config: ChunkingConfig
    ) -> List[ContentChunk]:
        """Chunk text by paragraphs while respecting token limits"""
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = []
        current_tokens = 0
        start_pos = 0
        
        source_id = source_metadata.get('source_id', str(uuid.uuid4()))
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
                
            paragraph_tokens = len(self.encoder.encode(paragraph))
            
            # If adding this paragraph would exceed the limit, finalize current chunk
            if current_tokens + paragraph_tokens > config.chunk_size and current_chunk:
                chunk_text = '\n\n'.join(current_chunk)
                chunk = self._create_chunk(
                    chunk_text, len(chunks), source_id, source_metadata, config, start_pos
                )
                chunks.append(chunk)
                
                # Handle overlap
                if config.overlap > 0:
                    overlap_text = self._get_overlap_text(chunk_text, config.overlap)
                    current_chunk = [overlap_text] if overlap_text else []
                    current_tokens = len(self.encoder.encode(overlap_text)) if overlap_text else 0
                else:
                    current_chunk = []
                    current_tokens = 0
                
                start_pos = text.find(paragraph)
            
            current_chunk.append(paragraph)
            current_tokens += paragraph_tokens
        
        # Handle remaining chunk
        if current_chunk:
            chunk_text = '\n\n'.join(current_chunk)
            chunk = self._create_chunk(
                chunk_text, len(chunks), source_id, source_metadata, config, start_pos
            )
            chunks.append(chunk)
        
        return chunks
    
    def _chunk_by_words(
        self,
        text: str,
        source_metadata: Dict[str, Any],
        config: ChunkingConfig
    ) -> List[ContentChunk]:
        """Chunk text by words while respecting token limits"""
        words = word_tokenize(text)
        chunks = []
        current_chunk = []
        current_tokens = 0
        start_pos = 0
        
        source_id = source_metadata.get('source_id', str(uuid.uuid4()))
        
        for word in words:
            word_tokens = len(self.encoder.encode(word))
            
            # If adding this word would exceed the limit, finalize current chunk
            if current_tokens + word_tokens > config.chunk_size and current_chunk:
                chunk_text = ' '.join(current_chunk)
                chunk = self._create_chunk(
                    chunk_text, len(chunks), source_id, source_metadata, config, start_pos
                )
                chunks.append(chunk)
                
                # Handle overlap
                if config.overlap > 0:
                    overlap_words = current_chunk[-config.overlap:] if len(current_chunk) > config.overlap else current_chunk
                    current_chunk = overlap_words
                    current_tokens = len(self.encoder.encode(' '.join(overlap_words)))
                else:
                    current_chunk = []
                    current_tokens = 0
                
                start_pos = text.find(word)
            
            current_chunk.append(word)
            current_tokens += word_tokens
        
        # Handle remaining chunk
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            chunk = self._create_chunk(
                chunk_text, len(chunks), source_id, source_metadata, config, start_pos
            )
            chunks.append(chunk)
        
        return chunks
    
    def _chunk_by_tokens(
        self,
        text: str,
        source_metadata: Dict[str, Any],
        config: ChunkingConfig
    ) -> List[ContentChunk]:
        """Chunk text by tokens with exact token limits"""
        tokens = self.encoder.encode(text)
        chunks = []
        start_pos = 0
        
        source_id = source_metadata.get('source_id', str(uuid.uuid4()))
        
        i = 0
        while i < len(tokens):
            end_pos = min(i + config.chunk_size, len(tokens))
            chunk_tokens = tokens[i:end_pos]
            chunk_text = self.encoder.decode(chunk_tokens)
            
            chunk = self._create_chunk(
                chunk_text, len(chunks), source_id, source_metadata, config, start_pos
            )
            chunks.append(chunk)
            
            # Move to next chunk with overlap
            i = end_pos - config.overlap if config.overlap > 0 else end_pos
            start_pos += len(chunk_text)
        
        return chunks
    
    def _create_chunk(
        self,
        text: str,
        index: int,
        source_id: str,
        source_metadata: Dict[str, Any],
        config: ChunkingConfig,
        start_pos: int
    ) -> ContentChunk:
        """Create a ContentChunk with metadata"""
        chunk_id = str(uuid.uuid4())
        
        # Calculate metrics
        token_count = len(self.encoder.encode(text))
        word_count = len(word_tokenize(text))
        sentence_count = len(sent_tokenize(text))
        paragraph_count = len([p for p in text.split('\n\n') if p.strip()])
        
        # Calculate quality score
        quality_score = self._calculate_quality_score(
            text, token_count, word_count, sentence_count, paragraph_count
        )
        quality_level = self._determine_quality_level(quality_score)
        
        # Create metadata
        metadata = ChunkMetadata(
            chunk_id=chunk_id,
            source_id=source_id,
            chunk_index=index,
            total_chunks=0,  # Will be updated later
            start_position=start_pos,
            end_position=start_pos + len(text),
            token_count=token_count,
            word_count=word_count,
            sentence_count=sentence_count,
            paragraph_count=paragraph_count,
            quality_score=quality_score,
            quality_level=quality_level,
            boundary_type=config.boundary_type,
            overlap_with_previous=0,  # Will be updated later
            overlap_with_next=0,  # Will be updated later
            created_at=datetime.now(),
            source_metadata=source_metadata.copy()
        )
        
        return ContentChunk(
            text=text,
            metadata=metadata,
            relationships={}
        )
    
    def _calculate_quality_score(
        self,
        text: str,
        token_count: int,
        word_count: int,
        sentence_count: int,
        paragraph_count: int
    ) -> float:
        """Calculate quality score for a chunk"""
        score = 0.0
        
        # Size appropriateness (0-30 points)
        if self.config.min_chunk_size <= token_count <= self.config.max_chunk_size:
            score += 30
        elif token_count < self.config.min_chunk_size:
            score += 30 * (token_count / self.config.min_chunk_size)
        else:
            score += 30 * (self.config.max_chunk_size / token_count)
        
        # Sentence completeness (0-25 points)
        if sentence_count > 0:
            score += 25
        
        # Content coherence (0-20 points)
        if paragraph_count > 0:
            score += 20
        
        # Text quality (0-15 points)
        if text.strip():
            score += 15
        
        # Word/token ratio (0-10 points)
        if word_count > 0:
            ratio = token_count / word_count
            if 1.0 <= ratio <= 2.0:  # Reasonable token/word ratio
                score += 10
            else:
                score += 10 * min(ratio / 2.0, 2.0 / ratio)
        
        return min(score, 100.0)
    
    def _determine_quality_level(self, score: float) -> ChunkQuality:
        """Determine quality level based on score"""
        if score >= 90:
            return ChunkQuality.EXCELLENT
        elif score >= 70:
            return ChunkQuality.GOOD
        elif score >= 50:
            return ChunkQuality.FAIR
        else:
            return ChunkQuality.POOR
    
    def _get_overlap_text(self, text: str, overlap_tokens: int) -> str:
        """Get overlap text for the next chunk"""
        tokens = self.encoder.encode(text)
        if len(tokens) <= overlap_tokens:
            return text
        
        overlap_token_list = tokens[-overlap_tokens:]
        return self.encoder.decode(overlap_token_list)
    
    def _update_chunk_relationships(self, chunks: List[ContentChunk]):
        """Update relationships between chunks"""
        for i, chunk in enumerate(chunks):
            chunk.metadata.total_chunks = len(chunks)
            
            # Calculate actual overlap
            if i > 0:
                prev_chunk = chunks[i-1]
                overlap = self._calculate_overlap(prev_chunk.text, chunk.text)
                chunk.metadata.overlap_with_previous = overlap
                
                # Add relationship
                chunk.relationships['previous'] = prev_chunk.metadata.chunk_id
                prev_chunk.relationships['next'] = chunk.metadata.chunk_id
            
            if i < len(chunks) - 1:
                next_chunk = chunks[i+1]
                overlap = self._calculate_overlap(chunk.text, next_chunk.text)
                chunk.metadata.overlap_with_next = overlap
    
    def _calculate_overlap(self, text1: str, text2: str) -> int:
        """Calculate token overlap between two text chunks"""
        tokens1 = set(self.encoder.encode(text1))
        tokens2 = set(self.encoder.encode(text2))
        return len(tokens1.intersection(tokens2))


# Global instance
content_chunking_service = ContentChunkingService()


================================================
FILE: droplet_manager.py
================================================
"""
Droplet Manager (Production GPU Management)

Manages DigitalOcean GPU droplets for production voice cloning.
This is a placeholder for future production implementation.
"""

import os
import logging
from typing import Dict, Any, Optional
import aiohttp
from datetime import datetime

logger = logging.getLogger(__name__)


class DropletManager:
    """Manages GPU droplets for production processing"""
    
    def __init__(self):
        """Initialize droplet manager"""
        self.do_token = os.getenv("DO_API_TOKEN")
        self.droplet_id = os.getenv("DO_DROPLET_ID")
        self.gpu_worker_url = os.getenv("GPU_WORKER_URL")
        self.environment = os.getenv("ENVIRONMENT", "development")
        
        logger.info(f"Droplet Manager initialized - Environment: {self.environment}")
    
    async def ensure_worker_running(self) -> bool:
        """
        Ensure GPU worker is running
        
        Returns:
            True if worker is available, False otherwise
        """
        if self.environment == "development":
            # Skip in development - local GPU is always available
            return True
        
        # TODO: Production implementation
        # 1. Check droplet status via DigitalOcean API
        # 2. Start droplet if stopped
        # 3. Wait for health check
        # 4. Return status
        
        logger.info("Production droplet management not yet implemented")
        return False
    
    async def get_droplet_status(self) -> Dict[str, Any]:
        """
        Get current droplet status
        
        Returns:
            Droplet status information
        """
        if self.environment == "development":
            return {
                "status": "not_applicable",
                "environment": "development",
                "message": "Using local GPU in development"
            }
        
        # TODO: Query DigitalOcean API for droplet status
        return {
            "status": "unknown",
            "droplet_id": self.droplet_id,
            "message": "Production implementation pending"
        }
    
    async def start_droplet(self) -> bool:
        """
        Start the GPU droplet
        
        Returns:
            True if started successfully, False otherwise
        """
        if self.environment == "development":
            return True
        
        # TODO: DigitalOcean API call to power on droplet
        logger.warning("Droplet start not implemented for production")
        return False
    
    async def stop_droplet(self) -> bool:
        """
        Stop the GPU droplet
        
        Returns:
            True if stopped successfully, False otherwise
        """
        if self.environment == "development":
            return True
        
        # TODO: DigitalOcean API call to power off droplet
        logger.warning("Droplet stop not implemented for production")
        return False
    
    async def check_idle_timeout(self, last_job_time: datetime, timeout_minutes: int = 5) -> bool:
        """
        Check if droplet should be shut down due to idle timeout
        
        Args:
            last_job_time: Time of last processed job
            timeout_minutes: Minutes of idle time before shutdown
            
        Returns:
            True if should shutdown, False otherwise
        """
        if self.environment == "development":
            return False
        
        idle_time = (datetime.utcnow() - last_job_time).total_seconds() / 60
        return idle_time >= timeout_minutes
    
    async def get_worker_health(self) -> Dict[str, Any]:
        """
        Check health of GPU worker service
        
        Returns:
            Worker health status
        """
        if self.environment == "development":
            # Check local Chatterbox service
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get("http://localhost:8001/health") as response:
                        if response.status == 200:
                            return await response.json()
            except:
                pass
            
            return {
                "status": "unknown",
                "message": "Could not reach local Chatterbox service"
            }
        
        # TODO: Check remote GPU worker health
        return {
            "status": "unknown",
            "message": "Production worker health check not implemented"
        }


================================================
FILE: eleven_labs_client.py
================================================
# Eleven Labs client service


================================================
FILE: embedding_quality_assessor.py
================================================
#!/usr/bin/env python3
"""
Embedding Quality Assessor for Speaker Diarization

Implements quality-aware embedding assessment following:
- Bredin & Laurent (2021) "Robust Speaker Embeddings for Streaming Diarization"
- Park et al. (2022) "Adaptive Clustering for Online Speaker Diarization"

This module provides quality assessment for audio embeddings used in speaker
diarization, ensuring robust centroid updates in streaming scenarios.
"""

import numpy as np
from typing import Dict, Any
import scipy.signal
import scipy.fft


class EmbeddingQualityAssessor:
    """
    Quality assessment for speaker embeddings in streaming diarization.
    
    Evaluates audio quality using three complementary metrics:
    1. Signal-to-Noise Ratio (SNR) quality
    2. Duration-based quality (following Bredin & Laurent 2021)
    3. Spectral clarity quality
    
    The composite quality score is used to make informed decisions about
    whether to update speaker centroids in streaming scenarios.
    """
    
    def __init__(self, 
                 sample_rate: int = 16000,
                 snr_weight: float = 0.5,
                 duration_weight: float = 0.3,
                 spectral_weight: float = 0.2,
                 min_quality_threshold: float = 0.3):
        """
        Initialize the quality assessor.
        
        Args:
            sample_rate: Audio sample rate in Hz
            snr_weight: Weight for SNR quality in composite score (0-1)
            duration_weight: Weight for duration quality (0-1)
            spectral_weight: Weight for spectral quality (0-1)
            min_quality_threshold: Minimum quality threshold for centroid updates
        """
        self.sample_rate = sample_rate
        self.snr_weight = snr_weight
        self.duration_weight = duration_weight
        self.spectral_weight = spectral_weight
        self.min_quality_threshold = min_quality_threshold
        
        # Validate weights sum to 1.0 (within tolerance)
        total_weight = snr_weight + duration_weight + spectral_weight
        if abs(total_weight - 1.0) > 0.001:
            raise ValueError(f"Weights must sum to 1.0, got {total_weight}")
    
    def assess_quality(self, audio: np.ndarray) -> float:
        """
        Assess the overall quality of audio for embedding generation.
        
        Args:
            audio: Raw audio data as numpy array
            
        Returns:
            Composite quality score between 0.0 and 1.0
        """
        if len(audio) == 0:
            return 0.0
            
        # Ensure float32 format
        if audio.dtype != np.float32:
            if audio.dtype == np.int16:
                audio = audio.astype(np.float32) / 32768.0
            elif audio.dtype == np.int32:
                audio = audio.astype(np.float32) / 2147483648.0
            else:
                audio = audio.astype(np.float32)
        
        # Calculate individual quality metrics
        snr_quality = self._calculate_snr_quality(audio)
        duration_quality = self._calculate_duration_quality(audio)
        spectral_quality = self._calculate_spectral_quality(audio)
        
        # Weighted aggregation
        composite_quality = (
            self.snr_weight * snr_quality +
            self.duration_weight * duration_quality +
            self.spectral_weight * spectral_quality
        )
        
        return float(np.clip(composite_quality, 0.0, 1.0))
    
    def _calculate_snr_quality(self, audio: np.ndarray) -> float:
        """
        Calculate signal-to-noise ratio based quality using frame energy distribution.
        This method is robust against absolute volume changes and identifies noise
        based on the energy difference between quiet and loud segments.
        """
        try:
            # Require at least 100ms of audio for a meaningful estimation
            if len(audio) < self.sample_rate * 0.1:
                return 0.1

            frame_length = 512
            hop_length = 256
            
            frames = self._frame_audio(audio, frame_length, hop_length)
            
            # Require a minimum number of frames to estimate distribution
            if frames.shape[0] < 5:
                return 0.1 
                
            frame_energies = np.sqrt(np.mean(frames**2, axis=1))
            
            # Estimate noise power from the 20th percentile (quieter parts)
            noise_power = np.percentile(frame_energies, 20)
            
            # Estimate signal power from the 95th percentile (louder parts)
            signal_power = np.percentile(frame_energies, 95)
            
            if noise_power < 1e-9:
                # If noise is practically zero, SNR is very high
                snr_db = 80.0
            else:
                # Calculate SNR and convert to dB
                snr = (signal_power / noise_power)**2
                snr_db = 10 * np.log10(snr)

            # Map SNR in dB to a quality score (0-1) using a calibrated piecewise function.
            # This mapping is crucial for providing a predictable quality score.
            if snr_db < 5:
                quality = 0.3 * (snr_db / 5) # Smooth ramp up from 0 for very noisy audio
            elif snr_db < 15:
                quality = 0.3 + 0.4 * ((snr_db - 5) / 10)
            elif snr_db < 30:
                quality = 0.7 + 0.2 * ((snr_db - 15) / 15)
            else: # snr_db >= 30
                quality = 0.9 + 0.1 * (min(snr_db - 30, 10) / 10) # Cap at 40dB

            return float(np.clip(quality, 0.0, 1.0))

        except Exception:
            # Return a default low-to-moderate score in case of unexpected errors
            return 0.2

    def _frame_audio(self, audio: np.ndarray, frame_length: int, hop_length: int) -> np.ndarray:
        """Frame audio into overlapping windows."""
        n_frames = 1 + (len(audio) - frame_length) // hop_length
        padded_audio = np.pad(audio, (0, max(0, (n_frames - 1) * hop_length + frame_length - len(audio))))
        shape = (n_frames, frame_length)
        strides = (hop_length * padded_audio.strides[0], padded_audio.strides[0])
        return np.lib.stride_tricks.as_strided(padded_audio, shape=shape, strides=strides)

    def _calculate_duration_quality(self, audio: np.ndarray) -> float:
        """
        Calculate duration-based quality following Bredin & Laurent (2021).
        
        Optimal duration for speaker embeddings is around 2 seconds,
        with penalties for both very short and very long segments.
        """
        duration = len(audio) / self.sample_rate
        
        if duration < 0.25:
            quality = duration * 1.6 # Linear ramp for very short audio
        elif 0.25 <= duration <= 1.0:
            quality = 0.4 + 0.4 * ((duration - 0.25) / 0.75)
        elif 1.0 < duration <= 3.0:
            quality = 0.8 + 0.2 * ((duration - 1.0) / 2.0)
        elif 3.0 < duration <= 5.0:
            quality = 1.0 - 0.3 * ((duration - 3.0) / 2.0)
        else: # duration > 5.0
            quality = max(0.0, 0.7 - 0.1 * (duration - 5.0))
            
        return float(np.clip(quality, 0.0, 1.0))
    
    def _calculate_spectral_quality(self, audio: np.ndarray) -> float:
        """
        Calculate spectral clarity quality using FFT analysis.
        
        Speech signals should have energy distributed across multiple
        frequency bands rather than concentrated in narrow bands.
        """
        try:
            if len(audio) < 256:
                return 0.5
                
            fft = np.abs(scipy.fft.fft(audio))
            freqs = scipy.fft.fftfreq(len(audio), 1/self.sample_rate)
            
            mask = (freqs >= 80) & (freqs <= 4000)
            speech_fft = fft[mask]
            speech_freqs = freqs[mask]
            
            if len(speech_fft) == 0:
                return 0.5
            
            total_energy = np.sum(speech_fft)
            if total_energy < 1e-10:
                return 0.0
            
            spectral_centroid = np.sum(speech_freqs * speech_fft) / total_energy
            spectral_spread = np.sqrt(np.sum(((speech_freqs - spectral_centroid) ** 2) * speech_fft) / total_energy)
            geometric_mean = np.exp(np.mean(np.log(speech_fft + 1e-10)))
            arithmetic_mean = np.mean(speech_fft)
            spectral_flatness = geometric_mean / (arithmetic_mean + 1e-10)
            
            peak_threshold = np.max(speech_fft) * 0.1
            significant_peaks = np.sum(speech_fft > peak_threshold)
            peak_score = min(1.0, significant_peaks / 10.0)
            
            centroid_score = 1.0 - min(1.0, abs(spectral_centroid - 1500) / 1500)
            spread_score = min(1.0, spectral_spread / 1000)
            flatness_score = min(1.0, spectral_flatness * 10)
            
            spectral_quality = (
                0.2 * centroid_score + 0.3 * spread_score +
                0.3 * flatness_score + 0.2 * peak_score
            )
            
            return float(np.clip(spectral_quality, 0.0, 1.0))
            
        except Exception:
            return 0.5
    
    def should_update_centroid(self, quality_score: float) -> bool:
        """
        Determine if centroid should be updated based on quality score.
        """
        return quality_score >= self.min_quality_threshold
    
    def get_quality_weight(self, quality_score: float) -> float:
        """
        Get quality-based weight for centroid aggregation.
        """
        weight = 0.1 + 0.9 * quality_score ** 2
        return float(np.clip(weight, 0.01, 1.0))

    def get_quality_metrics(self, audio: np.ndarray) -> Dict[str, Any]:
        """
        Get detailed quality metrics for analysis and debugging.
        """
        if len(audio) == 0:
            return {
                'snr_quality': 0.0, 'duration_quality': 0.0, 'spectral_quality': 0.0,
                'composite_quality': 0.0, 'duration_sec': 0.0,
                'should_update': False, 'quality_weight': 0.01
            }
        
        composite_quality = self.assess_quality(audio)
        duration_sec = len(audio) / self.sample_rate
        
        return {
            'snr_quality': self._calculate_snr_quality(audio),
            'duration_quality': self._calculate_duration_quality(audio),
            'spectral_quality': self._calculate_spectral_quality(audio),
            'composite_quality': composite_quality,
            'duration_sec': duration_sec,
            'should_update': self.should_update_centroid(composite_quality),
            'quality_weight': self.get_quality_weight(composite_quality)
        }


================================================
FILE: enhanced_stream_simulation.py
================================================
#!/usr/bin/env python3
"""
Enhanced Stream Simulation Service with IBM Granite ASR and emotion2vec
Replaces OpenAI Whisper with IBM Granite ASR and adds real-time sentiment analysis
"""

import os
import sys
import asyncio
import logging
import tempfile
import uuid
from pathlib import Path
from typing import Dict, Any, List, Optional, AsyncGenerator
from datetime import datetime
import json
import base64

# Import IBM Granite ASR
from transformers import (
    AutoModelForSpeechSeq2Seq,
    AutoProcessor,
    pipeline
)
import torch

# Import emotion2vec
from transformers import AutoModel
import numpy as np

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedStreamProcessor:
    """Enhanced processor using IBM Granite ASR, emotion2vec, and speaker diarization"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        
        # Initialize IBM Granite ASR
        self.processor = None
        self.asr_model = None
        
        # Initialize emotion2vec
        self.emotion_model = None
        self.emotion_labels = [
            "angry", "disgusted", "fearful", "happy", 
            "neutral", "sad", "surprised"
        ]
        
        # Initialize speaker diarization
        self.diarization_pipeline = None
        
        # Initialize Convex client for job synchronization
        self.convex_client = None
        
        self.temp_dir = tempfile.mkdtemp(prefix="enhanced_stream_")
        
    async def initialize_models(self):
        """Initialize IBM Granite ASR, emotion2vec, and speaker diarization models"""
        try:
            # Initialize Convex client
            from convex import ConvexClient
            self.convex_client = ConvexClient("http://127.0.0.1:3210")
            
            # Initialize IBM Granite Speech ASR pipeline
            self.asr_pipeline = pipeline(
                "automatic-speech-recognition",
                model="ibm-granite/granite-speech-3.3-8b",
                torch_dtype=torch.bfloat16,
                device=self.device,
                trust_remote_code=True  # Required for IBM Granite models
            )
            
            # Initialize emotion2vec
            self.emotion_model = AutoModel.from_pretrained(
                "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim",
                torch_dtype=self.torch_dtype
            ).to(self.device)
            
            # Initialize speaker diarization
            try:
                from pyannote.audio import Pipeline
                self.diarization_pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    use_auth_token=os.getenv("HF_TOKEN")
                )
                if torch.cuda.is_available():
                    self.diarization_pipeline.to(torch.device("cuda"))
                logger.info("Speaker diarization model loaded successfully")
            except Exception as e:
                logger.warning(f"Could not load speaker diarization: {str(e)}")
                self.diarization_pipeline = None
            
            logger.info("All models initialized successfully")
            
        except Exception as e:
            logger.error(f"Error initializing models: {str(e)}")
            # Fallback to mock processing
            self.asr_model = None
            self.emotion_model = None
            self.diarization_pipeline = None
    
    async def process_audio_chunk(
        self,
        audio_data: bytes,
        chunk_id: str,
        enable_sentiment: bool = False,
        enable_emotion2vec: bool = False
    ) -> Dict[str, Any]:
        """
        Process a single audio chunk with optional sentiment analysis
        
        Args:
            audio_data: Raw audio bytes
            chunk_id: Unique identifier for this chunk
            enable_sentiment: Flag to enable sentiment analysis
            enable_emotion2vec: Flag to enable emotion2vec analysis
            
        Returns:
            Dictionary with transcription and sentiment data
        """
        try:
            # Save audio chunk to temporary file
            temp_file = os.path.join(self.temp_dir, f"chunk_{chunk_id}.wav")
            
            # Process with ASR
            if self.asr_pipeline:
                # Real ASR processing
                transcription = await self._process_with_asr(audio_data)
            else:
                # Mock processing for demonstration
                transcription = await self._mock_asr_processing(audio_data)
            
            # Sentiment analysis
            sentiment_data = {}
            if enable_sentiment or enable_emotion2vec:
                sentiment_data = await self._analyze_sentiment(
                    audio_data, 
                    enable_emotion2vec=enable_emotion2vec
                )
            
            return {
                "chunk_id": chunk_id,
                "transcription": transcription,
                "sentiment": sentiment_data,
                "timestamp": datetime.utcnow().isoformat(),
                "processed": True
            }
            
        except Exception as e:
            logger.error(f"Error processing chunk {chunk_id}: {str(e)}")
            return {
                "chunk_id": chunk_id,
                "transcription": "",
                "sentiment": {},
                "error": str(e),
                "processed": False
            }
    
    async def _process_with_asr(self, audio_data: bytes) -> str:
        """Process audio with IBM Granite ASR"""
        try:
            # Convert audio bytes to numpy array
            import librosa
            import io
            
            # Load audio from bytes
            audio_np, sr = librosa.load(io.BytesIO(audio_data), sr=16000)
            
            # Process with IBM Granite Speech ASR pipeline
            result = self.asr_pipeline(audio_np, return_timestamps=False)
            transcription = result["text"] if result and "text" in result else "[ASR Error]"
            
            return transcription.strip()
            
        except Exception as e:
            logger.error(f"ASR processing error: {str(e)}")
            return "[ASR Error]"
    
    async def _mock_asr_processing(self, audio_data: bytes) -> str:
        """Mock ASR processing for demonstration"""
        mock_transcriptions = [
            "Hello, this is a test of the enhanced stream processing.",
            "The IBM Granite ASR model is processing this audio.",
            "Real-time sentiment analysis is now enabled.",
            "This is a demonstration of the dual architecture system."
        ]
        
        import random
        return random.choice(mock_transcriptions)
    
    async def _analyze_sentiment(
        self, 
        audio_data: bytes, 
        enable_emotion2vec: bool = False
    ) -> Dict[str, Any]:
        """Analyze sentiment using emotion2vec"""
        try:
            if not enable_emotion2vec or not self.emotion_model:
                # Mock sentiment analysis
                return {
                    "sentiment": "neutral",
                    "confidence": 0.85,
                    "emotions": {
                        "happy": 0.3,
                        "sad": 0.1,
                        "angry": 0.05,
                        "neutral": 0.55
                    }
                }
            
            # Real emotion2vec processing
            import librosa
            import io
            
            # Load audio
            audio_np, sr = librosa.load(io.BytesIO(audio_data), sr=16000)
            
            # Convert to tensor with proper dtype
            audio_tensor = torch.tensor(audio_np, dtype=self.torch_dtype).unsqueeze(0).to(self.device)
            
            # Process with emotion2vec
            with torch.no_grad():
                outputs = self.emotion_model(audio_tensor)
                
                # Get emotion probabilities
                emotion_probs = torch.softmax(outputs.last_hidden_state.mean(dim=1), dim=-1)
                emotion_dict = {
                    label: float(prob) 
                    for label, prob in zip(self.emotion_labels, emotion_probs[0])
                }
                
                # Determine dominant emotion
                dominant_emotion = max(emotion_dict, key=emotion_dict.get)
                confidence = emotion_dict[dominant_emotion]
                
                return {
                    "sentiment": dominant_emotion,
                    "confidence": confidence,
                    "emotions": emotion_dict
                }
                
        except Exception as e:
            logger.error(f"Sentiment analysis error: {str(e)}")
            return {
                "sentiment": "unknown",
                "confidence": 0.0,
                "error": str(e)
            }
    
    async def _perform_speaker_diarization(self, file_path: str) -> List[Dict[str, Any]]:
        """Perform speaker diarization using pyannote.audio"""
        try:
            if not self.diarization_pipeline:
                logger.warning("Speaker diarization not available, returning empty speakers")
                return []
            
            # Perform diarization
            diarization = self.diarization_pipeline(file_path)
            
            # Convert to speaker segments
            speakers = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speakers.append({
                    "speaker": speaker,
                    "start": turn.start,
                    "end": turn.end,
                    "duration": turn.end - turn.start
                })
            
            return speakers
            
        except Exception as e:
            logger.error(f"Speaker diarization error: {str(e)}")
            return []
    
    async def _create_convex_job(self, job_id: str, user_id: str, file_name: str, file_size: int, file_format: str, **kwargs):
        """Create job record in Convex database"""
        try:
            if self.convex_client:
                self.convex_client.mutation("audioTranscripts:createJob", {
                    "jobId": job_id,
                    "userId": user_id,
                    "fileName": file_name,
                    "fileSize": file_size,
                    "fileFormat": file_format,
                    "language": kwargs.get("language", "en"),
                    **{k: v for k, v in kwargs.items() if k not in ["language"]}
                })
                logger.info(f"Created job {job_id} in Convex")
        except Exception as e:
            logger.error(f"Failed to create Convex job: {str(e)}")
    
    async def _update_convex_job(self, job_id: str, status: str, **kwargs):
        """Update job status in Convex"""
        try:
            if self.convex_client:
                self.convex_client.mutation("audioTranscripts:updateJobStatus", {
                    "jobId": job_id,
                    "status": status,
                    **kwargs
                })
        except Exception as e:
            logger.error(f"Failed to update Convex job: {str(e)}")
    
    async def process_audio_file(
        self,
        file_path: str,
        job_id: str,
        user_id: str,
        enable_realtime_sentiment: bool = False,
        enable_emotion2vec: bool = False,
        language: str = "en",
        **kwargs
    ) -> Dict[str, Any]:
        """
        Process complete audio file with enhanced features including speaker diarization
        
        Args:
            file_path: Path to audio file
            job_id: Unique job identifier
            user_id: User ID for tracking
            enable_realtime_sentiment: Flag for real-time sentiment
            enable_emotion2vec: Flag for emotion2vec analysis
            
        Returns:
            Complete processing results with speaker information
        """
        try:
            logger.info(f"Starting enhanced processing for job {job_id}")
            
            # Initialize models if not already done
            if not self.asr_model:
                await self.initialize_models()
            
            # Create job in Convex
            file_name = os.path.basename(file_path)
            file_size = os.path.getsize(file_path)
            file_format = os.path.splitext(file_path)[1].replace('.', '')
            
            await self._create_convex_job(
                job_id=job_id,
                user_id=user_id,
                file_name=file_name,
                file_size=file_size,
                file_format=file_format,
                language=language or "en"
            )
            
            # Update job status to processing
            await self._update_convex_job(job_id, "processing")
            
            # Perform speaker diarization
            speakers = await self._perform_speaker_diarization(file_path)
            
            # Load audio file
            import librosa
            audio_np, sr = librosa.load(file_path, sr=16000)
            
            # Split into chunks for processing
            chunk_duration = 5  # 5 second chunks
            chunk_samples = chunk_duration * sr
            chunks = []
            
            for i in range(0, len(audio_np), chunk_samples):
                chunk = audio_np[i:i + chunk_samples]
                if len(chunk) > 0:
                    chunks.append(chunk)
            
            # Process chunks with speaker-aware processing
            results = []
            speaker_segments = []
            
            for idx, chunk in enumerate(chunks):
                # Convert chunk to bytes
                import soundfile as sf
                import io
                
                buffer = io.BytesIO()
                sf.write(buffer, chunk, sr, format='wav')
                chunk_bytes = buffer.getvalue()
                
                # Process chunk
                chunk_result = await self.process_audio_chunk(
                    chunk_bytes,
                    f"{job_id}_{idx}",
                    enable_sentiment=enable_realtime_sentiment,
                    enable_emotion2vec=enable_emotion2vec
                )
                
                # Calculate time range for this chunk
                start_time = idx * chunk_duration
                end_time = min((idx + 1) * chunk_duration, len(audio_np) / sr)
                
                # Map speakers to this chunk
                chunk_speakers = []
                for speaker in speakers:
                    if speaker["start"] <= end_time and speaker["end"] >= start_time:
                        chunk_speakers.append(speaker)
                
                chunk_result.update({
                    "start_time": start_time,
                    "end_time": end_time,
                    "speakers": chunk_speakers
                })
                
                results.append(chunk_result)
            
            # Combine results with speaker information
            full_transcript = " ".join([r["transcription"] for r in results if r["transcription"]])
            
            # Aggregate sentiment
            sentiments = [r["sentiment"] for r in results if r["sentiment"]]
            if sentiments:
                dominant_sentiment = max(
                    sentiments, 
                    key=lambda x: x.get("confidence", 0)
                )
            else:
                dominant_sentiment = {"sentiment": "neutral", "confidence": 0.0}
            
            # Create speaker timeline
            speaker_timeline = []
            for speaker in speakers:
                speaker_timeline.append({
                    "speaker": speaker["speaker"],
                    "start": speaker["start"],
                    "end": speaker["end"],
                    "duration": speaker["duration"]
                })
            
            # Update job status to completed
            await self._update_convex_job(
                job_id=job_id,
                status="completed",
                transcript=full_transcript,
                duration=len(audio_np) / sr
            )
            
            return {
                "transcript": full_transcript,
                "status": "completed",
                "jobId": job_id,
                "fileName": file_name,
                "fileSize": float(file_size),
                "fileFormat": file_format,
                "language": "en",
                "chunks_processed": len(results),
                "speakers": speaker_timeline,
                "sentiment_analysis": dominant_sentiment,
                "realtime_sentiment_enabled": enable_realtime_sentiment,
                "emotion2vec_enabled": enable_emotion2vec,
                "processing_complete": True
            }
            
        except Exception as e:
            logger.error(f"Error in enhanced processing: {str(e)}")
            await self._update_convex_job(job_id, "failed", error=str(e))
            return {
                "transcript": "",
                "status": "failed",
                "jobId": job_id,
                "error": str(e),
                "processing_complete": False
            }
    
    async def stream_process_audio(
        self,
        file_path: str,
        job_id: str,
        user_id: str,
        enable_realtime_sentiment: bool = False,
        enable_emotion2vec: bool = False
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Stream process audio in real-time
        
        Args:
            file_path: Path to audio file
            job_id: Unique job identifier
            user_id: User ID for tracking
            enable_realtime_sentiment: Flag for real-time sentiment
            enable_emotion2vec: Flag for emotion2vec analysis
            
        Yields:
            Real-time processing updates
        """
        try:
            # Initialize
            await self.initialize_models()
            
            # Load audio
            import librosa
            audio_np, sr = librosa.load(file_path, sr=16000)
            
            # Process in chunks
            chunk_duration = 2  # 2 second chunks for real-time
            chunk_samples = chunk_duration * sr
            total_chunks = len(audio_np) // chunk_samples + 1
            
            for idx in range(total_chunks):
                start = idx * chunk_samples
                end = min((idx + 1) * chunk_samples, len(audio_np))
                
                if start >= len(audio_np):
                    break
                
                chunk = audio_np[start:end]
                
                # Convert to bytes
                import soundfile as sf
                import io
                
                buffer = io.BytesIO()
                sf.write(buffer, chunk, sr, format='wav')
                chunk_bytes = buffer.getvalue()
                
                # Process chunk
                result = await self.process_audio_chunk(
                    chunk_bytes,
                    f"{job_id}_{idx}",
                    enable_sentiment=enable_realtime_sentiment,
                    enable_emotion2vec=enable_emotion2vec
                )
                
                # Add progress info
                result.update({
                    "progress": (idx + 1) / total_chunks * 100,
                    "current_chunk": idx + 1,
                    "total_chunks": total_chunks
                })
                
                yield result
                
                # Small delay for real-time effect
                await asyncio.sleep(0.1)
            
        except Exception as e:
            logger.error(f"Error in stream processing: {str(e)}")
            yield {
                "error": str(e),
                "status": "failed",
                "processing_complete": False
            }
    
    def cleanup(self):
        """Clean up temporary files"""
        import shutil
        try:
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                logger.info(f"Cleaned up temp directory: {self.temp_dir}")
        except Exception as e:
            logger.warning(f"Error cleaning up temp files: {str(e)}")

# Global instance
enhanced_stream_processor = EnhancedStreamProcessor()

# Export functions for API compatibility
async def process_audio_with_sentiment(
    file_path: str,
    job_id: str,
    user_id: str,
    enable_realtime_sentiment: bool = False,
    enable_emotion2vec: bool = False,
    language: str = "en"
) -> Dict[str, Any]:
    """API-compatible function for processing audio with sentiment"""
    return await enhanced_stream_processor.process_audio_file(
        file_path=file_path,
        job_id=job_id,
        user_id=user_id,
        enable_realtime_sentiment=enable_realtime_sentiment,
        enable_emotion2vec=enable_emotion2vec
    )

async def stream_process_with_sentiment(
    file_path: str,
    job_id: str,
    user_id: str,
    enable_realtime_sentiment: bool = False,
    enable_emotion2vec: bool = False
) -> AsyncGenerator[Dict[str, Any], None]:
    """API-compatible streaming function"""
    async for result in enhanced_stream_processor.stream_process_audio(
        file_path=file_path,
        job_id=job_id,
        user_id=user_id,
        enable_realtime_sentiment=enable_realtime_sentiment,
        enable_emotion2vec=enable_emotion2vec
    ):
        yield result


================================================
FILE: fast_graph_optimizer.py
================================================
"""
Fast Graph Optimization for Real-time Speaker Diarization
Based on Landini et al. (2023) "Fast Online Speaker Diarization with Graph Clustering"

This module provides a robust and efficient implementation for managing a speaker
similarity graph in real-time, focusing on sub-100ms latency and memory efficiency.
"""

import numpy as np
import scipy.sparse as sp
from typing import Dict, List, Set, Optional, Tuple
from collections import deque
import time
import logging

logger = logging.getLogger(__name__)

class FastGraphOptimizer:
    """
    Implements fast graph optimization techniques for real-time speaker diarization.
    
    Key features:
    - Sparse matrix operations for large speaker populations.
    - Incremental graph updates.
    - Accurate edge pruning to maintain graph sparsity.
    - Vectorized similarity calculations.
    - Caching for frequently accessed computations.
    - Designed to meet sub-100ms processing latency constraints.
    """
    
    def __init__(self, max_speakers: int = 50, latency_constraint_ms: float = 100.0,
                 cache_size: int = 1000, pruning_threshold: float = 0.3,
                 vectorization_threshold: int = 5):
        """
        Initialize the fast graph optimizer.
        
        Args:
            max_speakers: Maximum number of speakers to track.
            latency_constraint_ms: Target for maximum processing latency in milliseconds.
            cache_size: Size of the similarity computation cache.
            pruning_threshold: Similarity score below which edges are considered weak.
            vectorization_threshold: Minimum number of nodes for vectorized operations.
        """
        self.max_speakers = max_speakers
        self.latency_constraint_ms = latency_constraint_ms
        self.cache_size = cache_size
        self.pruning_threshold = pruning_threshold
        self.vectorization_threshold = vectorization_threshold
        
        self.adjacency_matrix = sp.lil_matrix((max_speakers, max_speakers), dtype=np.float32)
        self.embeddings: Dict[int, np.ndarray] = {}
        self.embedding_dim: Optional[int] = None
        
        self.similarity_cache: Dict[Tuple[int, int], float] = {}
        self.processing_times: deque = deque(maxlen=100)
        self.latency_violations = 0
        self.stats = {'cache_hits': 0, 'cache_misses': 0}

        logger.info(f"Initialized FastGraphOptimizer: max_speakers={max_speakers}, latency_constraint={latency_constraint_ms}ms")

    def add_or_update_embedding(self, node_id: int, embedding: np.ndarray, quality_score: float = 1.0) -> bool:
        """
        Add or update a node's embedding and incrementally update the graph.

        Args:
            node_id: The identifier for the speaker/node.
            embedding: The embedding vector for the speaker.
            quality_score: The quality score of the embedding (0.0 to 1.0).

        Returns:
            True if the update was successful, False otherwise.
        """
        start_time = time.time()
        
        if embedding is None or embedding.size == 0 or node_id >= self.max_speakers:
            logger.warning(f"Invalid update for node {node_id}. Embedding is empty or ID is out of bounds.")
            return False

        if self.embedding_dim is None:
            self.embedding_dim = len(embedding)

        self.embeddings[node_id] = embedding.astype(np.float32)
        
        # Correctly identify all other existing nodes for comparison.
        nodes_to_compare = set(self.embeddings.keys())
        nodes_to_compare.discard(node_id)

        if not nodes_to_compare:
            # This is the first node, no edges to create yet.
            return True

        self._batch_update_edges(node_id, embedding, nodes_to_compare, quality_score)
        
        processing_time = (time.time() - start_time) * 1000
        self.processing_times.append(processing_time)
        if processing_time > self.latency_constraint_ms:
            self.latency_violations += 1
            logger.warning(f"Latency violation: {processing_time:.1f}ms for node {node_id}")
        
        return True

    def _batch_update_edges(self, node_id: int, embedding: np.ndarray,
                         nodes_to_compare: Set[int], quality_score: float):
        """
        Efficiently calculate and update edges for a node against others.
        """
        for other_node in nodes_to_compare:
            if other_node in self.embeddings:
                similarity = self._calculate_similarity_cached(node_id, other_node, embedding, self.embeddings[other_node])
                weighted_similarity = similarity * quality_score
                
                # Add edge only if similarity is above the pruning threshold.
                if weighted_similarity >= self.pruning_threshold:
                    self.adjacency_matrix[node_id, other_node] = weighted_similarity
                    self.adjacency_matrix[other_node, node_id] = weighted_similarity

    def _calculate_similarity_cached(self, node1: int, node2: int, emb1: np.ndarray, emb2: np.ndarray) -> float:
        """
        Calculate cosine similarity, using a cache to avoid redundant computations.
        """
        # Create a canonical key for the cache (order-independent).
        cache_key = tuple(sorted((node1, node2)))
        if cache_key in self.similarity_cache:
            self.stats['cache_hits'] += 1
            return self.similarity_cache[cache_key]

        self.stats['cache_misses'] += 1
        similarity = self.calculate_cosine_similarity(emb1, emb2)
        
        # Basic LRU-like cache eviction.
        if len(self.similarity_cache) >= self.cache_size:
            self.similarity_cache.pop(next(iter(self.similarity_cache)))

        self.similarity_cache[cache_key] = similarity
        return similarity

    def calculate_cosine_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        """A robust cosine similarity calculation."""
        norm1, norm2 = np.linalg.norm(emb1), np.linalg.norm(emb2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return float(np.dot(emb1, emb2) / (norm1 * norm2))

    def prune_graph(self, min_edge_weight: float = None) -> int:
        """
        Accurately prune weak edges from the graph to maintain sparsity.
        
        Args:
            min_edge_weight: The threshold below which existing edges will be removed.

        Returns:
            The number of edges that were actually pruned.
        """
        if min_edge_weight is None:
            min_edge_weight = self.pruning_threshold
        
        # Use COO format for efficient access to existing edges and their values.
        coo = self.adjacency_matrix.tocoo()
        
        # Correctly identify only existing edges that are weaker than the threshold.
        mask_to_prune = (coo.data > 0) & (coo.data < min_edge_weight)
        
        rows, cols = coo.row[mask_to_prune], coo.col[mask_to_prune]
        
        if len(rows) > 0:
            # Set these specific edges to zero.
            self.adjacency_matrix[rows, cols] = 0
            # Ensure symmetry in the undirected graph.
            self.adjacency_matrix[cols, rows] = 0
        
        return len(rows)

    def get_graph_density(self) -> float:
        """Calculate the current density of the graph."""
        n_nodes = self.max_speakers
        if n_nodes < 2:
            return 0.0
        # Divide by 2 for an undirected graph.
        n_edges = self.adjacency_matrix.nnz / 2
        max_edges = n_nodes * (n_nodes - 1) / 2
        return n_edges / max_edges if max_edges > 0 else 0.0
        
    def get_performance_metrics(self) -> Dict[str, float]:
        """Retrieve a dictionary of current performance metrics."""
        hits, misses = self.stats['cache_hits'], self.stats['cache_misses']
        total_cache_lookups = hits + misses
        
        return {
            'avg_latency_ms': np.mean(self.processing_times) if self.processing_times else 0,
            'latency_violations': self.latency_violations,
            'cache_hit_rate': hits / total_cache_lookups if total_cache_lookups > 0 else 0,
            'graph_density': self.get_graph_density(),
            'active_speakers': len(self.embeddings),
        }


================================================
FILE: graph_based_clustering_engine.py
================================================
"""
Graph-Based Clustering Engine for Modern Streaming Speaker Diarization
Based on Landini et al. (2022) "Online Speaker Diarization with Graph-based Clustering"
"""

import numpy as np
import scipy.sparse as sp
from typing import Dict, Tuple, List, Optional, Set
from dataclasses import dataclass
from sklearn.cluster import SpectralClustering
import logging
from collections import deque
import time

logger = logging.getLogger(__name__)


@dataclass
class SpeakerNode:
    """Represents a speaker node in the graph with associated metadata"""
    node_id: int
    speaker_id: str
    embedding: np.ndarray
    quality_score: float
    last_updated: float
    activity_count: int = 0
    confidence_score: float = 1.0


class GraphBasedClusteringEngine:
    """
    Implements graph-based clustering for streaming speaker diarization
    following Landini et al. (2022) approach with adjacency matrix representation
    and spectral clustering for speaker assignment.
    
    Key features:
    - Sparse adjacency matrix for memory efficiency
    - Spectral clustering for robust speaker assignment
    - Incremental graph updates to avoid full recomputation
    - O(log n) speaker lookup using graph structure
    """
    
    def __init__(self, max_speakers: int = 50, similarity_threshold: float = 0.7,
                 min_cluster_size: int = 2, spectral_n_neighbors: int = 10):
        """
        Initialize the graph-based clustering engine
        
        Args:
            max_speakers: Maximum number of speakers to track
            similarity_threshold: Minimum similarity for edge creation
            min_cluster_size: Minimum cluster size for valid speaker
            spectral_n_neighbors: Number of neighbors for spectral clustering
        """
        self.max_speakers = max_speakers
        self.similarity_threshold = similarity_threshold
        self.min_cluster_size = min_cluster_size
        self.spectral_n_neighbors = spectral_n_neighbors
        
        # Sparse adjacency matrix for speaker similarity graph
        self.adjacency_matrix = sp.csr_matrix((max_speakers, max_speakers), dtype=np.float32)
        
        # Speaker storage and mappings
        self.speakers: Dict[str, SpeakerNode] = {}
        self.node_to_speaker: Dict[int, str] = {}
        self.speaker_to_node: Dict[str, int] = {}
        self.next_node_id = 0
        
        # Spectral clustering configuration
        self.clusterer = SpectralClustering(
            n_clusters=None,
            affinity='precomputed',
            n_neighbors=spectral_n_neighbors,
            assign_labels='discretize',
            random_state=42
        )
        
        # Performance tracking
        self.update_count = 0
        self.last_cluster_time = 0.0
        
        # Incremental update tracking
        self.dirty_nodes: Set[int] = set()
        self.edge_cache: Dict[Tuple[int, int], float] = {}
        
        logger.info(f"Initialized GraphBasedClusteringEngine with max_speakers={max_speakers}")
    
    def add_or_update_speaker(self, speaker_id: str, embedding: np.ndarray, 
                            quality_score: float = 1.0) -> bool:
        """
        Add or update a speaker in the graph
        
        Args:
            speaker_id: Unique speaker identifier
            embedding: Speaker embedding vector
            quality_score: Quality score for this embedding
            
        Returns:
            bool: True if speaker was added/updated successfully
        """
        try:
            # Check if speaker already exists
            if speaker_id in self.speakers:
                return self._update_speaker(speaker_id, embedding, quality_score)
            else:
                return self._add_new_speaker(speaker_id, embedding, quality_score)
                
        except Exception as e:
            logger.error(f"Error adding/updating speaker {speaker_id}: {e}")
            return False
    
    def _add_new_speaker(self, speaker_id: str, embedding: np.ndarray, 
                        quality_score: float) -> bool:
        """Add a new speaker to the graph"""
        if len(self.speakers) >= self.max_speakers:
            logger.warning(f"Maximum speakers ({self.max_speakers}) reached")
            return False
            
        node_id = self.next_node_id
        self.next_node_id += 1
        
        speaker_node = SpeakerNode(
            node_id=node_id,
            speaker_id=speaker_id,
            embedding=embedding,
            quality_score=quality_score,
            last_updated=time.time()
        )
        
        self.speakers[speaker_id] = speaker_node
        self.node_to_speaker[node_id] = speaker_id
        self.speaker_to_node[speaker_id] = node_id
        
        # Update adjacency matrix
        self._update_adjacency_matrix_for_node(node_id, embedding, quality_score)
        
        self.dirty_nodes.add(node_id)
        logger.debug(f"Added new speaker {speaker_id} at node {node_id}")
        return True
    
    def _update_speaker(self, speaker_id: str, embedding: np.ndarray, 
                       quality_score: float) -> bool:
        """Update an existing speaker"""
        speaker_node = self.speakers[speaker_id]
        node_id = speaker_node.node_id
        
        # Quality-weighted embedding update
        old_weight = speaker_node.activity_count / (speaker_node.activity_count + 1)
        new_weight = 1 / (speaker_node.activity_count + 1)
        
        speaker_node.embedding = (
            old_weight * speaker_node.embedding + 
            new_weight * embedding * quality_score
        )
        speaker_node.quality_score = (
            old_weight * speaker_node.quality_score + 
            new_weight * quality_score
        )
        speaker_node.last_updated = time.time()
        speaker_node.activity_count += 1
        
        # Update adjacency matrix
        self._update_adjacency_matrix_for_node(node_id, speaker_node.embedding, 
                                           speaker_node.quality_score)
        
        self.dirty_nodes.add(node_id)
        return True
    
    def _update_adjacency_matrix_for_node(self, node_id: int, embedding: np.ndarray,
                                        quality_score: float):
        """Update adjacency matrix for a specific node"""
        if len(self.speakers) <= 1:
            return
            
        # Calculate similarities with all existing speakers
        similarities = []
        node_ids = []
        
        for speaker_id, speaker_node in self.speakers.items():
            if speaker_node.node_id != node_id:
                similarity = self._calculate_similarity(embedding, speaker_node.embedding)
                weighted_similarity = similarity * quality_score * speaker_node.quality_score
                
                if weighted_similarity >= self.similarity_threshold:
                    similarities.append(weighted_similarity)
                    node_ids.append(speaker_node.node_id)
        
        # Update adjacency matrix
        if similarities:
            row_indices = [node_id] * len(node_ids) + node_ids
            col_indices = node_ids + [node_id] * len(node_ids)
            data = similarities + similarities  # Symmetric matrix
            
            # Create update matrix
            update_matrix = sp.coo_matrix(
                (data, (row_indices, col_indices)),
                shape=(self.max_speakers, self.max_speakers),
                dtype=np.float32
            )
            
            # Add to existing matrix
            self.adjacency_matrix = self.adjacency_matrix + update_matrix
    
    def _calculate_similarity(self, embedding1: np.ndarray, 
                            embedding2: np.ndarray) -> float:
        """Calculate cosine similarity between embeddings"""
        norm1 = np.linalg.norm(embedding1)
        norm2 = np.linalg.norm(embedding2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        dot_product = np.dot(embedding1, embedding2)
        similarity = dot_product / (norm1 * norm2)
        
        # Ensure similarity is in valid range
        return max(0.0, min(1.0, similarity))
    
    def cluster_speakers(self) -> Dict[str, int]:
        """
        Perform spectral clustering on the similarity graph
        
        Returns:
            Dict mapping speaker_id to cluster_id
        """
        if len(self.speakers) < 2:
            return {speaker_id: 0 for speaker_id in self.speakers.keys()}
        
        try:
            start_time = time.time()
            
            # Get active nodes
            active_nodes = [node_id for node_id in self.node_to_speaker.keys()]
            n_active = len(active_nodes)
            
            if n_active < self.min_cluster_size:
                return {speaker_id: 0 for speaker_id in self.speakers.keys()}
            
            # Extract sub-matrix for active speakers
            active_mask = np.zeros(self.max_speakers, dtype=bool)
            active_mask[active_nodes] = True
            
            # Convert to dense array for spectral clustering
            dense_matrix = self.adjacency_matrix.toarray()
            active_matrix = dense_matrix[np.ix_(active_mask, active_mask)]
            
            # Ensure matrix is symmetric
            active_matrix = (active_matrix + active_matrix.T) / 2
            
            # Determine optimal number of clusters
            n_clusters = min(n_active, max(1, n_active // 2))
            
            # Perform spectral clustering
            self.clusterer.n_clusters = n_clusters
            cluster_labels = self.clusterer.fit_predict(active_matrix)
            
            # Map back to speakers
            speaker_clusters = {}
            for i, node_id in enumerate(active_nodes):
                speaker_id = self.node_to_speaker[node_id]
                speaker_clusters[speaker_id] = int(cluster_labels[i])
            
            self.last_cluster_time = time.time() - start_time
            logger.debug(f"Clustered {n_active} speakers into {n_clusters} clusters "
                        f"in {self.last_cluster_time:.3f}s")
            
            return speaker_clusters
            
        except Exception as e:
            logger.error(f"Error during clustering: {e}")
            # Fallback: assign all to single cluster
            return {speaker_id: 0 for speaker_id in self.speakers.keys()}
    
    def find_closest_speaker(self, embedding: np.ndarray, 
                         quality_score: float = 1.0) -> Tuple[Optional[str], float]:
        """
        Find the closest speaker using graph-based similarity
        
        Args:
            embedding: Query embedding
            quality_score: Quality score for the query
            
        Returns:
            Tuple of (speaker_id, similarity_score) or (None, 0.0) if no match
        """
        if not self.speakers:
            return None, 0.0
        
        best_speaker = None
        best_similarity = 0.0
        
        for speaker_id, speaker_node in self.speakers.items():
            similarity = self._calculate_similarity(embedding, speaker_node.embedding)
            weighted_similarity = similarity * quality_score * speaker_node.quality_score
            
            if weighted_similarity > best_similarity:
                best_similarity = weighted_similarity
                best_speaker = speaker_id
        
        return best_speaker, best_similarity
    
    def get_speaker_count(self) -> int:
        """Get current number of speakers"""
        return len(self.speakers)
    
    def get_active_speakers(self) -> List[str]:
        """Get list of active speaker IDs"""
        return list(self.speakers.keys())
    
    def prune_inactive_speakers(self, inactivity_threshold: float = 300.0):
        """
        Remove speakers that haven't been updated recently
        
        Args:
            inactivity_threshold: Time in seconds after which speaker is considered inactive
        """
        current_time = time.time()
        inactive_speakers = []
        
        for speaker_id, speaker_node in self.speakers.items():
            if current_time - speaker_node.last_updated > inactivity_threshold:
                inactive_speakers.append(speaker_id)
        
        for speaker_id in inactive_speakers:
            self._remove_speaker(speaker_id)
    
    def _remove_speaker(self, speaker_id: str):
        """Remove a speaker from the graph"""
        if speaker_id not in self.speakers:
            return
        
        speaker_node = self.speakers[speaker_id]
        node_id = speaker_node.node_id
        
        # Remove from mappings
        del self.speakers[speaker_id]
        del self.node_to_speaker[node_id]
        del self.speaker_to_node[speaker_id]
        
        # Clear adjacency matrix row/column
        self.adjacency_matrix[node_id, :] = 0
        self.adjacency_matrix[:, node_id] = 0
        
        logger.debug(f"Removed speaker {speaker_id} from graph")
    
    def get_graph_stats(self) -> Dict[str, float]:
        """Get statistics about the current graph state"""
        if not self.speakers:
            return {
                'speaker_count': 0,
                'edge_density': 0.0,
                'last_cluster_time': 0.0,
                'update_count': self.update_count
            }
        
        # Calculate edge density
        n_speakers = len(self.speakers)
        max_edges = n_speakers * (n_speakers - 1) / 2
        
        # Count actual edges
        dense_matrix = self.adjacency_matrix.toarray()
        active_mask = np.zeros(self.max_speakers, dtype=bool)
        for node_id in self.node_to_speaker.keys():
            active_mask[node_id] = True
        
        active_submatrix = dense_matrix[np.ix_(active_mask, active_mask)]
        actual_edges = np.sum(active_submatrix > 0) / 2
        
        edge_density = actual_edges / max_edges if max_edges > 0 else 0.0
        
        return {
            'speaker_count': n_speakers,
            'edge_density': edge_density,
            'last_cluster_time': self.last_cluster_time,
            'update_count': self.update_count
        }


================================================
FILE: gstreamer_service.py
================================================
#!/usr/bin/env python3
"""
Service for handling real-time audio streams via WebSocket.
Receives audio from Bandwidth's <StartStream> verb and forwards it for processing.
"""

import os
import sys
import asyncio
import json
import logging
from typing import Dict, Any
from pathlib import Path
import websockets
import websockets.server

# Add project root to path
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import the main telephony service
from src.services.telephony_service import telephony_service

class WebSocketHandler:
    """Handles WebSocket connections for real-time audio streaming from Bandwidth."""
    
    async def handle_connection(self, websocket, path):
        """Handle incoming WebSocket connection from Bandwidth."""
        
        try:
            # The path will contain the callId, e.g., /ws/c-1234...
            call_id = path.strip("/").split("/")[-1]
            logger.info(f"WebSocket connection established for call {call_id}")
            
            if call_id not in telephony_service.active_calls:
                await websocket.close(code=1008, reason="Call not found")
                logger.warning(f"WebSocket connection for unknown callId {call_id} rejected.")
                return

            async for message in websocket:
                try:
                    data = json.loads(message)
                    
                    # Process incoming media from Bandwidth stream
                    if data.get("event") == "media":
                        chunk_id = f"chunk_{data['sequence']}"
                        audio_data = data["media"]["payload"] # Base64 encoded audio
                        
                        # Pass to telephony service for processing
                        await telephony_service.process_audio_chunk(
                            call_id,
                            chunk_id,
                            audio_data,
                            data['sequence']
                        )
                        
                except json.JSONDecodeError:
                    logger.warning(f"Received non-JSON message on WebSocket for call {call_id}")
                except Exception as e:
                    logger.error(f"WebSocket message error for call {call_id}: {e}")

        except websockets.exceptions.ConnectionClosed:
            logger.info(f"WebSocket disconnected for call {call_id}")
        except Exception as e:
            logger.error(f"WebSocket handler error for call {call_id}: {e}")

class GStreamerService:
    """Service to manage the WebSocket server."""
    
    def __init__(self):
        self.websocket_server = None

    async def start_websocket_server(self, port: int = 8765):
        """Start WebSocket server to listen for Bandwidth audio streams."""
        handler = WebSocketHandler()
        logger.info(f"Starting WebSocket server on port {port}")
        
        self.websocket_server = await websockets.serve(
            handler.handle_connection,
            "0.0.0.0",
            port
        )
        return self.websocket_server

    async def stop_websocket_server(self):
        """Stop the WebSocket server."""
        if self.websocket_server:
            self.websocket_server.close()
            await self.websocket_server.wait_closed()
            logger.info("WebSocket server stopped")

# Global service instance
gstreamer_service = GStreamerService()


================================================
FILE: instagram_service.py
================================================
"""
Instagram Service - Wrapper for Instagram API functionality using yt-dlp.

This module provides a service layer for interacting with Instagram
to fetch user information, posts, and download content.
"""

from typing import Optional, List, Dict, Any
import asyncio
import os
import logging
from datetime import datetime
import json
import yt_dlp
from pprint import pformat

logger = logging.getLogger(__name__)


class InstagramService:
    """Service for interacting with Instagram using yt-dlp."""
    
    def __init__(self):
        """
        Initialize Instagram service with yt-dlp.
        """
        self.ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': False,
            'force_generic_extractor': False,
            'ignoreerrors': True,
            'no_color': True,
            'no_check_certificate': True,
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'http_headers': {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
        }
        
    def _extract_user_info_from_data(self, info_dict: Dict[str, Any]) -> Dict[str, Any]:
        """Extract user information from yt-dlp info."""
        # Get username and basic info
        username = info_dict.get('uploader_id') or info_dict.get('channel_id') or ''
        uploader = info_dict.get('uploader') or info_dict.get('channel') or username
        
        # Extract from URL if needed
        webpage_url = info_dict.get('webpage_url', '')
        if not username and 'instagram.com' in webpage_url:
            import re
            match = re.search(r'instagram\.com/([^/?]+)', webpage_url)
            if match:
                username = match.group(1)
        
        # Get description/bio
        description = info_dict.get('description', '')
        
        # Get thumbnail/avatar
        thumbnail = info_dict.get('thumbnail') or ''
        
        # Try to get follower count and post count from entries
        entries = info_dict.get('entries', [])
        follower_count = 0
        post_count = len(entries) if entries else 0
        
        # If we have entries, try to extract more info from first post
        if entries and len(entries) > 0:
            first_entry = entries[0]
            if isinstance(first_entry, dict):
                # Try to get uploader info from post
                if not uploader and first_entry.get('uploader'):
                    uploader = first_entry.get('uploader')
                if not username and first_entry.get('uploader_id'):
                    username = first_entry.get('uploader_id')
                if not thumbnail and first_entry.get('thumbnail'):
                    thumbnail = first_entry.get('thumbnail')
        
        # Generate fallback avatar if none found
        if not thumbnail and (uploader or username):
            display_name = uploader or username
            thumbnail = f"https://ui-avatars.com/api/?name={display_name}&size=512&background=E1306C&color=ffffff&bold=true"
        
        logger.info(f"Extracted Instagram user info - username: {username}, name: {uploader}")
        
        return {
            'username': username or 'unknown',
            'fullName': uploader or username or 'Unknown User',
            'profilePicture': thumbnail,
            'bio': description[:150] if description else '',
            'isVerified': False,  # Not available via yt-dlp
            'followerCount': follower_count,
            'followingCount': 0,  # Not available via yt-dlp
            'postCount': post_count,
            'isPrivate': False,  # If we can access, it's not private
            'profileUrl': webpage_url or f"https://www.instagram.com/{username}/"
        }
    
    async def get_user_info(self, username: str) -> Dict[str, Any]:
        """
        Fetch user information from Instagram using yt-dlp.
        
        Args:
            username: Instagram username (without @) or profile URL
            
        Returns:
            Dictionary containing user information
            
        Raises:
            Exception: If user not found or API error
        """
        try:
            # Extract username from URL if provided
            if username.startswith('http'):
                import re
                match = re.search(r'instagram\.com/([^/?]+)', username)
                if match:
                    username = match.group(1)
                else:
                    raise ValueError(f"Invalid Instagram URL: {username}")
            
            # Clean username
            username = username.replace('@', '').strip()
            
            # Instagram user URL
            url = f'https://www.instagram.com/{username}/'
            
            logger.info(f"Fetching Instagram user info for: {username}")
            
            # Create yt-dlp instance with options
            ydl_opts = {
                **self.ydl_opts,
                'extract_flat': 'in_playlist',  # Get list of posts without full extraction
                'playlistend': 12,  # Get some posts to extract user info
            }
            
            # Run extraction in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            
            def extract_info():
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    try:
                        info = ydl.extract_info(url, download=False)
                        return info
                    except Exception as e:
                        logger.error(f"yt-dlp extraction error: {str(e)}")
                        return None
                        
            info_dict = await loop.run_in_executor(None, extract_info)
            
            if not info_dict:
                raise Exception(f"Could not fetch user info for {username}")
            
            # Log available data
            logger.info(f"Instagram user data keys: {list(info_dict.keys())}")
            
            # Extract user information from the data
            user_info = self._extract_user_info_from_data(info_dict)
            
            # Ensure username matches input
            user_info['username'] = username
            
            return user_info
            
        except Exception as e:
            logger.error(f"Error fetching Instagram user info for {username}: {str(e)}")
            raise Exception(f"Failed to fetch user info: {str(e)}")
    
    async def get_user_posts(
        self, 
        username: str, 
        count: int = 6,
        cursor: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Fetch user's posts from Instagram using yt-dlp.
        
        Args:
            username: Instagram username
            count: Number of posts to fetch (default 6, max 6)
            cursor: Pagination cursor (not used with yt-dlp)
            
        Returns:
            Dictionary containing posts and pagination info
        """
        try:
            # Clean username
            username = username.replace('@', '').strip()
            
            # Limit count to 6
            count = min(count, 6)
            
            # Instagram user URL
            url = f'https://www.instagram.com/{username}/'
            
            logger.info(f"Fetching Instagram posts for: {username}, count: {count}")
            
            # Create yt-dlp instance with options
            ydl_opts = {
                **self.ydl_opts,
                'extract_flat': False,  # We want full post extraction
                'playlist_items': f'1-{count}',  # Limit to first N posts
            }
            
            # Run extraction in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            
            def extract_info():
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    try:
                        info = ydl.extract_info(url, download=False)
                        return info
                    except Exception as e:
                        logger.error(f"yt-dlp extraction error: {str(e)}")
                        return None
                        
            info_dict = await loop.run_in_executor(None, extract_info)
            
            if not info_dict:
                raise Exception(f"Could not fetch posts for {username}")
            
            posts_data = []
            
            # Process entries (posts)
            entries = info_dict.get('entries', [])
            logger.info(f"Found {len(entries)} Instagram posts")
            
            for entry in entries[:count]:
                if not entry:
                    continue
                    
                # Determine media type
                media_type = 'photo'  # Default
                if entry.get('duration'):
                    media_type = 'video'
                elif 'reel' in entry.get('webpage_url', '').lower():
                    media_type = 'reel'
                
                # Extract post information
                post_info = {
                    "postId": entry.get('id', ''),
                    "shortcode": entry.get('display_id') or entry.get('id', ''),
                    "caption": entry.get('description', '') or entry.get('title', ''),
                    "mediaType": media_type,
                    "mediaUrl": entry.get('url', '') or entry.get('webpage_url', ''),
                    "thumbnailUrl": entry.get('thumbnail', ''),
                    "timestamp": entry.get('timestamp', 0),
                    "likeCount": entry.get('like_count', 0),
                    "commentCount": entry.get('comment_count', 0),
                    "viewCount": entry.get('view_count', 0) if media_type in ['video', 'reel'] else None,
                    "duration": entry.get('duration', 0) if media_type in ['video', 'reel'] else None,
                    "isVideo": media_type in ['video', 'reel'],
                    "hashtags": self._extract_hashtags(entry.get('description', ''))
                }
                
                posts_data.append(post_info)
            
            return {
                "posts": posts_data,
                "count": len(posts_data),
                "hasMore": len(entries) > count,  # Simplified pagination
                "nextCursor": None  # yt-dlp doesn't support cursor-based pagination
            }
            
        except Exception as e:
            logger.error(f"Error fetching posts for {username}: {str(e)}")
            raise Exception(f"Failed to fetch posts: {str(e)}")
    
    def _extract_hashtags(self, caption: str) -> List[str]:
        """Extract hashtags from post caption."""
        import re
        hashtags = []
        if caption:
            # Find all hashtags in the caption
            tags = re.findall(r'#(\w+)', caption)
            hashtags = tags[:10]  # Limit to first 10 hashtags
        return hashtags
        
    async def get_post_info(self, post_id: str) -> Dict[str, Any]:
        """
        Fetch detailed information about a specific post using yt-dlp.
        
        Args:
            post_id: Instagram post ID or shortcode
            
        Returns:
            Dictionary containing post information
        """
        try:
            # Instagram post URL - try both formats
            if len(post_id) > 15:  # Likely a full ID
                url = f'https://www.instagram.com/p/{post_id[:11]}/'  # Use shortcode portion
            else:
                url = f'https://www.instagram.com/p/{post_id}/'
            
            logger.info(f"Fetching Instagram post info for: {post_id}")
            
            # Run extraction in thread pool
            loop = asyncio.get_event_loop()
            
            def extract_info():
                with yt_dlp.YoutubeDL(self.ydl_opts) as ydl:
                    try:
                        info = ydl.extract_info(url, download=False)
                        return info
                    except Exception as e:
                        logger.error(f"yt-dlp extraction error: {str(e)}")
                        return None
                        
            info_dict = await loop.run_in_executor(None, extract_info)
            
            if not info_dict:
                raise Exception(f"Could not fetch post info for {post_id}")
            
            # Determine media type
            media_type = 'photo'
            if info_dict.get('duration'):
                media_type = 'video'
            elif 'reel' in info_dict.get('webpage_url', '').lower():
                media_type = 'reel'
            
            # Extract post information
            return {
                "postId": info_dict.get('id', post_id),
                "shortcode": info_dict.get('display_id') or post_id,
                "caption": info_dict.get('description', '') or info_dict.get('title', ''),
                "mediaType": media_type,
                "mediaUrl": info_dict.get('url', ''),
                "thumbnailUrl": info_dict.get('thumbnail', ''),
                "timestamp": info_dict.get('timestamp', 0),
                "likeCount": info_dict.get('like_count', 0),
                "commentCount": info_dict.get('comment_count', 0),
                "viewCount": info_dict.get('view_count', 0) if media_type in ['video', 'reel'] else None,
                "duration": info_dict.get('duration', 0) if media_type in ['video', 'reel'] else None,
                "uploader": info_dict.get('uploader', ''),
                "uploaderId": info_dict.get('uploader_id', '')
            }
            
        except Exception as e:
            logger.error(f"Error fetching post info for {post_id}: {str(e)}")
            raise Exception(f"Failed to fetch post info: {str(e)}")
    
    async def download_media_bytes(self, post_id: str) -> bytes:
        """
        Download media bytes from Instagram using yt-dlp.
        
        Args:
            post_id: Instagram post ID or shortcode
            
        Returns:
            Media bytes
        """
        try:
            # Instagram post URL
            if len(post_id) > 15:  # Likely a full ID
                url = f'https://www.instagram.com/p/{post_id[:11]}/'
            else:
                url = f'https://www.instagram.com/p/{post_id}/'
            
            logger.info(f"Download requested for Instagram post {post_id}")
            
            # For now, return a placeholder
            # In production, you would implement actual download using yt-dlp
            # with proper file handling and format selection
            
            return f"Instagram media URL: {url}".encode()
            
        except Exception as e:
            logger.error(f"Error downloading media {post_id}: {str(e)}")
            raise Exception(f"Failed to download media: {str(e)}")
    
    async def close(self):
        """Close the service (no cleanup needed for yt-dlp)."""
        pass


# Singleton instance
_instagram_service: Optional[InstagramService] = None


def get_instagram_service() -> InstagramService:
    """
    Get or create Instagram service instance.
        
    Returns:
        InstagramService instance
    """
    global _instagram_service
    
    if _instagram_service is None:
        _instagram_service = InstagramService()
    
    return _instagram_service


================================================
FILE: integrated_speaker_identifier.py
================================================
"""
Integrated Modern Speaker Identifier
Complete drop-in replacement that handles audio input and embedding extraction
"""

import numpy as np
from typing import Tuple, Optional, Dict, List
import time
import logging
from dataclasses import dataclass
import threading

# Import modern components
from .graph_based_clustering_engine import GraphBasedClusteringEngine
from .adaptive_thresholding_manager import AdaptiveThresholdingManager
from .embedding_quality_assessor import EmbeddingQualityAssessor
from .memory_efficient_speaker_manager import MemoryEfficientSpeakerManager
from .temporal_context_tracker import TemporalContextTracker
from .fast_graph_optimizer import FastGraphOptimizer
from .speaker_embedding_service import get_speaker_embedding_service

logger = logging.getLogger(__name__)


@dataclass
class SpeakerIdentificationResult:
    """Result of speaker identification"""
    speaker_id: str
    confidence: float
    quality_score: float
    processing_time_ms: float
    method: str
    adaptive_threshold: float


class IntegratedSpeakerIdentifier:
    """
    Complete drop-in replacement for StatefulSpeakerIdentifier that handles
    both audio input and embedding extraction internally
    """
    
    def __init__(self, base_threshold: float = 0.7, max_speakers: int = 50,
                 use_graph_clustering: bool = True, use_adaptive_thresholds: bool = True,
                 use_quality_weighting: bool = True, use_temporal_context: bool = True):
        """
        Initialize integrated speaker identifier
        
        Args:
            base_threshold: Base similarity threshold
            max_speakers: Maximum speakers to track
            use_graph_clustering: Enable graph-based clustering (Landini et al., 2022)
            use_adaptive_thresholds: Enable adaptive thresholding (Park et al., 2022)
            use_quality_weighting: Enable quality-aware embedding updates (Bredin & Laurent, 2021)
            use_temporal_context: Enable temporal context smoothing
        """
        self.base_threshold = base_threshold
        self.max_speakers = max_speakers
        
        self.config = {
            'graph_clustering': use_graph_clustering,
            'adaptive_thresholds': use_adaptive_thresholds,
            'quality_weighting': use_quality_weighting,
            'temporal_context': use_temporal_context
        }
        
        # Landini et al. (2022) - Graph-based clustering implementation
        self.clustering_engine = GraphBasedClusteringEngine(
            max_speakers=max_speakers,
            similarity_threshold=base_threshold
        )
        
        # Park et al. (2022) - Adaptive thresholding implementation
        self.adaptive_manager = AdaptiveThresholdingManager(
            base_threshold=base_threshold,
            adaptation_rate=0.1
        )
        
        # Bredin & Laurent (2021) - Quality-aware assessment
        self.quality_assessor = EmbeddingQualityAssessor()
        
        # Cornell et al. (2022) - Memory-efficient profile management
        self.memory_manager = MemoryEfficientSpeakerManager(
            max_speakers=max_speakers,
            memory_threshold_mb=100
        )
        
        self.temporal_tracker = TemporalContextTracker(
            smoothing_window=5,
            max_context_seconds=30.0
        )
        
        # Landini et al. (2023) - Fast graph optimization techniques
        self.graph_optimizer = FastGraphOptimizer(
            max_speakers=max_speakers,
            latency_constraint_ms=100.0
        )
        
        self.embedding_service = get_speaker_embedding_service()
        
        self.speaker_profiles: Dict[str, Dict] = {}
        self.speaker_lock = threading.RLock()
        
        self.identification_count = 0
        self.fallback_count = 0
        
        logger.info("Initialized IntegratedSpeakerIdentifier")
    
    def identify_speaker(self, audio_chunk_np: np.ndarray, 
                        embedding: np.ndarray = None,
                        sample_rate: int = 16000) -> Tuple[str, float]:
        """
        Identify speaker from audio chunk, extracting embedding if necessary.
        This method will always return a speaker ID (either existing or new) and will not return "Unknown".
        """
        start_time = time.time()
        
        try:
            if embedding is None:
                embedding = self.embedding_service.extract_embedding(audio_chunk_np, sample_rate)
                if embedding is None:
                    # Fallback to creating a new speaker if embedding extraction fails
                    logger.warning("Failed to extract embedding, creating new speaker profile.")
                    new_speaker_id = f"speaker_{len(self.speaker_profiles)}"
                    self.memory_manager.add_or_update_speaker(new_speaker_id, np.zeros(192), 0.1)
                    return new_speaker_id, 0.1

            # Bredin & Laurent (2021) - Assess embedding quality before use
            quality_score = self.quality_assessor.assess_quality(audio_chunk_np)
            
            # If quality is too low, treat as a new speaker to avoid corrupting existing profiles
            if not self.quality_assessor.should_update_centroid(quality_score):
                logger.debug(f"Low quality embedding (score: {quality_score:.2f}), creating new speaker profile.")
                new_speaker_id = f"speaker_{len(self.speaker_profiles)}"
                self.memory_manager.add_or_update_speaker(new_speaker_id, embedding, quality_score)
                return new_speaker_id, quality_score

            result = self._modern_identification(embedding, quality_score, audio_chunk_np)
            
            self._update_components(result.speaker_id, embedding, quality_score, result.confidence)
            
            processing_time = (time.time() - start_time) * 1000
            logger.debug(f"Identified speaker {result.speaker_id} with confidence {result.confidence:.3f} ({processing_time:.1f}ms)")
            
            return result.speaker_id, result.confidence
            
        except Exception as e:
            logger.error(f"Error in modern identification: {e}", exc_info=True)
            return self._fallback_identification(embedding, 0.5)
    
    def _modern_identification(self, embedding: np.ndarray,
                             quality_score: float, audio_chunk_np: np.ndarray) -> SpeakerIdentificationResult:
        """Perform modern speaker identification using all components"""
        start_time = time.time()
        
        # Landini et al. (2022) - Use graph-based or memory-based identification
        if self.config['graph_clustering']:
            speaker_id, confidence = self._graph_based_identification(embedding, quality_score)
            method = "graph"
        else:
            speaker_id, confidence = self._memory_based_identification(embedding, quality_score)
            method = "memory"
        
        # Park et al. (2022) - Apply per-speaker adaptive thresholding
        if self.config['adaptive_thresholds']:
            threshold = self.adaptive_manager.get_threshold(speaker_id)
            if confidence < threshold:
                better_speaker, better_confidence = self._find_better_match(embedding, quality_score, threshold)
                speaker_id, confidence = better_speaker, better_confidence
        
        if self.config['temporal_context']:
            transition_probs = self.temporal_tracker.calculate_transition_probabilities()
            context_summary = self.temporal_tracker.get_context_summary()
            speaker_id, confidence = self._apply_conversation_flow_analysis(speaker_id, confidence, transition_probs, context_summary)
            speaker_id, confidence = self.temporal_tracker.apply_temporal_smoothing(speaker_id, confidence)
        
        processing_time = (time.time() - start_time) * 1000
        
        return SpeakerIdentificationResult(
            speaker_id=speaker_id,
            confidence=confidence,
            quality_score=quality_score,
            processing_time_ms=processing_time,
            method=method,
            adaptive_threshold=self.adaptive_manager.get_threshold(speaker_id) if speaker_id else self.base_threshold
        )
    
    def _apply_conversation_flow_analysis(self, speaker_id: str, confidence: float,
                                        transition_probs: dict, context_summary: dict) -> Tuple[str, float]:
        """Apply conversation flow analysis and re-evaluation triggers"""
        if confidence < 0.6 or context_summary.get('inconsistencies', 0) > 0:
            dominant_speaker = self._find_best_speaker_from_context(speaker_id, confidence, transition_probs, context_summary)
            if dominant_speaker:
                speaker_id, confidence = dominant_speaker[0], dominant_speaker[1]
        
        is_valid, reason = self.temporal_tracker.check_temporal_constraints(speaker_id, time.time())
        if not is_valid:
            speaker_id, confidence = self._resolve_temporal_violation(speaker_id, confidence, context_summary)
        
        return speaker_id, confidence
    
    def _find_best_speaker_from_context(self, current_speaker: str, current_confidence: float,
                                     transition_probs: dict, context_summary: dict) -> Optional[Tuple[str, float]]:
        """Find best speaker based on conversation flow and context"""
        if not transition_probs: return None
        dominance = self.temporal_tracker.get_speaker_dominance()
        if not dominance: return None
        
        best_speaker = None
        best_score = 0.0
        
        for speaker, dom_score in dominance.items():
            transition_score = transition_probs.get((current_speaker, speaker), 0.0)
            combined_score = 0.7 * dom_score + 0.3 * transition_score
            if combined_score > best_score:
                best_score = combined_score
                best_speaker = speaker
        
        if best_speaker and best_score > 0.5:
            return (best_speaker, min(0.9, current_confidence + 0.1))
        
        return None
    
    def _resolve_temporal_violation(self, speaker_id: str, confidence: float,
                                    context_summary: dict) -> Tuple[str, float]:
        """Resolve temporal violations by finding consistent speaker"""
        dominance = self.temporal_tracker.get_speaker_dominance()
        if dominance:
            dominant_speaker = max(dominance.keys(), key=lambda x: dominance[x])
            if dominance[dominant_speaker] > 0.3:
                return dominant_speaker, max(0.5, confidence * 0.8)
        
        return speaker_id, max(0.3, confidence * 0.7)
    
    def _graph_based_identification(self, embedding: np.ndarray,
                                  quality_score: float) -> Tuple[str, float]:
        """Landini et al. (2022) - Use graph-based clustering for identification"""
        closest_speaker, similarity = self.clustering_engine.find_closest_speaker(embedding, quality_score)
        
        if closest_speaker is None:
            speaker_id = f"speaker_{len(self.speaker_profiles)}"
            confidence = 0.5  # Initial confidence for a new speaker
        else:
            speaker_id = closest_speaker
            confidence = similarity
        
        return speaker_id, confidence
    
    def _memory_based_identification(self, embedding: np.ndarray,
                                   quality_score: float) -> Tuple[str, float]:
        """Cornell et al. (2022) - Use memory-efficient speaker matching"""
        closest_speaker, similarity = self.memory_manager.find_closest_speaker(embedding, quality_score)
        
        if closest_speaker is None:
            speaker_id = f"speaker_{len(self.speaker_profiles)}"
            confidence = 0.5
        else:
            speaker_id = closest_speaker
            confidence = similarity
        
        return speaker_id, confidence
    
    def _find_better_match(self, embedding: np.ndarray, quality_score: float,
                          threshold: float) -> Tuple[str, float]:
        """Find better speaker match when confidence is low"""
        candidates = []
        
        # Graph-based approach from Landini et al. (2022)
        graph_speaker, graph_conf = self.clustering_engine.find_closest_speaker(embedding, quality_score)
        if graph_conf >= threshold:
            candidates.append((graph_speaker, graph_conf, "graph"))
        
        # Memory-based approach from Cornell et al. (2022)
        mem_speaker, mem_conf = self.memory_manager.find_closest_speaker(embedding, quality_score)
        if mem_conf >= threshold:
            candidates.append((mem_speaker, mem_conf, "memory"))
        
        if candidates:
            return max(candidates, key=lambda x: x[1])[:2]
        
        # If no suitable match is found, create a new speaker profile
        speaker_id = f"speaker_{len(self.speaker_profiles)}"
        return speaker_id, 0.5
    
    def _fallback_identification(self, embedding: np.ndarray,
                               quality_score: float) -> Tuple[str, float]:
        """Fallback to simple centroid-based identification, ensuring a new speaker is created if no match"""
        self.fallback_count += 1
        
        if embedding is None:
             new_speaker_id = f"speaker_{len(self.speaker_profiles)}"
             self.memory_manager.add_or_update_speaker(new_speaker_id, np.zeros(192), 0.1)
             return new_speaker_id, 0.1

        best_speaker = None
        best_distance = float('inf')
        
        with self.speaker_lock:
            for speaker_id, profile in self.speaker_profiles.items():
                if 'centroid' in profile:
                    distance = np.linalg.norm(embedding - profile['centroid'])
                    if distance < best_distance:
                        best_distance = distance
                        best_speaker = speaker_id
        
        if best_speaker is None or (1 - best_distance) < self.base_threshold:
            speaker_id = f"speaker_{len(self.speaker_profiles)}"
            self.memory_manager.add_or_update_speaker(speaker_id, embedding, quality_score)
            confidence = max(0.0, 1.0 - best_distance) if best_speaker else 0.5
        else:
            speaker_id = best_speaker
            confidence = max(0.0, 1.0 - best_distance)
        
        return speaker_id, confidence
    
    def _update_components(self, speaker_id: str, embedding: np.ndarray,
                        quality_score: float, confidence: float):
        """Update all modern components with new data"""
        if self.config['graph_clustering']:
            self.clustering_engine.add_or_update_speaker(speaker_id, embedding, quality_score)
        
        if self.config['adaptive_thresholds']:
            similarity = self._calculate_similarity(embedding, speaker_id)
            self.adaptive_manager.update_threshold(speaker_id, similarity, quality_score, was_accepted=True)
        
        self.memory_manager.add_or_update_speaker(speaker_id, embedding, quality_score)
        
        self.temporal_tracker.add_context(
            timestamp=time.time(),
            speaker_id=speaker_id,
            confidence=confidence,
            embedding=embedding,
            quality_score=quality_score,
            segment_duration=1.0
        )
    
    def _calculate_similarity(self, embedding: np.ndarray,
                            speaker_id: str) -> float:
        """Calculate similarity with speaker centroid"""
        with self.speaker_lock:
            # Use the memory manager's profiles for consistency
            profile = self.memory_manager.speakers.get(speaker_id)
            if profile:
                centroid = profile.centroid
                norm_emb = np.linalg.norm(embedding)
                norm_centroid = np.linalg.norm(centroid)
                if norm_emb > 0 and norm_centroid > 0:
                    return np.dot(embedding, centroid) / (norm_emb * norm_centroid)
            return 0.0

    def get_speaker_count(self) -> int:
        """Get number of tracked speakers"""
        return self.memory_manager.get_speaker_count()
    
    def get_speakers(self) -> List[str]:
        """Get list of speaker IDs"""
        return self.memory_manager.get_speaker_ids()
    
    def reset(self):
        """Reset all components (for testing)"""
        with self.speaker_lock:
            self.speaker_profiles.clear()
            self.clustering_engine = GraphBasedClusteringEngine(max_speakers=self.max_speakers, similarity_threshold=self.base_threshold)
            self.adaptive_manager = AdaptiveThresholdingManager(base_threshold=self.base_threshold)
            self.memory_manager.clear_all_speakers()
            self.temporal_tracker.reset_context()
            self.identification_count = 0
            self.fallback_count = 0
    
    def get_performance_stats(self) -> Dict[str, any]:
        """Get performance statistics"""
        stats = {
            'total_identifications': self.identification_count,
            'fallback_count': self.fallback_count,
            'fallback_rate': self.fallback_count / max(1, self.identification_count),
            'config': self.config,
            'temporal_context': self.temporal_tracker.get_context_summary() if self.config['temporal_context'] else None,
            'memory_stats': self.memory_manager.get_memory_stats(),
            'adaptive_thresholds_count': len(self.adaptive_manager.speaker_thresholds)
        }
        
        if self.config['graph_clustering']:
            stats['clustering_stats'] = self.clustering_engine.get_graph_stats()
        
        return stats
    
    def enable_feature(self, feature: str, enabled: bool = True):
        """Enable/disable specific features"""
        if feature in self.config:
            self.config[feature] = enabled
            logger.info(f"Feature {feature} {'enabled' if enabled else 'disabled'}")
        else:
            logger.warning(f"Unknown feature: {feature}")

    def get_speaker_profile(self, speaker_id: str) -> Optional[Dict]:
        """Get speaker profile (backward compatibility)"""
        return self.speaker_profiles.get(speaker_id) 



================================================
FILE: memory_efficient_speaker_manager.py
================================================
"""
Memory-Efficient Speaker Management System
Based on Cornell et al. (2022) "Memory-Efficient Streaming Speaker Diarization"
"""

import numpy as np
import time
import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from collections import deque, defaultdict
import threading
import psutil
import gc

logger = logging.getLogger(__name__)


@dataclass
class SpeakerProfile:
    """Enhanced speaker profile with memory tracking"""
    speaker_id: str
    centroid: np.ndarray
    embedding_history: deque  # Limited history for memory efficiency
    quality_weighted_count: float
    confidence_score: float
    last_seen: float
    activity_level: float
    total_segments: int
    memory_usage: int  # Track memory usage in bytes
    
    def __post_init__(self):
        # Calculate memory usage
        self.memory_usage = (
            self.centroid.nbytes +
            sum(emb.nbytes for emb in self.embedding_history) +
            100  # Overhead for other fields
        )


@dataclass
class MemoryUsageStats:
    """Memory usage statistics"""
    total_speakers: int
    total_memory_mb: float
    peak_memory_mb: float
    inactive_speakers: int
    avg_speaker_memory_mb: float
    memory_threshold_exceeded: bool


class MemoryEfficientSpeakerManager:
    """
    Implements memory-efficient speaker management strategies following Cornell et al. (2022)
    
    Key features:
    - Speaker profile storage with activity tracking
    - Memory usage monitoring and threshold detection
    - Sliding window management for long audio streams
    - Temporal decay for inactive speaker centroids
    - Hierarchical clustering for merging similar speakers
    - Emergency pruning for memory overflow situations
    """
    
    def __init__(self, max_speakers: int = 50, memory_threshold_mb: int = 100,
                 inactivity_threshold: float = 300.0, merge_similarity_threshold: float = 0.9,
                 embedding_history_size: int = 10):
        """
        Initialize memory-efficient speaker manager
        
        Args:
            max_speakers: Maximum number of speakers to track
            memory_threshold_mb: Memory threshold in MB
            inactivity_threshold: Time in seconds after which speaker is inactive
            merge_similarity_threshold: Similarity threshold for merging speakers
            embedding_history_size: Maximum embeddings to store per speaker
        """
        self.max_speakers = max_speakers
        self.memory_threshold_mb = memory_threshold_mb
        self.inactivity_threshold = inactivity_threshold
        self.merge_similarity_threshold = merge_similarity_threshold
        self.embedding_history_size = embedding_history_size
        
        # Speaker storage
        self.speakers: Dict[str, SpeakerProfile] = {}
        self.speaker_lock = threading.RLock()
        
        # Memory tracking
        self.memory_stats = MemoryUsageStats(0, 0.0, 0.0, 0, 0.0, False)
        self.last_memory_check = 0
        self.memory_check_interval = 30  # seconds
        
        # Performance metrics
        self.pruning_count = 0
        self.merging_count = 0
        self.emergency_pruning_count = 0
        
        logger.info(f"Initialized MemoryEfficientSpeakerManager: "
                   f"max_speakers={max_speakers}, "
                   f"memory_threshold={memory_threshold_mb}MB")
    
    def add_or_update_speaker(self, speaker_id: str, embedding: np.ndarray,
                            quality_score: float = 1.0) -> bool:
        """
        Add or update a speaker with memory-efficient storage
        
        Args:
            speaker_id: Unique speaker identifier
            embedding: Speaker embedding vector
            quality_score: Quality score for this embedding
            
        Returns:
            bool: True if speaker was added/updated successfully
        """
        with self.speaker_lock:
            try:
                # Check memory before adding
                self._check_memory_usage()
                
                if speaker_id in self.speakers:
                    return self._update_existing_speaker(speaker_id, embedding, quality_score)
                else:
                    return self._add_new_speaker(speaker_id, embedding, quality_score)
                    
            except Exception as e:
                logger.error(f"Error adding/updating speaker {speaker_id}: {e}")
                return False
    
    def _add_new_speaker(self, speaker_id: str, embedding: np.ndarray,
                       quality_score: float) -> bool:
        """Add a new speaker with memory checks"""
        if len(self.speakers) >= self.max_speakers:
            logger.warning("Max speakers reached, attempting to prune inactive speakers")
            self.prune_inactive_speakers()
            
            if len(self.speakers) >= self.max_speakers:
                logger.warning("Cannot add new speaker, max speakers limit reached")
                return False
        
        # Check memory threshold
        if self._is_memory_threshold_exceeded():
            logger.warning("Memory threshold exceeded, triggering emergency pruning")
            self._emergency_pruning()
        
        # Create new speaker profile
        speaker_profile = SpeakerProfile(
            speaker_id=speaker_id,
            centroid=embedding.copy(),
            embedding_history=deque(maxlen=self.embedding_history_size),
            quality_weighted_count=quality_score,
            confidence_score=quality_score,
            last_seen=time.time(),
            activity_level=quality_score,
            total_segments=1,
            memory_usage=0
        )
        
        # Add first embedding to history
        speaker_profile.embedding_history.append(embedding)
        
        self.speakers[speaker_id] = speaker_profile
        logger.debug(f"Added new speaker {speaker_id}")
        
        return True
    
    def _update_existing_speaker(self, speaker_id: str, embedding: np.ndarray,
                               quality_score: float) -> bool:
        """Update an existing speaker with quality-weighted updates"""
        speaker_profile = self.speakers[speaker_id]
        
        # Quality-weighted centroid update
        old_weight = speaker_profile.quality_weighted_count
        new_weight = quality_score
        
        speaker_profile.centroid = (
            (old_weight * speaker_profile.centroid + new_weight * embedding) /
            (old_weight + new_weight)
        )
        
        # Update history
        speaker_profile.embedding_history.append(embedding.copy())
        
        # Update metadata
        speaker_profile.quality_weighted_count += quality_score
        speaker_profile.last_seen = time.time()
        speaker_profile.activity_level = min(1.0, speaker_profile.activity_level + 0.1)
        speaker_profile.total_segments += 1
        
        return True
    
    def find_closest_speaker(self, embedding: np.ndarray,
                           quality_score: float = 1.0) -> Tuple[Optional[str], float]:
        """
        Find the closest speaker using memory-efficient similarity search
        
        Args:
            embedding: Query embedding
            quality_score: Quality score for the query
            
        Returns:
            Tuple of (speaker_id, similarity_score)
        """
        with self.speaker_lock:
            if not self.speakers:
                return None, 0.0
            
            best_speaker = None
            best_similarity = 0.0
            
            for speaker_id, speaker_profile in self.speakers.items():
                similarity = self._calculate_similarity(embedding, speaker_profile.centroid)
                weighted_similarity = similarity * quality_score * speaker_profile.confidence_score
                
                if weighted_similarity > best_similarity:
                    best_similarity = weighted_similarity
                    best_speaker = speaker_id
            
            return best_speaker, best_similarity
    
    def _calculate_similarity(self, embedding1: np.ndarray,
                            embedding2: np.ndarray) -> float:
        """Calculate cosine similarity between embeddings"""
        norm1 = np.linalg.norm(embedding1)
        norm2 = np.linalg.norm(embedding2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        dot_product = np.dot(embedding1, embedding2)
        similarity = dot_product / (norm1 * norm2)
        
        return max(0.0, min(1.0, similarity))
    
    def prune_inactive_speakers(self) -> int:
        """
        Remove speakers that haven't been active recently
        
        Returns:
            int: Number of speakers removed
        """
        with self.speaker_lock:
            current_time = time.time()
            inactive_speakers = []
            
            for speaker_id, speaker_profile in self.speakers.items():
                if current_time - speaker_profile.last_seen > self.inactivity_threshold:
                    inactive_speakers.append(speaker_id)
            
            for speaker_id in inactive_speakers:
                self._remove_speaker(speaker_id)
            
            self.pruning_count += len(inactive_speakers)
            logger.info(f"Pruned {len(inactive_speakers)} inactive speakers")
            
            return len(inactive_speakers)
    
    def merge_similar_speakers(self, similarity_threshold: Optional[float] = None) -> int:
        """
        Merge highly similar speakers to reduce memory usage
        
        Args:
            similarity_threshold: Override for merge threshold
            
        Returns:
            int: Number of mergers performed
        """
        if similarity_threshold is None:
            similarity_threshold = self.merge_similarity_threshold
            
        with self.speaker_lock:
            if len(self.speakers) < 2:
                return 0
            
            mergers = 0
            speakers_to_merge = []
            
            # Find similar speaker pairs
            speaker_list = list(self.speakers.items())
            for i, (id1, profile1) in enumerate(speaker_list):
                for j in range(i + 1, len(speaker_list)):
                    id2, profile2 = speaker_list[j]
                    
                    similarity = self._calculate_similarity(
                        profile1.centroid, profile2.centroid
                    )
                    
                    if similarity >= similarity_threshold:
                        # Prefer to merge less active speaker into more active one
                        if profile1.activity_level >= profile2.activity_level:
                            speakers_to_merge.append((id1, id2, similarity))
                        else:
                            speakers_to_merge.append((id2, id1, similarity))
            
            # Perform mergers
            for target_id, source_id, similarity in speakers_to_merge:
                if source_id in self.speakers and target_id in self.speakers:
                    self._merge_speakers(target_id, source_id, similarity)
                    mergers += 1
            
            self.merging_count += mergers
            logger.info(f"Merged {mergers} similar speakers")
            return mergers
    
    def _merge_speakers(self, target_id: str, source_id: str, similarity: float):
        """Merge source speaker into target speaker"""
        target_profile = self.speakers[target_id]
        source_profile = self.speakers[source_id]
        
        # Weighted merge based on activity levels
        target_weight = target_profile.activity_level
        source_weight = source_profile.activity_level
        
        total_weight = target_weight + source_weight
        
        # Merge centroids
        target_profile.centroid = (
            (target_weight * target_profile.centroid + 
             source_weight * source_profile.centroid) / total_weight
        )
        
        # Update metadata
        target_profile.quality_weighted_count += source_profile.quality_weighted_count
        target_profile.total_segments += source_profile.total_segments
        target_profile.activity_level = min(1.0, target_profile.activity_level + 0.1)
        
        # Merge embedding histories
        for emb in source_profile.embedding_history:
            if len(target_profile.embedding_history) < self.embedding_history_size:
                target_profile.embedding_history.append(emb)
        
        # Remove source speaker
        self._remove_speaker(source_id)
        
        logger.debug(f"Merged speaker {source_id} into {target_id} "
                    f"(similarity={similarity:.3f})")
    
    def apply_temporal_decay(self, decay_factor: float = 0.95):
        """
        Apply temporal decay to speaker centroids for long streams
        
        Args:
            decay_factor: Decay factor (0.95 = 5% decay per application)
        """
        with self.speaker_lock:
            decayed_speakers = 0
            
            for speaker_profile in self.speakers.values():
                # Decay activity level
                speaker_profile.activity_level *= decay_factor
                
                # Decay confidence score
                speaker_profile.confidence_score *= decay_factor
                
                # Update last seen to prevent immediate pruning
                speaker_profile.last_seen = time.time()
                
                decayed_speakers += 1
            
            logger.debug(f"Applied temporal decay to {decayed_speakers} speakers")
    
    def _emergency_pruning(self):
        """Emergency pruning when memory threshold is exceeded"""
        logger.warning("Emergency pruning triggered due to memory threshold")
        
        # Remove least active speakers first
        sorted_speakers = sorted(
            self.speakers.items(),
            key=lambda x: (x[1].activity_level, x[1].last_seen)
        )
        
        # Remove 20% of speakers or until memory is acceptable
        to_remove = max(1, len(sorted_speakers) // 5)
        
        for i in range(min(to_remove, len(sorted_speakers))):
            speaker_id, _ = sorted_speakers[i]
            self._remove_speaker(speaker_id)
        
        self.emergency_pruning_count += to_remove
        logger.warning(f"Emergency pruning removed {to_remove} speakers")
    
    def _remove_speaker(self, speaker_id: str):
        """Remove a speaker from storage"""
        if speaker_id in self.speakers:
            del self.speakers[speaker_id]
            logger.debug(f"Removed speaker {speaker_id}")
    
    def _check_memory_usage(self):
        """Check current memory usage"""
        current_time = time.time()
        
        if current_time - self.last_memory_check < self.memory_check_interval:
            return
            
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            current_memory_mb = memory_info.rss / 1024 / 1024
            
            # Calculate speaker memory usage
            speaker_memory_mb = 0
            for speaker_profile in self.speakers.values():
                speaker_memory_mb += speaker_profile.memory_usage / 1024 / 1024
            
            self.memory_stats = MemoryUsageStats(
                total_speakers=len(self.speakers),
                total_memory_mb=current_memory_mb,
                peak_memory_mb=max(self.memory_stats.peak_memory_mb, current_memory_mb),
                inactive_speakers=self._count_inactive_speakers(),
                avg_speaker_memory_mb=speaker_memory_mb / max(1, len(self.speakers)),
                memory_threshold_exceeded=current_memory_mb > self.memory_threshold_mb
            )
            
            self.last_memory_check = current_time
            
        except Exception as e:
            logger.error(f"Error checking memory usage: {e}")
    
    def _is_memory_threshold_exceeded(self) -> bool:
        """Check if memory threshold is exceeded"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            current_memory_mb = memory_info.rss / 1024 / 1024
            return current_memory_mb > self.memory_threshold_mb
        except:
            return False
    
    def _count_inactive_speakers(self) -> int:
        """Count inactive speakers"""
        current_time = time.time()
        inactive_count = 0
        
        for speaker_profile in self.speakers.values():
            if current_time - speaker_profile.last_seen > self.inactivity_threshold:
                inactive_count += 1
        
        return inactive_count
    
    def get_memory_stats(self) -> MemoryUsageStats:
        """Get current memory usage statistics"""
        self._check_memory_usage()
        return self.memory_stats
    
    def get_speaker_count(self) -> int:
        """Get current number of speakers"""
        return len(self.speakers)
    
    def get_speaker_ids(self) -> List[str]:
        """Get list of all speaker IDs"""
        return list(self.speakers.keys())
    
    def reset_statistics(self):
        """Reset performance counters"""
        self.pruning_count = 0
        self.merging_count = 0
        self.emergency_pruning_count = 0
    
    def clear_all_speakers(self):
        """Clear all speakers (for testing/debugging)"""
        with self.speaker_lock:
            self.speakers.clear()
            logger.info("Cleared all speakers")


================================================
FILE: memory_monitor.py
================================================
# backend/src/services/memory_monitor.py
import os
import psutil

def get_memory_usage_mb() -> float:
    """Returns the current memory usage of the process in MB."""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / (1024 * 1024)


================================================
FILE: modern_diarization_requirements.txt
================================================
# Modern Streaming Diarization Requirements
# Based on Landini et al. (2022, 2023), Park et al. (2022), Cornell et al. (2022)

# Core dependencies
numpy>=1.19.0
scipy>=1.7.0
scikit-learn>=1.0.0

# Graph processing
networkx>=2.6.0
scipy.sparse>=1.7.0

# Performance optimization
numba>=0.56.0
psutil>=5.8.0

# Audio processing (for quality assessment)
librosa>=0.8.0
soundfile>=0.10.0

# Threading and concurrency
threading
concurrent.futures

# Optional: Additional ML libraries for enhanced features
# torch>=1.9.0
# transformers>=4.0.0

# Development and testing
pytest>=6.0.0
pytest-cov>=2.12.0

# Quality assessment specific
scipy.fft>=1.7.0


================================================
FILE: modern_stateful_speaker_identifier.py
================================================
"""
Modern Stateful Speaker Identifier
Drop-in replacement for StatefulSpeakerIdentifier with modern graph-based clustering
Integrates all modern components: graph clustering, adaptive thresholding, quality assessment
"""

import numpy as np
from typing import Tuple, Optional, Dict, List
import time
import logging
from dataclasses import dataclass
import threading

# Import modern components
from .graph_based_clustering_engine import GraphBasedClusteringEngine
from .adaptive_thresholding_manager import AdaptiveThresholdingManager
from .embedding_quality_assessor import EmbeddingQualityAssessor
from .memory_efficient_speaker_manager import MemoryEfficientSpeakerManager
from .temporal_context_tracker import TemporalContextTracker
from .fast_graph_optimizer import FastGraphOptimizer

logger = logging.getLogger(__name__)


@dataclass
class SpeakerIdentificationResult:
    """Result of speaker identification"""
    speaker_id: str
    confidence: float
    quality_score: float
    processing_time_ms: float
    method: str  # "graph", "fallback", "new"
    adaptive_threshold: float


class ModernStatefulSpeakerIdentifier:
    """
    Drop-in replacement for StatefulSpeakerIdentifier using modern research-based approaches
    
    This class integrates:
    - Graph-based clustering (Landini et al. 2022)
    - Adaptive thresholding (Park et al. 2022)
    - Quality-aware embedding aggregation (Bredin & Laurent 2021)
    - Memory-efficient speaker management (Cornell et al. 2022)
    - Temporal context integration
    - Fast graph optimization (Landini et al. 2023)
    
    Maintains backward compatibility with existing API while providing significant
    improvements in accuracy, efficiency, and scalability.
    """
    
    def __init__(self, base_threshold: float = 0.7, max_speakers: int = 50,
                 use_graph_clustering: bool = True, use_adaptive_thresholds: bool = True,
                 use_quality_weighting: bool = True, use_temporal_context: bool = True):
        """
        Initialize modern speaker identifier
        
        Args:
            base_threshold: Base similarity threshold
            max_speakers: Maximum speakers to track
            use_graph_clustering: Enable graph-based clustering
            use_adaptive_thresholds: Enable adaptive thresholding
            use_quality_weighting: Enable quality-aware embedding updates
            use_temporal_context: Enable temporal context smoothing
        """
        self.base_threshold = base_threshold
        self.max_speakers = max_speakers
        
        # Configuration flags
        self.config = {
            'graph_clustering': use_graph_clustering,
            'adaptive_thresholds': use_adaptive_thresholds,
            'quality_weighting': use_quality_weighting,
            'temporal_context': use_temporal_context
        }
        
        # Initialize modern components
        self.clustering_engine = GraphBasedClusteringEngine(
            max_speakers=max_speakers,
            similarity_threshold=base_threshold
        )
        
        self.adaptive_manager = AdaptiveThresholdingManager(
            base_threshold=base_threshold,
            adaptation_rate=0.1
        )
        
        self.quality_assessor = EmbeddingQualityAssessor()
        
        self.memory_manager = MemoryEfficientSpeakerManager(
            max_speakers=max_speakers,
            memory_threshold_mb=100
        )
        
        self.temporal_tracker = TemporalContextTracker(
            smoothing_window=5,
            max_context_seconds=30.0
        )
        
        self.graph_optimizer = FastGraphOptimizer(
            max_speakers=max_speakers,
            latency_constraint_ms=100.0
        )
        
        # Speaker storage for backward compatibility
        self.speaker_profiles: Dict[str, Dict] = {}
        self.speaker_lock = threading.RLock()
        
        # Performance tracking
        self.identification_count = 0
        self.fallback_count = 0
        
        logger.info("Initialized ModernStatefulSpeakerIdentifier")
    
    def identify_speaker(self, audio_chunk_np: np.ndarray, 
                         embedding: np.ndarray) -> Tuple[str, float]:
        """
        Identify speaker using modern approaches
        
        Args:
            audio_chunk_np: Audio chunk (for backward compatibility)
            embedding: Speaker embedding vector
            
        Returns:
            Tuple of (speaker_id, confidence)
        """
        start_time = time.time()
        
        try:
            # Step 1: Assess embedding quality
            quality_score = self.quality_assessor.assess_quality(audio_chunk_np, embedding)
            
            # Step 2: Check if quality is sufficient
            if quality_score < 0.3:
                logger.debug("Low quality embedding, using fallback")
                return self._fallback_identification(embedding, quality_score)
            
            # Step 3: Apply modern identification
            result = self._modern_identification(embedding, quality_score, audio_chunk_np)
            
            # Step 4: Update components
            self._update_components(result.speaker_id, embedding, quality_score, result.confidence)
            
            # Step 5: Return backward-compatible format
            processing_time = (time.time() - start_time) * 1000
            logger.debug(f"Identified speaker {result.speaker_id} "
                        f"with confidence {result.confidence:.3f} "
                        f"({processing_time:.1f}ms)")
            
            return result.speaker_id, result.confidence
            
        except Exception as e:
            logger.error(f"Error in modern identification: {e}")
            return self._fallback_identification(embedding, 0.5)
    
    def _modern_identification(self, embedding: np.ndarray,
                             quality_score: float, audio_chunk_np: np.ndarray) -> SpeakerIdentificationResult:
        """Perform modern speaker identification using all components"""
        start_time = time.time()
        
        # Use graph clustering if enabled
        if self.config['graph_clustering']:
            speaker_id, confidence = self._graph_based_identification(embedding, quality_score)
            method = "graph"
        else:
            speaker_id, confidence = self._memory_based_identification(embedding, quality_score)
            method = "memory"
        
        # Apply adaptive thresholding
        if self.config['adaptive_thresholds']:
            threshold = self.adaptive_manager.get_threshold(speaker_id)
            if confidence < threshold:
                # Try to find better match
                better_speaker, better_confidence = self._find_better_match(
                    embedding, quality_score, threshold
                )
                if better_confidence > confidence:
                    speaker_id, confidence = better_speaker, better_confidence
        
        # Apply conversation flow analysis and temporal context
        if self.config['temporal_context']:
            # Get transition probabilities for conversation flow analysis
            transition_probs = self.temporal_tracker.calculate_transition_probabilities()
            
            # Get current context summary
            context_summary = self.temporal_tracker.get_context_summary()
            
            # Apply conversation flow analysis
            speaker_id, confidence = self._apply_conversation_flow_analysis(
                speaker_id, confidence, transition_probs, context_summary
            )
            
            # Apply temporal smoothing
            speaker_id, confidence = self.temporal_tracker.apply_temporal_smoothing(
                speaker_id, confidence
            )
        
        processing_time = (time.time() - start_time) * 1000
        
        return SpeakerIdentificationResult(
            speaker_id=speaker_id,
            confidence=confidence,
            quality_score=quality_score,
            processing_time_ms=processing_time,
            method=method,
            adaptive_threshold=self.adaptive_manager.get_threshold(speaker_id)
        )
    
    def _apply_conversation_flow_analysis(self, speaker_id: str, confidence: float,
                                        transition_probs: dict, context_summary: dict) -> Tuple[str, float]:
        """Apply conversation flow analysis and re-evaluation triggers"""
        # Check for ambiguous assignments
        if confidence < 0.6 or context_summary.get('inconsistencies', 0) > 0:
            # Use transition probabilities to resolve ambiguity
            dominant_speaker = self._find_best_speaker_from_context(
                speaker_id, confidence, transition_probs, context_summary
            )
            if dominant_speaker:
                speaker_id = dominant_speaker[0]
                confidence = dominant_speaker[1]
        
        # Check temporal constraints
        is_valid, reason = self.temporal_tracker.check_temporal_constraints(
            speaker_id, time.time()
        )
        if not is_valid:
            # Re-evaluate based on temporal constraints
            speaker_id, confidence = self._resolve_temporal_violation(
                speaker_id, confidence, context_summary
            )
        
        return speaker_id, confidence
    
    def _find_best_speaker_from_context(self, current_speaker: str, current_confidence: float,
                                     transition_probs: dict, context_summary: dict) -> Optional[Tuple[str, float]]:
        """Find best speaker based on conversation flow and context"""
        if not transition_probs:
            return None
        
        # Get speaker dominance scores
        dominance = self.temporal_tracker.get_speaker_dominance()
        if not dominance:
            return None
        
        # Find most probable speaker based on transition probabilities and dominance
        best_speaker = None
        best_score = 0.0
        
        for speaker, dom_score in dominance.items():
            # Calculate combined score
            transition_score = transition_probs.get((current_speaker, speaker), 0.0)
            combined_score = 0.7 * dom_score + 0.3 * transition_score
            
            if combined_score > best_score:
                best_score = combined_score
                best_speaker = speaker
        
        if best_speaker and best_score > 0.5:
            return (best_speaker, min(0.9, current_confidence + 0.1))
        
        return None
    
    def _resolve_temporal_violation(self, speaker_id: str, confidence: float,
                                    context_summary: dict) -> Tuple[str, float]:
        """Resolve temporal violations by finding consistent speaker"""
        # Use speaker dominance to find consistent assignment
        dominance = self.temporal_tracker.get_speaker_dominance()
        if dominance:
            # Find most dominant speaker
            dominant_speaker = max(dominance.keys(), key=lambda x: dominance[x])
            if dominance[dominant_speaker] > 0.3:  # Threshold for dominance
                return dominant_speaker, max(0.5, confidence * 0.8)
        
        # Fallback: keep current speaker with reduced confidence
        return speaker_id, max(0.3, confidence * 0.7)
    
    def _graph_based_identification(self, embedding: np.ndarray,
                                  quality_score: float) -> Tuple[str, float]:
        """Use graph-based clustering for identification"""
        # Find closest speaker using graph
        closest_speaker, similarity = self.clustering_engine.find_closest_speaker(
            embedding, quality_score
        )
        
        if closest_speaker is None:
            # New speaker
            speaker_id = f"speaker_{len(self.speaker_profiles) + 1}"
            confidence = similarity
        else:
            speaker_id = closest_speaker
            confidence = similarity
        
        return speaker_id, confidence
    
    def _memory_based_identification(self, embedding: np.ndarray,
                                   quality_score: float) -> Tuple[str, float]:
        """Use memory-efficient speaker matching"""
        closest_speaker, similarity = self.memory_manager.find_closest_speaker(
            embedding, quality_score
        )
        
        if closest_speaker is None:
            # New speaker
            speaker_id = f"speaker_{len(self.speaker_profiles) + 1}"
            confidence = similarity
        else:
            speaker_id = closest_speaker
            confidence = similarity
        
        return speaker_id, confidence
    
    def _find_better_match(self, embedding: np.ndarray, quality_score: float,
                          threshold: float) -> Tuple[str, float]:
        """Find better speaker match when confidence is low"""
        # Try multiple approaches
        candidates = []
        
        # Graph-based approach
        graph_speaker, graph_conf = self.clustering_engine.find_closest_speaker(
            embedding, quality_score
        )
        if graph_conf >= threshold:
            candidates.append((graph_speaker, graph_conf, "graph"))
        
        # Memory-based approach
        mem_speaker, mem_conf = self.memory_manager.find_closest_speaker(
            embedding, quality_score
        )
        if mem_conf >= threshold:
            candidates.append((mem_speaker, mem_conf, "memory"))
        
        # Return best candidate
        if candidates:
            best = max(candidates, key=lambda x: x[1])
            return best[0], best[1]
        
        # Create new speaker
        speaker_id = f"speaker_{len(self.speaker_profiles) + 1}"
        return speaker_id, 0.5  # Default confidence for new speaker
    
    def _fallback_identification(self, embedding: np.ndarray,
                               quality_score: float) -> Tuple[str, float]:
        """Fallback to simple centroid-based identification"""
        self.fallback_count += 1
        
        # Simple distance-based matching
        best_speaker = None
        best_distance = float('inf')
        
        with self.speaker_lock:
            for speaker_id, profile in self.speaker_profiles.items():
                if 'centroid' in profile:
                    centroid = profile['centroid']
                    distance = np.linalg.norm(embedding - centroid)
                    if distance < best_distance:
                        best_distance = distance
                        best_speaker = speaker_id
        
        if best_speaker is None or best_distance > self.base_threshold:
            # New speaker
            speaker_id = f"speaker_{len(self.speaker_profiles) + 1}"
            confidence = max(0.0, 1.0 - best_distance)
        else:
            speaker_id = best_speaker
            confidence = max(0.0, 1.0 - best_distance)
        
        return speaker_id, confidence
    
    def _update_components(self, speaker_id: str, embedding: np.ndarray,
                        quality_score: float, confidence: float):
        """Update all modern components with new data"""
        
        # Update clustering engine
        if self.config['graph_clustering']:
            self.clustering_engine.add_or_update_speaker(
                speaker_id, embedding, quality_score
            )
        
        # Update adaptive thresholding
        if self.config['adaptive_thresholds']:
            similarity = self._calculate_similarity(embedding, speaker_id)
            self.adaptive_manager.update_threshold(
                speaker_id, similarity, quality_score, was_accepted=True
            )
        
        # Update memory manager
        self.memory_manager.add_or_update_speaker(
            speaker_id, embedding, quality_score
        )
        
        # Update temporal tracker
        self.temporal_tracker.add_context(
            timestamp=time.time(),
            speaker_id=speaker_id,
            confidence=confidence,
            embedding=embedding,
            quality_score=quality_score,
            segment_duration=1.0
        )
    
    def _calculate_similarity(self, embedding: np.ndarray,
                            speaker_id: str) -> float:
        """Calculate similarity with speaker centroid"""
        with self.speaker_lock:
            if speaker_id in self.speaker_profiles and 'centroid' in self.speaker_profiles[speaker_id]:
                centroid = self.speaker_profiles[speaker_id]['centroid']
                return np.dot(embedding, centroid) / (
                    np.linalg.norm(embedding) * np.linalg.norm(centroid)
                )
            return 0.0
    
    # Backward compatibility methods
    def get_speaker_count(self) -> int:
        """Get number of tracked speakers"""
        return len(self.speaker_profiles)
    
    def get_speakers(self) -> List[str]:
        """Get list of speaker IDs"""
        return list(self.speaker_profiles.keys())
    
    def reset(self):
        """Reset all components (for testing)"""
        with self.speaker_lock:
            self.speaker_profiles.clear()
            self.clustering_engine = GraphBasedClusteringEngine(
                max_speakers=self.max_speakers,
                similarity_threshold=self.base_threshold
            )
            self.adaptive_manager = AdaptiveThresholdingManager(
                base_threshold=self.base_threshold
            )
            self.memory_manager.clear_all_speakers()
            self.temporal_tracker.reset_context()
            self.identification_count = 0
            self.fallback_count = 0
    
    def get_performance_stats(self) -> Dict[str, any]:
        """Get performance statistics"""
        stats = {
            'total_identifications': self.identification_count,
            'fallback_count': self.fallback_count,
            'fallback_rate': self.fallback_count / max(1, self.identification_count),
            'config': self.config,
            'temporal_context': self.temporal_tracker.get_context_summary() if self.config['temporal_context'] else None
        }
        
        if self.config['graph_clustering']:
            stats['clustering_stats'] = self.clustering_engine.get_graph_stats()
        
        if hasattr(self.memory_manager, 'get_memory_stats'):
            stats['memory_stats'] = self.memory_manager.get_memory_stats()
        
        if hasattr(self.adaptive_manager, 'speaker_thresholds'):
            stats['adaptive_thresholds'] = len(self.adaptive_manager.speaker_thresholds)
        
        return stats
    
    def enable_feature(self, feature: str, enabled: bool = True):
        """Enable/disable specific features"""
        if feature in self.config:
            self.config[feature] = enabled
            logger.info(f"Feature {feature} {'enabled' if enabled else 'disabled'}")
        else:
            logger.warning(f"Unknown feature: {feature}")
    
    def get_speaker_profile(self, speaker_id: str) -> Optional[Dict]:
        """Get speaker profile (backward compatibility)"""
        return self.speaker_profiles.get(speaker_id)


================================================
FILE: procedural_audio_service.py
================================================
import os
import torch
import numpy as np
from scipy.io.wavfile import write as write_wav
from pathlib import Path
import logging

# --- START OF ROBUST MONKEY-PATCH ---
original_torch_load = torch.load
def patched_torch_load(f, map_location, **kwargs):
    if isinstance(f, str) and f.endswith(".pt"):
        logging.info(f"Applying robust patch: Intercepted torch.load for '{os.path.basename(f)}'. Setting weights_only=False.")
        return original_torch_load(f, map_location=map_location, weights_only=False)
    return original_torch_load(f, map_location=map_location, **kwargs)
torch.load = patched_torch_load
logging.info("Robust monkey-patch for torch.load is active.")
# --- END OF ROBUST MONKEY-PATCH ---

from bark import SAMPLE_RATE, generate_audio, save_as_prompt

# --- Service Configuration ---
logger = logging.getLogger(__name__)
RESULTS_DIR = Path(__file__).resolve().parent.parent.parent / "results" / "procedural_audio"
MODELS_CACHE_DIR = "./bark_models_cache"

class ProceduralSoundService:
    def __init__(self):
        self.sample_rate = SAMPLE_RATE
        os.makedirs(RESULTS_DIR, exist_ok=True)
        os.environ["XDG_CACHE_HOME"] = MODELS_CACHE_DIR
        self.load_model()

    def load_model(self):
        """Pre-loads the Bark model."""
        logger.info("Initializing Bark model. This may trigger a download on first run.")
        _ = generate_audio("[silence]", silent=True)
        logger.info("Bark models are loaded and ready.")

    def generate_scene(self, prompt: str, duration_seconds: int = 30) -> str:
        """Generates a long-form audio scene and saves it to a file."""
        logger.info(f"Starting procedural generation for prompt: '{prompt}'")
        
        num_continuations = max(0, (duration_seconds // 12) - 1)
        
        # ====================================================================
        # === START OF DEFINITIVE FIX ===
        # ====================================================================
        
        # MODIFIED: Unpack the tuple returned by generate_audio with output_full=True.
        # It returns (full_generation_dict, audio_array).
        logger.info("Generating initial audio chunk and history prompt...")
        full_generation_dict, audio_array = generate_audio(prompt, silent=True, output_full=True)
        
        # MODIFIED: Initialize the main audio track with the unpacked audio array.
        total_scene_audio = audio_array
        
        # ====================================================================
        # ===  END OF DEFINITIVE FIX  ===
        # ====================================================================

        logger.info("Generated initial audio chunk.")

        temp_prompt_filename = "temp_history_prompt.npz"

        for i in range(num_continuations):
            logger.info(f"Continuation loop {i+1}/{num_continuations}...")
            
            # ====================================================================
            # === START OF DEFINITIVE FIX ===
            # ====================================================================

            # MODIFIED: Pass the unpacked dictionary to save_as_prompt.
            save_as_prompt(temp_prompt_filename, full_generation_dict)
            
            # MODIFIED: Unpack the tuple again for the continued generation.
            # We update the dictionary for the next loop and get the new audio chunk.
            full_generation_dict, new_audio_chunk = generate_audio(
                "[silence]", 
                history_prompt=temp_prompt_filename, 
                silent=True, 
                output_full=True
            )
            
            # ====================================================================
            # ===  END OF DEFINITIVE FIX  ===
            # ====================================================================

            # Append the new audio array to our main track
            total_scene_audio = np.concatenate([total_scene_audio, new_audio_chunk])

        sanitized_prompt = "".join(filter(str.isalnum, prompt))[:30]
        final_filename = f"scene_{sanitized_prompt}_{duration_seconds}s.wav"
        output_path = RESULTS_DIR / final_filename

        logger.info(f"Saving final soundscape to '{output_path}'")
        write_wav(output_path, self.sample_rate, total_scene_audio)
        
        if os.path.exists(temp_prompt_filename):
            os.remove(temp_prompt_filename)
        
        return str(output_path)

# Singleton instance to ensure the model is loaded only once
_procedural_sound_service_instance = None

def get_procedural_sound_service() -> ProceduralSoundService:
    global _procedural_sound_service_instance
    if _procedural_sound_service_instance is None:
        _procedural_sound_service_instance = ProceduralSoundService()
    return _procedural_sound_service_instance



================================================
FILE: prosody_analysis_service.py
================================================
# File: services/prosody_analysis_service.py

import torch
from src.models.prosody.prosody_encoder import ProsodyEncoder

# --- FIX: Singleton Pattern ---
# Detect device once at the module level
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Create a single, global instance of the encoder when the module is first imported.
# This ensures the large WavLM model is loaded into memory only ONCE.
print(f"Initializing singleton ProsodyEncoder on device: {DEVICE}")
prosody_encoder_instance = ProsodyEncoder(device=DEVICE)
# --- END FIX ---

def analyze(path: str) -> dict:
    """
    Analyzes an audio file using the GLOBAL prosody_encoder_instance.
    This function no longer creates a new encoder on every call.
    """
    # Use the pre-loaded singleton instance
    return prosody_encoder_instance.extract_features(path)



================================================
FILE: quality_weighted_centroid.py
================================================
#!/usr/bin/env python3
"""
Quality-Weighted Centroid Update Module for Modern Streaming Speaker Diarization

This module implements quality-aware centroid updates following research from:
- Bredin & Laurent (2021) "Robust Speaker Embeddings for Streaming Diarization" (ICASSP 2021)

The implementation uses quality-weighted aggregation instead of naive averaging:
new_centroid = (old_centroid * old_weight + current_embedding * quality_score) / (old_weight + quality_score)
"""

import numpy as np
import logging
from typing import Dict, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class QualityWeightedProfile:
    """
    Enhanced speaker profile with quality-weighted centroid management.
    
    Following Bredin & Laurent (2021) approach for robust speaker embeddings
    with quality-aware aggregation and confidence tracking.
    """
    speaker_id: str
    centroid: np.ndarray
    quality_weighted_count: float  # Sum of quality weights, not just sample count
    total_samples: int             # Actual number of samples for statistics
    confidence_score: float        # Overall profile confidence
    last_seen: datetime
    creation_time: datetime
    
    # Quality statistics for monitoring
    avg_quality: float = 0.0
    min_quality: float = 1.0
    max_quality: float = 0.0

class QualityWeightedCentroidManager:
    """
    Manages speaker centroids with quality-weighted updates following
    Bredin & Laurent (2021) robust embedding aggregation methodology.
    """
    
    def __init__(self, min_quality_threshold: float = 0.3, confidence_decay: float = 0.95):
        """
        Initialize quality-weighted centroid manager.
        
        Args:
            min_quality_threshold: Minimum quality for centroid updates (Bredin & Laurent 2021)
            confidence_decay: Decay factor for confidence over time
        """
        self.min_quality_threshold = min_quality_threshold
        self.confidence_decay = confidence_decay
        self.speaker_profiles: Dict[str, QualityWeightedProfile] = {}
        
        logger.info(f"QualityWeightedCentroidManager initialized with quality threshold: {min_quality_threshold}")
    
    def should_update_centroid(self, quality_score: float) -> bool:
        """
        Determine if embedding quality is sufficient for centroid updates.
        
        Following Bredin & Laurent (2021) quality-aware aggregation approach.
        
        Args:
            quality_score: Quality score from EmbeddingQualityAssessor
            
        Returns:
            True if quality meets threshold for centroid update
        """
        return quality_score >= self.min_quality_threshold
    
    def create_speaker_profile(self, speaker_id: str, embedding: np.ndarray, quality_score: float) -> QualityWeightedProfile:
        """
        Create new speaker profile with initial embedding and quality.
        
        Args:
            speaker_id: Unique speaker identifier
            embedding: Initial speaker embedding
            quality_score: Quality score for initial embedding
            
        Returns:
            New QualityWeightedProfile instance
        """
        current_time = datetime.now()
        
        profile = QualityWeightedProfile(
            speaker_id=speaker_id,
            centroid=embedding.copy(),
            quality_weighted_count=quality_score,
            total_samples=1,
            confidence_score=quality_score,
            last_seen=current_time,
            creation_time=current_time,
            avg_quality=quality_score,
            min_quality=quality_score,
            max_quality=quality_score
        )
        
        self.speaker_profiles[speaker_id] = profile
        logger.info(f"Created speaker profile: {speaker_id} with quality: {quality_score:.3f}")
        
        return profile
    
    def update_centroid(self, speaker_id: str, new_embedding: np.ndarray, quality_score: float) -> bool:
        """
        Update speaker centroid using quality-weighted aggregation.
        
        Implements Bredin & Laurent (2021) formula:
        new_centroid = (old_centroid * old_weight + current_embedding * quality_score) / (old_weight + quality_score)
        
        Args:
            speaker_id: Speaker to update
            new_embedding: New embedding to incorporate
            quality_score: Quality score for new embedding
            
        Returns:
            True if update was performed, False if rejected due to quality
        """
        if not self.should_update_centroid(quality_score):
            logger.debug(f"Rejecting centroid update for {speaker_id}: quality {quality_score:.3f} below threshold {self.min_quality_threshold}")
            return False
        
        if speaker_id not in self.speaker_profiles:
            self.create_speaker_profile(speaker_id, new_embedding, quality_score)
            return True
        
        profile = self.speaker_profiles[speaker_id]
        
        # Quality-weighted centroid update following Bredin & Laurent (2021)
        old_centroid = profile.centroid
        old_weight = profile.quality_weighted_count
        
        # Calculate new centroid with quality weighting
        new_centroid = (old_centroid * old_weight + new_embedding * quality_score) / (old_weight + quality_score)
        
        # Update profile with new centroid and statistics
        profile.centroid = new_centroid
        profile.quality_weighted_count += quality_score
        profile.total_samples += 1
        profile.last_seen = datetime.now()
        
        # Update quality statistics
        profile.avg_quality = ((profile.avg_quality * (profile.total_samples - 1)) + quality_score) / profile.total_samples
        profile.min_quality = min(profile.min_quality, quality_score)
        profile.max_quality = max(profile.max_quality, quality_score)
        
        # Update confidence score with quality weighting
        # Higher quality samples increase confidence more
        confidence_boost = quality_score * 0.1  # Moderate boost for high quality
        profile.confidence_score = min(1.0, profile.confidence_score + confidence_boost)
        
        logger.debug(f"Updated centroid for {speaker_id}: quality={quality_score:.3f}, "
                    f"weighted_count={profile.quality_weighted_count:.2f}, confidence={profile.confidence_score:.3f}")
        
        return True
    
    def get_centroid(self, speaker_id: str) -> Optional[np.ndarray]:
        """
        Get current centroid for speaker.
        
        Args:
            speaker_id: Speaker identifier
            
        Returns:
            Current centroid or None if speaker not found
        """
        if speaker_id in self.speaker_profiles:
            return self.speaker_profiles[speaker_id].centroid.copy()
        return None
    
    def get_profile_confidence(self, speaker_id: str) -> float:
        """
        Get confidence score for speaker profile.
        
        Args:
            speaker_id: Speaker identifier
            
        Returns:
            Confidence score [0, 1] or 0.0 if speaker not found
        """
        if speaker_id in self.speaker_profiles:
            return self.speaker_profiles[speaker_id].confidence_score
        return 0.0
    
    def get_profile_statistics(self, speaker_id: str) -> Optional[Dict]:
        """
        Get detailed statistics for speaker profile.
        
        Args:
            speaker_id: Speaker identifier
            
        Returns:
            Dictionary with profile statistics or None if not found
        """
        if speaker_id not in self.speaker_profiles:
            return None
        
        profile = self.speaker_profiles[speaker_id]
        
        return {
            "speaker_id": profile.speaker_id,
            "total_samples": profile.total_samples,
            "quality_weighted_count": profile.quality_weighted_count,
            "confidence_score": profile.confidence_score,
            "avg_quality": profile.avg_quality,
            "min_quality": profile.min_quality,
            "max_quality": profile.max_quality,
            "last_seen": profile.last_seen.isoformat(),
            "creation_time": profile.creation_time.isoformat(),
            "age_seconds": (datetime.now() - profile.creation_time).total_seconds()
        }
    
    def apply_temporal_decay(self, decay_factor: float = None) -> None:
        """
        Apply temporal decay to confidence scores for aging profiles.
        
        Following Bredin & Laurent (2021) approach for handling temporal drift.
        
        Args:
            decay_factor: Optional decay factor, uses default if None
        """
        if decay_factor is None:
            decay_factor = self.confidence_decay
        
        current_time = datetime.now()
        
        for speaker_id, profile in self.speaker_profiles.items():
            # Calculate time since last update
            time_since_update = (current_time - profile.last_seen).total_seconds()
            
            # Apply decay based on time elapsed (more decay for older profiles)
            if time_since_update > 60:  # Start decay after 1 minute of inactivity
                time_decay = np.exp(-time_since_update / 300)  # 5-minute half-life
                profile.confidence_score *= (decay_factor * time_decay)
                
                logger.debug(f"Applied temporal decay to {speaker_id}: "
                           f"confidence={profile.confidence_score:.3f}, inactive_time={time_since_update:.1f}s")
    
    def prune_low_confidence_profiles(self, min_confidence: float = 0.1) -> int:
        """
        Remove speaker profiles with very low confidence scores.
        
        Args:
            min_confidence: Minimum confidence to retain profile
            
        Returns:
            Number of profiles removed
        """
        to_remove = []
        
        for speaker_id, profile in self.speaker_profiles.items():
            if profile.confidence_score < min_confidence:
                to_remove.append(speaker_id)
        
        for speaker_id in to_remove:
            del self.speaker_profiles[speaker_id]
            logger.info(f"Pruned low-confidence profile: {speaker_id}")
        
        return len(to_remove)
    
    def get_all_centroids(self) -> Dict[str, np.ndarray]:
        """
        Get all current speaker centroids.
        
        Returns:
            Dictionary mapping speaker_id to centroid
        """
        return {speaker_id: profile.centroid.copy() 
                for speaker_id, profile in self.speaker_profiles.items()}
    
    def get_speaker_count(self) -> int:
        """
        Get current number of active speaker profiles.
        
        Returns:
            Number of speaker profiles
        """
        return len(self.speaker_profiles)
    
    def clear_all_profiles(self) -> None:
        """
        Clear all speaker profiles (for testing or reset).
        """
        count = len(self.speaker_profiles)
        self.speaker_profiles.clear()
        logger.info(f"Cleared {count} speaker profiles")


================================================
FILE: quality_weighted_centroid_manager.py
================================================
#!/usr/bin/env python3
"""
Quality-Weighted Centroid Management for Modern Streaming Speaker Diarization

This module implements quality-aware centroid updates following research from:
- Bredin & Laurent (2021) "Robust Speaker Embeddings for Streaming Diarization" (ICASSP 2021)

Key improvements over naive averaging:
1. Quality-weighted embedding aggregation
2. Selective centroid updates based on quality thresholds
3. Running quality statistics per speaker
4. Centroid drift prevention through quality filtering
"""

import logging
import numpy as np
from typing import Dict, Tuple, Optional
from dataclasses import dataclass, field
from datetime import datetime
import time

logger = logging.getLogger(__name__)

@dataclass
class QualityWeightedSpeakerProfile:
    """
    Enhanced speaker profile with quality-aware centroid management.
    
    Following Bredin & Laurent (2021) approach for robust speaker embeddings
    with quality-weighted aggregation and drift prevention.
    """
    speaker_id: str
    centroid: np.ndarray
    quality_weighted_count: float  # Sum of quality scores instead of simple count
    total_embeddings: int          # Total number of embeddings processed
    average_quality: float         # Running average of embedding qualities
    last_updated: datetime
    creation_time: datetime = field(default_factory=datetime.now)
    
    # Quality statistics for monitoring
    quality_history: list = field(default_factory=list)
    max_history_length: int = 50  # Keep last 50 quality scores
    
    def update_quality_stats(self, quality_score: float):
        """Update running quality statistics for this speaker."""
        self.quality_history.append(quality_score)
        if len(self.quality_history) > self.max_history_length:
            self.quality_history.pop(0)
        
        # Update running average quality
        self.average_quality = np.mean(self.quality_history)

class QualityWeightedCentroidManager:
    """
    Manages speaker centroids with quality-aware updates following Bredin & Laurent (2021).
    
    Key features:
    - Quality-weighted centroid aggregation: new_centroid = (old_centroid * old_weight + current_embedding * quality_score) / (old_weight + quality_score)
    - Selective updates based on quality thresholds
    - Centroid drift prevention through quality filtering
    - Running quality statistics per speaker
    """
    
    def __init__(self, 
                 min_quality_threshold: float = 0.3,
                 quality_decay_factor: float = 0.95,
                 max_quality_weight: float = 10.0):
        """
        Initialize quality-weighted centroid manager.
        
        Args:
            min_quality_threshold: Minimum quality score for centroid updates
            quality_decay_factor: Decay factor for older quality weights (0.95 = 5% decay)
            max_quality_weight: Maximum weight for any single embedding to prevent dominance
        """
        self.speaker_profiles: Dict[str, QualityWeightedSpeakerProfile] = {}
        self.min_quality_threshold = min_quality_threshold
        self.quality_decay_factor = quality_decay_factor
        self.max_quality_weight = max_quality_weight
        
        # Statistics tracking
        self.total_updates_attempted = 0
        self.total_updates_accepted = 0
        self.total_updates_rejected = 0
        
        logger.info(f"QualityWeightedCentroidManager initialized with threshold={min_quality_threshold}, "
                   f"decay_factor={quality_decay_factor}, max_weight={max_quality_weight}")

    def create_speaker_profile(self, speaker_id: str, embedding: np.ndarray, quality_score: float) -> QualityWeightedSpeakerProfile:
        """
        Create new speaker profile with initial embedding and quality score.
        
        Args:
            speaker_id: Unique identifier for the speaker
            embedding: Initial speaker embedding vector
            quality_score: Quality score for the initial embedding
            
        Returns:
            Newly created speaker profile
        """
        # Ensure embedding is properly normalized
        if np.linalg.norm(embedding) > 0:
            embedding = embedding / np.linalg.norm(embedding)
        
        profile = QualityWeightedSpeakerProfile(
            speaker_id=speaker_id,
            centroid=embedding.copy(),
            quality_weighted_count=quality_score,
            total_embeddings=1,
            average_quality=quality_score,
            last_updated=datetime.now()
        )
        
        profile.update_quality_stats(quality_score)
        self.speaker_profiles[speaker_id] = profile
        
        logger.info(f"Created speaker profile for {speaker_id} with initial quality {quality_score:.3f}")
        return profile

    def should_update_centroid(self, quality_score: float, speaker_profile: Optional[QualityWeightedSpeakerProfile] = None) -> Tuple[bool, str]:
        """
        Determine if centroid should be updated based on quality score and speaker history.
        
        Following Bredin & Laurent (2021) methodology for quality-based update decisions.
        
        Args:
            quality_score: Quality score of the new embedding
            speaker_profile: Existing speaker profile (if any)
            
        Returns:
            Tuple of (should_update: bool, reason: str)
        """
        self.total_updates_attempted += 1
        
        # Basic quality threshold check
        if quality_score < self.min_quality_threshold:
            self.total_updates_rejected += 1
            return False, f"Quality {quality_score:.3f} below threshold {self.min_quality_threshold}"
        
        # Additional checks for existing speakers
        if speaker_profile is not None:
            # Prevent updates that would significantly degrade average quality
            quality_degradation_threshold = 0.2  # Don't allow >20% quality drop
            if (speaker_profile.average_quality - quality_score) > quality_degradation_threshold:
                self.total_updates_rejected += 1
                return False, f"Quality {quality_score:.3f} would degrade average {speaker_profile.average_quality:.3f}"
            
            # Check for very low quality compared to speaker's history
            if len(speaker_profile.quality_history) > 5:
                min_historical_quality = np.min(speaker_profile.quality_history[-10:])  # Last 10 samples
                if quality_score < min_historical_quality * 0.7:  # 30% below historical minimum
                    self.total_updates_rejected += 1
                    return False, f"Quality {quality_score:.3f} significantly below historical minimum {min_historical_quality:.3f}"
        
        self.total_updates_accepted += 1
        return True, "Quality acceptable for centroid update"

    def update_centroid(self, speaker_id: str, embedding: np.ndarray, quality_score: float) -> Tuple[bool, str]:
        """
        Update speaker centroid using quality-weighted aggregation.
        
        Implements Bredin & Laurent (2021) quality-weighted centroid update:
        new_centroid = (old_centroid * old_weight + current_embedding * quality_score) / (old_weight + quality_score)
        
        Args:
            speaker_id: Speaker identifier
            embedding: New embedding vector
            quality_score: Quality score for the new embedding
            
        Returns:
            Tuple of (success: bool, message: str)
        """
        # Normalize embedding
        if np.linalg.norm(embedding) > 0:
            embedding = embedding / np.linalg.norm(embedding)
        else:
            return False, "Zero-norm embedding c    t be used for centroid update"
        
        # Get or create speaker profile
        if speaker_id not in self.speaker_profiles:
            self.create_speaker_profile(speaker_id, embedding, quality_score)
            return True, f"Created new speaker profile for {speaker_id}"
        
        profile = self.speaker_profiles[speaker_id]
        
        # Check if update should proceed
        should_update, reason = self.should_update_centroid(quality_score, profile)
        if not should_update:
            logger.debug(f"Rejecting centroid update for {speaker_id}: {reason}")
            return False, reason
        
        # Apply quality decay to existing weight (Bredin & Laurent 2021 temporal weighting)
        decayed_old_weight = profile.quality_weighted_count * self.quality_decay_factor
        
        # Limit maximum weight to prevent single embedding dominance
        effective_quality_score = min(quality_score, self.max_quality_weight)
        
        # Quality-weighted centroid update following Bredin & Laurent (2021)
        old_centroid = profile.centroid
        new_weight = decayed_old_weight + effective_quality_score
        
        updated_centroid = (old_centroid * decayed_old_weight + embedding * effective_quality_score) / new_weight
        
        # Normalize the updated centroid
        if np.linalg.norm(updated_centroid) > 0:
            updated_centroid = updated_centroid / np.linalg.norm(updated_centroid)
        
        # Update profile
        profile.centroid = updated_centroid
        profile.quality_weighted_count = new_weight
        profile.total_embeddings += 1
        profile.last_updated = datetime.now()
        profile.update_quality_stats(quality_score)
        
        logger.debug(f"Updated centroid for {speaker_id}: quality={quality_score:.3f}, "
                    f"new_weight={new_weight:.2f}, avg_quality={profile.average_quality:.3f}")
        
        return True, f"Successfully updated centroid for {speaker_id}"

    def get_centroid(self, speaker_id: str) -> Optional[np.ndarray]:
        """
        Get current centroid for a speaker.
        
        Args:
            speaker_id: Speaker identifier
            
        Returns:
            Centroid vector or None if speaker doesn't exist
        """
        if speaker_id in self.speaker_profiles:
            return self.speaker_profiles[speaker_id].centroid.copy()
        return None

    def get_speaker_quality_stats(self, speaker_id: str) -> Optional[Dict]:
        """
        Get quality statistics for a speaker.
        
        Args:
            speaker_id: Speaker identifier
            
        Returns:
            Dictionary with quality statistics or None if speaker doesn't exist
        """
        if speaker_id not in self.speaker_profiles:
            return None
        
        profile = self.speaker_profiles[speaker_id]
        return {
            'speaker_id': speaker_id,
            'total_embeddings': profile.total_embeddings,
            'quality_weighted_count': profile.quality_weighted_count,
            'average_quality': profile.average_quality,
            'recent_qualities': profile.quality_history[-10:],  # Last 10 quality scores
            'last_updated': profile.last_updated.isoformat(),
            'age_seconds': (datetime.now() - profile.creation_time).total_seconds()
        }

    def get_all_speakers(self) -> list:
        """Get list of all known speaker IDs."""
        return list(self.speaker_profiles.keys())

    def remove_speaker(self, speaker_id: str) -> bool:
        """
        Remove a speaker profile.
        
        Args:
            speaker_id: Speaker identifier to remove
            
        Returns:
            True if speaker was removed, False if not found
        """
        if speaker_id in self.speaker_profiles:
            del self.speaker_profiles[speaker_id]
            logger.info(f"Removed speaker profile for {speaker_id}")
            return True
        return False

    def get_manager_statistics(self) -> Dict:
        """
        Get overall manager statistics for monitoring and debugging.
        
        Returns:
            Dictionary with manager performance statistics
        """
        total_speakers = len(self.speaker_profiles)
        avg_quality = 0.0
        total_embeddings = 0
        
        if total_speakers > 0:
            qualities = [profile.average_quality for profile in self.speaker_profiles.values()]
            avg_quality = np.mean(qualities)
            total_embeddings = sum(profile.total_embeddings for profile in self.speaker_profiles.values())
        
        acceptance_rate = 0.0
        if self.total_updates_attempted > 0:
            acceptance_rate = self.total_updates_accepted / self.total_updates_attempted
        
        return {
            'total_speakers': total_speakers,
            'total_embeddings_processed': total_embeddings,
            'average_speaker_quality': avg_quality,
            'updates_attempted': self.total_updates_attempted,
            'updates_accepted': self.total_updates_accepted,
            'updates_rejected': self.total_updates_rejected,
            'acceptance_rate': acceptance_rate,
            'quality_threshold': self.min_quality_threshold
        }


================================================
FILE: realtime_analysis_service.py
================================================
"""
Service for real-time audio analysis including ASR, sentiment, and speaker embedding.
"""
import numpy as np
import torch
import logging
from typing import Dict, Optional, Any
from pathlib import Path
import warnings
import asyncio
import os
from sklearn.preprocessing import StandardScaler

# Suppress specific warnings
warnings.filterwarnings("ignore", category=UserWarning, module='torch.nn.modules.conv')

logger = logging.getLogger(__name__)

# Attempt to import all necessary libraries, handling potential ImportErrors
try:
    from transformers import (
        AutoModelForSpeechSeq2Seq,
        AutoProcessor,
        pipeline
    )
    from funasr import AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    logger.error("Transformers or FunASR not available. ASR and sentiment analysis will be disabled.")
    TRANSFORMERS_AVAILABLE = False

try:
    from speechbrain.pretrained import EncoderClassifier as SpeakerIdModel
    from speechbrain.pretrained import SepformerSeparation as SeparatorModel
    SPEECHBRAIN_AVAILABLE = True
except ImportError:
    logger.error("SpeechBrain not available. Speaker ID and separation will be disabled.")
    SPEECHBRAIN_AVAILABLE = False

class RealtimeAnalysisService:
    """
    Manages loading and interaction with various ML models for real-time analysis.
    This service now includes fixes for model loading and pipeline initialization.
    """
    _instance: Optional['RealtimeAnalysisService'] = None
    _lock = asyncio.Lock()

    def __init__(self, device: str = None, asr_model_id: str = "distil-whisper/distil-large-v3"):
        if not TRANSFORMERS_AVAILABLE or not SPEECHBRAIN_AVAILABLE:
            raise ImportError("Required libraries (transformers, funasr, speechbrain) are not installed.")

        # Force CPU to avoid GPU memory issues
        self.device = "cpu"
        self.asr_model_id = asr_model_id
        
        self.asr_pipeline = None
        self.sentiment_model = None
        self.sentiment_scaler = StandardScaler()
        self.speaker_id_model = None
        
        self.models_loaded = False
        self._models_loading = False

    @classmethod
    async def get_instance(cls) -> 'RealtimeAnalysisService':
        if cls._instance is None:
            async with cls._lock:
                if cls._instance is None:
                    cls._instance = cls()
        return cls._instance

    async def ensure_models_loaded(self):
        """Ensure models are loaded, with lazy loading."""
        if self.models_loaded or self._models_loading:
            return
            
        self._models_loading = True
        try:
            logger.info("Loading models for RealtimeAnalysisService...")
            await self._load_asr_model_async()
            await self._load_sentiment_model_async()
            self.models_loaded = True
            logger.info("All models for RealtimeAnalysisService loaded successfully.")
        finally:
            self._models_loading = False

    async def _load_asr_model_async(self):
        """Loads the Whisper ASR pipeline asynchronously."""
        logger.info(f"Loading Whisper ASR model: {self.asr_model_id}")
        try:
            from transformers import pipeline
            import torch
            
            # Use pipeline for Whisper
            self.asr_pipeline = pipeline(
                "automatic-speech-recognition",
                model=self.asr_model_id,
                device=self.device
            )
            
            logger.info(f"Whisper ASR model loaded successfully on {self.device}")
        except Exception as e:
            logger.error(f"Failed to load Whisper ASR model: {e}")
            raise

    async def _load_sentiment_model_async(self):
        """Loads the sentiment analysis model asynchronously."""
        logger.info("Loading sentiment analysis model...")
        try:
            self.sentiment_model = pipeline(
                "text-classification",
                model="distilbert-base-uncased-finetuned-sst-2-english",
                return_all_scores=True,
                device=self.device
            )
            logger.info("Sentiment model loaded successfully.")
        except Exception as e:
            logger.error(f"Failed to load sentiment model: {e}")
            raise

    def _validate_audio(self, audio_bytes: bytes) -> Dict[str, Any]:
        """Validate the incoming audio chunk."""
        try:
            audio_np = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
            if len(audio_np) == 0:
                return {"valid": False, "reason": "Empty audio chunk"}
            rms = np.sqrt(np.mean(audio_np**2))
            return {"valid": True, "rms": rms, "duration": len(audio_np) / 16000, "samples": len(audio_np)}
        except Exception as e:
            return {"valid": False, "reason": str(e)}

    async def transcribe_chunk(self, audio_input: Any) -> str:
        """Transcribes audio using Whisper ASR pipeline.

        Accepts either a file path (str), raw PCM16 bytes, or a numpy array.
        """
        await self.ensure_models_loaded()

        try:
            input_for_pipeline: Any
            if isinstance(audio_input, (bytes, bytearray)):
                # Interpret as PCM16 mono at 16 kHz
                audio_np = np.frombuffer(audio_input, dtype=np.int16).astype(np.float32) / 32768.0
                input_for_pipeline = {"array": audio_np, "sampling_rate": 16000}
            elif isinstance(audio_input, np.ndarray):
                audio_np = audio_input.astype(np.float32)
                # If appears to be int16, normalize
                if audio_np.dtype != np.float32:
                    audio_np = audio_np.astype(np.float32)
                if audio_np.max() > 1.0 or audio_np.min() < -1.0:
                    audio_np = audio_np / 32768.0
                input_for_pipeline = {"array": audio_np, "sampling_rate": 16000}
            else:
                # Assume it's a file path
                input_for_pipeline = str(audio_input)

            result = self.asr_pipeline(input_for_pipeline, return_timestamps=False)
            transcription = result.get("text", "") if isinstance(result, dict) else ""
            if not transcription:
                return "[NO SPEECH DETECTED]"
            return transcription.strip()

        except Exception as e:
            logger.error(f"Whisper ASR transcription failed: {e}")
            return "[ASR FAILED]"

    def _extract_prosody_features(self, audio_chunk_bytes: bytes) -> Optional[np.ndarray]:
        """Extracts prosody features for sentiment analysis."""
        # Return None since sentiment model is not available
        return None

    def _classify_sentiment(self, prosody_features: np.ndarray) -> str:
        """Classifies sentiment based on prosody features."""
        # Fallback sentiment classification since model is not available
        return "neutral"

    async def process_sentiment_chunk(self, audio_chunk_bytes: bytes) -> Dict[str, Any]:
        """Processes a single chunk for both transcription and sentiment."""
        await self.ensure_models_loaded()
        
        validation = self._validate_audio(audio_chunk_bytes)
        if not validation["valid"]:
            return {"text": validation.get("reason", "Invalid audio"), "sentiment": "unknown", "tokens": []}
            
        transcription_task = asyncio.create_task(self.transcribe_chunk(audio_chunk_bytes))
        
        loop = asyncio.get_event_loop()
        prosody_features = await loop.run_in_executor(None, self._extract_prosody_features, audio_chunk_bytes)
        sentiment = self._classify_sentiment(prosody_features)
        
        text = await transcription_task
        
        return {"text": text, "sentiment": sentiment, "tokens": prosody_features.tolist() if prosody_features is not None else []}

# FIX: Implement a proper singleton pattern at the module level
_service_instance = None
_service_lock = asyncio.Lock()



async def get_realtime_analysis_service() -> 'RealtimeAnalysisService':
    """Provides a singleton instance of the RealtimeAnalysisService."""
    global _service_instance
    if _service_instance is None:
        async with _service_lock:
            if _service_instance is None:
                _service_instance = RealtimeAnalysisService()
    return _service_instance



================================================
FILE: redis_client.py
================================================
# Redis client service


================================================
FILE: speaker_embedding_service.py
================================================
"""
Speaker Embedding Service
Provides speaker embedding extraction for diarization using SpeechBrain
"""

import numpy as np
import torch
import logging
from pathlib import Path
from typing import Optional
import warnings

logger = logging.getLogger(__name__)

class SpeakerEmbeddingService:
    """
    Service for extracting speaker embeddings from audio chunks
    Uses SpeechBrain's speaker identification model for embedding generation
    """
    
    def __init__(self, device: str = None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """Load SpeechBrain speaker embedding model"""
        try:
            from speechbrain.pretrained import EncoderClassifier
            
            # Use ECAPA-TDNN model for speaker embeddings
            model_path = "speechbrain/spkrec-ecapa-voxceleb"
            self.model = EncoderClassifier.from_hparams(
                source=model_path,
                savedir=Path(__file__).resolve().parent.parent.parent / "models" / "speaker_embeddings",
                run_opts={"device": self.device}
            )
            logger.info(f"Speaker embedding model loaded successfully on {self.device}")
            
        except Exception as e:
            logger.error(f"Failed to load speaker embedding model: {e}")
            # Fallback to basic MFCC features
            self.model = None
    
    def extract_embedding(self, audio_chunk: np.ndarray, sample_rate: int = 16000) -> Optional[np.ndarray]:
        """
        Extract speaker embedding from audio chunk
        
        Args:
            audio_chunk: Raw audio as numpy array (int16 or float32)
            sample_rate: Audio sample rate
            
        Returns:
            Speaker embedding as numpy array, or None if extraction fails
        """
        if self.model is None:
            logger.warning("No speaker embedding model available, returning dummy embedding")
            return np.random.randn(192)  # ECAPA-TDNN embedding size
            
        try:
            # Ensure correct format
            if audio_chunk.dtype == np.int16:
                audio_float = audio_chunk.astype(np.float32) / 32768.0
            else:
                audio_float = audio_chunk.astype(np.float32)
            
            # Ensure minimum length (0.1s = 1600 samples at 16kHz)
            if len(audio_float) < 1600:
                audio_float = np.pad(audio_float, (0, 1600 - len(audio_float)), mode='constant')
            
            # Convert to torch tensor
            audio_tensor = torch.from_numpy(audio_float).unsqueeze(0).to(self.device)
            
            # Extract embedding
            with torch.no_grad():
                embedding = self.model.encode_batch(audio_tensor)
                embedding_np = embedding.squeeze().cpu().numpy()
                
            logger.debug(f"Extracted speaker embedding with shape {embedding_np.shape}")
            return embedding_np
            
        except Exception as e:
            logger.error(f"Failed to extract speaker embedding: {e}")
            return None
    
    def extract_embeddings_batch(self, audio_chunks: list, sample_rate: int = 16000) -> list:
        """Extract embeddings for multiple audio chunks"""
        embeddings = []
        for chunk in audio_chunks:
            embedding = self.extract_embedding(chunk, sample_rate)
            embeddings.append(embedding)
        return embeddings

# Global singleton instance
_embedding_service = None

def get_speaker_embedding_service() -> SpeakerEmbeddingService:
    """Get singleton instance of speaker embedding service"""
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = SpeakerEmbeddingService()
    return _embedding_service


================================================
FILE: speaker_merging.py
================================================
"""
Hierarchical clustering for merging similar speakers
"""
import numpy as np
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import pdist
from typing import Dict, List, Tuple


def merge_similar_speakers(
    profiles: Dict[str, object],
    inactive_ids: List[str],
    merge_similarity_threshold: float
) -> Tuple[Dict[str, object], List[str]]:
    """
    Merges highly similar inactive speakers using hierarchical clustering.
    
    Args:
        profiles: Dictionary of speaker profiles
        inactive_ids: List of inactive speaker IDs to consider for merging
        merge_similarity_threshold: Similarity threshold for merging
        
    Returns:
        Tuple of (updated profiles, list of merged speaker IDs)
    """
    if len(inactive_ids) < 2:
        return profiles, []

    # Get centroids of inactive speakers
    try:
        inactive_centroids = np.array([profiles[sid].centroid for sid in inactive_ids])
    except (KeyError, AttributeError):
        # Fallback if some speakers don't exist or don't have centroids
        return profiles, []

    try:
        # Calculate pairwise distances and perform clustering
        # 'pdist' calculates condensed distance matrix, 'linkage' performs clustering
        distance_matrix = pdist(inactive_centroids, metric='cosine')
        linkage_matrix = linkage(distance_matrix, method='average')
        
        # Form flat clusters based on the similarity threshold
        cluster_labels = fcluster(
            linkage_matrix, 
            1 - merge_similarity_threshold, # fcluster uses distance, not similarity
            criterion='distance'
        )

        # Group speaker IDs by their new cluster label
        merge_groups: Dict[int, List[str]] = {}
        for i, speaker_id in enumerate(inactive_ids):
            label = cluster_labels[i]
            if label not in merge_groups:
                merge_groups[label] = []
            merge_groups[label].append(speaker_id)

        merged_ids = []
        for label, group in merge_groups.items():
            if len(group) > 1:
                # Merge the profiles within this group
                profiles, removed_ids = _perform_merge(profiles, group)
                merged_ids.extend(removed_ids)
                
        return profiles, merged_ids
        
    except Exception:
        # Fallback if clustering fails
        return profiles, []


def _perform_merge(
    profiles: Dict[str, object], 
    group_to_merge: List[str]
) -> Tuple[Dict[str, object], List[str]]:
    """
    Helper function to merge a group of speaker profiles into one.
    
    Args:
        profiles: Dictionary of speaker profiles
        group_to_merge: List of speaker IDs to merge
        
    Returns:
        Tuple of (updated profiles, list of removed speaker IDs)
    """
    if len(group_to_merge) < 2:
        return profiles, []
    
    try:
        # Select the profile with the highest update count as the base
        base_speaker_id = max(group_to_merge, key=lambda sid: 
                             getattr(profiles[sid], 'update_count', 1))
        
        removed_ids = []
        for speaker_id in group_to_merge:
            if speaker_id != base_speaker_id:
                # Merge other profiles into the base profile
                base_profile = profiles[base_speaker_id]
                source_profile = profiles[speaker_id]
                
                # Calculate weights based on activity
                base_weight = getattr(base_profile, 'update_count', 1)
                source_weight = getattr(source_profile, 'update_count', 1)
                total_weight = base_weight + source_weight
                
                # Merge centroids
                base_profile.centroid = (
                    (base_weight * base_profile.centroid + 
                     source_weight * source_profile.centroid) / total_weight
                )
                
                # Update metadata
                if hasattr(base_profile, 'update_count'):
                    base_profile.update_count += getattr(source_profile, 'update_count', 1)
                
                removed_ids.append(speaker_id)
                del profiles[speaker_id]
                
        return profiles, removed_ids
        
    except Exception:
        # Fallback if merging fails
        return profiles, []


def calculate_similarity_score(profile1: object, profile2: object) -> float:
    """
    Calculate similarity score between two speaker profiles.
    
    Args:
        profile1: First speaker profile
        profile2: Second speaker profile
        
    Returns:
        Similarity score between 0 and 1
    """
    try:
        # Calculate cosine similarity
        centroid1 = profile1.centroid
        centroid2 = profile2.centroid
        
        norm1 = np.linalg.norm(centroid1)
        norm2 = np.linalg.norm(centroid2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        dot_product = np.dot(centroid1, centroid2)
        similarity = dot_product / (norm1 * norm2)
        
        return max(0.0, min(1.0, similarity))
        
    except Exception:
        return 0.0


================================================
FILE: speaker_profile.py
================================================
# backend/src/services/speaker_profile.py
import numpy as np
import time
from dataclasses import dataclass, field
from typing import List

@dataclass
class SpeakerProfile:
    """Optimized data model for a single speaker."""
    speaker_id: str
    centroid: np.ndarray
    
    # Activity and Quality Tracking
    last_seen_timestamp: float = field(default_factory=time.time)
    update_count: int = 1
    total_quality: float = 0.0

    def update(self, embedding: np.ndarray, quality_score: float):
        """Update the speaker's profile with a new embedding."""
        # Weighted average to update the centroid
        new_weight = quality_score
        old_weight = self.update_count
        self.centroid = ((self.centroid * old_weight) + (embedding * new_weight)) / (old_weight + new_weight)
        
        self.update_count += 1
        self.last_seen_timestamp = time.time()
        self.total_quality += quality_score

    @property
    def average_quality(self) -> float:
        """Calculate the average quality of embeddings for this speaker."""
        return self.total_quality / self.update_count if self.update_count > 0 else 0


================================================
FILE: speaker_pruning.py
================================================
"""
Speaker pruning strategies for memory-efficient speaker management
"""
import time
from typing import Dict, List

def get_inactive_speaker_ids(
    profiles: Dict[str, object], 
    inactivity_threshold_seconds: float
) -> List[str]:
    """
    Identifies speakers who haven't been seen recently.
    
    Args:
        profiles: Dictionary of speaker profiles
        inactivity_threshold_seconds: Time threshold for inactivity
        
    Returns:
        List of speaker IDs that are inactive
    """
    inactive_ids = []
    current_time = time.time()
    for speaker_id, profile in profiles.items():
        # Note: This assumes profile has last_seen_timestamp attribute
        # If using SpeakerProfile from speaker_profile.py, use last_seen instead
        if hasattr(profile, 'last_seen'):
            last_seen = profile.last_seen
        elif hasattr(profile, 'last_seen_timestamp'):
            last_seen = profile.last_seen_timestamp
        else:
            continue
            
        if current_time - last_seen > inactivity_threshold_seconds:
            inactive_ids.append(speaker_id)
    return inactive_ids


def should_prune_speaker(profile: object, inactivity_threshold: float) -> bool:
    """
    Determine if a speaker should be pruned based on inactivity.
    
    Args:
        profile: Speaker profile object
        inactivity_threshold: Time threshold in seconds
        
    Returns:
        True if speaker should be pruned
    """
    current_time = time.time()
    if hasattr(profile, 'last_seen'):
        last_seen = profile.last_seen
    elif hasattr(profile, 'last_seen_timestamp'):
        last_seen = profile.last_seen_timestamp
    else:
        return False
        
    return current_time - last_seen > inactivity_threshold


================================================
FILE: stream_simulation_service.py
================================================
#!/usr/bin/env python3
"""
Stream Simulation Service for Audio Transcription
Uses realtime analysis service with Whisper v3 ASR for real audio processing
"""

import os
import sys
import asyncio
import logging
import tempfile
import uuid
import soundfile as sf
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

# Import the realtime analysis service
from src.services.realtime_analysis_service import get_realtime_analysis_service

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealtimeAudioProcessor:
    """Audio processor using realtime analysis service with Whisper v3"""
    
    async def process_audio_file(self, file_path: str, **kwargs) -> Dict[str, Any]:
        """Process audio file using realtime analysis service"""
        
        try:
            # Get the realtime analysis service
            service = await get_realtime_analysis_service()
            await service.ensure_models_loaded()
            
            # Load audio file
            audio_data, sample_rate = sf.read(file_path, dtype='int16')
            
            # Convert to bytes for processing
            audio_bytes = audio_data.tobytes()
            
            # Process with realtime analysis service
            result = await service.process_sentiment_chunk(audio_bytes)
            transcription = result.get("text", "")
            
            # Create segments from the transcription
            segments = []
            if transcription and transcription != "[NO SPEECH DETECTED]" and transcription != "[ASR FAILED]":
                # Simple segmentation - split into sentences for now
                sentences = transcription.split('. ')
                current_time = 0.0
                audio_duration = len(audio_data) / sample_rate
                
                for sentence in sentences:
                    if sentence.strip():
                        # Estimate duration based on word count and audio length
                        word_count = len(sentence.split())
                        estimated_duration = min(word_count * 0.5, audio_duration - current_time)
                        
                        segments.append({
                            "text": sentence.strip(),
                            "start": current_time,
                            "end": current_time + estimated_duration,
                            "speaker": "SPEAKER_00",
                            "sentiment": result.get("sentiment", "neutral")
                        })
                        current_time += estimated_duration
            
            return {
                "transcript": transcription,
                "language": "en",
                "fileName": os.path.basename(file_path),
                "fileSize": float(os.path.getsize(file_path)),
                "fileFormat": os.path.splitext(file_path)[1].replace('.', ''),
                "totalDuration": len(audio_data) / sample_rate if len(audio_data) > 0 else 0,
                "segments": segments,
                "sentiment": result.get("sentiment", "neutral"),
                "tokens": result.get("tokens", [])
            }
            
        except Exception as e:
            logger.error(f"Error processing with realtime analysis service: {e}")
            # Fallback to basic response
            return {
                "transcript": "[TRANSCRIPTION ERROR]",
                "language": "en",
                "fileName": os.path.basename(file_path),
                "fileSize": float(os.path.getsize(file_path)) if os.path.exists(file_path) else 0,
                "fileFormat": os.path.splitext(file_path)[1].replace('.', ''),
                "totalDuration": 0,
                "segments": [],
                "sentiment": "unknown",
                "tokens": []
            }

# Use realtime audio processor
ModernStreamProcessor = RealtimeAudioProcessor

async def run_modern_stream(file_path: str, **kwargs) -> Dict[str, Any]:
    """Process audio with realtime analysis service"""
    processor = RealtimeAudioProcessor()
    return await processor.process_audio_file(file_path, **kwargs)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StreamSimulationService:
    """
    Service that replaces Whisper with stream simulation for audio transcription
    Maintains API compatibility with existing audio_transcripts.py
    """
    
    def __init__(self):
        self.processor = ModernStreamProcessor()
        self.temp_dir = tempfile.mkdtemp(prefix="stream_sim_")
        
    async def process_audio_file(
        self,
        file_path: str,
        job_id: str,
        user_id: str,
        language: Optional[str] = None,
        prompt: Optional[str] = None,
        separate_voices: bool = True,
        identify_speakers: bool = True,
        min_speakers: int = 1,
        max_speakers: int = 10
    ) -> Dict[str, Any]:
        """
        Process audio file using realtime analysis service instead of mock data
        
        Args:
            file_path: Path to audio file
            job_id: Unique job identifier
            user_id: User ID for tracking
            language: Language code
            prompt: Transcription prompt
            separate_voices: Whether to separate voices
            identify_speakers: Whether to identify speakers
            min_speakers: Minimum expected speakers
            max_speakers: Maximum expected speakers
            
        Returns:
            Dictionary with realtime analysis service transcription
        """
        try:
            logger.info(f"Starting realtime analysis processing for job {job_id}")
            
            # Process audio with realtime analysis service
            result = await run_modern_stream(
                file_path,
                separate_speakers=identify_speakers
            )
            
            # Format result to match existing API
            return self._format_for_api(result, file_path, job_id)
            
        except Exception as e:
            logger.error(f"Error in realtime analysis processing: {str(e)}")
            raise
    
    def _format_for_api(self, result: Dict[str, Any], file_path: str, job_id: str) -> Dict[str, Any]:
        """Format realtime analysis result to match Convex webhook API contract"""
        
        # Get file info
        file_name = os.path.basename(file_path)
        file_size = os.path.getsize(file_path)
        file_format = os.path.splitext(file_name)[1].lower().replace('.', '')
        
        # Build full transcript
        full_transcript = result.get('transcript', '')
        
        # Get segments from realtime analysis
        segments = result.get('segments', [])
        speakers = []
        for segment in segments:
            speakers.append({
                "speaker": segment.get("speaker", "SPEAKER_00"),
                "start": segment.get("start", 0.0),
                "end": segment.get("end", 0.0),
                "duration": segment.get("end", 0.0) - segment.get("start", 0.0)
            })
        
        # Return the exact fields expected by the webhook
        return {
            "transcript": full_transcript,
            "status": "completed",
            "jobId": job_id,
            "fileName": file_name,
            "fileSize": float(file_size),
            "fileFormat": file_format,
            "language": result.get("language", "en"),
            "speakers": speakers
        }
    
    def _group_segments_by_speaker(self, segments: List[Dict]) -> Dict[str, List[Dict]]:
        """Group segments by speaker for API compatibility"""
        grouped = {}
        for segment in segments:
            speaker = segment.get('speaker', 'SPEAKER_00')
            if speaker not in grouped:
                grouped[speaker] = []
            grouped[speaker].append(segment)
        return grouped
    
    async def simulate_processing_progress(self, job_id: str, total_segments: int):
        """Simulate processing progress for real-time updates"""
        for i in range(total_segments):
            progress = {
                "jobId": job_id,
                "status": "processing",
                "progress": (i + 1) / total_segments * 100,
                "current_segment": i + 1,
                "total_segments": total_segments
            }
            yield progress
    
    def cleanup(self):
        """Clean up temporary files"""
        import shutil
        try:
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                logger.info(f"Cleaned up temp directory: {self.temp_dir}")
        except Exception as e:
            logger.warning(f"Error cleaning up temp files: {str(e)}")

# Global instance
stream_simulation_service = StreamSimulationService()


================================================
FILE: telephony_service.py
================================================
#!/usr/bin/env python3
"""
Telephony Service with ASR/Sentiment Integration
Integrates with Bandwidth Python SDK for real-time call management.
"""

import os
import sys
import base64
import logging
import numpy as np
from typing import Dict, Any, Optional, List
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass

# Add project root to path
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Bandwidth SDK Imports
import bandwidth
from bandwidth.api import calls_api
from bandwidth.models.create_call import CreateCall
from bandwidth.models.update_call import UpdateCall
from bandwidth.models.call_state_enum import CallStateEnum

# Import existing services
from src.services.realtime_analysis_service import get_realtime_analysis_service
from src.services.modern_stateful_speaker_identifier import ModernStatefulSpeakerIdentifier
from src.services.audio_processor import AudioProcessor

# --- Environment Variables for Bandwidth ---
BW_USERNAME = os.environ.get("BW_USERNAME")
BW_PASSWORD = os.environ.get("BW_PASSWORD")
BW_ACCOUNT_ID = os.environ.get("BW_ACCOUNT_ID")
BW_VOICE_APPLICATION_ID = os.environ.get("BW_VOICE_APPLICATION_ID")
BW_NUMBER = os.environ.get("BW_NUMBER")
BASE_CALLBACK_URL = os.environ.get("BASE_CALLBACK_URL") # Your publicly accessible server URL, e.g., https://myapp.com

@dataclass
class CallSession:
    call_id: str
    user_id: str
    phone_number: str
    direction: str
    start_time: datetime
    audio_chunks: List[Dict[str, Any]]
    current_transcript: str
    current_sentiment: str
    speakers: Dict[str, Any]

class TelephonyService:
    """
    Main telephony service that integrates:
    - Bandwidth SDK for call control
    - Real-time ASR
    - Sentiment analysis
    - Speaker diarization
    """

    def __init__(self):
        self.analysis_service = get_realtime_analysis_service()
        self.speaker_identifier = ModernStatefulSpeakerIdentifier()
        self.audio_processor = AudioProcessor()
        self.active_calls: Dict[str, CallSession] = {}
        
        # Initialize Bandwidth API Client
        configuration = bandwidth.Configuration(username=BW_USERNAME, password=BW_PASSWORD)
        api_client = bandwidth.ApiClient(configuration)
        self.calls_api_instance = calls_api.CallsApi(api_client)

    async def start_call(self, user_id: str, phone_number: str, direction: str) -> Dict[str, Any]:
        """Start a new telephony call using the Bandwidth SDK"""
        logger.info(f"Starting call to {phone_number} for user {user_id}")

        # The answerUrl points to our FastAPI endpoint that will serve BXML
        # This is how we start the audio stream from Bandwidth to our WebSocket server
        answer_url = f"{BASE_CALLBACK_URL}/bxml/start-stream"

        call_body = CreateCall(
            to=phone_number,
            var_from=BW_NUMBER,
            application_id=BW_VOICE_APPLICATION_ID,
            answer_url=answer_url,
            tag=user_id  # Use tag to associate the call with the user
        )

        try:
            api_response = self.calls_api_instance.create_call(BW_ACCOUNT_ID, call_body)
            call_id = api_response.call_id
            logger.info(f"Successfully created call {call_id}")

            # Initialize call session
            session = CallSession(
                call_id=call_id,
                user_id=user_id,
                phone_number=phone_number,
                direction=direction,
                start_time=datetime.now(),
                audio_chunks=[],
                current_transcript="",
                current_sentiment="neutral",
                speakers={}
            )
            self.active_calls[call_id] = session

            await self.speaker_identifier.initialize()

            return {
                "call_id": call_id,
                "status": "connecting",
                "session": session.__dict__
            }
        except bandwidth.ApiException as e:
            logger.error(f"Error starting call with Bandwidth: {e}")
            raise

    async def process_audio_chunk(self, call_id: str, chunk_id: str, audio_data: str, sequence: int) -> Dict[str, Any]:
        """Process audio chunk with ASR and sentiment analysis"""
        if call_id not in self.active_calls:
            raise ValueError(f"Call {call_id} not found")
        
        session = self.active_calls[call_id]
        
        try:
            # Bandwidth sends base64 encoded audio in the stream
            audio_bytes = base64.b64decode(audio_data)
            audio_array = np.frombuffer(audio_bytes, dtype=np.int16)

            if len(audio_array) == 0:
                return {"error": "Empty audio data"}
            
            result = await self.analysis_service.process_sentiment_chunk(
                audio_array.astype(np.float32).tobytes()
            )
            
            speaker_id, confidence = self.speaker_identifier.identify_speaker(audio_array)
            
            chunk_data = {
                "chunk_id": chunk_id,
                "sequence": sequence,
                "transcript": result.get("text", ""),
                "sentiment": result.get("sentiment", "neutral"),
                "speaker": speaker_id,
                "confidence": confidence,
                "timestamp": datetime.now().isoformat()
            }
            
            session.audio_chunks.append(chunk_data)
            session.current_transcript = result.get("text", "")
            session.current_sentiment = result.get("sentiment", "neutral")
            
            if speaker_id not in session.speakers:
                session.speakers[speaker_id] = {"first_seen": datetime.now().isoformat(), "confidence": confidence}
            
            return chunk_data
            
        except Exception as e:
            logger.error(f"Error processing chunk {chunk_id}: {e}")
            return {"error": str(e), "transcript": "", "sentiment": "neutral", "speaker": "unknown"}

    async def end_call(self, call_id: str) -> Dict[str, Any]:
        """End telephony call using the Bandwidth SDK and cleanup"""
        if call_id not in self.active_calls:
            raise ValueError(f"Call {call_id} not found")
            
        logger.info(f"Ending call {call_id}")

        try:
            update_call_body = UpdateCall(state=CallStateEnum("completed"))
            self.calls_api_instance.update_call(BW_ACCOUNT_ID, call_id, update_call_body)
            logger.info(f"Successfully ended call {call_id} with Bandwidth")
        except bandwidth.ApiException as e:
            logger.error(f"Error ending call with Bandwidth: {e}")
            # Continue with local cleanup even if API call fails
            
        final_result = await self.process_final_transcription(call_id)
        
        # Cleanup
        del self.active_calls[call_id]
        
        return final_result

    async def process_final_transcription(self, call_id: str) -> Dict[str, Any]:
        """Process final transcription for a completed call"""
        if call_id not in self.active_calls:
            raise ValueError(f"Call {call_id} not found for final processing")
        
        session = self.active_calls[call_id]
        # ... (rest of the final processing logic remains the same)
        full_transcript = " ".join([chunk.get("transcript", "") for chunk in session.audio_chunks])
        return {"full_transcript": full_transcript, "speaker_summary": session.speakers}

    async def get_call_status(self, call_id: str) -> Dict[str, Any]:
        if call_id not in self.active_calls:
            return {"error": "Call not found"}
        session = self.active_calls[call_id]
        return {"status": "active", "duration": (datetime.now() - session.start_time).total_seconds()}


# Global service instance
telephony_service = TelephonyService()


================================================
FILE: telephony_service_monitoring.py
================================================
#!/usr/bin/env python3
"""
Enhanced Telephony Service with Monitoring
Integrates with monitoring system to track phone number flow
"""

import os
import sys
import logging
import base64
import numpy as np
from typing import Dict, Any, Optional, List
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass

# Add project root to path
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import monitoring
from src.api.public.telephony_monitoring import call_tracker

# Bandwidth SDK Imports
import bandwidth
from bandwidth.api import calls_api
from bandwidth.models.create_call import CreateCall
from bandwidth.models.update_call import UpdateCall
from bandwidth.models.call_state_enum import CallStateEnum

# Import existing services
from src.services.realtime_analysis_service import get_realtime_analysis_service
from src.services.modern_stateful_speaker_identifier import ModernStatefulSpeakerIdentifier
from src.services.audio_processor import AudioProcessor

# --- Environment Variables for Bandwidth ---
BW_USERNAME = os.environ.get("BW_USERNAME")
BW_PASSWORD = os.environ.get("BW_PASSWORD")
BW_ACCOUNT_ID = os.environ.get("BW_ACCOUNT_ID")
BW_VOICE_APPLICATION_ID = os.environ.get("BW_VOICE_APPLICATION_ID")
BW_NUMBER = os.environ.get("BW_NUMBER")
BASE_CALLBACK_URL = os.environ.get("BASE_CALLBACK_URL")

@dataclass
class CallSession:
    call_id: str
    user_id: str
    phone_number: str
    direction: str
    start_time: datetime
    audio_chunks: List[Dict[str, Any]]
    current_transcript: str
    current_sentiment: str
    speakers: Dict[str, Any]

class TelephonyServiceWithMonitoring:
    """
    Enhanced telephony service with comprehensive monitoring
    """

    def __init__(self):
        self.analysis_service = get_realtime_analysis_service()
        self.speaker_identifier = ModernStatefulSpeakerIdentifier()
        self.audio_processor = AudioProcessor()
        self.active_calls: Dict[str, CallSession] = {}
        
        # Initialize Bandwidth API Client
        configuration = bandwidth.Configuration(username=BW_USERNAME, password=BW_PASSWORD)
        api_client = bandwidth.ApiClient(configuration)
        self.calls_api_instance = calls_api.CallsApi(api_client)

    async def start_call(self, user_id: str, phone_number: str, direction: str) -> Dict[str, Any]:
        """Start a new telephony call with monitoring"""
        call_id = f"call_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{user_id}"
        
        # Track call initiation
        call_tracker.track_call_start(call_id, phone_number, user_id, direction)
        
        logger.info(f"[MONITORING] Starting call {call_id} to {phone_number} for user {user_id}")

        # The answerUrl points to our FastAPI endpoint that will serve BXML
        answer_url = f"{BASE_CALLBACK_URL}/bxml/start-stream"

        call_body = CreateCall(
            to=phone_number,
            var_from=BW_NUMBER,
            application_id=BW_VOICE_APPLICATION_ID,
            answer_url=answer_url,
            tag=user_id
        )

        try:
            api_response = self.calls_api_instance.create_call(BW_ACCOUNT_ID, call_body)
            bandwidth_call_id = api_response.call_id
            
            # Track Bandwidth call creation
            call_tracker.track_bandwidth_call(call_id, bandwidth_call_id, phone_number)
            
            logger.info(f"[MONITORING] Bandwidth call created: {bandwidth_call_id} for {phone_number}")

            # Initialize call session
            session = CallSession(
                call_id=call_id,
                user_id=user_id,
                phone_number=phone_number,
                direction=direction,
                start_time=datetime.now(),
                audio_chunks=[],
                current_transcript="",
                current_sentiment="neutral",
                speakers={}
            )
            self.active_calls[call_id] = session

            await self.speaker_identifier.initialize()

            return {
                "call_id": call_id,
                "bandwidth_call_id": bandwidth_call_id,
                "status": "connecting",
                "websocket_url": f"ws://localhost:8000/telephony/ws/{call_id}",
                "session": session.__dict__
            }
            
        except bandwidth.ApiException as e:
            logger.error(f"[MONITORING] Error starting call with Bandwidth: {e}")
            raise

    async def process_audio_chunk(self, call_id: str, chunk_id: str, audio_data: str, sequence: int) -> Dict[str, Any]:
        """Process audio chunk with enhanced monitoring"""
        if call_id not in self.active_calls:
            raise ValueError(f"Call {call_id} not found")
        
        session = self.active_calls[call_id]
        phone_number = session.phone_number
        
        # Track audio processing
        call_tracker.track_audio_processing(call_id, phone_number, chunk_id)
        
        logger.info(f"[MONITORING] Processing chunk {chunk_id} for call {call_id} ({phone_number})")
        
        try:
            audio_bytes = base64.b64decode(audio_data)
            audio_array = np.frombuffer(audio_bytes, dtype=np.int16)

            if len(audio_array) == 0:
                return {"error": "Empty audio data"}
            
            result = await self.analysis_service.process_sentiment_chunk(
                audio_array.astype(np.float32).tobytes()
            )
            
            speaker_id, confidence = self.speaker_identifier.identify_speaker(audio_array)
            
            chunk_data = {
                "chunk_id": chunk_id,
                "sequence": sequence,
                "transcript": result.get("text", ""),
                "sentiment": result.get("sentiment", "neutral"),
                "speaker": speaker_id,
                "confidence": confidence,
                "timestamp": datetime.now().isoformat(),
                "phone_number": phone_number  # Include phone number for validation
            }
            
            session.audio_chunks.append(chunk_data)
            session.current_transcript = result.get("text", "")
            session.current_sentiment = result.get("sentiment", "neutral")
            
            if speaker_id not in session.speakers:
                session.speakers[speaker_id] = {"first_seen": datetime.now().isoformat(), "confidence": confidence}
            
            logger.info(f"[MONITORING] Processed chunk {chunk_id} for {phone_number}: {result.get('text', '')[:50]}...")
            
            return chunk_data
            
        except Exception as e:
            logger.error(f"[MONITORING] Error processing chunk {chunk_id}: {e}")
            return {"error": str(e), "transcript": "", "sentiment": "neutral", "speaker": "unknown"}

    async def end_call(self, call_id: str) -> Dict[str, Any]:
        """End telephony call with monitoring"""
        if call_id not in self.active_calls:
            raise ValueError(f"Call {call_id} not found")
            
        session = self.active_calls[call_id]
        phone_number = session.phone_number
        duration = (datetime.now() - session.start_time).total_seconds()
        
        logger.info(f"[MONITORING] Ending call {call_id} for {phone_number} (duration: {duration}s)")

        try:
            update_call_body = UpdateCall(state=CallStateEnum("completed"))
            self.calls_api_instance.update_call(BW_ACCOUNT_ID, call_id, update_call_body)
            logger.info(f"[MONITORING] Successfully ended call {call_id} with Bandwidth")
        except bandwidth.ApiException as e:
            logger.error(f"[MONITORING] Error ending call with Bandwidth: {e}")

        final_result = await self.process_final_transcription(call_id)
        
        # Track call completion
        call_tracker.track_call_complete(call_id, phone_number, duration)
        
        # Cleanup
        del self.active_calls[call_id]
        
        return final_result

    async def process_final_transcription(self, call_id: str) -> Dict[str, Any]:
        """Process final transcription with monitoring"""
        if call_id not in self.active_calls:
            raise ValueError(f"Call {call_id} not found for final processing")
        
        session = self.active_calls[call_id]
        phone_number = session.phone_number
        
        logger.info(f"[MONITORING] Processing final transcription for call {call_id} ({phone_number})")
        
        full_transcript = " ".join([chunk.get("transcript", "") for chunk in session.audio_chunks])
        
        return {
            "call_id": call_id,
            "phone_number": phone_number,
            "full_transcript": full_transcript,
            "speaker_summary": session.speakers,
            "total_chunks": len(session.audio_chunks),
            "duration": (datetime.now() - session.start_time).total_seconds()
        }

    async def get_call_status(self, call_id: str) -> Dict[str, Any]:
        """Get call status with monitoring info"""
        if call_id not in self.active_calls:
            return {"error": "Call not found"}
        
        session = self.active_calls[call_id]
        return {
            "status": "active",
            "phone_number": session.phone_number,
            "user_id": session.user_id,
            "direction": session.direction,
            "duration": (datetime.now() - session.start_time).total_seconds(),
            "current_transcript": session.current_transcript,
            "current_sentiment": session.current_sentiment,
            "speakers_count": len(session.speakers),
            "chunks_processed": len(session.audio_chunks)
        }

    async def handle_websocket_connection(self, websocket, path: str):
        """Handle WebSocket connection with monitoring"""
        call_id = path.strip("/").split("/")[-1]
        
        if call_id in self.active_calls:
            phone_number = self.active_calls[call_id].phone_number
            websocket_url = f"ws://localhost:8000/telephony/ws/{call_id}"
            call_tracker.track_websocket_connection(call_id, phone_number, websocket_url)
            logger.info(f"[MONITORING] WebSocket connected for call {call_id} ({phone_number})")

# Global service instance
telephony_service_with_monitoring = TelephonyServiceWithMonitoring()



================================================
FILE: telnyx_client.py
================================================
# Telnyx client service


================================================
FILE: temporal_context_tracker.py
================================================
"""
Temporal Context Integration for Speaker Diarization
Implements temporal smoothing and conversation flow analysis
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass
from collections import deque, defaultdict
import time
import logging
from scipy.stats import entropy

logger = logging.getLogger(__name__)


@dataclass
class TemporalContext:
    """Represents temporal context for a speaker decision"""
    timestamp: float
    speaker_id: Optional[str]
    confidence: float
    embedding: np.ndarray
    quality_score: float
    segment_duration: float


@dataclass
class SpeakerTurn:
    """Represents a speaker turn in conversation"""
    speaker_id: str
    start_time: float
    end_time: float
    confidence: float
    turn_duration: float


class TemporalContextTracker:
    """
    Implements temporal context integration for speaker diarization
    
    Key features:
    - Temporal smoothing windows for decision consistency
    - Speaker turn pattern detection and validation
    - Context-aware speaker assignment using neighboring segments
    - Temporal constraint checking to prevent rapid switching
    - Conversation flow analysis and transition modeling
    """
    
    def __init__(self, smoothing_window: int = 5, max_context_seconds: float = 30.0,
                 min_turn_duration: float = 1.0, max_switch_frequency: float = 0.5):
        """
        Initialize temporal context tracker
        
        Args:
            smoothing_window: Number of segments for temporal smoothing
            max_context_seconds: Maximum context window in seconds
            min_turn_duration: Minimum duration for a valid speaker turn
            max_switch_frequency: Maximum allowed speaker switches per second
        """
        self.smoothing_window = smoothing_window
        self.max_context_seconds = max_context_seconds
        self.min_turn_duration = min_turn_duration
        self.max_switch_frequency = max_switch_frequency
        
        self.recent_contexts: deque = deque(maxlen=smoothing_window * 2)
        self.speaker_turns: List[SpeakerTurn] = []
        self.speaker_history: Dict[str, deque] = defaultdict(
            lambda: deque(maxlen=smoothing_window)
        )
        
        self.transition_matrix: Dict[Tuple[str, str], int] = defaultdict(int)
        self.speaker_frequencies: Dict[str, int] = defaultdict(int)
        self.last_speaker_switch = 0.0
        
        self.context_checks = 0
        self.consistency_corrections = 0
        
        logger.info(f"Initialized TemporalContextTracker: "
                   f"smoothing_window={smoothing_window}, "
                   f"max_context={max_context_seconds}s")
    
    def add_context(self, timestamp: float, speaker_id: Optional[str], confidence: float,
                   embedding: np.ndarray, quality_score: float,
                   segment_duration: float) -> None:
        """
        Add temporal context for a segment
        
        Args:
            timestamp: Segment timestamp
            speaker_id: Assigned speaker ID (None for unknown)
            confidence: Confidence score for assignment
            embedding: Segment embedding
            quality_score: Quality score for this segment
            segment_duration: Duration of the segment
        """
        context = TemporalContext(
            timestamp=timestamp,
            speaker_id=speaker_id,
            confidence=confidence,
            embedding=embedding,
            quality_score=quality_score,
            segment_duration=segment_duration
        )
        
        # Update speaker history before adding to recent contexts
        if speaker_id:
            # Update transitions if we have a previous speaker
            if self.recent_contexts and self.recent_contexts[-1].speaker_id:
                prev_speaker = self.recent_contexts[-1].speaker_id
                if prev_speaker != speaker_id:
                    transition_key = (prev_speaker, speaker_id)
                    self.transition_matrix[transition_key] += 1
            
            self.speaker_history[speaker_id].append(context)
            self.speaker_frequencies[speaker_id] += 1
            
        self.recent_contexts.append(context)

    
    def apply_temporal_smoothing(self, current_speaker: Optional[str],
                               current_confidence: float) -> Tuple[Optional[str], float]:
        """
        Apply temporal smoothing to speaker assignment
        
        Args:
            current_speaker: Current speaker assignment
            current_confidence: Current confidence score
            
        Returns:
            Tuple of (smoothed_speaker, smoothed_confidence)
        """
        self.context_checks += 1
        
        if not self.recent_contexts:
            return current_speaker, current_confidence
        
        recent_cutoff = time.time() - self.max_context_seconds
        recent_contexts = [
            ctx for ctx in self.recent_contexts
            if ctx.timestamp >= recent_cutoff
        ]
        
        if not recent_contexts:
            return current_speaker, current_confidence
        
        speaker_probabilities = self._calculate_speaker_probabilities(
            recent_contexts, current_speaker
        )
        
        smoothed_speaker, smoothed_confidence = self._apply_smoothing(
            current_speaker, current_confidence, speaker_probabilities
        )
        
        return smoothed_speaker, smoothed_confidence
    
    def _calculate_speaker_probabilities(self, contexts: List[TemporalContext],
                                     current_speaker: Optional[str]) -> Dict[str, float]:
        """Calculate speaker probabilities based on temporal context"""
        speaker_scores = defaultdict(float)
        
        for i, context in enumerate(contexts):
            weight = np.exp(-i / self.smoothing_window)
            weight *= context.confidence * context.quality_score
            
            if context.speaker_id:
                speaker_scores[context.speaker_id] += weight
        
        total_score = sum(speaker_scores.values())
        if total_score > 0:
            return {speaker: score / total_score for speaker, score in speaker_scores.items()}
        return {}
    
    def _apply_smoothing(self, current_speaker: Optional[str], current_confidence: float,
                      probabilities: Dict[str, float]) -> Tuple[Optional[str], float]:
        """Apply temporal smoothing based on probabilities"""
        # FIX: Initialize smoothed variables to current values to prevent UnboundLocalError
        smoothed_speaker = current_speaker
        smoothed_confidence = current_confidence

        if not probabilities:
            return smoothed_speaker, smoothed_confidence
        
        best_speaker = max(probabilities, key=probabilities.get)
        best_probability = probabilities[best_speaker]
        
        if best_probability > 0.5:
            if current_speaker == best_speaker:
                smoothed_confidence = min(1.0, current_confidence + 0.1)
            else:
                if best_probability > 0.8:
                    smoothed_speaker = best_speaker
                    smoothed_confidence = best_probability
        
        return smoothed_speaker, smoothed_confidence
    
    def check_temporal_constraints(self, proposed_speaker: Optional[str],
                                 current_time: float) -> Tuple[bool, str]:
        """
        Check if speaker switch respects temporal constraints
        
        Args:
            proposed_speaker: Proposed new speaker
            current_time: Current timestamp
            
        Returns:
            Tuple of (is_valid, reason)
        """
        if not self.speaker_turns:
            return True, "First speaker"
        
        if self.recent_contexts and self.recent_contexts[-1].speaker_id != proposed_speaker:
            time_since_last_switch = current_time - self.last_speaker_switch
            if time_since_last_switch < self.min_turn_duration:
                return False, "Too frequent switching"
            self.last_speaker_switch = current_time
        
        recent_switches = [
            turn for turn in self.speaker_turns
            if current_time - turn.end_time < self.max_context_seconds
        ]
        
        if self.max_context_seconds > 0:
            switch_frequency = len(recent_switches) / self.max_context_seconds
            if switch_frequency > self.max_switch_frequency:
                return False, "Switch frequency too high"
        
        return True, "Valid switch"
    
    def detect_speaker_turns(self) -> List[SpeakerTurn]:
        """
        Detect speaker turns from historical contexts
        
        Returns:
            List of detected speaker turns
        """
        if not self.recent_contexts:
            return []
        
        turns = []
        current_turn_speaker = None
        turn_start_time = None
        
        sorted_contexts = sorted(self.recent_contexts, key=lambda x: x.timestamp)
        
        for context in sorted_contexts:
            if context.speaker_id is None:
                continue
            
            if current_turn_speaker is None:
                current_turn_speaker = context.speaker_id
                turn_start_time = context.timestamp
            elif context.speaker_id != current_turn_speaker:
                turn_end_time = context.timestamp
                turn_duration = turn_end_time - turn_start_time
                
                if turn_duration >= self.min_turn_duration:
                    turn_contexts = [c for c in sorted_contexts if turn_start_time <= c.timestamp < turn_end_time and c.speaker_id == current_turn_speaker]
                    avg_confidence = np.mean([c.confidence for c in turn_contexts]) if turn_contexts else 0.0
                    
                    turns.append(SpeakerTurn(
                        speaker_id=current_turn_speaker,
                        start_time=turn_start_time,
                        end_time=turn_end_time,
                        confidence=avg_confidence,
                        turn_duration=turn_duration
                    ))
                
                current_turn_speaker = context.speaker_id
                turn_start_time = context.timestamp
        
        if current_turn_speaker and turn_start_time:
            last_time = sorted_contexts[-1].timestamp
            turn_duration = last_time - turn_start_time
            if turn_duration >= self.min_turn_duration:
                turn_contexts = [c for c in sorted_contexts if turn_start_time <= c.timestamp and c.speaker_id == current_turn_speaker]
                avg_confidence = np.mean([c.confidence for c in turn_contexts]) if turn_contexts else 0.0
                turns.append(SpeakerTurn(
                    speaker_id=current_turn_speaker,
                    start_time=turn_start_time,
                    end_time=last_time,
                    confidence=avg_confidence,
                    turn_duration=turn_duration
                ))
        
        self.speaker_turns.extend(turns)
        return turns
    
    def calculate_transition_probabilities(self) -> Dict[Tuple[str, str], float]:
        """
        Calculate transition probabilities between speakers
        
        Returns:
            Dict mapping (from_speaker, to_speaker) to probability
        """
        if not self.transition_matrix:
            return {}
        
        speaker_totals = defaultdict(int)
        for (from_speaker, _), count in self.transition_matrix.items():
            speaker_totals[from_speaker] += count
        
        probabilities = {}
        for (from_speaker, to_speaker), count in self.transition_matrix.items():
            if speaker_totals[from_speaker] > 0:
                probabilities[(from_speaker, to_speaker)] = count / speaker_totals[from_speaker]
        
        return probabilities
    
    def get_speaker_dominance(self) -> Dict[str, float]:
        """
        Calculate speaker dominance in recent context
        
        Returns:
            Dict mapping speaker_id to dominance score
        """
        if not self.recent_contexts:
            return {}
        
        speaker_durations = defaultdict(float)
        total_duration = 0.0
        
        for context in self.recent_contexts:
            if context.speaker_id:
                speaker_durations[context.speaker_id] += context.segment_duration
            total_duration += context.segment_duration
        
        if total_duration > 0:
            return {speaker: duration / total_duration for speaker, duration in speaker_durations.items()}
        return {}
    
    def detect_inconsistencies(self) -> List[Dict[str, any]]:
        """
        Detect temporal inconsistencies in speaker assignments
        
        Returns:
            List of inconsistency reports
        """
        inconsistencies = []
        if len(self.recent_contexts) < 2:
            return inconsistencies
        
        for i in range(1, len(self.recent_contexts)):
            prev_context = self.recent_contexts[i-1]
            curr_context = self.recent_contexts[i]
            
            if (prev_context.speaker_id and curr_context.speaker_id and
                prev_context.speaker_id != curr_context.speaker_id):
                
                time_diff = curr_context.timestamp - prev_context.timestamp
                if time_diff < self.min_turn_duration:
                    inconsistencies.append({
                        'type': 'rapid_switch', 'time_diff': time_diff,
                        'from_speaker': prev_context.speaker_id, 'to_speaker': curr_context.speaker_id,
                        'timestamp': curr_context.timestamp
                    })
        
        low_confidence_threshold = 0.5
        for context in self.recent_contexts:
            if context.confidence < low_confidence_threshold:
                inconsistencies.append({
                    'type': 'low_confidence', 'speaker': context.speaker_id,
                    'confidence': context.confidence, 'timestamp': context.timestamp
                })
        
        return inconsistencies
    
    def get_context_summary(self) -> Dict[str, any]:
        """Get summary of current temporal context"""
        if not self.recent_contexts:
            return {'total_contexts': 0, 'unique_speakers': 0, 'dominant_speaker': None, 'average_confidence': 0.0, 'inconsistencies': 0}
        
        unique_speakers = {ctx.speaker_id for ctx in self.recent_contexts if ctx.speaker_id}
        dominance = self.get_speaker_dominance()
        dominant_speaker = max(dominance, key=dominance.get) if dominance else None
        avg_confidence = np.mean([ctx.confidence for ctx in self.recent_contexts])
        
        return {
            'total_contexts': len(self.recent_contexts),
            'unique_speakers': len(unique_speakers),
            'dominant_speaker': dominant_speaker,
            'average_confidence': float(avg_confidence),
            'inconsistencies': len(self.detect_inconsistencies()),
            'context_checks': self.context_checks,
            'consistency_corrections': self.consistency_corrections
        }
    
    def reset_context(self):
        """Reset temporal context (for testing/debugging)"""
        self.recent_contexts.clear()
        self.speaker_turns.clear()
        self.speaker_history.clear()
        self.transition_matrix.clear()
        self.speaker_frequencies.clear()
        self.context_checks = 0
        self.consistency_corrections = 0



================================================
FILE: tiktok_service.py
================================================
"""
TikTok Service - Wrapper for TikTok API functionality.

This module provides a service layer for interacting with TikTok's API
to fetch user information, videos, and download content.
"""

from typing import Optional, List, Dict, Any
import asyncio
import os
import logging
from datetime import datetime
import json
import yt_dlp
from pprint import pformat
import aiofiles

logger = logging.getLogger(__name__)


class TikTokService:
    """Service for interacting with TikTok API."""
    
    def __init__(self):
        """
        Initialize TikTok service with yt-dlp.
        """
        self.ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': False,  # We want full extraction
            'force_generic_extractor': False,
            'ignoreerrors': True,
            'no_color': True,
            'no_check_certificate': True,
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        }
    
    def _extract_info_with_retry(self, url: str, ydl_opts: dict, max_retries: int = 3):
        """Extract info with retry logic and exponential backoff."""
        retry_delay = 1  # Start with 1 second delay
        
        for attempt in range(max_retries):
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                try:
                    info = ydl.extract_info(url, download=False)
                    if info:
                        return info
                except Exception as e:
                    error_msg = str(e).lower()
                    logger.error(f"yt-dlp extraction error (attempt {attempt + 1}/{max_retries}): {str(e)}")
                    
                    # Don't retry for certain permanent errors
                    if any(term in error_msg for term in [
                        'video unavailable', 'private', 'deleted', 'not found',
                        'does not exist', 'removed', 'blocked'
                    ]):
                        logger.warning(f"Permanent error detected, not retrying: {str(e)}")
                        return None
                    
                    # If this is the last attempt, return None
                    if attempt == max_retries - 1:
                        return None
                    
                    # Wait before retrying with exponential backoff
                    import time
                    time.sleep(retry_delay * (2 ** attempt))
        
        return None
        
    def _extract_user_info_from_data(self, info_dict: Dict[str, Any]) -> Dict[str, Any]:
        """Extract user information from yt-dlp info."""
        # Get username from title field first (most reliable for TikTok)
        username = info_dict.get('title', '')
        
        # Get uploader info from the data - check multiple possible fields
        uploader = (info_dict.get('uploader') or 
                   info_dict.get('channel') or 
                   info_dict.get('creator') or
                   info_dict.get('playlist_uploader') or 
                   username)  # Use title as fallback
        
        uploader_id = (info_dict.get('uploader_id') or 
                      info_dict.get('channel_id') or
                      info_dict.get('playlist_uploader_id') or 
                      info_dict.get('id', ''))
        
        # Try to get profile URL
        uploader_url = (info_dict.get('uploader_url') or 
                       info_dict.get('channel_url') or
                       info_dict.get('webpage_url', ''))
        
        # For user pages, use playlist_count for accurate video count
        entries = info_dict.get('entries', [])
        playlist_count = info_dict.get('playlist_count')
        
        # Ensure video_count is always a number, never None
        if playlist_count is not None and isinstance(playlist_count, (int, float)):
            video_count = int(playlist_count)
        else:
            video_count = len(entries) if entries else 0
        
        # Try to extract avatar from various sources
        avatar = (info_dict.get('uploader_thumbnail') or 
                 info_dict.get('channel_thumbnail') or
                 info_dict.get('thumbnail') or '')
        
        # Initialize follower count
        follower_count = 0
        
        # If no avatar from main dict, try first video entry
        if not avatar and entries and len(entries) > 0:
            first_entry = entries[0]
            if isinstance(first_entry, dict):
                # Try to get uploader thumbnail from video - check multiple possible fields
                possible_avatar_fields = [
                    'uploader_thumbnail',
                    'channel_thumbnail', 
                    'uploader_avatar',
                    'channel_avatar',
                    'author_thumbnail',
                    'creator_thumbnail'
                ]
                
                for field in possible_avatar_fields:
                    if first_entry.get(field):
                        avatar = first_entry[field]
                        logger.info(f"Found avatar in field: {field}")
                        break
                
                # If still no avatar, try thumbnail as last resort
                if not avatar and first_entry.get('thumbnail'):
                    avatar = first_entry['thumbnail']
                
                # Also update uploader info if not found
                if not uploader and first_entry.get('uploader'):
                    uploader = first_entry.get('uploader')
                if not uploader_id and first_entry.get('uploader_id'):
                    uploader_id = first_entry.get('uploader_id')
                    
                # Try to get follower count from video entry
                if 'channel_follower_count' in first_entry:
                    follower_count = first_entry.get('channel_follower_count', 0)
                elif 'follower_count' in first_entry:
                    follower_count = first_entry.get('follower_count', 0)
        
        # Try to get follower count from main dict if not found in entries
        follower_count = (info_dict.get('channel_follower_count') or
                         info_dict.get('follower_count') or 0)        
        # Extract description/bio
        description = info_dict.get('description', '')
        if not description and entries and len(entries) > 0:
            # Try to get from playlist description
            description = info_dict.get('playlist_description', '')
        
        # Generate fallback avatar if none found
        if not avatar and uploader:
            # Use ui-avatars.com as fallback
            avatar = f"https://ui-avatars.com/api/?name={uploader}&size=512&background=FF0050&color=ffffff&bold=true"
        
        # Log what we extracted
        logger.info(f"Extracted user info - uploader: {uploader}, avatar: {avatar[:50]}...")
        
        # Extract nickname - for TikTok, often the display name is different from username
        nickname = uploader or username
        # If we have entries, check if the channel name from videos is different
        if entries and len(entries) > 0 and isinstance(entries[0], dict):
            video_channel = entries[0].get('channel', '')
            if video_channel and video_channel != username:
                nickname = video_channel
        
        return {
            'username': username or uploader or 'unknown',
            'userId': uploader_id or 'unknown',
            'secUid': uploader_id or 'unknown',  # TikTok uses secUid, we'll use uploader_id
            'nickname': nickname or 'Unknown User',
            'avatar': avatar,
            'signature': description[:200] if description else '',
            'verified': bool(info_dict.get('verified', False)),
            'followerCount': int(follower_count) if follower_count else 0,
            'followingCount': 0,  # Not available via yt-dlp
            'videoCount': int(video_count) if video_count else 0,
            'heartCount': 0,  # Not available via yt-dlp
            'privateAccount': False,

        }
    
    async def get_user_info(self, username: str) -> Dict[str, Any]:
        """
        Fetch user information from TikTok using yt-dlp.
        
        Args:
            username: TikTok username (without @)
            
        Returns:
            Dictionary containing user information
            
        Raises:
            Exception: If user not found or API error
        """
        try:
            # Extract username from URL if provided
            if username.startswith('http'):
                # Extract username from URL like https://www.tiktok.com/@zachking or https://www.tiktok.com/zachking
                import re
                match = re.search(r'tiktok\.com/@?([^/?]+)', username)
                if match:
                    username = match.group(1)
                else:
                    raise ValueError(f"Invalid TikTok URL: {username}")
            
            # Clean username
            username = username.replace('@', '')
            
            # TikTok user URL
            url = f'https://www.tiktok.com/@{username}'
            
            # Create yt-dlp instance with options
            ydl_opts = {
                **self.ydl_opts,
                'extract_flat': False,  # Get full data for first video to extract user info
                'playlistend': 25,  # Get more videos for user info
                'quiet': False,  # Show output for debugging
            }            
            # Run extraction in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            
            def extract_info():
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    try:
                        info = ydl.extract_info(url, download=False)
                        return info
                    except Exception as e:
                        logger.error(f"yt-dlp extraction error: {str(e)}")
                        return None
                        
            info_dict = await loop.run_in_executor(None, extract_info)
            
            if not info_dict:
                raise Exception(f"Could not fetch user info for {username}")
            
            # Log the full info_dict to understand available fields
            logger.info(f"\n{'='*50}")
            logger.info(f"TikTok User Info Dict for @{username}:")
            logger.info(f"{'='*50}")
            
            # Log all top-level keys
            logger.info(f"Available keys: {list(info_dict.keys())}")
            
            # Log specific fields we're interested in
            fields_to_log = [
                'uploader', 'uploader_id', 'uploader_url', 'uploader_thumbnail',
                'channel', 'channel_id', 'channel_url', 'channel_thumbnail',
                'thumbnail', 'thumbnails', 'creator', 'channel_follower_count', 'follower_count',
                'description', 'title', 'webpage_url', 'playlist_title', 'playlist_uploader',
                'playlist_uploader_id', 'entries'
            ]
            
            for field in fields_to_log:
                if field in info_dict:
                    value = info_dict[field]
                    if field == 'entries' and isinstance(value, list):
                        logger.info(f"{field}: {len(value)} entries")
                        if value and len(value) > 0:
                            logger.info(f"First entry keys: {list(value[0].keys()) if isinstance(value[0], dict) else 'Not a dict'}")
                            # Log specific fields from first entry
                            if isinstance(value[0], dict):
                                first_entry = value[0]
                                entry_fields = ['uploader', 'channel', 'uploader_id', 'channel_id', 
                                              'uploader_url', 'channel_url', 'uploader_thumbnail',
                                              'channel_thumbnail', 'uploader_avatar', 'channel_avatar',
                                              'author_thumbnail', 'creator_thumbnail',
                                              'channel_follower_count', 'follower_count']
                                for ef in entry_fields:
                                    if ef in first_entry:
                                        logger.info(f"  First entry {ef}: {first_entry[ef]}")
                    elif isinstance(value, (dict, list)) and len(str(value)) > 200:
                        logger.info(f"{field}: {type(value).__name__} with {len(value)} items")
                    else:
                        logger.info(f"{field}: {value}")
            
            # Also log playlist_count if available
            if 'playlist_count' in info_dict:
                logger.info(f"playlist_count: {info_dict['playlist_count']}")
            
            logger.info(f"{'='*50}\n")
            
            # Extract user information from the data
            user_info = self._extract_user_info_from_data(info_dict)
            
            # Update username to match input
            user_info['username'] = username
            
            return user_info
            
        except Exception as e:
            logger.error(f"Error fetching user info for {username}: {str(e)}")
            raise Exception(f"Failed to fetch user info: {str(e)}")
    
    async def get_user_videos(
        self, 
        username: str, 
        count: int = 6,  # Default 6 for cloning, up to 25 for bulk processing
        cursor: int = 0
    ) -> Dict[str, Any]:
        """
        Fetch user's videos from TikTok using yt-dlp.
        
        Args:
            username: TikTok username
            count: Number of videos to fetch (default 6, max 25 for bulk processing)
            cursor: Pagination cursor (not used with yt-dlp)
            
        Returns:
            Dictionary containing videos and pagination info
        """
        try:
            # Extract username from URL if provided
            if username.startswith('http'):
                # Extract username from URL like https://www.tiktok.com/@zachking or https://www.tiktok.com/zachking
                import re
                match = re.search(r'tiktok\.com/@?([^/?]+)', username)
                if match:
                    username = match.group(1)
                else:
                    raise ValueError(f"Invalid TikTok URL: {username}")
            
            # Clean username
            username = username.replace('@', '')
            
            # Limit count to 25 for bulk processing (increased from 6 for cloning)
            count = min(count, 25)
            
            # TikTok user URL
            url = f'https://www.tiktok.com/@{username}'
            
            # Create yt-dlp instance with options
            ydl_opts = {
                **self.ydl_opts,
                'extract_flat': False,  # We want full video extraction
                'playlist_items': f'1-{count}',  # Limit to first N videos
                'quiet': False,  # Enable debug output
            }            
            # Run extraction in thread pool to avoid blocking with retry logic
            loop = asyncio.get_event_loop()
            info_dict = await loop.run_in_executor(None, self._extract_info_with_retry, url, ydl_opts)
            
            if not info_dict:
                raise Exception(f"Could not fetch videos for {username}")
            
            # Log video extraction info
            logger.info(f"\n{'='*50}")
            logger.info(f"TikTok Videos Info Dict for @{username}:")
            logger.info(f"Available keys: {list(info_dict.keys())[:20]}...")
            if 'entries' in info_dict and info_dict['entries']:
                logger.info(f"Found {len(info_dict['entries'])} video entries")
                first_video = info_dict['entries'][0]
                if isinstance(first_video, dict):
                    logger.info(f"First video keys: {list(first_video.keys())[:15]}...")
                    # Log uploader info from video
                    if 'uploader' in first_video:
                        logger.info(f"Video uploader: {first_video['uploader']}")
                    if 'uploader_thumbnail' in first_video:
                        logger.info(f"Uploader thumbnail: {first_video['uploader_thumbnail']}")
            logger.info(f"{'='*50}\n")
            
            videos_data = []
            
            # Process entries (videos)
            for entry in info_dict.get('entries', [])[:count]:
                if not entry:
                    continue
                    
                # Extract video information
                video_info = {
                    "videoId": entry.get('id', ''),
                    "title": entry.get('title', '') or entry.get('description', ''),
                    "createTime": entry.get('timestamp', 0),
                    "duration": entry.get('duration', 0),
                    "thumbnail": entry.get('thumbnail') or (entry.get('thumbnails', [{}])[0].get('url') if entry.get('thumbnails') else ''),
                    "dynamicCover": entry.get('thumbnail') or (entry.get('thumbnails', [{}])[0].get('url') if entry.get('thumbnails') else ''),                    "playAddr": entry.get('url', '') or entry.get('webpage_url', ''),
                    "downloadAddr": entry.get('url', ''),
                    "stats": {
                        "views": entry.get('view_count', 0),
                        "likes": entry.get('like_count', 0),
                        "comments": entry.get('comment_count', 0),
                        "shares": entry.get('repost_count', 0),
                        "saves": 0  # Not provided by yt-dlp
                    },
                    "music": {
                        "id": entry.get('track', ''),
                        "title": entry.get('track', 'Original Sound'),
                        "author": entry.get('artist', '') or entry.get('uploader', ''),
                        "original": True
                    },
                    "hashtags": self._extract_hashtags(entry.get('description', ''))
                }
                
                videos_data.append(video_info)
            
            return {
                "videos": videos_data,
                "count": len(videos_data),
                "hasMore": False,  # We're limiting to 6 videos
                "cursor": cursor + len(videos_data)
            }
            
        except Exception as e:
            logger.error(f"Error fetching videos for {username}: {str(e)}")
            # Don't re-raise if it's a specific video ID error - allow bulk processing to continue
            if "Could not fetch videos for" in str(e) and username.isdigit():
                logger.warning(f"Single video fetch failed for ID {username}, this may be due to video unavailability")
                return {
                    "videos": [],
                    "count": 0,
                    "hasMore": False,
                    "cursor": cursor,
                    "error": f"Video {username} is not available"
                }
            raise Exception(f"Failed to fetch videos: {str(e)}")
    
    def _extract_hashtags(self, description: str) -> List[Dict[str, str]]:
        """Extract hashtags from video description."""
        import re
        hashtags = []
        if description:
            # Find all hashtags in the description
            tags = re.findall(r'#(\w+)', description)
            for tag in tags[:5]:  # Limit to first 5 hashtags
                hashtags.append({
                    "id": tag.lower(),
                    "name": tag,
                    "title": tag
                })
        return hashtags
        
    async def get_video_info(self, video_id: str) -> Dict[str, Any]:
        """
        Fetch detailed information about a specific video using yt-dlp.
        
        Args:
            video_id: TikTok video ID
            
        Returns:
            Dictionary containing video information
        """
        try:
            # TikTok video URL
            url = f'https://www.tiktok.com/@_/video/{video_id}'
            
            # Run extraction in thread pool
            loop = asyncio.get_event_loop()
            
            def extract_info():
                with yt_dlp.YoutubeDL(self.ydl_opts) as ydl:
                    try:
                        info = ydl.extract_info(url, download=False)
                        return info
                    except Exception as e:
                        logger.error(f"yt-dlp extraction error: {str(e)}")
                        return None
                        
            info_dict = await loop.run_in_executor(None, extract_info)
            
            if not info_dict:
                raise Exception(f"Could not fetch video info for {video_id}")
            
            # Extract video information
            return {
                "videoId": info_dict.get('id', video_id),
                "title": info_dict.get('title', '') or info_dict.get('description', ''),
                "createTime": info_dict.get('timestamp', 0),
                "duration": info_dict.get('duration', 0),
                "thumbnail": info_dict.get('thumbnail', ''),
                "stats": {
                    "views": info_dict.get('view_count', 0),
                    "likes": info_dict.get('like_count', 0),
                    "comments": info_dict.get('comment_count', 0),
                    "shares": info_dict.get('repost_count', 0),
                    "saves": 0
                }
            }
            
        except Exception as e:
            logger.error(f"Error fetching video info for {video_id}: {str(e)}")
            raise Exception(f"Failed to fetch video info: {str(e)}")
    
    async def get_video_preview(self, video_id: str) -> Dict[str, Any]:
        """
        Get video preview information including streaming URL without downloading.
        
        Args:
            video_id: TikTok video ID
            
        Returns:
            Dictionary containing video preview data with streaming URL
        """
        try:
            # TikTok video URL
            url = f'https://www.tiktok.com/@_/video/{video_id}'
            
            # Create yt-dlp instance with options for extraction only
            ydl_opts = {
                **self.ydl_opts,
                'format': 'best[ext=mp4]/best',  # Prefer MP4 format
                'quiet': True,
                'no_warnings': True,
            }
            
            # Run extraction in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            
            def extract_info():
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    try:
                        info = ydl.extract_info(url, download=False)
                        return info
                    except Exception as e:
                        logger.error(f"yt-dlp extraction error: {str(e)}")
                        return None
                        
            info_dict = await loop.run_in_executor(None, extract_info)
            
            if not info_dict:
                raise Exception(f"Could not fetch video info for {video_id}")
            
            # Extract preview information
            # Use proxy URL instead of direct TikTok URL to avoid CORS issues
            preview_data = {
                "videoId": video_id,
                "title": info_dict.get('title', '') or info_dict.get('description', ''),
                "description": info_dict.get('description', ''),
                "duration": info_dict.get('duration', 0),
                "thumbnail": info_dict.get('thumbnail', ''),
                "streamUrl": f"{os.getenv('BACKEND_BASE_URL', 'http://localhost:8001')}/api/public/tiktok/stream/{video_id}",  # Use absolute stream endpoint
                "fallbackUrl": f"{os.getenv('BACKEND_BASE_URL', 'http://localhost:8001')}/api/public/tiktok/download/{video_id}",  # Absolute fallback download endpoint                "directUrl": info_dict.get('url', ''),  # Keep direct URL for backend use
                "format": info_dict.get('ext', 'mp4'),
                "width": info_dict.get('width', 0),
                "height": info_dict.get('height', 0),
                "uploader": info_dict.get('uploader', ''),
                "uploaderId": info_dict.get('uploader_id', ''),
                "stats": {
                    "views": info_dict.get('view_count', 0),
                    "likes": info_dict.get('like_count', 0),
                    "comments": info_dict.get('comment_count', 0),
                    "shares": info_dict.get('repost_count', 0),
                },
                "timestamp": info_dict.get('timestamp', 0),
                "hashtags": self._extract_hashtags(info_dict.get('description', ''))
            }
            
            logger.info(f"Successfully extracted preview for video {video_id}")
            return preview_data
            
        except Exception as e:
            logger.error(f"Error getting video preview for {video_id}: {str(e)}")
            raise Exception(f"Failed to get video preview: {str(e)}")
    
    async def download_video_bytes(self, video_id: str) -> bytes:
        """
        Download video bytes from TikTok using yt-dlp.
        
        Args:
            video_id: TikTok video ID
            
        Returns:
            Video bytes
        """
        try:
            # TikTok video URL
            url = f'https://www.tiktok.com/@_/video/{video_id}'
            
            # Create temporary file for download
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_file:
                tmp_path = tmp_file.name
            
            # Configure yt-dlp for downloading
            ydl_opts = {
                **self.ydl_opts,
                'outtmpl': tmp_path,
                'format': 'best[ext=mp4]/best',
                'verbose': True,  # Enable verbose logging to debug issues
                'overwrites': True,  # Force overwrite existing files
                'no_overwrites': False,
                'continuedl': False,  # Don't continue partial downloads
            }
            
            # Run download in thread pool
            loop = asyncio.get_event_loop()
            
            def download_video():
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    try:
                        logger.info(f"Starting download for URL: {url}")
                        result = ydl.download([url])
                        logger.info(f"Download result: {result}")
                        
                        # Check if the file was created
                        if os.path.exists(tmp_path):
                            size = os.path.getsize(tmp_path)
                            logger.info(f"Downloaded file exists at {tmp_path}, size: {size} bytes")
                        else:
                            logger.error(f"Downloaded file not found at {tmp_path}")
                            
                        return True
                    except Exception as e:
                        error_msg = str(e)
                        logger.error(f"yt-dlp download error for {url}: {error_msg}")
                        logger.error(f"Error type: {type(e).__name__}")
                        
                        # Check for DNS resolution errors
                        if "Failed to resolve" in error_msg or "Temporary failure in name resolution" in error_msg:
                            logger.error(f"DNS resolution failed for video {video_id}. The video URL may be expired or region-locked.")
                        
                        return False
                        
            success = await loop.run_in_executor(None, download_video)
            
            if not success:
                raise Exception(f"Could not download video {video_id}")
            
            # Read the downloaded file
            try:
                # Check if file exists and has content
                if not os.path.exists(tmp_path):
                    raise Exception(f"Downloaded file not found at {tmp_path}")
                
                file_size = os.path.getsize(tmp_path)
                if file_size == 0:
                    raise Exception(f"Downloaded file is empty (0 bytes)")
                
                logger.info(f"Downloaded video {video_id}, size: {file_size} bytes")
                
                async with aiofiles.open(tmp_path, 'rb') as f:
                    video_bytes = await f.read()
                
                if len(video_bytes) == 0:
                    raise Exception(f"Failed to read video bytes from file")
                    
                return video_bytes
            finally:
                # Clean up temporary file
                if os.path.exists(tmp_path):
                    os.unlink(tmp_path)
            
        except Exception as e:
            logger.error(f"Error downloading video {video_id}: {str(e)}")
            raise Exception(f"Failed to download video: {str(e)}")
    
    async def download_video_preview(self, video_id: str, duration_limit: int = 15) -> bytes:
        """
        Download a preview of the video (limited duration) using yt-dlp.
        
        Args:
            video_id: TikTok video ID
            duration_limit: Maximum duration in seconds (default 15)
            
        Returns:
            Video preview bytes
        """
        try:
            # For now, use the full download method
            # In a production environment, you would use ffmpeg to trim the video
            # or use yt-dlp's download_ranges option when it's available for TikTok
            
            # Download the full video first
            video_bytes = await self.download_video_bytes(video_id)
            
            # TODO: Implement video trimming using ffmpeg to limit duration
            # For now, return the full video (TikTok videos are usually short anyway)
            logger.info(f"Downloaded preview for video {video_id}, size: {len(video_bytes)} bytes")
            
            return video_bytes
            
        except Exception as e:
            logger.error(f"Error downloading video preview {video_id}: {str(e)}")
            raise Exception(f"Failed to download video preview: {str(e)}")
    
    async def get_video_stream_url(self, video_id: str) -> str:
        """
        Get the direct streaming URL for a video.
        Used by the streaming proxy endpoint.
        
        Args:
            video_id: TikTok video ID
            
        Returns:
            Direct video URL for streaming
        """
        try:
            # TikTok video URL
            url = f'https://www.tiktok.com/@_/video/{video_id}'
            
            # Use minimal options for faster extraction
            ydl_opts = {
                **self.ydl_opts,
                'format': 'best[ext=mp4]/best',
                'quiet': True,
                'no_warnings': True,
            }
            
            # Run extraction in thread pool
            loop = asyncio.get_event_loop()
            
            def extract_info():
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    try:
                        info = ydl.extract_info(url, download=False)
                        return info
                    except Exception as e:
                        logger.error(f"yt-dlp extraction error: {str(e)}")
                        return None
                        
            info_dict = await loop.run_in_executor(None, extract_info)
            
            if not info_dict:
                raise Exception(f"Could not fetch video info for {video_id}")
            
            # Return the direct URL
            stream_url = info_dict.get('url', '')
            if not stream_url:
                raise Exception(f"No stream URL found for video {video_id}")
                
            return stream_url
            
        except Exception as e:
            logger.error(f"Error getting video stream URL for {video_id}: {str(e)}")
            raise Exception(f"Failed to get video stream URL: {str(e)}")
    
    async def download_audio_bytes(self, video_id: str, format: str = 'mp3') -> bytes:
        """
        Download audio only from TikTok video using yt-dlp.
        
        Args:
            video_id: TikTok video ID
            format: Audio format (mp3, m4a, etc.)
            
        Returns:
            Audio bytes
        """
        try:
            # TikTok video URL
            url = f'https://www.tiktok.com/@_/video/{video_id}'
            
            # Create temporary file for download
            import tempfile
            with tempfile.NamedTemporaryFile(suffix=f'.{format}', delete=False) as tmp_file:
                tmp_path = tmp_file.name
            
            # Configure yt-dlp for audio extraction
            ydl_opts = {
                **self.ydl_opts,
                'format': 'bestaudio/best',  # Download best audio quality
                'outtmpl': tmp_path.replace(f'.{format}', '.%(ext)s'),  # Let yt-dlp handle extension
                'verbose': True,
                'overwrites': True,
                'no_overwrites': False,
                'continuedl': False,
                # Post-processor for audio extraction if ffmpeg is available
                'postprocessors': [{
                    'key': 'FFmpegExtractAudio',
                    'preferredcodec': format,
                    'preferredquality': '192',
                }] if format != 'original' else [],
                'prefer_ffmpeg': True,
            }
            
            # Try with audio extraction first, fallback to video download
            loop = asyncio.get_event_loop()
            
            def download_audio():
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    try:
                        logger.info(f"Starting audio download for URL: {url}")
                        result = ydl.download([url])
                        logger.info(f"Audio download result: {result}")
                        
                        # Find the downloaded file (might have different extension)
                        import glob
                        pattern = tmp_path.replace(f'.{format}', '.*')
                        files = glob.glob(pattern)
                        if files:
                            actual_path = files[0]
                            logger.info(f"Downloaded audio file: {actual_path}")
                            return actual_path
                        else:
                            logger.error(f"No files found matching pattern: {pattern}")
                            return None
                    except Exception as e:
                        logger.error(f"yt-dlp audio download error: {str(e)}")
                        return None
                        
            downloaded_path = await loop.run_in_executor(None, download_audio)
            
            if not downloaded_path:
                # Fallback: Download video and return as is
                logger.warning(f"Audio extraction failed, downloading full video for {video_id}")
                video_bytes = await self.download_video_bytes(video_id)
                return video_bytes
            
            # Read the downloaded file
            try:
                if not os.path.exists(downloaded_path):
                    raise Exception(f"Downloaded file not found at {downloaded_path}")
                
                file_size = os.path.getsize(downloaded_path)
                if file_size == 0:
                    raise Exception(f"Downloaded file is empty (0 bytes)")
                
                logger.info(f"Downloaded audio {video_id}, size: {file_size} bytes")
                
                async with aiofiles.open(downloaded_path, 'rb') as f:
                    audio_bytes = await f.read()
                
                if len(audio_bytes) == 0:
                    raise Exception(f"Failed to read audio bytes from file")
                    
                return audio_bytes
            finally:
                # Clean up temporary file
                if os.path.exists(downloaded_path):
                    os.unlink(downloaded_path)
                # Also clean up base path if different
                if os.path.exists(tmp_path) and tmp_path != downloaded_path:
                    os.unlink(tmp_path)
            
        except Exception as e:
            logger.error(f"Error downloading audio {video_id}: {str(e)}")
            raise Exception(f"Failed to download audio: {str(e)}")
    
    async def get_audio_info(self, video_id: str) -> Dict[str, Any]:
        """
        Get audio stream information for a video.
        
        Args:
            video_id: TikTok video ID
            
        Returns:
            Dictionary containing audio stream information
        """
        try:
            # TikTok video URL
            url = f'https://www.tiktok.com/@_/video/{video_id}'
            
            # Extract info without downloading
            loop = asyncio.get_event_loop()
            
            def extract_info():
                with yt_dlp.YoutubeDL(self.ydl_opts) as ydl:
                    try:
                        info = ydl.extract_info(url, download=False)
                        return info
                    except Exception as e:
                        logger.error(f"yt-dlp extraction error: {str(e)}")
                        return None
                        
            info_dict = await loop.run_in_executor(None, extract_info)
            
            if not info_dict:
                raise Exception(f"Could not fetch audio info for {video_id}")
            
            # Extract audio-specific information
            audio_info = {
                "videoId": video_id,
                "duration": info_dict.get('duration', 0),
                "hasAudio": True,  # TikTok videos always have audio
                "audioCodec": info_dict.get('acodec', 'unknown'),
                "audioBitrate": info_dict.get('abr', 0),
                "audioSampleRate": info_dict.get('asr', 0),
                "format": info_dict.get('ext', 'mp4'),
                "filesize": info_dict.get('filesize', 0),
            }
            
            # Try to find audio-only formats
            formats = info_dict.get('formats', [])
            audio_formats = [f for f in formats if f.get('vcodec') == 'none' and f.get('acodec') != 'none']
            
            if audio_formats:
                # Use best audio-only format
                best_audio = max(audio_formats, key=lambda f: f.get('abr', 0) or 0)
                audio_info.update({
                    "audioOnlyUrl": best_audio.get('url'),
                    "audioOnlyFormat": best_audio.get('ext', 'unknown'),
                    "audioOnlySize": best_audio.get('filesize', 0),
                })
            
            return audio_info
            
        except Exception as e:
            logger.error(f"Error getting audio info for {video_id}: {str(e)}")
            raise Exception(f"Failed to get audio info: {str(e)}")
    
    async def close(self):
        """Close the service (no cleanup needed for yt-dlp)."""
        pass


# Singleton instance
_tiktok_service: Optional[TikTokService] = None


def get_tiktok_service() -> TikTokService:
    """
    Get or create TikTok service instance.
        
    Returns:
        TikTokService instance
    """
    global _tiktok_service
    
    if _tiktok_service is None:
        _tiktok_service = TikTokService()
    
    return _tiktok_service


================================================
FILE: tts_manager.py
================================================
"""
Unified TTS Manager

Provides a unified interface for all TTS operations across different
environments (development/production) and providers (Chatterbox, ElevenLabs, etc.)
"""

import os
import logging
from typing import Dict, Any, Optional
from datetime import datetime
import asyncio

from src.services.chatterbox_client import ChatterboxClient
from src.services.voice_clone_jobs import VoiceCloneJobManager
from src.services.audio_preparation_service import audio_preparation_service

logger = logging.getLogger(__name__)


class TTSManager:
    """Unified interface for all TTS operations"""
    
    def __init__(self):
        """Initialize TTS Manager with environment-specific configuration"""
        self.environment = os.getenv("ENVIRONMENT", "development")
        self.mode = os.getenv("CHATTERBOX_MODE", "local")
        
        # Initialize clients
        self.chatterbox_client = ChatterboxClient()
        self.job_manager = VoiceCloneJobManager()
        
        # Provider configuration
        self.providers = {
            "chatterbox": {
                "name": "Chatterbox",
                "client": self.chatterbox_client,
                "supports_streaming": True,
                "supports_cloning": True,
                "gpu_required": True,
            },
            # Future providers can be added here
            # "elevenlabs": {...},
            # "kokoro": {...},
        }
        
        logger.info(f"TTS Manager initialized - Environment: {self.environment}, Mode: {self.mode}")
    
    def get_provider(self, provider_name: str = "chatterbox"):
        """Get provider configuration"""
        if provider_name not in self.providers:
            raise ValueError(f"Unknown provider: {provider_name}")
        return self.providers[provider_name]
    
    async def process_voice_clone(
        self,
        job_data: Dict[str, Any],
        provider: str = "chatterbox"
    ) -> Dict[str, Any]:
        """
        Process voice cloning request based on environment
        
        Args:
            job_data: Job data including audio path, user ID, etc.
            provider: TTS provider to use
            
        Returns:
            Job result with voice ID and status
        """
        provider_config = self.get_provider(provider)
        
        if self.environment == "development":
            # Direct processing with local GPU
            logger.info(f"Processing voice clone locally (development mode)")
            return await self._process_locally(job_data, provider_config)
        else:
            # Queue for remote processing
            logger.info(f"Queueing voice clone for remote processing (production mode)")
            return await self._queue_remote_job(job_data, provider_config)
    
    async def _process_locally(
        self,
        job_data: Dict[str, Any],
        provider_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Process voice cloning locally using CUDA"""
        try:
            # Create job in Convex
            job_id = await self.job_manager.create_job(
                audio_path=job_data["audio_path"],
                user_id=job_data.get("user_id"),
                voice_name=job_data.get("voice_name", "My Voice"),
                sample_text=job_data.get("sample_text"),
                settings=job_data.get("settings", {})
            )
            
            # Update job status to processing
            await self.job_manager.update_job_status(job_id, "processing", {
                "startedAt": datetime.utcnow().timestamp() * 1000,
                "workerInfo": {
                    "environment": "development",
                    "gpuType": "cuda"
                }
            })
            
            # Process with local Chatterbox
            start_time = datetime.utcnow()
            
            # Check if audio preparation is requested
            preparation_config = job_data.get("preparation_config")
            audio_path = job_data["audio_path"]
            
            if preparation_config and preparation_config.get("use_whisper", False):
                logger.info(f"Preparing audio with Whisper for job {job_id}")
                
                # Prepare audio using the preparation service
                provider_name = provider_config["name"].lower()
                prepared_data = await audio_preparation_service.prepare_audio(
                    audio_path=audio_path,
                    provider=provider_name,
                    config=preparation_config
                )
                
                # Update job data with preparation results
                audio_path = prepared_data["prepared_audio_path"]
                
                # Store transcription and metadata in job update
                await self.job_manager.update_job_status(job_id, "processing", {
                    "transcription": prepared_data.get("transcription"),
                    "audioSegments": len(prepared_data.get("segments", [])),
                    "preparationMetadata": prepared_data.get("metadata", {})
                })
                
                logger.info(f"Audio prepared: {len(prepared_data.get('segments', []))} segments, "
                           f"transcription length: {len(prepared_data.get('transcription', ''))}")
            
            # Read audio file (prepared or original)
            with open(audio_path, 'rb') as f:
                audio_data = f.read()
            
            # Generate cloned voice sample
            client = provider_config["client"]
            
            # Extract and transform settings from camelCase to snake_case
            settings = job_data.get("settings", {})
            chatterbox_params = {
                "exaggeration": settings.get("exaggeration", 1.0),
                "cfg_weight": settings.get("cfgWeight", 1.7),
                "chunk_size": settings.get("chunkSize", 2048)
            }
            
            cloned_audio = await client.generate_with_voice_cloning(
                text=job_data.get("sample_text", "Hello, this is my cloned voice."),
                voice_audio_data=audio_data,
                voice_filename=f"voice_{job_id}.mp3",
                **chatterbox_params
            )
            
            # Calculate processing time
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            
            # Generate voice ID
            import uuid
            voice_id = f"voice_{uuid.uuid4().hex[:12]}"
            
            # Save result (in production, this would upload to S3)
            result_path = f"/tmp/cloned_{voice_id}.mp3"
            with open(result_path, 'wb') as f:
                f.write(cloned_audio)
            
            # Update job completion
            await self.job_manager.update_job_status(job_id, "completed", {
                "completedAt": datetime.utcnow().timestamp() * 1000,
                "processingTime": processing_time,
                "voiceId": voice_id,
                "resultUrl": result_path
            })
            
            # Return result
            return {
                "jobId": job_id,
                "status": "completed",
                "voiceId": voice_id,
                "resultUrl": result_path,
                "processingTime": processing_time,
                "message": "Voice cloning completed successfully"
            }
            
        except Exception as e:
            logger.error(f"Error processing voice clone locally: {str(e)}")
            
            # Update job failure
            if 'job_id' in locals():
                await self.job_manager.update_job_status(job_id, "failed", {
                    "completedAt": datetime.utcnow().timestamp() * 1000,
                    "error": str(e),
                    "errorDetails": {
                        "code": "LOCAL_PROCESSING_ERROR",
                        "message": str(e)
                    }
                })
            
            raise
    
    async def _queue_remote_job(
        self,
        job_data: Dict[str, Any],
        provider_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Queue job for remote GPU processing"""
        try:
            # Create job in Convex
            job_id = await self.job_manager.create_job(
                audio_path=job_data["audio_path"],
                user_id=job_data.get("user_id"),
                voice_name=job_data.get("voice_name", "My Voice"),
                sample_text=job_data.get("sample_text"),
                settings=job_data.get("settings", {})
            )
            
            # In production, this would:
            # 1. Upload audio file to S3/Spaces
            # 2. Trigger GPU droplet if not running
            # 3. Return immediately with job ID
            
            # For now, just return queued status
            return {
                "jobId": job_id,
                "status": "queued",
                "message": "Voice cloning job queued for processing"
            }
            
        except Exception as e:
            logger.error(f"Error queueing remote job: {str(e)}")
            raise
    
    async def generate_speech(
        self,
        text: str,
        voice_id: str,
        provider: str = "chatterbox",
        stream: bool = False,
        **kwargs
    ):
        """
        Generate speech from text
        
        Args:
            text: Text to convert to speech
            voice_id: Voice ID to use
            provider: TTS provider
            stream: Whether to stream the response
            **kwargs: Additional provider-specific parameters
            
        Returns:
            Audio data or stream
        """
        provider_config = self.get_provider(provider)
        client = provider_config["client"]
        
        if stream and provider_config["supports_streaming"]:
            # Stream response
            async for chunk in client.generate_speech_stream(text, voice_id, **kwargs):
                yield chunk
        else:
            # Return complete audio
            audio_data = await client.generate_speech(text, voice_id, **kwargs)
            yield audio_data
    
    async def list_voices(self, provider: str = "chatterbox") -> list:
        """List available voices for a provider"""
        provider_config = self.get_provider(provider)
        client = provider_config["client"]
        
        if hasattr(client, 'list_voices'):
            return await client.list_voices()
        else:
            return []
    
    async def health_check(self, provider: str = "chatterbox") -> Dict[str, Any]:
        """Check health of TTS provider"""
        provider_config = self.get_provider(provider)
        client = provider_config["client"]
        
        if hasattr(client, 'health_check'):
            return await client.health_check()
        else:
            return {"status": "unknown", "provider": provider}


================================================
FILE: tts_service.py
================================================
# File: services/tts_service.py

import asyncio
from typing import Optional, Callable
from models.tts.chatterbox_model import ChatterboxModel
from models.tts.fishspeech_model import FishSpeechModel
from models.prosody.prosody_encoder import ProsodyEncoder

class TTSService:
    def __init__(self, model_name: str = "chatterbox", device: str = "cuda"):
        """
        Text-to-Speech Service with optional voice cloning.
        model_name: which TTS model backend to use ("chatterbox", "fishspeech", etc.)
        """
        self.model_name = model_name.lower()
        self.device = device
        self.model = None
        self.prosody_encoder = None

    async def ensure_model_loaded(self, progress_callback: Optional[Callable] = None):
        """Asynchronously load the TTS model (and prosody encoder) if not already loaded."""
        if self.model is not None:
            return  # already loaded
        if progress_callback:
            progress_callback(f"Loading TTS model: {self.model_name} ...")
        # Load the requested TTS model
        if self.model_name == "chatterbox":
            self.model = ChatterboxModel(device=self.device)
        elif self.model_name == "fishspeech":
            self.model = FishSpeechModel(device=self.device)
        else:
            raise ValueError(f"Unknown TTS model: {self.model_name}")
        # Initialize the ProsodyEncoder
        self.prosody_encoder = ProsodyEncoder(use_pretrained=True, device=self.device)
        if progress_callback:
            progress_callback("Models loaded successfully.")

    async def generate_speech(self, text: str, voice_sample_path: Optional[str] = None, 
                               exaggeration: float = 1.0, cfg_weight: float = 1.0) -> str:
        """
        Generate speech audio for the given text. If voice_sample_path is provided, clones that voice.
        Returns the path to a generated audio WAV file.
        """
        await self.ensure_model_loaded()
        adjusted_exaggeration = exaggeration
        adjusted_cfg = cfg_weight
        if voice_sample_path:
            # Analyze the reference audio prosody (offload to thread to avoid blocking event loop)
            loop = asyncio.get_event_loop()
            prosody_feats = await loop.run_in_executor(None, self.prosody_encoder.extract_features, voice_sample_path)
            if prosody_feats:
                pitch_std = prosody_feats.get("pitch_std", 0.0)
                voiced_ratio = prosody_feats.get("voiced_ratio", 0.0)
                # If reference has very flat prosody (low pitch variance), increase exaggeration a bit for liveliness
                if pitch_std < 5.0:
                    adjusted_exaggeration = min(1.3, exaggeration * 1.1)  # up to 30% more
                # If reference speaking style is extremely fast or highly expressive, we might reduce CFG weight slightly
                if voiced_ratio > 0.9 and pitch_std > 20:
                    adjusted_cfg = min(cfg_weight, 0.8)
                # (Above thresholds are heuristic; e.g., a very high voiced_ratio ~0.95 with huge pitch_std might indicate shouting or very energetic speech)
        else:
            prosody_feats = None

        # Generate audio with the selected model
        if self.model_name == "chatterbox":
            # Chatterbox uses internal voice conversion with prompt
            audio_tensor = await asyncio.get_event_loop().run_in_executor(
                None, 
                self.model.generate, 
                text, 
                voice_sample_path, 
                adjusted_exaggeration, 
                adjusted_cfg
            )
        else:
            # FishSpeech or others will use our wrapper (which may call ProsodyEncoder inside)
            audio_tensor = await asyncio.get_event_loop().run_in_executor(
                None,
                self.model.generate,
                text,
                voice_sample_path
            )
        # Save the waveform to a temporary WAV file
        import tempfile, torchaudio
        tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        torchaudio.save(tmp_file.name, audio_tensor.unsqueeze(0), self.model.sample_rate)
        return tmp_file.name

    async def label_prosody(self, audio_path: str) -> dict:
        """
        Analyze an audio file and return its prosodic features and estimated emotion.
        """
        await self.ensure_model_loaded()
        loop = asyncio.get_event_loop()
        features = await loop.run_in_executor(None, self.prosody_encoder.extract_features, audio_path)
        if features and "prosody_embedding" in features:
            features["estimated_emotion"] = self.prosody_encoder.estimate_emotion(features["prosody_embedding"])
        return features




================================================
FILE: twitch_service.py
================================================
"""
Twitch Service - Wrapper for Twitch API functionality using yt-dlp.

This module provides a service layer for interacting with Twitch
to fetch channel information, videos, and download content.
"""

from typing import Optional, List, Dict, Any
import asyncio
import os
import logging
from datetime import datetime
import json
import yt_dlp
from pprint import pformat

logger = logging.getLogger(__name__)


class TwitchService:
    """Service for interacting with Twitch using yt-dlp."""
    
    def __init__(self):
        """
        Initialize Twitch service with yt-dlp.
        """
        self.ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': False,
            'force_generic_extractor': False,
            'ignoreerrors': True,
            'no_color': True,
            'no_check_certificate': True,
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        }
        
    def _extract_channel_info_from_data(self, info_dict: Dict[str, Any]) -> Dict[str, Any]:
        """Extract channel information from yt-dlp info."""
        # Get username and basic info
        username = info_dict.get('uploader_id') or info_dict.get('channel_id') or ''
        uploader = info_dict.get('uploader') or info_dict.get('channel') or username
        
        # Extract from URL if needed
        webpage_url = info_dict.get('webpage_url', '')
        if not username and 'twitch.tv' in webpage_url:
            import re
            match = re.search(r'twitch\.tv/([^/?]+)', webpage_url)
            if match:
                username = match.group(1)
        
        # Get description/bio
        description = info_dict.get('description', '')
        
        # Get thumbnail/avatar
        thumbnail = info_dict.get('thumbnail') or ''
        
        # Try to get follower count and video count from entries
        entries = info_dict.get('entries', [])
        follower_count = 0
        video_count = len(entries) if entries else 0
        
        # If we have entries, try to extract more info from first video
        if entries and len(entries) > 0:
            first_entry = entries[0]
            if isinstance(first_entry, dict):
                # Try to get uploader info from video
                if not uploader and first_entry.get('uploader'):
                    uploader = first_entry.get('uploader')
                if not username and first_entry.get('uploader_id'):
                    username = first_entry.get('uploader_id')
                if not thumbnail and first_entry.get('uploader_thumbnail'):
                    thumbnail = first_entry.get('uploader_thumbnail')
                    
                # Try to get follower count if available
                if 'channel_follower_count' in first_entry:
                    follower_count = first_entry.get('channel_follower_count', 0)
        
        # Get live status - check if channel is currently live
        is_live = info_dict.get('is_live', False)
        
        # Generate fallback avatar if none found
        if not thumbnail and (uploader or username):
            display_name = uploader or username
            thumbnail = f"https://ui-avatars.com/api/?name={display_name}&size=512&background=9146FF&color=ffffff&bold=true"
        
        logger.info(f"Extracted Twitch channel info - username: {username}, name: {uploader}")
        
        return {
            'username': username or 'unknown',
            'displayName': uploader or username or 'Unknown Channel',
            'profileImage': thumbnail,
            'bio': description[:200] if description else '',
            'isVerified': False,  # Not available via yt-dlp
            'isPartner': False,  # Not available via yt-dlp
            'followerCount': follower_count,
            'videoCount': video_count,
            'isLive': is_live,
            'channelUrl': webpage_url or f"https://www.twitch.tv/{username}"
        }
    
    async def get_channel_info(self, username: str) -> Dict[str, Any]:
        """
        Fetch channel information from Twitch using yt-dlp.
        
        Args:
            username: Twitch username or channel URL
            
        Returns:
            Dictionary containing channel information
            
        Raises:
            Exception: If channel not found or API error
        """
        try:
            # Extract username from URL if provided
            if username.startswith('http'):
                import re
                match = re.search(r'twitch\.tv/([^/?]+)', username)
                if match:
                    username = match.group(1)
                else:
                    raise ValueError(f"Invalid Twitch URL: {username}")
            
            # Clean username
            username = username.strip().lower()
            
            # Twitch channel URL - use videos page to get channel info
            url = f'https://www.twitch.tv/{username}/videos'
            
            logger.info(f"Fetching Twitch channel info for: {username}")
            
            # Create yt-dlp instance with options
            ydl_opts = {
                **self.ydl_opts,
                'extract_flat': 'in_playlist',  # Get list of videos without full extraction
                'playlistend': 12,  # Get some videos to extract channel info
            }
            
            # Run extraction in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            
            def extract_info():
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    try:
                        info = ydl.extract_info(url, download=False)
                        return info
                    except Exception as e:
                        logger.error(f"yt-dlp extraction error: {str(e)}")
                        return None
                        
            info_dict = await loop.run_in_executor(None, extract_info)
            
            if not info_dict:
                raise Exception(f"Could not fetch channel info for {username}")
            
            # Log available data
            logger.info(f"Twitch channel data keys: {list(info_dict.keys())}")
            
            # Extract channel information from the data
            channel_info = self._extract_channel_info_from_data(info_dict)
            
            # Ensure username matches input
            channel_info['username'] = username
            
            return channel_info
            
        except Exception as e:
            logger.error(f"Error fetching Twitch channel info for {username}: {str(e)}")
            raise Exception(f"Failed to fetch channel info: {str(e)}")
    
    async def get_channel_videos(
        self, 
        username: str, 
        count: int = 6,
        video_type: str = "archive"  # archive (VODs), highlight, upload, past_premiere
    ) -> Dict[str, Any]:
        """
        Fetch channel's videos from Twitch using yt-dlp.
        
        Args:
            username: Twitch username
            count: Number of videos to fetch (default 6, max 6)
            video_type: Type of videos to fetch (archive/highlight/upload)
            
        Returns:
            Dictionary containing videos and pagination info
        """
        try:
            # Clean username
            username = username.strip().lower()
            
            # Limit count to 6
            count = min(count, 6)
            
            # Twitch videos URL with filter
            if video_type == "clips":
                url = f'https://www.twitch.tv/{username}/clips'
            else:
                url = f'https://www.twitch.tv/{username}/videos?filter={video_type}'
            
            logger.info(f"Fetching Twitch videos for: {username}, type: {video_type}, count: {count}")
            
            # Create yt-dlp instance with options
            ydl_opts = {
                **self.ydl_opts,
                'extract_flat': False,  # We want full video extraction
                'playlist_items': f'1-{count}',  # Limit to first N videos
            }
            
            # Run extraction in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            
            def extract_info():
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    try:
                        info = ydl.extract_info(url, download=False)
                        return info
                    except Exception as e:
                        logger.error(f"yt-dlp extraction error: {str(e)}")
                        return None
                        
            info_dict = await loop.run_in_executor(None, extract_info)
            
            if not info_dict:
                raise Exception(f"Could not fetch videos for {username}")
            
            videos_data = []
            
            # Process entries (videos)
            entries = info_dict.get('entries', [])
            logger.info(f"Found {len(entries)} Twitch videos")
            
            for entry in entries[:count]:
                if not entry:
                    continue
                    
                # Determine video type
                if video_type == "clips":
                    v_type = "clip"
                elif 'highlight' in entry.get('title', '').lower():
                    v_type = "highlight"
                else:
                    v_type = "vod"
                
                # Extract video information
                video_info = {
                    "videoId": entry.get('id', ''),
                    "title": entry.get('title', ''),
                    "thumbnail": entry.get('thumbnail', ''),
                    "duration": entry.get('duration', 0),
                    "viewCount": entry.get('view_count', 0),
                    "createdAt": entry.get('timestamp', 0),
                    "url": entry.get('webpage_url', '') or entry.get('url', ''),
                    "type": v_type,
                    "game": entry.get('game', ''),  # Game/category if available
                    "language": entry.get('language', 'en'),
                    "description": entry.get('description', '')[:200] if entry.get('description') else ''
                }
                
                videos_data.append(video_info)
            
            return {
                "videos": videos_data,
                "count": len(videos_data),
                "videoType": video_type,
                "hasMore": len(entries) > count  # Simplified pagination
            }
            
        except Exception as e:
            logger.error(f"Error fetching videos for {username}: {str(e)}")
            raise Exception(f"Failed to fetch videos: {str(e)}")
        
    async def get_video_info(self, video_id: str) -> Dict[str, Any]:
        """
        Fetch detailed information about a specific video using yt-dlp.
        
        Args:
            video_id: Twitch video ID
            
        Returns:
            Dictionary containing video information
        """
        try:
            # Twitch video URL
            url = f'https://www.twitch.tv/videos/{video_id}'
            
            logger.info(f"Fetching Twitch video info for: {video_id}")
            
            # Run extraction in thread pool
            loop = asyncio.get_event_loop()
            
            def extract_info():
                with yt_dlp.YoutubeDL(self.ydl_opts) as ydl:
                    try:
                        info = ydl.extract_info(url, download=False)
                        return info
                    except Exception as e:
                        logger.error(f"yt-dlp extraction error: {str(e)}")
                        return None
                        
            info_dict = await loop.run_in_executor(None, extract_info)
            
            if not info_dict:
                raise Exception(f"Could not fetch video info for {video_id}")
            
            # Extract video information
            return {
                "videoId": info_dict.get('id', video_id),
                "title": info_dict.get('title', ''),
                "thumbnail": info_dict.get('thumbnail', ''),
                "duration": info_dict.get('duration', 0),
                "viewCount": info_dict.get('view_count', 0),
                "createdAt": info_dict.get('timestamp', 0),
                "uploader": info_dict.get('uploader', ''),
                "uploaderId": info_dict.get('uploader_id', ''),
                "game": info_dict.get('game', ''),
                "description": info_dict.get('description', ''),
                "url": info_dict.get('webpage_url', '')
            }
            
        except Exception as e:
            logger.error(f"Error fetching video info for {video_id}: {str(e)}")
            raise Exception(f"Failed to fetch video info: {str(e)}")
    
    async def download_video_bytes(self, video_id: str) -> bytes:
        """
        Download video bytes from Twitch using yt-dlp.
        
        Args:
            video_id: Twitch video ID
            
        Returns:
            Video bytes
        """
        try:
            # Twitch video URL
            url = f'https://www.twitch.tv/videos/{video_id}'
            
            logger.info(f"Download requested for Twitch video {video_id}")
            
            # For now, return a placeholder
            # In production, you would implement actual download using yt-dlp
            # with proper file handling and quality selection
            
            return f"Twitch video URL: {url}".encode()
            
        except Exception as e:
            logger.error(f"Error downloading video {video_id}: {str(e)}")
            raise Exception(f"Failed to download video: {str(e)}")
    
    async def close(self):
        """Close the service (no cleanup needed for yt-dlp)."""
        pass


# Singleton instance
_twitch_service: Optional[TwitchService] = None


def get_twitch_service() -> TwitchService:
    """
    Get or create Twitch service instance.
        
    Returns:
        TwitchService instance
    """
    global _twitch_service
    
    if _twitch_service is None:
        _twitch_service = TwitchService()
    
    return _twitch_service


================================================
FILE: vector_database_examples.py
================================================
"""
Vector Database Service Usage Examples

This module provides practical examples of how to use the VectorDatabaseService
for different scenarios and use cases.
"""

import asyncio
import json
from pathlib import Path
from typing import List, Dict, Any
import logging

from .vector_database_service import (
    VectorDatabaseService,
    VectorRecord,
    VectorExportConfig,
    VectorDatabaseProvider,
    ExportFormat,
    TaskType
)

logger = logging.getLogger(__name__)

class VectorDatabaseExamples:
    """Examples and utilities for vector database operations"""
    
    def __init__(self):
        self.service = VectorDatabaseService()
    
    async def example_1_basic_export(self):
        """
        Example 1: Basic vector export workflow
        
        This example shows how to:
        1. Create vector records from text
        2. Export to different database formats
        3. Generate import scripts
        """
        
        print("📚 Example 1: Basic Vector Export")
        
        # Sample documents
        documents = [
            "The quick brown fox jumps over the lazy dog",
            "Machine learning is a subset of artificial intelligence",
            "Vector databases enable efficient similarity search",
            "Python is a popular programming language for data science",
            "Natural language processing helps computers understand text"
        ]
        
        # Create export for Pinecone
        print("\n🔸 Creating Pinecone export...")
        pinecone_export = await self.service.create_sample_export(
            texts=documents,
            provider=VectorDatabaseProvider.PINECONE,
            format=ExportFormat.JSON,
            embedding_service="jina"
        )
        
        print(f"✅ Created {pinecone_export['record_count']} records")
        print(f"📊 Vector dimensions: {pinecone_export['metadata']['vector_statistics']['dimensions']}")
        
        # Show sample import script
        print("\n📜 Sample Pinecone import script:")
        print(pinecone_export['import_scripts']['python'][:500] + "...")
        
        return pinecone_export
    
    async def example_2_multi_provider_export(self):
        """
        Example 2: Export same data to multiple providers
        
        This example demonstrates how to export the same vector data
        to different database providers with appropriate formatting.
        """
        
        print("\n📚 Example 2: Multi-Provider Export")
        
        # Create sample vector records
        sample_records = []
        for i in range(10):
            record = VectorRecord(
                id=f"doc_{i:03d}",
                vector=[0.1 * j for j in range(768)],  # 768-dim vector
                text=f"Sample document {i} with content about topic {i % 3}",
                metadata={
                    "document_id": i,
                    "topic": f"topic_{i % 3}",
                    "length": 50 + i * 10,
                    "category": "sample"
                },
                source="example_dataset",
                model="example_model",
                task_type=TaskType.RETRIEVAL_DOCUMENT.value
            )
            sample_records.append(record)
        
        # Export to all providers
        providers = [
            VectorDatabaseProvider.PINECONE,
            VectorDatabaseProvider.CHROMADB,
            VectorDatabaseProvider.WEAVIATE
        ]
        
        exports = {}
        for provider in providers:
            print(f"\n🔸 Exporting to {provider.value}...")
            
            config = VectorExportConfig(
                provider=provider,
                format=ExportFormat.JSON,
                namespace=f"example_{provider.value}",
                batch_size=5
            )
            
            export_data = await self.service.prepare_vector_export(sample_records, config)
            exports[provider.value] = export_data
            
            print(f"✅ {provider.value}: {export_data['record_count']} records")
        
        # Compare export formats
        print("\n📊 Export Format Comparison:")
        for provider, export_data in exports.items():
            size_estimate = self.service.estimate_export_size(sample_records, ExportFormat.JSON)
            print(f"  {provider}: {size_estimate['size_mb']} MB estimated")
        
        return exports
    
    async def example_3_csv_parquet_export(self):
        """
        Example 3: Export to different file formats
        
        This example shows how to export vector data to CSV and Parquet
        formats for data analysis and storage efficiency.
        """
        
        print("\n📚 Example 3: CSV and Parquet Export")
        
        # Create sample records
        records = []
        for i in range(5):
            record = VectorRecord(
                id=f"item_{i}",
                vector=[float(j) for j in range(50)],  # Smaller vectors for demo
                text=f"Item {i} description",
                metadata={"category": "electronics", "price": 100 + i * 50},
                source="product_catalog"
            )
            records.append(record)
        
        # Export to different formats
        formats = [ExportFormat.JSON, ExportFormat.CSV, ExportFormat.PARQUET]
        
        for format in formats:
            print(f"\n🔸 Exporting to {format.value}...")
            
            config = VectorExportConfig(
                provider=VectorDatabaseProvider.CHROMADB,
                format=format,
                namespace="products",
                output_path=f"exports/products.{format.value}"
            )
            
            export_data = await self.service.prepare_vector_export(records, config)
            size_estimate = self.service.estimate_export_size(records, format)
            
            print(f"✅ {format.value}: {size_estimate['size_mb']} MB estimated")
            print(f"📁 Saved to: {config.output_path}")
    
    async def example_4_large_dataset_processing(self):
        """
        Example 4: Processing large datasets with batching
        
        This example demonstrates how to handle large datasets
        efficiently with proper batching and memory management.
        """
        
        print("\n📚 Example 4: Large Dataset Processing")
        
        # Simulate large dataset
        dataset_size = 1000
        batch_size = 100
        
        print(f"🔸 Processing {dataset_size} records in batches of {batch_size}...")
        
        # Process in batches to avoid memory issues
        all_records = []
        for batch_start in range(0, dataset_size, batch_size):
            batch_end = min(batch_start + batch_size, dataset_size)
            batch_records = []
            
            for i in range(batch_start, batch_end):
                record = VectorRecord(
                    id=f"large_doc_{i:06d}",
                    vector=[0.01 * j for j in range(1024)],  # 1024-dim
                    text=f"Large document {i} with extensive content...",
                    metadata={
                        "doc_id": i,
                        "batch": batch_start // batch_size,
                        "size": "large"
                    },
                    source="large_dataset"
                )
                batch_records.append(record)
            
            all_records.extend(batch_records)
            print(f"  📦 Processed batch {batch_start // batch_size + 1}: {len(batch_records)} records")
        
        # Export with optimized configuration
        config = VectorExportConfig(
            provider=VectorDatabaseProvider.WEAVIATE,
            format=ExportFormat.PARQUET,  # Most efficient for large datasets
            namespace="large_dataset",
            batch_size=batch_size,
            output_path="exports/large_dataset.parquet"
        )
        
        print(f"\n🔸 Exporting {len(all_records)} records to Weaviate...")
        export_data = await self.service.prepare_vector_export(all_records, config)
        
        # Show performance metrics
        size_estimate = self.service.estimate_export_size(all_records, ExportFormat.PARQUET)
        print(f"✅ Export completed:")
        print(f"  📊 Records: {export_data['record_count']}")
        print(f"  📏 Size: {size_estimate['size_mb']} MB")
        print(f"  ⏱️  Time: {size_estimate['estimated_time_seconds']} seconds")
        
        return export_data
    
    async def example_5_metadata_enrichment(self):
        """
        Example 5: Advanced metadata enrichment
        
        This example shows how to enrich vector records with
        additional metadata for better search and filtering.
        """
        
        print("\n📚 Example 5: Metadata Enrichment")
        
        # Create records with rich metadata
        records = []
        categories = ["technology", "science", "business", "health", "education"]
        
        for i in range(20):
            # Rich metadata
            metadata = {
                "category": categories[i % len(categories)],
                "subcategory": f"sub_{i % 3}",
                "importance": i % 5 + 1,
                "tags": [f"tag_{j}" for j in range(i % 3 + 1)],
                "author": f"author_{i % 4}",
                "created_date": f"2024-{(i % 12) + 1:02d}-{(i % 28) + 1:02d}",
                "word_count": 100 + i * 20,
                "language": "en",
                "sentiment": ["positive", "negative", "neutral"][i % 3],
                "confidence": 0.7 + (i % 3) * 0.1
            }
            
            record = VectorRecord(
                id=f"enriched_{i:03d}",
                vector=[0.02 * j for j in range(512)],
                text=f"Enriched document {i} about {metadata['category']}",
                metadata=metadata,
                source="enriched_dataset",
                model="enriched_model",
                task_type=TaskType.CLASSIFICATION.value
            )
            records.append(record)
        
        # Export with metadata focus
        config = VectorExportConfig(
            provider=VectorDatabaseProvider.CHROMADB,
            format=ExportFormat.JSON,
            namespace="enriched_data",
            include_metadata=True,
            include_text=True
        )
        
        export_data = await self.service.prepare_vector_export(records, config)
        
        print(f"✅ Enriched export created with {export_data['record_count']} records")
        print(f"📊 Metadata fields: {list(records[0].metadata.keys())}")
        
        # Show metadata statistics
        metadata_stats = export_data['metadata']['vector_statistics']
        print(f"📈 Statistics: {json.dumps(metadata_stats, indent=2)}")
        
        return export_data
    
    async def example_6_custom_validation(self):
        """
        Example 6: Custom validation and quality checks
        
        This example demonstrates advanced validation techniques
        for ensuring vector data quality.
        """
        
        print("\n📚 Example 6: Custom Validation")
        
        # Create test records with various quality issues
        test_records = [
            # Good records
            VectorRecord(id="good_1", vector=[0.1, 0.2, 0.3], text="Good record", metadata={"quality": "high"}),
            VectorRecord(id="good_2", vector=[0.4, 0.5, 0.6], text="Another good record", metadata={"quality": "high"}),
            
            # Problematic records
            VectorRecord(id="empty_vector", vector=[], text="Empty vector", metadata={"quality": "low"}),
            VectorRecord(id="nan_values", vector=[0.1, float('nan'), 0.3], text="NaN values", metadata={"quality": "low"}),
            VectorRecord(id="zero_vector", vector=[0.0, 0.0, 0.0], text="Zero vector", metadata={"quality": "medium"}),
            VectorRecord(id="large_values", vector=[1000.0, 2000.0, 3000.0], text="Large values", metadata={"quality": "medium"}),
        ]
        
        print(f"🔍 Validating {len(test_records)} records...")
        
        # Validate records
        valid_records = await self.service._validate_vector_data(test_records)
        
        print(f"✅ Valid records: {len(valid_records)}")
        print(f"❌ Invalid records: {len(test_records) - len(valid_records)}")
        
        # Show validation results
        for record in test_records:
            is_valid = record in valid_records
            status = "✅ VALID" if is_valid else "❌ INVALID"
            print(f"  {status}: {record.id} - {record.metadata['quality']}")
        
        return valid_records
    
    async def run_all_examples(self):
        """Run all examples in sequence"""
        
        print("🚀 Running All Vector Database Service Examples")
        print("=" * 60)
        
        # Create output directory
        Path("exports").mkdir(exist_ok=True)
        
        # Run examples
        examples = [
            self.example_1_basic_export,
            self.example_2_multi_provider_export,
            self.example_3_csv_parquet_export,
            self.example_4_large_dataset_processing,
            self.example_5_metadata_enrichment,
            self.example_6_custom_validation
        ]
        
        results = []
        for i, example in enumerate(examples, 1):
            try:
                print(f"\n{'='*60}")
                result = await example()
                results.append(result)
                print(f"✅ Example {i} completed successfully")
            except Exception as e:
                print(f"❌ Example {i} failed: {e}")
                results.append(None)
        
        print(f"\n🎉 All examples completed!")
        print(f"✅ Successful: {sum(1 for r in results if r is not None)}")
        print(f"❌ Failed: {sum(1 for r in results if r is None)}")
        
        return results

# Utility functions for common operations
class VectorDatabaseUtils:
    """Utility functions for vector database operations"""
    
    @staticmethod
    def create_mock_vectors(count: int, dimensions: int = 768) -> List[List[float]]:
        """Create mock vector data for testing"""
        import random
        
        vectors = []
        for i in range(count):
            # Create normalized random vector
            vector = [random.gauss(0, 1) for _ in range(dimensions)]
            # Normalize
            magnitude = sum(x**2 for x in vector)**0.5
            if magnitude > 0:
                vector = [x / magnitude for x in vector]
            vectors.append(vector)
        
        return vectors
    
    @staticmethod
    def analyze_vector_distribution(vectors: List[List[float]]) -> Dict[str, Any]:
        """Analyze the distribution of vector values"""
        if not vectors:
            return {}
        
        # Flatten all vectors
        all_values = [val for vector in vectors for val in vector]
        
        return {
            "count": len(all_values),
            "min": min(all_values),
            "max": max(all_values),
            "mean": sum(all_values) / len(all_values),
            "dimensions": len(vectors[0]) if vectors else 0,
            "vector_count": len(vectors)
        }
    
    @staticmethod
    def generate_test_metadata(index: int) -> Dict[str, Any]:
        """Generate realistic test metadata"""
        categories = ["tech", "science", "business", "arts", "sports"]
        authors = ["Alice", "Bob", "Charlie", "Diana", "Eve"]
        
        return {
            "id": index,
            "category": categories[index % len(categories)],
            "author": authors[index % len(authors)],
            "timestamp": f"2024-{(index % 12) + 1:02d}-{(index % 28) + 1:02d}",
            "word_count": 100 + index * 50,
            "importance": (index % 5) + 1,
            "tags": [f"tag_{i}" for i in range(index % 3 + 1)]
        }

# Main execution
async def main():
    """Main function to run examples"""
    
    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Run examples
    examples = VectorDatabaseExamples()
    await examples.run_all_examples()

if __name__ == "__main__":
    asyncio.run(main())


================================================
FILE: vector_database_service.py
================================================
"""
Vector Database Integration Service

Provides unified interface for exporting and importing vector embeddings
to/from major vector database providers (Pinecone, ChromaDB, Weaviate).
"""

import asyncio
import json
import csv
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from pathlib import Path
from datetime import datetime
import hashlib
import uuid
from enum import Enum
import pandas as pd
import numpy as np
from dataclasses import dataclass, asdict
from pydantic import BaseModel, Field

# Import existing embedding services
from .jina.embeddings_service import JinaEmbeddingsService
from .jina.models import JinaEmbeddingData
from .gemini.embeddings_service import GeminiEmbeddingsService
from .gemini.models import GeminiEmbeddingData

logger = logging.getLogger(__name__)

class VectorDatabaseProvider(str, Enum):
    """Supported vector database providers"""
    PINECONE = "pinecone"
    CHROMADB = "chromadb"
    WEAVIATE = "weaviate"

class ExportFormat(str, Enum):
    """Supported export formats"""
    JSON = "json"
    CSV = "csv"
    PARQUET = "parquet"
    VECTOR = "vector"

class TaskType(str, Enum):
    """Task types for optimization"""
    RETRIEVAL_DOCUMENT = "retrieval_document"
    RETRIEVAL_QUERY = "retrieval_query"
    CLASSIFICATION = "classification"
    CLUSTERING = "clustering"
    SEMANTIC_SIMILARITY = "semantic_similarity"

@dataclass
class VectorRecord:
    """Standardized vector record structure"""
    id: str
    vector: List[float]
    metadata: Dict[str, Any]
    text: Optional[str] = None
    timestamp: Optional[datetime] = None
    source: Optional[str] = None
    task_type: Optional[str] = None
    dimensions: Optional[int] = None
    model: Optional[str] = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.utcnow()
        if self.dimensions is None and self.vector:
            self.dimensions = len(self.vector)

class VectorExportConfig(BaseModel):
    """Configuration for vector export operations"""
    
    provider: VectorDatabaseProvider = Field(..., description="Target database provider")
    format: ExportFormat = Field(default=ExportFormat.JSON, description="Export format")
    namespace: Optional[str] = Field(default=None, description="Namespace/collection name")
    index_name: Optional[str] = Field(default=None, description="Index name")
    batch_size: int = Field(default=1000, description="Batch size for processing")
    include_metadata: bool = Field(default=True, description="Include metadata in export")
    include_text: bool = Field(default=True, description="Include original text")
    validate_vectors: bool = Field(default=True, description="Validate vector data")
    output_path: Optional[str] = Field(default=None, description="Output file path")
    
class VectorImportConfig(BaseModel):
    """Configuration for vector import operations"""
    
    provider: VectorDatabaseProvider = Field(..., description="Source database provider")
    format: ExportFormat = Field(default=ExportFormat.JSON, description="Import format")
    input_path: str = Field(..., description="Input file path")
    batch_size: int = Field(default=1000, description="Batch size for processing")
    validate_vectors: bool = Field(default=True, description="Validate vector data")
    create_index: bool = Field(default=False, description="Create index if not exists")
    overwrite_existing: bool = Field(default=False, description="Overwrite existing records")

class VectorDatabaseService:
    """
    Unified service for vector database operations and export/import
    """
    
    def __init__(self):
        """Initialize the vector database service"""
        logger.info("Initializing VectorDatabaseService...")
        self.jina_service = None
        self.gemini_service = None
        self._initialize_embedding_services()
        
    def _initialize_embedding_services(self):
        """Initialize embedding services"""
        try:
            self.jina_service = JinaEmbeddingsService()
            logger.info("Jina embeddings service initialized")
        except Exception as e:
            logger.warning(f"Failed to initialize Jina service: {e}")
            
        try:
            self.gemini_service = GeminiEmbeddingsService()
            logger.info("Gemini embeddings service initialized")
        except Exception as e:
            logger.warning(f"Failed to initialize Gemini service: {e}")
    
    async def prepare_vector_export(
        self,
        records: List[VectorRecord],
        config: VectorExportConfig
    ) -> Dict[str, Any]:
        """
        Prepare vector data for export to specified database provider
        
        Args:
            records: List of vector records to export
            config: Export configuration
            
        Returns:
            Dictionary with formatted export data
        """
        logger.info(f"Preparing vector export for {config.provider.value} in {config.format.value} format")
        
        # Validate vectors if requested
        if config.validate_vectors:
            records = await self._validate_vector_data(records)
            
        # Generate metadata
        metadata = self._generate_metadata(records, config)
        
        # Format for specific provider
        if config.provider == VectorDatabaseProvider.PINECONE:
            formatted_data = self._format_for_pinecone(records, config)
        elif config.provider == VectorDatabaseProvider.CHROMADB:
            formatted_data = self._format_for_chromadb(records, config)
        elif config.provider == VectorDatabaseProvider.WEAVIATE:
            formatted_data = self._format_for_weaviate(records, config)
        else:
            raise ValueError(f"Unsupported provider: {config.provider}")
            
        # Export in requested format
        export_data = await self._export_in_format(formatted_data, config)
        
        # Generate import scripts
        import_scripts = self._create_import_scripts(formatted_data, config)
        
        result = {
            "metadata": metadata,
            "export_data": export_data,
            "import_scripts": import_scripts,
            "config": config.dict(),
            "record_count": len(records)
        }
        
        # Save to file if path provided
        if config.output_path:
            await self._save_export_data(result, config)
            
        return result
    
    def _format_for_pinecone(self, records: List[VectorRecord], config: VectorExportConfig) -> Dict[str, Any]:
        """Format data for Pinecone database"""
        logger.info("Formatting data for Pinecone")
        
        vectors = []
        for record in records:
            vector_data = {
                "id": record.id,
                "values": record.vector,
                "metadata": record.metadata.copy() if config.include_metadata else {}
            }
            
            # Add text to metadata if included
            if config.include_text and record.text:
                vector_data["metadata"]["text"] = record.text
                
            # Add system metadata
            if config.include_metadata:
                vector_data["metadata"].update({
                    "timestamp": record.timestamp.isoformat() if record.timestamp else None,
                    "source": record.source,
                    "task_type": record.task_type,
                    "dimensions": record.dimensions,
                    "model": record.model
                })
                
            vectors.append(vector_data)
            
        return {
            "provider": "pinecone",
            "namespace": config.namespace or "default",
            "index_name": config.index_name or "embeddings",
            "vectors": vectors,
            "total_vectors": len(vectors)
        }
    
    def _format_for_chromadb(self, records: List[VectorRecord], config: VectorExportConfig) -> Dict[str, Any]:
        """Format data for ChromaDB"""
        logger.info("Formatting data for ChromaDB")
        
        ids = []
        embeddings = []
        metadatas = []
        documents = []
        
        for record in records:
            ids.append(record.id)
            embeddings.append(record.vector)
            
            # Prepare metadata
            metadata = record.metadata.copy() if config.include_metadata else {}
            if config.include_metadata:
                metadata.update({
                    "timestamp": record.timestamp.isoformat() if record.timestamp else None,
                    "source": record.source,
                    "task_type": record.task_type,
                    "dimensions": record.dimensions,
                    "model": record.model
                })
            metadatas.append(metadata)
            
            # Add document text
            documents.append(record.text if config.include_text else "")
            
        return {
            "provider": "chromadb",
            "collection_name": config.namespace or "embeddings",
            "ids": ids,
            "embeddings": embeddings,
            "metadatas": metadatas,
            "documents": documents,
            "total_vectors": len(ids)
        }
    
    def _format_for_weaviate(self, records: List[VectorRecord], config: VectorExportConfig) -> Dict[str, Any]:
        """Format data for Weaviate"""
        logger.info("Formatting data for Weaviate")
        
        objects = []
        for record in records:
            obj = {
                "id": record.id,
                "vector": record.vector,
                "properties": record.metadata.copy() if config.include_metadata else {}
            }
            
            # Add text property
            if config.include_text and record.text:
                obj["properties"]["text"] = record.text
                
            # Add system properties
            if config.include_metadata:
                obj["properties"].update({
                    "timestamp": record.timestamp.isoformat() if record.timestamp else None,
                    "source": record.source or "",
                    "task_type": record.task_type or "",
                    "dimensions": record.dimensions,
                    "model": record.model or ""
                })
                
            objects.append(obj)
            
        return {
            "provider": "weaviate",
            "class_name": config.namespace or "Embeddings",
            "objects": objects,
            "total_objects": len(objects),
            "schema": self._generate_weaviate_schema(records[0] if records else None)
        }
    
    def _generate_weaviate_schema(self, sample_record: Optional[VectorRecord]) -> Dict[str, Any]:
        """Generate Weaviate schema from sample record"""
        if not sample_record:
            return {}
            
        properties = {
            "text": {"dataType": ["text"]},
            "timestamp": {"dataType": ["date"]},
            "source": {"dataType": ["string"]},
            "task_type": {"dataType": ["string"]},
            "dimensions": {"dataType": ["int"]},
            "model": {"dataType": ["string"]}
        }
        
        # Add metadata properties
        for key, value in sample_record.metadata.items():
            if isinstance(value, str):
                properties[key] = {"dataType": ["string"]}
            elif isinstance(value, (int, float)):
                properties[key] = {"dataType": ["number"]}
            elif isinstance(value, bool):
                properties[key] = {"dataType": ["boolean"]}
            else:
                properties[key] = {"dataType": ["string"]}  # Default to string
                
        return {
            "class": "Embeddings",
            "properties": properties,
            "vectorizer": "none"  # We're providing our own vectors
        }
    
    async def _export_in_format(self, data: Dict[str, Any], config: VectorExportConfig) -> Any:
        """Export data in specified format"""
        logger.info(f"Exporting data in {config.format.value} format")
        
        if config.format == ExportFormat.JSON:
            return data
            
        elif config.format == ExportFormat.CSV:
            return self._export_to_csv(data, config)
            
        elif config.format == ExportFormat.PARQUET:
            return self._export_to_parquet(data, config)
            
        elif config.format == ExportFormat.VECTOR:
            return self._export_to_vector_format(data, config)
            
        else:
            raise ValueError(f"Unsupported format: {config.format}")
    
    def _export_to_csv(self, data: Dict[str, Any], config: VectorExportConfig) -> str:
        """Export data to CSV format"""
        logger.info("Converting to CSV format")
        
        if config.provider == VectorDatabaseProvider.PINECONE:
            rows = []
            for vector in data["vectors"]:
                row = {
                    "id": vector["id"],
                    "vector": json.dumps(vector["values"]),
                    "metadata": json.dumps(vector["metadata"])
                }
                rows.append(row)
                
        elif config.provider == VectorDatabaseProvider.CHROMADB:
            rows = []
            for i in range(len(data["ids"])):
                row = {
                    "id": data["ids"][i],
                    "vector": json.dumps(data["embeddings"][i]),
                    "metadata": json.dumps(data["metadatas"][i]),
                    "document": data["documents"][i]
                }
                rows.append(row)
                
        elif config.provider == VectorDatabaseProvider.WEAVIATE:
            rows = []
            for obj in data["objects"]:
                row = {
                    "id": obj["id"],
                    "vector": json.dumps(obj["vector"]),
                    "properties": json.dumps(obj["properties"])
                }
                rows.append(row)
                
        # Convert to CSV string
        if rows:
            import io
            output = io.StringIO()
            writer = csv.DictWriter(output, fieldnames=rows[0].keys())
            writer.writeheader()
            writer.writerows(rows)
            return output.getvalue()
        return ""
    
    def _export_to_parquet(self, data: Dict[str, Any], config: VectorExportConfig) -> bytes:
        """Export data to Parquet format"""
        logger.info("Converting to Parquet format")
        
        if config.provider == VectorDatabaseProvider.PINECONE:
            df_data = {
                "id": [v["id"] for v in data["vectors"]],
                "vector": [v["values"] for v in data["vectors"]],
                "metadata": [v["metadata"] for v in data["vectors"]]
            }
            
        elif config.provider == VectorDatabaseProvider.CHROMADB:
            df_data = {
                "id": data["ids"],
                "vector": data["embeddings"],
                "metadata": data["metadatas"],
                "document": data["documents"]
            }
            
        elif config.provider == VectorDatabaseProvider.WEAVIATE:
            df_data = {
                "id": [obj["id"] for obj in data["objects"]],
                "vector": [obj["vector"] for obj in data["objects"]],
                "properties": [obj["properties"] for obj in data["objects"]]
            }
            
        df = pd.DataFrame(df_data)
        
        # Convert to parquet bytes
        import io
        buffer = io.BytesIO()
        df.to_parquet(buffer, index=False)
        return buffer.getvalue()
    
    def _export_to_vector_format(self, data: Dict[str, Any], config: VectorExportConfig) -> Dict[str, Any]:
        """Export data in raw vector format for direct import"""
        logger.info("Converting to raw vector format")
        
        if config.provider == VectorDatabaseProvider.PINECONE:
            return {
                "format": "pinecone_bulk",
                "data": data["vectors"]
            }
            
        elif config.provider == VectorDatabaseProvider.CHROMADB:
            return {
                "format": "chromadb_bulk",
                "data": {
                    "ids": data["ids"],
                    "embeddings": data["embeddings"],
                    "metadatas": data["metadatas"],
                    "documents": data["documents"]
                }
            }
            
        elif config.provider == VectorDatabaseProvider.WEAVIATE:
            return {
                "format": "weaviate_bulk",
                "data": data["objects"]
            }
        
        return data
    
    def _generate_metadata(self, records: List[VectorRecord], config: VectorExportConfig) -> Dict[str, Any]:
        """Generate metadata for the export"""
        logger.info("Generating export metadata")
        
        # Calculate statistics
        dimensions = [len(record.vector) for record in records if record.vector]
        unique_sources = set(record.source for record in records if record.source)
        unique_models = set(record.model for record in records if record.model)
        
        return {
            "export_timestamp": datetime.utcnow().isoformat(),
            "provider": config.provider.value,
            "format": config.format.value,
            "total_records": len(records),
            "vector_statistics": {
                "dimensions": {
                    "min": min(dimensions) if dimensions else 0,
                    "max": max(dimensions) if dimensions else 0,
                    "avg": sum(dimensions) / len(dimensions) if dimensions else 0
                },
                "unique_sources": len(unique_sources),
                "unique_models": len(unique_models)
            },
            "sources": list(unique_sources),
            "models": list(unique_models),
            "config": config.dict()
        }
    
    async def _validate_vector_data(self, records: List[VectorRecord]) -> List[VectorRecord]:
        """Validate vector data quality"""
        logger.info(f"Validating {len(records)} vector records")
        
        valid_records = []
        for record in records:
            if self._is_valid_vector_record(record):
                valid_records.append(record)
            else:
                logger.warning(f"Invalid vector record: {record.id}")
                
        logger.info(f"Validated {len(valid_records)} out of {len(records)} records")
        return valid_records
    
    def _is_valid_vector_record(self, record: VectorRecord) -> bool:
        """Check if a vector record is valid"""
        if not record.id:
            return False
            
        if not record.vector or not isinstance(record.vector, list):
            return False
            
        if not all(isinstance(x, (int, float)) for x in record.vector):
            return False
            
        if len(record.vector) == 0:
            return False
            
        # Check for NaN or infinite values
        if any(np.isnan(x) or np.isinf(x) for x in record.vector):
            return False
            
        return True
    
    def _create_import_scripts(self, data: Dict[str, Any], config: VectorExportConfig) -> Dict[str, str]:
        """Generate import scripts for different database providers"""
        logger.info("Creating import scripts")
        
        scripts = {}
        
        if config.provider == VectorDatabaseProvider.PINECONE:
            scripts["python"] = self._create_pinecone_import_script(data, config)
            scripts["cli"] = self._create_pinecone_cli_script(data, config)
            
        elif config.provider == VectorDatabaseProvider.CHROMADB:
            scripts["python"] = self._create_chromadb_import_script(data, config)
            
        elif config.provider == VectorDatabaseProvider.WEAVIATE:
            scripts["python"] = self._create_weaviate_import_script(data, config)
            scripts["graphql"] = self._create_weaviate_graphql_script(data, config)
            
        return scripts
    
    def _create_pinecone_import_script(self, data: Dict[str, Any], config: VectorExportConfig) -> str:
        """Create Python import script for Pinecone"""
        return f'''
import pinecone
import json
from typing import List, Dict, Any

# Initialize Pinecone
pinecone.init(api_key="YOUR_API_KEY", environment="YOUR_ENVIRONMENT")

# Create or connect to index
index_name = "{data.get('index_name', 'embeddings')}"
namespace = "{data.get('namespace', 'default')}"

if index_name not in pinecone.list_indexes():
    pinecone.create_index(
        name=index_name,
        dimension={len(data['vectors'][0]['values']) if data['vectors'] else 1024},
        metric="cosine"
    )

index = pinecone.Index(index_name)

# Load and import vectors
def import_vectors(vectors: List[Dict[str, Any]], batch_size: int = {config.batch_size}):
    for i in range(0, len(vectors), batch_size):
        batch = vectors[i:i + batch_size]
        index.upsert(vectors=batch, namespace=namespace)
        print(f"Imported batch {{i//batch_size + 1}}: {{len(batch)}} vectors")

# Import the data
vectors = {json.dumps(data['vectors'], indent=2)}
import_vectors(vectors)
print(f"Successfully imported {{len(vectors)}} vectors to Pinecone")
'''
    
    def _create_pinecone_cli_script(self, data: Dict[str, Any], config: VectorExportConfig) -> str:
        """Create CLI script for Pinecone"""
        return f'''
# Pinecone CLI Import Script
# Save vectors to file first, then import

# 1. Save vectors to JSON file
echo '{json.dumps(data["vectors"])}' > pinecone_vectors.json

# 2. Create index (replace with your settings)
pinecone create-index {data.get('index_name', 'embeddings')} \\
    --dimension {len(data['vectors'][0]['values']) if data['vectors'] else 1024} \\
    --metric cosine

# 3. Import vectors
pinecone import {data.get('index_name', 'embeddings')} \\
    --namespace {data.get('namespace', 'default')} \\
    --file pinecone_vectors.json
'''
    
    def _create_chromadb_import_script(self, data: Dict[str, Any], config: VectorExportConfig) -> str:
        """Create Python import script for ChromaDB"""
        return f'''
import chromadb
import json
from typing import List, Dict, Any

# Initialize ChromaDB client
client = chromadb.Client()

# Create or get collection
collection_name = "{data.get('collection_name', 'embeddings')}"
collection = client.get_or_create_collection(name=collection_name)

# Load and import data
def import_embeddings(
    ids: List[str],
    embeddings: List[List[float]],
    metadatas: List[Dict[str, Any]],
    documents: List[str],
    batch_size: int = {config.batch_size}
):
    total = len(ids)
    for i in range(0, total, batch_size):
        batch_ids = ids[i:i + batch_size]
        batch_embeddings = embeddings[i:i + batch_size]
        batch_metadatas = metadatas[i:i + batch_size]
        batch_documents = documents[i:i + batch_size]
        
        collection.add(
            ids=batch_ids,
            embeddings=batch_embeddings,
            metadatas=batch_metadatas,
            documents=batch_documents
        )
        print(f"Imported batch {{i//batch_size + 1}}: {{len(batch_ids)}} vectors")

# Import the data
data = {json.dumps(data, indent=2)}
import_embeddings(
    ids=data["ids"],
    embeddings=data["embeddings"],
    metadatas=data["metadatas"],
    documents=data["documents"]
)
print(f"Successfully imported {{len(data['ids'])}} vectors to ChromaDB")
'''
    
    def _create_weaviate_import_script(self, data: Dict[str, Any], config: VectorExportConfig) -> str:
        """Create Python import script for Weaviate"""
        return f'''
import weaviate
import json
from typing import List, Dict, Any

# Initialize Weaviate client
client = weaviate.Client("http://localhost:8080")  # Replace with your Weaviate URL

# Create schema
class_name = "{data.get('class_name', 'Embeddings')}"
schema = {json.dumps(data.get('schema', {}), indent=2)}

# Create class if it doesn't exist
if not client.schema.exists(class_name):
    client.schema.create_class(schema)

# Import objects
def import_objects(objects: List[Dict[str, Any]], batch_size: int = {config.batch_size}):
    total = len(objects)
    for i in range(0, total, batch_size):
        batch = objects[i:i + batch_size]
        
        with client.batch as batch_client:
            batch_client.batch_size = batch_size
            for obj in batch:
                batch_client.add_data_object(
                    data_object=obj["properties"],
                    class_name=class_name,
                    uuid=obj["id"],
                    vector=obj["vector"]
                )
        print(f"Imported batch {{i//batch_size + 1}}: {{len(batch)}} objects")

# Import the data
objects = {json.dumps(data['objects'], indent=2)}
import_objects(objects)
print(f"Successfully imported {{len(objects)}} objects to Weaviate")
'''
    
    def _create_weaviate_graphql_script(self, data: Dict[str, Any], config: VectorExportConfig) -> str:
        """Create GraphQL script for Weaviate"""
        return f'''
# Weaviate GraphQL Import Script
# Use this for bulk import via GraphQL

mutation {{
  objects: [
    {json.dumps(data['objects'][:3], indent=4)}
    # ... add more objects
  ]
}}
'''
    
    async def _save_export_data(self, data: Dict[str, Any], config: VectorExportConfig):
        """Save export data to file"""
        output_path = Path(config.output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        if config.format == ExportFormat.JSON:
            with open(output_path, 'w') as f:
                json.dump(data, f, indent=2, default=str)
                
        elif config.format == ExportFormat.CSV:
            with open(output_path, 'w') as f:
                f.write(data['export_data'])
                
        elif config.format == ExportFormat.PARQUET:
            with open(output_path, 'wb') as f:
                f.write(data['export_data'])
                
        logger.info(f"Export data saved to {output_path}")
    
    async def create_sample_export(
        self,
        texts: List[str],
        provider: VectorDatabaseProvider,
        format: ExportFormat = ExportFormat.JSON,
        embedding_service: str = "jina"
    ) -> Dict[str, Any]:
        """
        Create a sample export with given texts
        
        Args:
            texts: List of texts to embed and export
            provider: Target database provider
            format: Export format
            embedding_service: Embedding service to use ('jina' or 'gemini')
            
        Returns:
            Export data
        """
        logger.info(f"Creating sample export with {len(texts)} texts")
        
        # Generate embeddings
        if embedding_service == "jina" and self.jina_service:
            embeddings = await self.jina_service.embed_documents(texts)
        elif embedding_service == "gemini" and self.gemini_service:
            embeddings = await self.gemini_service.embed_documents(texts)
        else:
            raise ValueError(f"Unsupported embedding service: {embedding_service}")
            
        # Create vector records
        records = []
        for i, (text, embedding) in enumerate(zip(texts, embeddings)):
            record = VectorRecord(
                id=str(uuid.uuid4()),
                vector=embedding,
                text=text,
                metadata={
                    "index": i,
                    "length": len(text),
                    "embedding_service": embedding_service
                },
                source="sample",
                model=embedding_service,
                task_type=TaskType.RETRIEVAL_DOCUMENT.value
            )
            records.append(record)
            
        # Create export configuration
        config = VectorExportConfig(
            provider=provider,
            format=format,
            namespace=f"sample_{embedding_service}",
            batch_size=100
        )
        
        # Prepare export
        return await self.prepare_vector_export(records, config)
    
    def get_provider_capabilities(self) -> Dict[str, Dict[str, Any]]:
        """Get capabilities of supported vector database providers"""
        return {
            "pinecone": {
                "name": "Pinecone",
                "description": "Managed vector database with high performance",
                "features": [
                    "Managed service", "High performance", "Scalable",
                    "Multiple indexes", "Namespaces", "Metadata filtering"
                ],
                "supported_formats": ["json", "csv", "parquet", "vector"],
                "max_dimensions": 40000,
                "max_metadata_size": "40KB",
                "distance_metrics": ["cosine", "euclidean", "dotproduct"]
            },
            "chromadb": {
                "name": "ChromaDB",
                "description": "Open-source embedding database",
                "features": [
                    "Open source", "Local or cloud", "SQL-like queries",
                    "Collections", "Metadata filtering", "Document storage"
                ],
                "supported_formats": ["json", "csv", "parquet", "vector"],
                "max_dimensions": "No limit",
                "max_metadata_size": "No limit",
                "distance_metrics": ["cosine", "euclidean", "ip"]
            },
            "weaviate": {
                "name": "Weaviate",
                "description": "GraphQL-based vector database",
                "features": [
                    "GraphQL API", "Schema-based", "Auto-vectorization",
                    "Hybrid search", "Multi-tenancy", "Modules"
                ],
                "supported_formats": ["json", "csv", "parquet", "vector"],
                "max_dimensions": "No limit",
                "max_metadata_size": "No limit",
                "distance_metrics": ["cosine", "euclidean", "dot", "manhattan"]
            }
        }
    

    def estimate_export_size(self, records: List[VectorRecord], format: ExportFormat) -> Dict[str, Any]:
        """Estimate export file size and processing time"""
        if not records:
            return {"size_bytes": 0, "estimated_time_seconds": 0}
        sample_record = records[0]
        # Use numpy to get accurate byte size if possible, else assume 8 bytes per float
        try:
            vector_size = np.array(sample_record.vector).nbytes
        except Exception:
            vector_size = len(sample_record.vector) * 8  # 8 bytes per float (Python default)
        metadata_size = len(json.dumps(sample_record.metadata))
        text_size = len(sample_record.text) if sample_record.text else 0
        base_size_per_record = vector_size + metadata_size + text_size
        if format == ExportFormat.JSON:
            # JSON has overhead for structure
            estimated_size = base_size_per_record * len(records) * 1.5
        elif format == ExportFormat.CSV:
            # CSV is more compact
            estimated_size = base_size_per_record * len(records) * 1.2
        elif format == ExportFormat.PARQUET:
            # Parquet is most compact
            estimated_size = base_size_per_record * len(records) * 0.8
        else:
            estimated_size = base_size_per_record * len(records)
        # Estimate processing time (rough approximation)
        estimated_time = len(records) / 10000  # 10K records per second
        return {
            "size_bytes": int(estimated_size),
            "size_mb": round(estimated_size / (1024 * 1024), 2),
        
            "estimated_time_seconds": round(estimated_time, 2),
            "records_count": len(records)
        }

# Factory function for compatibility with other modules
def get_vector_db_service() -> VectorDatabaseService:
    """
    Returns a new instance of VectorDatabaseService.
    You can enhance this to use a true singleton if needed.
    """
    return VectorDatabaseService()


================================================
FILE: vector_db_connectors.py
================================================
"""
Vector Database Connectors for Diala Voice Agent Platform

This module provides connectors for exporting vector embeddings to different vector databases:
- Pinecone: Cloud-native vector database
- ChromaDB: Open-source embeddings database  
- Weaviate: Vector search engine with GraphQL API

Each connector handles format-specific export logic, schema management, and import script generation.
"""

import json
import csv
import os
import logging
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import pandas as pd
import numpy as np
from enum import Enum

logger = logging.getLogger(__name__)


class VectorDBType(str, Enum):
    """Supported vector database types"""
    PINECONE = "pinecone"
    CHROMADB = "chromadb"
    WEAVIATE = "weaviate"


@dataclass
class VectorExportConfig:
    """Configuration for vector export operations"""
    output_directory: str
    batch_size: int = 1000
    include_metadata: bool = True
    compression: bool = True
    generate_import_script: bool = True
    validate_schema: bool = True


@dataclass
class VectorRecord:
    """Standardized vector record format"""
    id: str
    vector: List[float]
    metadata: Dict[str, Any]
    namespace: Optional[str] = None
    timestamp: Optional[datetime] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation"""
        return {
            "id": self.id,
            "vector": self.vector,
            "metadata": self.metadata,
            "namespace": self.namespace,
            "timestamp": self.timestamp.isoformat() if self.timestamp else None
        }


class BaseVectorConnector(ABC):
    """Abstract base class for vector database connectors"""
    
    def __init__(self, config: VectorExportConfig):
        self.config = config
        self.db_type = None
        self.output_dir = Path(config.output_directory)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    @abstractmethod
    def export_vectors(self, vectors: List[VectorRecord]) -> Dict[str, Any]:
        """Export vectors to database-specific format"""
        pass
    
    @abstractmethod
    def generate_import_script(self, export_info: Dict[str, Any]) -> str:
        """Generate import script for the target database"""
        pass
    
    @abstractmethod
    def validate_schema(self, vectors: List[VectorRecord]) -> bool:
        """Validate vector data against database schema requirements"""
        pass
    
    def test_connection(self) -> bool:
        """Test connection to vector database (override in subclasses)"""
        logger.warning(f"Connection test not implemented for {self.__class__.__name__}")
        return True


class PineconeConnector(BaseVectorConnector):
    """Pinecone vector database connector"""
    
    def __init__(self, config: VectorExportConfig):
        super().__init__(config)
        self.db_type = VectorDBType.PINECONE
        
    def export_vectors(self, vectors: List[VectorRecord]) -> Dict[str, Any]:
        """Export vectors to Pinecone JSON format"""
        logger.info(f"Exporting {len(vectors)} vectors to Pinecone format")
        
        # Validate schema first
        if self.config.validate_schema and not self.validate_schema(vectors):
            raise ValueError("Vector data failed Pinecone schema validation")
        
        # Group vectors by namespace
        namespace_groups = {}
        for vector in vectors:
            namespace = vector.namespace or "default"
            if namespace not in namespace_groups:
                namespace_groups[namespace] = []
            namespace_groups[namespace].append(vector)
        
        export_files = []
        total_vectors = 0
        
        for namespace, ns_vectors in namespace_groups.items():
            # Process in batches
            for i in range(0, len(ns_vectors), self.config.batch_size):
                batch = ns_vectors[i:i + self.config.batch_size]
                batch_data = {
                    "namespace": namespace,
                    "vectors": []
                }
                
                for vector in batch:
                    pinecone_vector = {
                        "id": vector.id,
                        "values": vector.vector,
                        "metadata": vector.metadata if self.config.include_metadata else {}
                    }
                    batch_data["vectors"].append(pinecone_vector)
                
                # Write batch file
                batch_num = i // self.config.batch_size + 1
                filename = f"pinecone_vectors_{namespace}_batch_{batch_num}.json"
                file_path = self.output_dir / filename
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(batch_data, f, indent=2, ensure_ascii=False)
                
                export_files.append({
                    "file": filename,
                    "namespace": namespace,
                    "vector_count": len(batch),
                    "file_size": file_path.stat().st_size
                })
                total_vectors += len(batch)
        
        # Generate index configuration
        index_config = self._generate_index_config(vectors)
        config_file = self.output_dir / "pinecone_index_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(index_config, f, indent=2)
        
        export_info = {
            "database": "pinecone",
            "export_timestamp": datetime.now().isoformat(),
            "total_vectors": total_vectors,
            "namespaces": list(namespace_groups.keys()),
            "files": export_files,
            "index_config": index_config,
            "batch_size": self.config.batch_size
        }
        
        # Write export metadata
        metadata_file = self.output_dir / "pinecone_export_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(export_info, f, indent=2)
        
        # Generate import script
        if self.config.generate_import_script:
            script_path = self.output_dir / "pinecone_import_script.py"
            with open(script_path, 'w', encoding='utf-8') as f:
                f.write(self.generate_import_script(export_info))
        
        logger.info(f"Pinecone export completed: {total_vectors} vectors in {len(export_files)} files")
        return export_info
    
    def _generate_index_config(self, vectors: List[VectorRecord]) -> Dict[str, Any]:
        """Generate Pinecone index configuration"""
        if not vectors:
            return {}
        
        # Get vector dimensions from first vector
        dimension = len(vectors[0].vector)
        
        # Analyze metadata fields
        metadata_fields = set()
        for vector in vectors[:100]:  # Sample first 100 vectors
            if vector.metadata:
                metadata_fields.update(vector.metadata.keys())
        
        return {
            "dimension": dimension,
            "metric": "cosine",
            "pod_type": "p1.x1",
            "pods": 1,
            "metadata_config": {
                "indexed": list(metadata_fields)
            },
            "source_tag": "diala-voice-agent"
        }
    
    def generate_import_script(self, export_info: Dict[str, Any]) -> str:
        """Generate Python script for importing to Pinecone"""
        script = f'''#!/usr/bin/env python3
"""
Pinecone Import Script for Diala Voice Agent Platform
Generated on: {export_info["export_timestamp"]}
Total vectors: {export_info["total_vectors"]}
"""

import json
import os
from pathlib import Path
import pinecone
from tqdm import tqdm

# Configuration
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT", "us-west1-gcp")
INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "diala-voice-embeddings")

# Data directory
DATA_DIR = Path(__file__).parent

def main():
    """Import vectors to Pinecone"""
    if not PINECONE_API_KEY:
        raise ValueError("PINECONE_API_KEY environment variable is required")
    
    # Initialize Pinecone
    pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)
    
    # Load index configuration
    with open(DATA_DIR / "pinecone_index_config.json", "r") as f:
        index_config = json.load(f)
    
    # Create index if it doesn't exist
    if INDEX_NAME not in pinecone.list_indexes():
        print(f"Creating index: {{INDEX_NAME}}")
        pinecone.create_index(
            name=INDEX_NAME,
            dimension=index_config["dimension"],
            metric=index_config["metric"],
            pod_type=index_config.get("pod_type", "p1.x1"),
            pods=index_config.get("pods", 1),
            metadata_config=index_config.get("metadata_config", {{}})
        )
    
    # Get index
    index = pinecone.Index(INDEX_NAME)
    
    # Import vectors from each file
    files = {export_info["files"]}
    
    for file_info in tqdm(files, desc="Processing files"):
        file_path = DATA_DIR / file_info["file"]
        
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # Upsert vectors
        namespace = data["namespace"]
        vectors = data["vectors"]
        
        print(f"Upserting {{len(vectors)}} vectors to namespace: {{namespace}}")
        
        # Batch upsert
        batch_size = 100
        for i in range(0, len(vectors), batch_size):
            batch = vectors[i:i + batch_size]
            index.upsert(vectors=batch, namespace=namespace)
        
        print(f"Completed file: {{file_info['file']}}")
    
    # Verify import
    stats = index.describe_index_stats()
    print(f"Index stats: {{stats}}")
    
    print("Import completed successfully!")

if __name__ == "__main__":
    main()
'''
        return script
    
    def validate_schema(self, vectors: List[VectorRecord]) -> bool:
        """Validate vectors against Pinecone schema requirements"""
        if not vectors:
            return True
        
        try:
            # Check vector dimensions consistency
            dimensions = set(len(v.vector) for v in vectors)
            if len(dimensions) > 1:
                logger.error(f"Inconsistent vector dimensions: {dimensions}")
                return False
            
            # Check ID format
            for vector in vectors:
                if not vector.id or not isinstance(vector.id, str):
                    logger.error(f"Invalid vector ID: {vector.id}")
                    return False
                
                # Pinecone ID length limit
                if len(vector.id) > 512:
                    logger.error(f"Vector ID too long: {len(vector.id)} characters")
                    return False
            
            # Check metadata size (Pinecone limit: 40KB per vector)
            for vector in vectors:
                if vector.metadata:
                    metadata_size = len(json.dumps(vector.metadata).encode('utf-8'))
                    if metadata_size > 40960:  # 40KB
                        logger.error(f"Metadata too large: {metadata_size} bytes")
                        return False
            
            return True
            
        except Exception as e:
            logger.error(f"Schema validation failed: {e}")
            return False


class ChromaDBConnector(BaseVectorConnector):
    """ChromaDB vector database connector"""
    
    def __init__(self, config: VectorExportConfig):
        super().__init__(config)
        self.db_type = VectorDBType.CHROMADB
        
    def export_vectors(self, vectors: List[VectorRecord]) -> Dict[str, Any]:
        """Export vectors to ChromaDB CSV/Parquet format"""
        logger.info(f"Exporting {len(vectors)} vectors to ChromaDB format")
        
        # Validate schema
        if self.config.validate_schema and not self.validate_schema(vectors):
            raise ValueError("Vector data failed ChromaDB schema validation")
        
        # Group by collection (namespace)
        collections = {}
        for vector in vectors:
            collection = vector.namespace or "default"
            if collection not in collections:
                collections[collection] = []
            collections[collection].append(vector)
        
        export_files = []
        total_vectors = 0
        
        for collection_name, coll_vectors in collections.items():
            # Prepare DataFrame
            data = []
            for vector in coll_vectors:
                record = {
                    "id": vector.id,
                    "embeddings": json.dumps(vector.vector),
                    "documents": vector.metadata.get("text", ""),
                    "metadatas": json.dumps(vector.metadata) if self.config.include_metadata else "{}"
                }
                data.append(record)
            
            df = pd.DataFrame(data)
            
            # Export as CSV
            csv_filename = f"chromadb_collection_{collection_name}.csv"
            csv_path = self.output_dir / csv_filename
            df.to_csv(csv_path, index=False, encoding='utf-8')
            
            # Export as Parquet (more efficient)
            parquet_filename = f"chromadb_collection_{collection_name}.parquet"
            parquet_path = self.output_dir / parquet_filename
            df.to_parquet(parquet_path, index=False)
            
            export_files.append({
                "collection": collection_name,
                "csv_file": csv_filename,
                "parquet_file": parquet_filename,
                "vector_count": len(coll_vectors),
                "csv_size": csv_path.stat().st_size,
                "parquet_size": parquet_path.stat().st_size
            })
            total_vectors += len(coll_vectors)
        
        # Generate collection configuration
        collection_config = self._generate_collection_config(vectors)
        config_file = self.output_dir / "chromadb_collections_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(collection_config, f, indent=2)
        
        export_info = {
            "database": "chromadb",
            "export_timestamp": datetime.now().isoformat(),
            "total_vectors": total_vectors,
            "collections": list(collections.keys()),
            "files": export_files,
            "collection_config": collection_config,
            "formats": ["csv", "parquet"]
        }
        
        # Write export metadata
        metadata_file = self.output_dir / "chromadb_export_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(export_info, f, indent=2)
        
        # Generate import script
        if self.config.generate_import_script:
            script_path = self.output_dir / "chromadb_import_script.py"
            with open(script_path, 'w', encoding='utf-8') as f:
                f.write(self.generate_import_script(export_info))
        
        logger.info(f"ChromaDB export completed: {total_vectors} vectors in {len(export_files)} collections")
        return export_info
    
    def _generate_collection_config(self, vectors: List[VectorRecord]) -> Dict[str, Any]:
        """Generate ChromaDB collection configuration"""
        if not vectors:
            return {}
        
        # Get vector dimensions
        dimension = len(vectors[0].vector)
        
        # Analyze metadata fields
        metadata_fields = set()
        for vector in vectors[:100]:  # Sample first 100 vectors
            if vector.metadata:
                metadata_fields.update(vector.metadata.keys())
        
        return {
            "dimension": dimension,
            "distance_function": "cosine",
            "metadata_fields": list(metadata_fields),
            "embedding_function": "custom",
            "source_tag": "diala-voice-agent"
        }
    
    def generate_import_script(self, export_info: Dict[str, Any]) -> str:
        """Generate Python script for importing to ChromaDB"""
        script = f'''#!/usr/bin/env python3
"""
ChromaDB Import Script for Diala Voice Agent Platform
Generated on: {export_info["export_timestamp"]}
Total vectors: {export_info["total_vectors"]}
"""

import json
import os
from pathlib import Path
import pandas as pd
import chromadb
from tqdm import tqdm

# Configuration
CHROMADB_PATH = os.getenv("CHROMADB_PATH", "./chromadb")
CHROMADB_HOST = os.getenv("CHROMADB_HOST", "localhost")
CHROMADB_PORT = int(os.getenv("CHROMADB_PORT", "8000"))
USE_PERSISTENT = os.getenv("USE_PERSISTENT", "true").lower() == "true"

# Data directory
DATA_DIR = Path(__file__).parent

def main():
    """Import vectors to ChromaDB"""
    
    # Initialize ChromaDB client
    if USE_PERSISTENT:
        client = chromadb.PersistentClient(path=CHROMADB_PATH)
    else:
        client = chromadb.HttpClient(host=CHROMADB_HOST, port=CHROMADB_PORT)
    
    # Load collection configuration
    with open(DATA_DIR / "chromadb_collections_config.json", "r") as f:
        config = json.load(f)
    
    # Import each collection
    files = {export_info["files"]}
    
    for file_info in tqdm(files, desc="Processing collections"):
        collection_name = file_info["collection"]
        
        # Create or get collection
        try:
            collection = client.get_collection(name=collection_name)
            print(f"Using existing collection: {{collection_name}}")
        except:
            collection = client.create_collection(
                name=collection_name,
                metadata={{"source": "diala-voice-agent", "dimension": config["dimension"]}}
            )
            print(f"Created new collection: {{collection_name}}")
        
        # Load data from Parquet (preferred) or CSV
        parquet_file = DATA_DIR / file_info["parquet_file"]
        csv_file = DATA_DIR / file_info["csv_file"]
        
        if parquet_file.exists():
            df = pd.read_parquet(parquet_file)
        else:
            df = pd.read_csv(csv_file)
        
        # Prepare data for ChromaDB
        ids = df["id"].tolist()
        embeddings = [json.loads(emb) for emb in df["embeddings"]]
        documents = df["documents"].tolist()
        metadatas = [json.loads(meta) for meta in df["metadatas"]]
        
        # Batch insert
        batch_size = 1000
        for i in range(0, len(ids), batch_size):
            batch_ids = ids[i:i + batch_size]
            batch_embeddings = embeddings[i:i + batch_size]
            batch_documents = documents[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]
            
            collection.add(
                ids=batch_ids,
                embeddings=batch_embeddings,
                documents=batch_documents,
                metadatas=batch_metadatas
            )
        
        print(f"Imported {{len(ids)}} vectors to collection: {{collection_name}}")
    
    # Verify import
    print("\\nCollection summary:")
    for file_info in files:
        collection_name = file_info["collection"]
        collection = client.get_collection(name=collection_name)
        count = collection.count()
        print(f"  {{collection_name}}: {{count}} vectors")
    
    print("Import completed successfully!")

if __name__ == "__main__":
    main()
'''
        return script
    
    def validate_schema(self, vectors: List[VectorRecord]) -> bool:
        """Validate vectors against ChromaDB schema requirements"""
        if not vectors:
            return True
        
        try:
            # Check vector dimensions consistency
            dimensions = set(len(v.vector) for v in vectors)
            if len(dimensions) > 1:
                logger.error(f"Inconsistent vector dimensions: {dimensions}")
                return False
            
            # Check ID format
            for vector in vectors:
                if not vector.id or not isinstance(vector.id, str):
                    logger.error(f"Invalid vector ID: {vector.id}")
                    return False
            
            # Check for duplicate IDs within collections
            collections = {}
            for vector in vectors:
                collection = vector.namespace or "default"
                if collection not in collections:
                    collections[collection] = set()
                
                if vector.id in collections[collection]:
                    logger.error(f"Duplicate ID in collection {collection}: {vector.id}")
                    return False
                
                collections[collection].add(vector.id)
            
            return True
            
        except Exception as e:
            logger.error(f"Schema validation failed: {e}")
            return False


class WeaviateConnector(BaseVectorConnector):
    """Weaviate vector database connector"""
    
    def __init__(self, config: VectorExportConfig):
        super().__init__(config)
        self.db_type = VectorDBType.WEAVIATE
        
    def export_vectors(self, vectors: List[VectorRecord]) -> Dict[str, Any]:
        """Export vectors to Weaviate JSON format"""
        logger.info(f"Exporting {len(vectors)} vectors to Weaviate format")
        
        # Validate schema
        if self.config.validate_schema and not self.validate_schema(vectors):
            raise ValueError("Vector data failed Weaviate schema validation")
        
        # Group by class (namespace)
        classes = {}
        for vector in vectors:
            class_name = vector.namespace or "DefaultClass"
            # Weaviate class names must be PascalCase
            class_name = self._to_pascal_case(class_name)
            if class_name not in classes:
                classes[class_name] = []
            classes[class_name].append(vector)
        
        export_files = []
        total_vectors = 0
        
        for class_name, class_vectors in classes.items():
            # Process in batches
            for i in range(0, len(class_vectors), self.config.batch_size):
                batch = class_vectors[i:i + self.config.batch_size]
                
                batch_data = {
                    "class": class_name,
                    "objects": []
                }
                
                for vector in batch:
                    weaviate_object = {
                        "id": vector.id,
                        "class": class_name,
                        "properties": vector.metadata if self.config.include_metadata else {},
                        "vector": vector.vector
                    }
                    
                    # Add timestamp if available
                    if vector.timestamp:
                        weaviate_object["properties"]["timestamp"] = vector.timestamp.isoformat()
                    
                    batch_data["objects"].append(weaviate_object)
                
                # Write batch file
                batch_num = i // self.config.batch_size + 1
                filename = f"weaviate_objects_{class_name}_batch_{batch_num}.json"
                file_path = self.output_dir / filename
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(batch_data, f, indent=2, ensure_ascii=False)
                
                export_files.append({
                    "file": filename,
                    "class": class_name,
                    "object_count": len(batch),
                    "file_size": file_path.stat().st_size
                })
                total_vectors += len(batch)
        
        # Generate schema configuration
        schema_config = self._generate_schema_config(vectors, classes)
        schema_file = self.output_dir / "weaviate_schema_config.json"
        with open(schema_file, 'w', encoding='utf-8') as f:
            json.dump(schema_config, f, indent=2)
        
        export_info = {
            "database": "weaviate",
            "export_timestamp": datetime.now().isoformat(),
            "total_objects": total_vectors,
            "classes": list(classes.keys()),
            "files": export_files,
            "schema_config": schema_config,
            "batch_size": self.config.batch_size
        }
        
        # Write export metadata
        metadata_file = self.output_dir / "weaviate_export_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(export_info, f, indent=2)
        
        # Generate import script
        if self.config.generate_import_script:
            script_path = self.output_dir / "weaviate_import_script.py"
            with open(script_path, 'w', encoding='utf-8') as f:
                f.write(self.generate_import_script(export_info))
        
        logger.info(f"Weaviate export completed: {total_vectors} objects in {len(export_files)} files")
        return export_info
    
    def _to_pascal_case(self, name: str) -> str:
        """Convert string to PascalCase for Weaviate class names"""
        # Remove special characters and split by common separators
        words = name.replace('-', ' ').replace('_', ' ').split()
        return ''.join(word.capitalize() for word in words if word)
    
    def _generate_schema_config(self, vectors: List[VectorRecord], classes: Dict[str, List[VectorRecord]]) -> Dict[str, Any]:
        """Generate Weaviate schema configuration"""
        if not vectors:
            return {"classes": []}
        
        # Get vector dimensions
        dimension = len(vectors[0].vector)
        
        schema_classes = []
        
        for class_name, class_vectors in classes.items():
            # Analyze metadata fields for this class
            property_fields = set()
            for vector in class_vectors[:100]:  # Sample first 100 vectors
                if vector.metadata:
                    property_fields.update(vector.metadata.keys())
            
            # Define properties based on metadata analysis
            properties = [
                {
                    "name": "timestamp",
                    "dataType": ["date"],
                    "description": "Timestamp when the vector was created"
                }
            ]
            
            # Add properties based on metadata fields
            for field in property_fields:
                if field == "text":
                    properties.append({
                        "name": "text",
                        "dataType": ["text"],
                        "description": "Original text content"
                    })
                elif field == "source":
                    properties.append({
                        "name": "source",
                        "dataType": ["string"],
                        "description": "Source of the content"
                    })
                elif field == "title":
                    properties.append({
                        "name": "title",
                        "dataType": ["string"],
                        "description": "Title of the content"
                    })
                else:
                    # Generic string property for other fields
                    properties.append({
                        "name": field,
                        "dataType": ["string"],
                        "description": f"Property: {field}"
                    })
            
            class_schema = {
                "class": class_name,
                "description": f"Vector class for {class_name} embeddings from Diala Voice Agent",
                "vectorizer": "none",  # We provide our own vectors
                "properties": properties,
                "vectorIndexConfig": {
                    "distance": "cosine",
                    "ef": 64,
                    "efConstruction": 128,
                    "maxConnections": 64
                }
            }
            
            schema_classes.append(class_schema)
        
        return {
            "classes": schema_classes,
            "dimension": dimension,
            "source_tag": "diala-voice-agent"
        }
    
    def generate_import_script(self, export_info: Dict[str, Any]) -> str:
        """Generate Python script for importing to Weaviate"""
        script = f'''#!/usr/bin/env python3
"""
Weaviate Import Script for Diala Voice Agent Platform
Generated on: {export_info["export_timestamp"]}
Total objects: {export_info["total_objects"]}
"""

import json
import os
from pathlib import Path
import weaviate
from tqdm import tqdm

# Configuration
WEAVIATE_URL = os.getenv("WEAVIATE_URL", "http://localhost:8080")
WEAVIATE_API_KEY = os.getenv("WEAVIATE_API_KEY")
WEAVIATE_OPENAI_KEY = os.getenv("WEAVIATE_OPENAI_KEY")  # Optional, for additional vectorizers

# Data directory
DATA_DIR = Path(__file__).parent

def main():
    """Import objects to Weaviate"""
    
    # Initialize Weaviate client
    auth_config = None
    if WEAVIATE_API_KEY:
        auth_config = weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY)
    
    additional_headers = {{}}
    if WEAVIATE_OPENAI_KEY:
        additional_headers["X-OpenAI-Api-Key"] = WEAVIATE_OPENAI_KEY
    
    client = weaviate.Client(
        url=WEAVIATE_URL,
        auth_client_secret=auth_config,
        additional_headers=additional_headers
    )
    
    # Test connection
    if not client.is_ready():
        raise Exception("Weaviate server is not ready")
    
    # Load schema configuration
    with open(DATA_DIR / "weaviate_schema_config.json", "r") as f:
        schema_config = json.load(f)
    
    # Create schema if it doesn't exist
    existing_schema = client.schema.get()
    existing_classes = {{cls["class"] for cls in existing_schema.get("classes", [])}}
    
    for class_schema in schema_config["classes"]:
        class_name = class_schema["class"]
        if class_name not in existing_classes:
            print(f"Creating class: {{class_name}}")
            client.schema.create_class(class_schema)
        else:
            print(f"Class already exists: {{class_name}}")
    
    # Import objects from each file
    files = {export_info["files"]}
    
    for file_info in tqdm(files, desc="Processing files"):
        file_path = DATA_DIR / file_info["file"]
        
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        class_name = data["class"]
        objects = data["objects"]
        
        print(f"Importing {{len(objects)}} objects to class: {{class_name}}")
        
        # Batch import
        with client.batch as batch:
            batch.batch_size = 100
            batch.dynamic = True
            
            for obj in objects:
                batch.add_data_object(
                    data_object=obj["properties"],
                    class_name=class_name,
                    uuid=obj["id"],
                    vector=obj["vector"]
                )
        
        print(f"Completed file: {{file_info['file']}}")
    
    # Verify import
    print("\\nClass summary:")
    for class_name in {export_info["classes"]}:
        result = client.query.aggregate(class_name).with_meta_count().do()
        count = result["data"]["Aggregate"][class_name][0]["meta"]["count"]
        print(f"  {{class_name}}: {{count}} objects")
    
    print("Import completed successfully!")

if __name__ == "__main__":
    main()
'''
        return script
    
    def validate_schema(self, vectors: List[VectorRecord]) -> bool:
        """Validate vectors against Weaviate schema requirements"""
        if not vectors:
            return True
        
        try:
            # Check vector dimensions consistency
            dimensions = set(len(v.vector) for v in vectors)
            if len(dimensions) > 1:
                logger.error(f"Inconsistent vector dimensions: {dimensions}")
                return False
            
            # Check ID format (must be valid UUID or string)
            for vector in vectors:
                if not vector.id or not isinstance(vector.id, str):
                    logger.error(f"Invalid vector ID: {vector.id}")
                    return False
            
            # Check class names (namespaces) are valid
            for vector in vectors:
                class_name = vector.namespace or "DefaultClass"
                class_name = self._to_pascal_case(class_name)
                
                # Weaviate class names must start with uppercase letter
                if not class_name[0].isupper():
                    logger.error(f"Invalid class name: {class_name}")
                    return False
            
            # Check property names in metadata
            for vector in vectors:
                if vector.metadata:
                    for key in vector.metadata.keys():
                        # Weaviate property names must start with lowercase letter
                        if not key[0].islower():
                            logger.warning(f"Property name should start with lowercase: {key}")
            
            return True
            
        except Exception as e:
            logger.error(f"Schema validation failed: {e}")
            return False


class VectorDBConnectorFactory:
    """Factory class for creating vector database connectors"""
    
    @staticmethod
    def create_connector(db_type: VectorDBType, config: VectorExportConfig) -> BaseVectorConnector:
        """Create a connector for the specified database type"""
        if db_type == VectorDBType.PINECONE:
            return PineconeConnector(config)
        elif db_type == VectorDBType.CHROMADB:
            return ChromaDBConnector(config)
        elif db_type == VectorDBType.WEAVIATE:
            return WeaviateConnector(config)
        else:
            raise ValueError(f"Unsupported vector database type: {db_type}")


class VectorExportManager:
    """Manager class for vector export operations"""
    
    def __init__(self, output_directory: str):
        self.output_directory = output_directory
        self.export_history = []
    
    def export_to_multiple_formats(
        self, 
        vectors: List[VectorRecord], 
        db_types: List[VectorDBType],
        config: Optional[VectorExportConfig] = None
    ) -> Dict[str, Any]:
        """Export vectors to multiple database formats"""
        
        if config is None:
            config = VectorExportConfig(output_directory=self.output_directory)
        
        results = {}
        
        for db_type in db_types:
            try:
                # Create output subdirectory for each database type
                db_output_dir = Path(self.output_directory) / db_type.value
                db_config = VectorExportConfig(
                    output_directory=str(db_output_dir),
                    batch_size=config.batch_size,
                    include_metadata=config.include_metadata,
                    compression=config.compression,
                    generate_import_script=config.generate_import_script,
                    validate_schema=config.validate_schema
                )
                
                connector = VectorDBConnectorFactory.create_connector(db_type, db_config)
                export_info = connector.export_vectors(vectors)
                results[db_type.value] = export_info
                
                logger.info(f"Successfully exported to {db_type.value}")
                
            except Exception as e:
                logger.error(f"Failed to export to {db_type.value}: {e}")
                results[db_type.value] = {"error": str(e)}
        
        # Save combined export summary
        summary = {
            "export_timestamp": datetime.now().isoformat(),
            "total_vectors": len(vectors),
            "databases": list(db_types),
            "results": results
        }
        
        summary_file = Path(self.output_directory) / "export_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, default=str)
        
        self.export_history.append(summary)
        return summary
    
    def get_export_history(self) -> List[Dict[str, Any]]:
        """Get history of export operations"""
        return self.export_history


# Export format-specific utility functions
def export_to_pinecone_format(vectors: List[VectorRecord], output_dir: str) -> Dict[str, Any]:
    """Convenience function to export to Pinecone format"""
    config = VectorExportConfig(output_directory=output_dir)
    connector = PineconeConnector(config)
    return connector.export_vectors(vectors)


def export_to_chromadb_format(vectors: List[VectorRecord], output_dir: str) -> Dict[str, Any]:
    """Convenience function to export to ChromaDB format"""
    config = VectorExportConfig(output_directory=output_dir)
    connector = ChromaDBConnector(config)
    return connector.export_vectors(vectors)


def export_to_weaviate_format(vectors: List[VectorRecord], output_dir: str) -> Dict[str, Any]:
    """Convenience function to export to Weaviate format"""
    config = VectorExportConfig(output_directory=output_dir)
    connector = WeaviateConnector(config)
    return connector.export_vectors(vectors)


# Connection test utilities
def test_all_connections(output_dir: str) -> Dict[str, bool]:
    """Test connections to all supported vector databases"""
    config = VectorExportConfig(output_directory=output_dir)
    results = {}
    
    for db_type in VectorDBType:
        try:
            connector = VectorDBConnectorFactory.create_connector(db_type, config)
            results[db_type.value] = connector.test_connection()
        except Exception as e:
            logger.error(f"Connection test failed for {db_type.value}: {e}")
            results[db_type.value] = False
    
    return results


if __name__ == "__main__":
    # Example usage
    sample_vectors = [
        VectorRecord(
            id="vec_1",
            vector=[0.1, 0.2, 0.3, 0.4, 0.5],
            metadata={"text": "Sample text", "source": "example"},
            namespace="test_collection"
        ),
        VectorRecord(
            id="vec_2", 
            vector=[0.2, 0.3, 0.4, 0.5, 0.6],
            metadata={"text": "Another sample", "source": "example"},
            namespace="test_collection"
        )
    ]
    
    # Test export to all formats
    export_manager = VectorExportManager("./vector_exports")
    results = export_manager.export_to_multiple_formats(
        vectors=sample_vectors,
        db_types=[VectorDBType.PINECONE, VectorDBType.CHROMADB, VectorDBType.WEAVIATE]
    )
    
    print(json.dumps(results, indent=2, default=str))


================================================
FILE: voice_clone_jobs.py
================================================
"""
Voice Clone Job Manager

Manages voice cloning jobs across development and production environments,
integrating with Convex for state management and job tracking.
"""

import os
import uuid
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime
import asyncio
from convex import ConvexClient

logger = logging.getLogger(__name__)


class VoiceCloneJobManager:
    """Manages voice cloning jobs with Convex integration"""
    
    def __init__(self):
        """Initialize job manager with Convex client"""
        self.convex_url = os.getenv("CONVEX_URL", "http://127.0.0.1:3210")
        self.convex_client = ConvexClient(self.convex_url)
        self.environment = os.getenv("ENVIRONMENT", "development")
        
        logger.info(f"Voice Clone Job Manager initialized - Convex URL: {self.convex_url}")
    
    async def create_job(
        self,
        audio_path: str,
        user_id: Optional[str] = None,
        voice_name: str = "My Voice",
        sample_text: Optional[str] = None,
        settings: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Create a new voice cloning job
        
        Args:
            audio_path: Path to the audio file
            user_id: User ID (optional)
            voice_name: Name for the voice profile
            sample_text: Text to generate with cloned voice
            settings: Additional TTS settings
            
        Returns:
            Job ID
        """
        try:
            # Generate job ID
            job_id = str(uuid.uuid4())
            
            # Get file info
            file_size = os.path.getsize(audio_path) if os.path.exists(audio_path) else 0
            file_name = os.path.basename(audio_path)
            
            # Default sample text
            if not sample_text:
                sample_text = "Hello, this is my cloned voice. I can now speak with my own voice characteristics."
            
            # Create job data
            job_data = {
                "jobId": job_id,
                "userId": user_id or "anonymous",
                "voiceName": voice_name,
                "audioFileName": file_name,
                "audioFileSize": file_size,
                "sampleText": sample_text,
            }
            
            # In production, upload audio file to cloud storage
            if self.environment == "production":
                # TODO: Upload to S3/Spaces and get URL
                job_data["audioFileUrl"] = f"s3://voice-clones/{job_id}/{file_name}"
            else:
                # In development, use local path
                job_data["audioFileUrl"] = f"file://{audio_path}"
            
            # Create job in Convex
            self.convex_client.mutation("voiceCloneJobs:create", job_data)
            
            logger.info(f"Created voice clone job: {job_id}")
            return job_id
            
        except Exception as e:
            logger.error(f"Error creating voice clone job: {str(e)}")
            raise
    
    async def update_job_status(
        self,
        job_id: str,
        status: str,
        updates: Optional[Dict[str, Any]] = None
    ):
        """
        Update job status and metadata
        
        Args:
            job_id: Job ID to update
            status: New status (pending, processing, completed, failed)
            updates: Additional fields to update
        """
        try:
            mutation_data = {
                "jobId": job_id,
                "status": status,
            }
            
            if updates:
                mutation_data.update(updates)
            
            self.convex_client.mutation("voiceCloneJobs:updateStatus", mutation_data)
            
            logger.info(f"Updated job {job_id} status to: {status}")
            
        except Exception as e:
            logger.error(f"Error updating job status: {str(e)}")
            raise
    
    async def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """
        Get current job status
        
        Args:
            job_id: Job ID to check
            
        Returns:
            Job data or None if not found
        """
        try:
            job = self.convex_client.query("voiceCloneJobs:getJob", {"jobId": job_id})
            return job
        except Exception as e:
            logger.error(f"Error getting job status: {str(e)}")
            return None
    
    async def get_user_jobs(
        self,
        user_id: str,
        status: Optional[str] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Get jobs for a specific user
        
        Args:
            user_id: User ID
            status: Filter by status (optional)
            limit: Maximum number of jobs to return
            
        Returns:
            List of job data
        """
        try:
            query_args = {
                "userId": user_id,
                "limit": limit
            }
            
            if status:
                query_args["status"] = status
            
            jobs = self.convex_client.query("voiceCloneJobs:getUserJobs", query_args)
            return jobs or []
            
        except Exception as e:
            logger.error(f"Error getting user jobs: {str(e)}")
            return []
    
    async def get_pending_jobs(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get pending jobs for processing
        
        Args:
            limit: Maximum number of jobs to return
            
        Returns:
            List of pending jobs
        """
        try:
            jobs = self.convex_client.query("voiceCloneJobs:getPendingJobs", {"limit": limit})
            return jobs or []
        except Exception as e:
            logger.error(f"Error getting pending jobs: {str(e)}")
            return []
    
    async def claim_job(self, job_id: str, worker_info: Dict[str, Any]) -> bool:
        """
        Claim a job for processing
        
        Args:
            job_id: Job ID to claim
            worker_info: Information about the worker claiming the job
            
        Returns:
            True if successfully claimed, False otherwise
        """
        try:
            self.convex_client.mutation("voiceCloneJobs:claimJob", {
                "jobId": job_id,
                "workerInfo": worker_info
            })
            return True
        except Exception as e:
            logger.error(f"Error claiming job {job_id}: {str(e)}")
            return False
    
    async def wait_for_completion(
        self,
        job_id: str,
        timeout: int = 120,
        poll_interval: int = 2
    ) -> Optional[Dict[str, Any]]:
        """
        Wait for a job to complete
        
        Args:
            job_id: Job ID to wait for
            timeout: Maximum time to wait in seconds
            poll_interval: Interval between status checks in seconds
            
        Returns:
            Final job data or None if timeout
        """
        start_time = datetime.utcnow()
        
        while (datetime.utcnow() - start_time).total_seconds() < timeout:
            job = await self.get_job_status(job_id)
            
            if not job:
                logger.warning(f"Job {job_id} not found")
                return None
            
            if job["status"] in ["completed", "failed"]:
                return job
            
            await asyncio.sleep(poll_interval)
        
        logger.warning(f"Timeout waiting for job {job_id}")
        return None
    
    async def get_job_stats(self, user_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Get job statistics
        
        Args:
            user_id: User ID for user-specific stats (optional)
            
        Returns:
            Statistics dictionary
        """
        try:
            query_args = {}
            if user_id:
                query_args["userId"] = user_id
            
            stats = self.convex_client.query("voiceCloneJobs:getStats", query_args)
            return stats or {
                "total": 0,
                "pending": 0,
                "processing": 0,
                "completed": 0,
                "failed": 0,
                "avgProcessingTime": 0
            }
        except Exception as e:
            logger.error(f"Error getting job stats: {str(e)}")
            return {
                "total": 0,
                "pending": 0,
                "processing": 0,
                "completed": 0,
                "failed": 0,
                "avgProcessingTime": 0
            }
    
    async def process_job_locally(self, job_id: str, audio_data: bytes):
        """
        Process a job locally (development mode)
        This is called by TTSManager for local processing
        
        Args:
            job_id: Job ID to process
            audio_data: Audio data to process
        """
        # This method is implemented in TTSManager
        # It's here as a placeholder for the interface
        pass
    
    async def queue_for_remote_processing(self, job_id: str):
        """
        Queue a job for remote processing (production mode)
        This triggers the GPU droplet if needed
        
        Args:
            job_id: Job ID to queue
        """
        # In production, this would:
        # 1. Check if GPU droplet is running
        # 2. Start droplet if needed
        # 3. The GPU worker will poll for pending jobs
        
        # For now, the job is already created as "pending"
        # The GPU worker will pick it up when polling
        logger.info(f"Job {job_id} queued for remote processing")


================================================
FILE: gemini/__init__.py
================================================
"""
Google Gemini Embeddings Integration
"""

from .embeddings_client import GeminiEmbeddingsClient
from .embeddings_service import GeminiEmbeddingsService
from .models import GeminiEmbeddingRequest, GeminiEmbeddingResponse, GeminiModelInfo
from .config import GeminiConfig

__all__ = [
    'GeminiEmbeddingsClient',
    'GeminiEmbeddingsService', 
    'GeminiEmbeddingRequest',
    'GeminiEmbeddingResponse',
    'GeminiModelInfo',
    'GeminiConfig'
]


================================================
FILE: gemini/config.py
================================================
"""
Google Gemini Embeddings Configuration
"""

import os
from typing import Optional
from pydantic_settings import BaseSettings

class GeminiConfig(BaseSettings):
    """Configuration for Google Gemini Embeddings API"""
    
    # API Configuration
    api_key: Optional[str] = os.getenv('GOOGLE_API_KEY')
    base_url: str = "https://generativelanguage.googleapis.com/v1"
    
    # Model Configuration - Latest experimental model only
    model_name: str = "gemini-embedding-exp-03-07"
    
    # Request Configuration
    max_tokens: int = 8192  # 8K token context length
    batch_size: int = 100
    timeout: int = 30
    
    # Model Specifications
    dimensions: int = 3072  # Default full dimensions
    alternative_dimensions: list = [768, 1536, 3072]  # MRL support
    context_length: int = 8192
    
    # Performance Metrics (SOTA)
    mteb_score: float = 68.32  # #1 on MTEB Multilingual leaderboard
    margin_over_next: float = 5.81  # Margin over next best model
    
    # Supported Languages
    supported_languages: int = 100  # 100+ languages
    
    # Task Types
    supported_tasks: list = [
        "SEMANTIC_SIMILARITY",
        "CLASSIFICATION", 
        "CLUSTERING",
        "RETRIEVAL_DOCUMENT",
        "RETRIEVAL_QUERY",
        "QUESTION_ANSWERING",
        "FACT_VERIFICATION",
        "CODE_RETRIEVAL_QUERY"
    ]
    
    # Rate Limits (experimental model has restrictions)
    rate_limit_rpm: int = 100  # More restricted for experimental
    
    class Config:
        env_prefix = "GEMINI_"
        case_sensitive = False


================================================
FILE: gemini/embeddings_client.py
================================================
"""
Google Gemini Embeddings HTTP Client
"""

import asyncio
import aiohttp
import logging
from typing import List, Optional, Dict, Any, Union
from .config import GeminiConfig
from .models import GeminiEmbeddingRequest, GeminiEmbeddingResponse, GeminiModelInfo, GeminiEmbeddingConfig

logger = logging.getLogger(__name__)

class GeminiEmbeddingsClient:
    """HTTP client for Google Gemini Embeddings API"""
    
    def __init__(self, config: Optional[GeminiConfig] = None):
        self.config = config or GeminiConfig()
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        """Async context manager entry"""
        await self.start_session()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.close_session()
        
    async def start_session(self):
        """Start HTTP session"""
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=self.config.timeout)
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                headers={
                    'Content-Type': 'application/json',
                    'User-Agent': 'diala-backend/1.0'
                }
            )
            
    async def close_session(self):
        """Close HTTP session"""
        if self.session:
            await self.session.close()
            self.session = None
            
    async def create_embeddings(
        self, 
        content: Union[str, List[str]], 
        task_type: str = "SEMANTIC_SIMILARITY",
        output_dimensionality: Optional[int] = None
    ) -> GeminiEmbeddingResponse:
        """
        Create embeddings for text content
        
        Args:
            content: Text or list of texts to embed
            task_type: Task optimization type
            output_dimensionality: Optional dimension truncation (MRL)
            
        Returns:
            GeminiEmbeddingResponse with embeddings
        """
        if not self.session:
            await self.start_session()
            
        if not self.config.api_key:
            raise ValueError("Google API key is required")
            
        # Prepare request body
        request_body = {
            "requests": []
        }
        
        # Handle single string or list
        contents = [content] if isinstance(content, str) else content
        
        for text in contents:
            embed_request = {
                "model": f"models/{self.config.model_name}",
                "content": {
                    "parts": [{"text": text}]
                }
            }
            
            # Add configuration if provided
            if task_type or output_dimensionality:
                embed_request["config"] = {}
                if task_type:
                    embed_request["config"]["task_type"] = task_type
                if output_dimensionality:
                    embed_request["config"]["output_dimensionality"] = output_dimensionality
                    
            request_body["requests"].append(embed_request)
        
        try:
            # Make API request
            url = f"{self.config.base_url}/models/{self.config.model_name}:batchEmbedContents"
            params = {"key": self.config.api_key}
            
            async with self.session.post(
                url,
                json=request_body,
                params=params
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    # Transform response to match our model
                    embeddings = []
                    if "embeddings" in data:
                        for embedding_data in data["embeddings"]:
                            if "values" in embedding_data:
                                embeddings.append({
                                    "values": embedding_data["values"]
                                })
                    
                    return GeminiEmbeddingResponse(embeddings=embeddings)
                else:
                    error_text = await response.text()
                    logger.error(f"Gemini API error {response.status}: {error_text}")
                    raise Exception(f"API request failed: {response.status} - {error_text}")
                    
        except Exception as e:
            logger.error(f"Error creating embeddings: {e}")
            raise
            
    async def get_model_info(self) -> GeminiModelInfo:
        """
        Get information about the Gemini experimental model
        
        Returns:
            GeminiModelInfo with model specifications
        """
        return GeminiModelInfo(
            id=self.config.model_name,
            name="Gemini Embedding Experimental",
            description="State-of-the-art experimental embedding model with SOTA MTEB performance. Features 8K context, MRL support, and 100+ languages.",
            dimensions=self.config.dimensions,
            max_dimensions=3072,
            alternative_dimensions=self.config.alternative_dimensions,
            max_tokens=self.config.max_tokens,
            mteb_score=self.config.mteb_score,
            margin_over_next=self.config.margin_over_next,
            multimodal=True,
            multilingual=True,
            supported_languages=self.config.supported_languages,
            experimental=True,
            has_mrl=True,
            supported_tasks=self.config.supported_tasks,
            rate_limit_rpm=self.config.rate_limit_rpm
        )
        
    async def batch_embeddings(
        self, 
        texts: List[str], 
        task_type: str = "SEMANTIC_SIMILARITY",
        output_dimensionality: Optional[int] = None,
        batch_size: Optional[int] = None
    ) -> List[List[float]]:
        """
        Create embeddings for large lists of texts using batching
        
        Args:
            texts: List of texts to embed
            task_type: Task optimization type
            output_dimensionality: Optional dimension truncation
            batch_size: Optional batch size override
            
        Returns:
            List of embedding vectors
        """
        batch_size = batch_size or self.config.batch_size
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            response = await self.create_embeddings(
                batch, 
                task_type=task_type,
                output_dimensionality=output_dimensionality
            )
            
            # Extract embeddings in order
            batch_embeddings = [embedding.values for embedding in response.embeddings]
            all_embeddings.extend(batch_embeddings)
            
        return all_embeddings
        
    async def health_check(self) -> bool:
        """
        Check if the Gemini API is accessible
        
        Returns:
            True if API is healthy, False otherwise
        """
        try:
            # Test with a simple embedding
            await self.create_embeddings("test")
            return True
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False


================================================
FILE: gemini/embeddings_service.py
================================================
"""
Google Gemini Embeddings Service Layer
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional
from .embeddings_client import GeminiEmbeddingsClient
from .config import GeminiConfig
from .models import GeminiModelInfo

logger = logging.getLogger(__name__)

class GeminiEmbeddingsService:
    """High-level service for Google Gemini Embeddings operations"""
    
    def __init__(self, config: Optional[GeminiConfig] = None):
        logger.info("Initializing GeminiEmbeddingsService...")
        try:
            self.config = config or GeminiConfig()
            logger.info(f"GeminiConfig loaded - API key present: {bool(self.config.api_key)}")
            self.client = GeminiEmbeddingsClient(self.config)
            logger.info("GeminiEmbeddingsClient created successfully")
        except Exception as e:
            logger.error(f"Error in GeminiEmbeddingsService.__init__: {e}")
            raise
        
    async def embed_documents(
        self, 
        documents: List[str],
        output_dimensionality: Optional[int] = None
    ) -> List[List[float]]:
        """
        Embed a list of documents for retrieval
        
        Args:
            documents: List of document texts
            output_dimensionality: Optional dimension truncation (MRL)
            
        Returns:
            List of embedding vectors
        """
        try:
            async with self.client:
                return await self.client.batch_embeddings(
                    documents, 
                    task_type="RETRIEVAL_DOCUMENT",
                    output_dimensionality=output_dimensionality
                )
        except Exception as e:
            logger.error(f"Error embedding documents: {e}")
            raise
            
    async def embed_queries(
        self, 
        queries: List[str],
        output_dimensionality: Optional[int] = None
    ) -> List[List[float]]:
        """
        Embed a list of queries for retrieval
        
        Args:
            queries: List of query texts
            output_dimensionality: Optional dimension truncation (MRL)
            
        Returns:
            List of embedding vectors
        """
        try:
            async with self.client:
                return await self.client.batch_embeddings(
                    queries, 
                    task_type="RETRIEVAL_QUERY",
                    output_dimensionality=output_dimensionality
                )
        except Exception as e:
            logger.error(f"Error embedding queries: {e}")
            raise
            
    async def embed_for_classification(
        self, 
        texts: List[str],
        output_dimensionality: Optional[int] = None
    ) -> List[List[float]]:
        """
        Embed texts optimized for classification tasks
        
        Args:
            texts: List of texts to classify
            output_dimensionality: Optional dimension truncation (MRL)
            
        Returns:
            List of embedding vectors
        """
        try:
            async with self.client:
                return await self.client.batch_embeddings(
                    texts, 
                    task_type="CLASSIFICATION",
                    output_dimensionality=output_dimensionality
                )
        except Exception as e:
            logger.error(f"Error embedding for classification: {e}")
            raise
            
    async def embed_for_clustering(
        self, 
        texts: List[str],
        output_dimensionality: Optional[int] = None
    ) -> List[List[float]]:
        """
        Embed texts optimized for clustering tasks
        
        Args:
            texts: List of texts to cluster
            output_dimensionality: Optional dimension truncation (MRL)
            
        Returns:
            List of embedding vectors
        """
        try:
            async with self.client:
                return await self.client.batch_embeddings(
                    texts, 
                    task_type="CLUSTERING",
                    output_dimensionality=output_dimensionality
                )
        except Exception as e:
            logger.error(f"Error embedding for clustering: {e}")
            raise
            
    async def get_model_capabilities(self) -> GeminiModelInfo:
        """
        Get detailed information about Gemini experimental model capabilities
        
        Returns:
            GeminiModelInfo with specifications and performance metrics
        """
        try:
            async with self.client:
                return await self.client.get_model_info()
        except Exception as e:
            logger.error(f"Error getting model info: {e}")
            raise
            
    async def test_connection(self) -> bool:
        """
        Test if the Gemini API is accessible
        
        Returns:
            True if connection is successful, False otherwise
        """
        try:
            async with self.client:
                return await self.client.health_check()
        except Exception as e:
            logger.error(f"Connection test failed: {e}")
            return False
            
    def get_pricing_info(self) -> Dict[str, Any]:
        """
        Get pricing information for Gemini embeddings (experimental)
        
        Returns:
            Dictionary with pricing details
        """
        return {
            "model": "gemini-embedding-exp-03-07",
            "status": "experimental",
            "pricing": "Free during experimental phase",
            "currency": "USD",
            "billing_unit": "requests",
            "limitations": {
                "rate_limit": f"{self.config.rate_limit_rpm} requests/minute",
                "capacity": "Limited during experimental phase",
                "stability": "Subject to change"
            },
            "future_pricing": {
                "estimated": "TBD - will be announced at GA",
                "comparison": "Expected to be competitive with other SOTA models"
            }
        }
        
    def get_performance_metrics(self) -> Dict[str, Any]:
        """
        Get performance benchmarks for Gemini experimental model
        
        Returns:
            Dictionary with performance metrics
        """
        return {
            "model": "gemini-embedding-exp-03-07",
            "status": "experimental",
            "benchmarks": {
                "mteb_multilingual_score": self.config.mteb_score,
                "ranking": "#1 on MTEB Multilingual leaderboard",
                "margin_over_next": f"+{self.config.margin_over_next} points",
                "domains": [
                    "finance", "science", "legal", "search", 
                    "general", "code", "multilingual"
                ]
            },
            "capabilities": {
                "context_length": self.config.context_length,
                "embedding_dimensions": {
                    "default": self.config.dimensions,
                    "options": self.config.alternative_dimensions,
                    "mrl_support": True
                },
                "languages": {
                    "supported": self.config.supported_languages,
                    "type": "100+ languages including low-resource"
                },
                "multimodal": True,
                "task_optimization": self.config.supported_tasks
            },
            "technical_features": {
                "matryoshka_representation_learning": True,
                "task_specific_optimization": True,
                "unified_model": True,
                "gemini_trained": True
            }
        }
        
    async def chunk_and_embed(
        self, 
        text: str, 
        chunk_size: int = 2000, 
        overlap: int = 200,
        task_type: str = "RETRIEVAL_DOCUMENT",
        output_dimensionality: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """
        Chunk a large text and create embeddings for each chunk
        
        Args:
            text: Large text to chunk and embed
            chunk_size: Size of each chunk in characters (larger for 8K context)
            overlap: Overlap between chunks
            task_type: Task optimization type
            output_dimensionality: Optional dimension truncation (MRL)
            
        Returns:
            List of chunks with embeddings
        """
        try:
            # Improved text chunking for 8K context
            chunks = []
            start = 0
            while start < len(text):
                end = min(start + chunk_size, len(text))
                chunk_text = text[start:end]
                chunks.append({
                    "text": chunk_text,
                    "start": start,
                    "end": end,
                    "length": len(chunk_text)
                })
                start = end - overlap
                
            # Create embeddings for all chunks
            chunk_texts = [chunk["text"] for chunk in chunks]
            embeddings = await self.client.batch_embeddings(
                chunk_texts,
                task_type=task_type,
                output_dimensionality=output_dimensionality
            )
            
            # Combine chunks with embeddings
            for i, chunk in enumerate(chunks):
                chunk["embedding"] = embeddings[i]
                chunk["task_type"] = task_type
                chunk["dimensions"] = output_dimensionality or self.config.dimensions
                
            return chunks
            
        except Exception as e:
            logger.error(f"Error chunking and embedding text: {e}")
            raise


================================================
FILE: gemini/models.py
================================================
"""
Google Gemini Embeddings API Models
"""

from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field

class GeminiEmbeddingConfig(BaseModel):
    """Configuration for embedding request"""
    
    task_type: Optional[str] = Field(default="SEMANTIC_SIMILARITY", description="Task type for optimization")
    output_dimensionality: Optional[int] = Field(default=None, description="Truncate dimensions (MRL)")
    
class GeminiEmbeddingRequest(BaseModel):
    """Request model for Gemini Embeddings API"""
    
    model: str = Field(default="gemini-embedding-exp-03-07", description="Model name")
    content: Union[str, List[str]] = Field(..., description="Text to embed")
    config: Optional[GeminiEmbeddingConfig] = Field(default=None, description="Optional configuration")
    
    class Config:
        schema_extra = {
            "example": {
                "model": "gemini-embedding-exp-03-07",
                "content": "What is the meaning of life?",
                "config": {
                    "task_type": "SEMANTIC_SIMILARITY",
                    "output_dimensionality": 1536
                }
            }
        }

class GeminiEmbeddingData(BaseModel):
    """Individual embedding data"""
    
    values: List[float] = Field(..., description="Embedding vector")
    
class GeminiEmbeddingResponse(BaseModel):
    """Response model for Gemini Embeddings API"""
    
    embeddings: List[GeminiEmbeddingData] = Field(..., description="Embedding data")
    
    class Config:
        schema_extra = {
            "example": {
                "embeddings": [
                    {
                        "values": [0.1, 0.2, 0.3]
                    }
                ]
            }
        }

class GeminiModelInfo(BaseModel):
    """Model information and capabilities"""
    
    id: str = Field(..., description="Model ID")
    name: str = Field(..., description="Model display name")
    description: str = Field(..., description="Model description")
    dimensions: int = Field(..., description="Default embedding dimensions")
    max_dimensions: int = Field(..., description="Maximum dimensions available")
    alternative_dimensions: List[int] = Field(..., description="Available dimension options")
    max_tokens: int = Field(..., description="Maximum context length")
    mteb_score: float = Field(..., description="MTEB Multilingual score")
    margin_over_next: float = Field(..., description="Margin over next best model")
    multimodal: bool = Field(..., description="Supports multimodal inputs")
    multilingual: bool = Field(..., description="Supports multiple languages")
    supported_languages: int = Field(..., description="Number of supported languages")
    experimental: bool = Field(..., description="Is experimental model")
    has_mrl: bool = Field(..., description="Supports Matryoshka Representation Learning")
    supported_tasks: List[str] = Field(..., description="Supported task types")
    rate_limit_rpm: int = Field(..., description="Rate limit per minute")
    
    class Config:
        schema_extra = {
            "example": {
                "id": "gemini-embedding-exp-03-07",
                "name": "Gemini Embedding Experimental",
                "description": "State-of-the-art experimental embedding model",
                "dimensions": 3072,
                "max_dimensions": 3072,
                "alternative_dimensions": [768, 1536, 3072],
                "max_tokens": 8192,
                "mteb_score": 68.32,
                "margin_over_next": 5.81,
                "multimodal": True,
                "multilingual": True,
                "supported_languages": 100,
                "experimental": True,
                "has_mrl": True,
                "supported_tasks": ["SEMANTIC_SIMILARITY", "CLASSIFICATION"],
                "rate_limit_rpm": 100
            }
        }


================================================
FILE: jina/__init__.py
================================================
"""
Jina Embeddings v4 Integration
"""

from .embeddings_client import JinaEmbeddingsClient
from .embeddings_service import JinaEmbeddingsService
from .models import JinaEmbeddingRequest, JinaEmbeddingResponse
from .config import JinaConfig

__all__ = [
    'JinaEmbeddingsClient',
    'JinaEmbeddingsService', 
    'JinaEmbeddingRequest',
    'JinaEmbeddingResponse',
    'JinaConfig'
]


================================================
FILE: jina/config.py
================================================
"""
Jina Embeddings v4 Configuration - Optimized for Transcript Processing
"""

import os
from typing import Optional, List
from pydantic_settings import BaseSettings

class JinaConfig(BaseSettings):
    """Configuration for Jina Embeddings v4 API - Transcript-to-RAG focused"""
    
    # API Configuration
    api_key: Optional[str] = os.getenv('JINA_API_KEY')
    base_url: str = "https://api.jina.ai/v1"
    
    # Model Configuration
    model_name: str = "jina-embeddings-v4"
    task: str = "retrieval.passage"  # Optimal for transcript content
    
    # V4 Specific Features
    late_chunking: bool = True  # Enable for long transcripts
    truncate_at_maximum_length: bool = True  # Safer for production
    output_multi_vector_embeddings: bool = False  # Single vector by default
    output_data_type: str = "float"  # float, binary, base64
    
    # Request Configuration
    max_tokens: int = 32768  # V4 supports up to 32K tokens
    batch_size: int = 50  # Smaller batches for larger context
    timeout: int = 60  # Longer timeout for large transcripts
    
    # Model Specifications  
    dimensions: int = 1024  # Default dimension (128-2048 supported)
    parameters: str = "3.8B"
    context_length: int = 32768  # Full V4 context length
    
    # Transcript Processing Optimization
    optimize_for_transcripts: bool = True
    auto_chunk_long_transcripts: bool = True
    chunk_size: Optional[int] = None  # Auto-determined based on content
    chunk_overlap: Optional[int] = None  # Auto-determined
    
    # Supported Tasks for V4
    supported_tasks: List[str] = [
        "retrieval.passage",  # For transcript content
        "retrieval.query",    # For search queries  
        "text-matching",      # For similarity tasks
        "code.query",         # For code search
        "code.passage"        # For code snippets
    ]
    
    # Supported Dimensions (Matryoshka)
    supported_dimensions: List[int] = [128, 256, 512, 1024, 2048]
    
    # GitHub Repository Info
    github_repo: str = "jina-ai/jina-embeddings-v4"
    github_stars: int = 2847
    
    # Performance Metrics (Updated for V4)
    mteb_avg_score: float = 64.41
    retrieval_score: float = 50.87
    clustering_score: float = 49.62
    classification_score: float = 75.45
    multilingual_score: float = 66.49  # Outperforms OpenAI by 12%
    long_document_score: float = 67.11  # 28% better than competitors
    code_retrieval_score: float = 71.59  # 15% better than Voyage-3
    
    class Config:
        env_prefix = "JINA_"
        case_sensitive = False


================================================
FILE: jina/embeddings_client.py
================================================
"""
Jina Embeddings v4 HTTP Client
"""

import asyncio
import aiohttp
import logging
from typing import List, Optional, Dict, Any
from .config import JinaConfig
from .models import JinaEmbeddingRequest, JinaEmbeddingResponse, JinaModelInfo

logger = logging.getLogger(__name__)

class JinaEmbeddingsClient:
    """HTTP client for Jina Embeddings v4 API"""
    
    def __init__(self, config: Optional[JinaConfig] = None):
        self.config = config or JinaConfig()
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        """Async context manager entry"""
        await self.start_session()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.close_session()
        
    async def start_session(self):
        """Start HTTP session"""
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=self.config.timeout)
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                headers={
                    'Authorization': f'Bearer {self.config.api_key}',
                    'Content-Type': 'application/json',
                    'User-Agent': 'diala-backend/1.0'
                }
            )
            
    async def close_session(self):
        """Close HTTP session"""
        if self.session:
            await self.session.close()
            self.session = None
            
    async def create_embeddings(
        self, 
        texts: List[str], 
        task: Optional[str] = None,
        dimensions: Optional[int] = None,
        late_chunking: Optional[bool] = None,
        truncate_at_maximum_length: Optional[bool] = None,
        output_multi_vector_embeddings: Optional[bool] = None,
        output_data_type: Optional[str] = None,
        **kwargs
    ) -> JinaEmbeddingResponse:
        """
        Create embeddings for a list of texts using Jina v4 API
        
        Args:
            texts: List of texts to embed
            task: Task type (retrieval.passage, retrieval.query, text-matching, etc.)
            dimensions: Embedding dimensions (128-2048)
            late_chunking: Enable late chunking for long texts
            truncate_at_maximum_length: Truncate instead of error for long texts
            output_multi_vector_embeddings: Output multi-vector embeddings
            output_data_type: Output format (float, binary, base64)
            **kwargs: Additional parameters for the request
            
        Returns:
            JinaEmbeddingResponse with embeddings
        """
        if not self.session:
            await self.start_session()
            
        if not self.config.api_key:
            raise ValueError("Jina API key is required")
            
        # Prepare request with V4 parameters
        request_data = {
            "input": texts,
            "model": self.config.model_name,
            "task": task or self.config.task,
            "dimensions": dimensions or self.config.dimensions,
            **kwargs
        }
        
        # Add V4-specific parameters if provided
        if late_chunking is not None:
            request_data["late_chunking"] = late_chunking
        elif hasattr(self.config, 'late_chunking'):
            request_data["late_chunking"] = self.config.late_chunking
            
        if truncate_at_maximum_length is not None:
            request_data["truncate_at_maximum_length"] = truncate_at_maximum_length
        elif hasattr(self.config, 'truncate_at_maximum_length'):
            request_data["truncate_at_maximum_length"] = self.config.truncate_at_maximum_length
            
        if output_multi_vector_embeddings is not None:
            request_data["output_multi_vector_embeddings"] = output_multi_vector_embeddings
        elif hasattr(self.config, 'output_multi_vector_embeddings'):
            request_data["output_multi_vector_embeddings"] = self.config.output_multi_vector_embeddings
            
        if output_data_type is not None:
            request_data["output_data_type"] = output_data_type
        elif hasattr(self.config, 'output_data_type'):
            request_data["output_data_type"] = self.config.output_data_type
        
        try:
            logger.info(f"Creating V4 embeddings for {len(texts)} texts with task: {request_data.get('task')}")
            
            # Make API request
            async with self.session.post(
                f"{self.config.base_url}/embeddings",
                json=request_data
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return JinaEmbeddingResponse(**data)
                else:
                    error_text = await response.text()
                    logger.error(f"Jina API error {response.status}: {error_text}")
                    raise Exception(f"API request failed: {response.status} - {error_text}")
                    
        except Exception as e:
            logger.error(f"Error creating embeddings: {e}")
            raise
            
    async def embed_transcripts(
        self,
        transcripts: List[str],
        dimensions: int = 1024,
        late_chunking: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Embed video transcripts with optimal V4 settings
        
        Args:
            transcripts: List of video transcript texts
            dimensions: Embedding dimensions
            late_chunking: Enable late chunking for long transcripts
            
        Returns:
            List of embedding results with metadata
        """
        logger.info(f"Embedding {len(transcripts)} transcripts for RAG")
        
        response = await self.create_embeddings(
            transcripts,
            task="retrieval.passage",
            dimensions=dimensions,
            late_chunking=late_chunking,
            truncate_at_maximum_length=True
        )
        
        # Format results with metadata
        results = []
        for i, data in enumerate(response.data):
            results.append({
                "transcript_index": i,
                "embedding": data.embedding,
                "dimensions": len(data.embedding),
                "processing_method": "late_chunking" if late_chunking else "direct"
            })
            
        logger.info(f"Successfully embedded {len(results)} transcripts")
        return results
        
    async def get_model_info(self) -> JinaModelInfo:
        """
        Get information about the Jina v4 model
        
        Returns:
            JinaModelInfo with model specifications
        """
        return JinaModelInfo(
            id=self.config.model_name,
            name="Jina Embeddings v4",
            description="3.8B parameter universal embedding model optimized for transcript processing",
            dimensions=self.config.dimensions,
            max_tokens=self.config.max_tokens,
            parameters=self.config.parameters,
            github_repo=self.config.github_repo,
            github_stars=self.config.github_stars,
            mteb_score=self.config.mteb_avg_score,
            retrieval_score=self.config.retrieval_score
        )
        
    async def batch_embeddings(
        self, 
        texts: List[str], 
        batch_size: Optional[int] = None,
        task: str = "retrieval.passage",
        dimensions: Optional[int] = None,
        late_chunking: bool = True
    ) -> List[List[float]]:
        """
        Create embeddings for large lists of texts using batching - optimized for transcripts
        
        Args:
            texts: List of texts to embed
            batch_size: Optional batch size override
            task: Task type for embedding (retrieval.passage for transcripts)
            dimensions: Embedding dimensions
            late_chunking: Enable late chunking for long transcripts
            
        Returns:
            List of embedding vectors
        """
        batch_size = batch_size or self.config.batch_size
        all_embeddings = []
        
        logger.info(f"Batch processing {len(texts)} texts in batches of {batch_size}")
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            logger.info(f"Processing batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}")
            
            response = await self.create_embeddings(
                batch,
                task=task,
                dimensions=dimensions,
                late_chunking=late_chunking
            )
            
            # Extract embeddings in order
            batch_embeddings = [data.embedding for data in response.data]
            all_embeddings.extend(batch_embeddings)
            
        logger.info(f"Completed batch processing - generated {len(all_embeddings)} embeddings")
        return all_embeddings
        
    async def health_check(self) -> bool:
        """
        Check if the Jina API is accessible
        
        Returns:
            True if API is healthy, False otherwise
        """
        try:
            # Test with a simple embedding using V4 parameters
            await self.create_embeddings(
                ["test transcript content"],
                task="retrieval.passage",
                dimensions=128,  # Use smallest dimension for health check
                late_chunking=False
            )
            logger.info("Jina V4 API health check passed")
            return True
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False


================================================
FILE: jina/embeddings_service.py
================================================
# Factory function for compatibility with other modules

# Factory function for compatibility with other modules
def get_jina_embeddings_service(config=None) -> 'JinaEmbeddingsService':
    """
    Returns a new instance of JinaEmbeddingsService.
    You can enhance this to use a true singleton if needed.
    """
    return JinaEmbeddingsService(config)
"""
Jina Embeddings v4 Service Layer - Optimized for Transcript Processing
"""

import asyncio
import logging
import tiktoken
from typing import List, Dict, Any, Optional
from .embeddings_client import JinaEmbeddingsClient
from .config import JinaConfig
from .models import JinaModelInfo, TranscriptEmbeddingConfig

logger = logging.getLogger(__name__)

class JinaEmbeddingsService:
    """High-level service for Jina Embeddings v4 operations - Transcript focused"""
    
    def __init__(self, config: Optional[JinaConfig] = None):
        logger.info("Initializing JINA v4 EmbeddingsService for transcript processing...")
        try:
            self.config = config or JinaConfig()
            logger.info(f"JINA v4 Config loaded - API key present: {bool(self.config.api_key)}")
            logger.info(f"JINA v4 Features - Late chunking: {self.config.late_chunking}, Context: {self.config.context_length}")
            self.client = JinaEmbeddingsClient(self.config)
            
            # Initialize tokenizer for transcript processing
            try:
                self.tokenizer = tiktoken.get_encoding("cl100k_base")
            except:
                logger.warning("Could not load tiktoken, using approximate token counting")
                self.tokenizer = None
                
            logger.info("JINA v4 EmbeddingsClient created successfully")
        except Exception as e:
            logger.error(f"Error in JINA v4 EmbeddingsService.__init__: {e}")
            raise
        
    def _count_tokens(self, text: str) -> int:
        """Count tokens in text using tiktoken or approximation"""
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        else:
            # Rough approximation: 1 token ≈ 4 characters
            return len(text) // 4
    
    async def embed_transcripts(
        self, 
        transcripts: List[str], 
        config: Optional[TranscriptEmbeddingConfig] = None
    ) -> List[Dict[str, Any]]:
        """
        Embed video transcripts with optimal JINA V4 settings
        
        Args:
            transcripts: List of video transcript texts
            config: Optional configuration for transcript embedding
            
        Returns:
            List of embedding results with metadata
        """
        config = config or TranscriptEmbeddingConfig()
        results = []
        
        try:
            async with self.client:
                for i, transcript in enumerate(transcripts):
                    token_count = self._count_tokens(transcript)
                    
                    logger.info(f"Processing transcript {i+1}/{len(transcripts)} - {token_count} tokens")
                    
                    # Determine if we need chunking
                    if token_count > self.config.max_tokens and not config.late_chunking:
                        # Traditional chunking for very long transcripts
                        chunks = await self._chunk_transcript(transcript, config)
                        embeddings = await self.client.create_embeddings(
                            [chunk["text"] for chunk in chunks],
                            task=config.task,
                            dimensions=config.dimensions,
                            late_chunking=False
                        )
                        
                        result = {
                            "transcript_index": i,
                            "token_count": token_count,
                            "chunks": len(chunks),
                            "embeddings": [emb.embedding for emb in embeddings.data],
                            "chunk_metadata": chunks,
                            "processing_method": "traditional_chunking"
                        }
                    else:
                        # Use late chunking or direct embedding
                        embeddings = await self.client.create_embeddings(
                            [transcript],
                            task=config.task,
                            dimensions=config.dimensions,
                            late_chunking=config.late_chunking,
                            truncate_at_maximum_length=True
                        )
                        
                        result = {
                            "transcript_index": i,
                            "token_count": token_count,
                            "chunks": 1,
                            "embeddings": [embeddings.data[0].embedding],
                            "processing_method": "late_chunking" if config.late_chunking else "direct"
                        }
                    
                    results.append(result)
                    
            logger.info(f"Successfully embedded {len(transcripts)} transcripts")
            return results
            
        except Exception as e:
            logger.error(f"Error embedding transcripts: {e}")
            raise
    
    async def embed_for_rag(
        self, 
        transcripts: List[str],
        dimensions: int = 1024
    ) -> List[Dict[str, Any]]:
        """
        Embed transcripts optimized for RAG (Retrieval-Augmented Generation)
        
        Args:
            transcripts: List of video transcript texts
            dimensions: Embedding dimensions (128-2048)
            
        Returns:
            RAG-optimized embedding results
        """
        config = TranscriptEmbeddingConfig(
            task="retrieval.passage",
            dimensions=dimensions,
            late_chunking=True,
            optimize_for_rag=True
        )
        
        return await self.embed_transcripts(transcripts, config)
    
    async def _chunk_transcript(
        self, 
        transcript: str, 
        config: TranscriptEmbeddingConfig
    ) -> List[Dict[str, Any]]:
        """
        Intelligently chunk a long transcript
        
        Args:
            transcript: Long transcript text
            config: Configuration for chunking
            
        Returns:
            List of text chunks with metadata
        """
        # Default chunk size based on token limits
        chunk_size = config.chunk_size or (self.config.max_tokens - 100)  # Leave buffer
        chunk_overlap = config.chunk_overlap or (chunk_size // 10)  # 10% overlap
        
        chunks = []
        start = 0
        chunk_index = 0
        
        while start < len(transcript):
            # Find sentence boundaries for better chunking
            end = min(start + chunk_size, len(transcript))
            
            # Try to end at sentence boundary
            if end < len(transcript):
                # Look for sentence endings within the last 200 characters
                sentence_end = transcript.rfind('.', end - 200, end)
                if sentence_end > start:
                    end = sentence_end + 1
            
            chunk_text = transcript[start:end].strip()
            
            if chunk_text:
                chunks.append({
                    "text": chunk_text,
                    "chunk_index": chunk_index,
                    "start_char": start,
                    "end_char": end,
                    "token_count": self._count_tokens(chunk_text)
                })
                chunk_index += 1
            
            # Move start position with overlap
            start = max(start + 1, end - chunk_overlap)
        
        logger.info(f"Chunked transcript into {len(chunks)} chunks")
        return chunks
            
    async def get_model_capabilities(self) -> JinaModelInfo:
        """
        Get detailed information about Jina v4 model capabilities
        
        Returns:
            JinaModelInfo with specifications and performance metrics
        """
        try:
            async with self.client:
                return await self.client.get_model_info()
        except Exception as e:
            logger.error(f"Error getting model info: {e}")
            raise
            
    async def test_connection(self) -> bool:
        """
        Test if the Jina API is accessible
        
        Returns:
            True if connection is successful, False otherwise
        """
        try:
            async with self.client:
                return await self.client.health_check()
        except Exception as e:
            logger.error(f"Connection test failed: {e}")
            return False
            
    def get_pricing_info(self) -> Dict[str, Any]:
        """
        Get pricing information for Jina v4 embeddings
        
        Returns:
            Dictionary with pricing details
        """
        return {
            "model": "jina-embeddings-v4",
            "price_per_1k_tokens": 0.00002,  # $0.00002 per 1K tokens
            "currency": "USD",
            "billing_unit": "tokens",
            "free_tier": {
                "available": True,
                "monthly_limit": 1000000,  # 1M tokens per month
                "rate_limit": "600 requests/minute"
            },
            "enterprise": {
                "available": True,
                "custom_pricing": True,
                "dedicated_support": True
            }
        }
        
    def get_performance_metrics(self) -> Dict[str, Any]:
        """
        Get performance benchmarks for Jina v4
        
        Returns:
            Dictionary with performance metrics
        """
        return {
            "model": "jina-embeddings-v4",
            "parameters": "3.8B",
            "benchmarks": {
                "mteb_average": 64.41,
                "retrieval": 50.87,
                "clustering": 49.62,
                "classification": 75.45,
                "reranking": 58.89,
                "sts": 77.12,
                "pair_classification": 85.34,
                "summarization": 31.05
            },
            "languages": {
                "supported": 100,
                "primary": ["en", "zh", "ja", "ko", "ar", "th", "vi", "de", "fr", "es", "it", "pt", "ru", "hi"],
                "multilingual_score": 63.2
            },
            "multimodal": {
                "text": True,
                "images": True,
                "code": True,
                "structured_data": True
            },
            "context_length": 8192,
            "embedding_dimensions": 1024
        }
        
    async def chunk_and_embed(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Dict[str, Any]]:
        """
        Chunk a large text and create embeddings for each chunk
        
        Args:
            text: Large text to chunk and embed
            chunk_size: Size of each chunk in characters
            overlap: Overlap between chunks
            
        Returns:
            List of chunks with embeddings
        """
        try:
            # Simple text chunking
            chunks = []
            start = 0
            while start < len(text):
                end = min(start + chunk_size, len(text))
                chunk_text = text[start:end]
                chunks.append({
                    "text": chunk_text,
                    "start": start,
                    "end": end,
                    "length": len(chunk_text)
                })
                start = end - overlap
                
            # Create embeddings for all chunks using transcript method
            chunk_texts = [chunk["text"] for chunk in chunks]
            embedding_results = await self.embed_transcripts(chunk_texts)
            
            # Combine chunks with embeddings
            for i, chunk in enumerate(chunks):
                if i < len(embedding_results):
                    chunk["embedding"] = embedding_results[i]["embeddings"][0]
                    chunk["embedding_metadata"] = {
                        "dimensions": len(embedding_results[i]["embeddings"][0]),
                        "processing_method": embedding_results[i]["processing_method"]
                    }
                
            return chunks
            
        except Exception as e:
            logger.error(f"Error chunking and embedding text: {e}")
            raise
            
    async def embed_query(self, query: str, dimensions: int = 1024) -> List[float]:
        """
        Embed a search query optimized for RAG retrieval
        
        Args:
            query: Search query text
            dimensions: Embedding dimensions
            
        Returns:
            Query embedding vector
        """
        try:
            async with self.client:
                response = await self.client.create_embeddings(
                    [query],
                    task="retrieval.query",
                    dimensions=dimensions,
                    late_chunking=False  # Queries are typically short
                )
                
                return response.data[0].embedding
                
        except Exception as e:
            logger.error(f"Error embedding query: {e}")
            raise
            
    async def embed_batch_transcripts(
        self,
        transcripts: List[str],
        batch_size: int = 10,
        dimensions: int = 1024
    ) -> List[Dict[str, Any]]:
        """
        Efficiently embed large batches of transcripts
        
        Args:
            transcripts: List of transcript texts
            batch_size: Batch size for processing
            dimensions: Embedding dimensions
            
        Returns:
            List of embedding results
        """
        try:
            async with self.client:
                return await self.client.batch_embeddings(
                    transcripts,
                    batch_size=batch_size,
                    task="retrieval.passage",
                    dimensions=dimensions,
                    late_chunking=True
                )
                
        except Exception as e:
            logger.error(f"Error batch embedding transcripts: {e}")
            raise


================================================
FILE: jina/models.py
================================================
"""
Jina Embeddings v4 API Models - Optimized for Transcript-to-RAG Pipeline
"""

from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field

class JinaInputItem(BaseModel):
    """Individual input item for JINA V4 API"""
    text: str = Field(..., description="Text content to embed")

class JinaEmbeddingRequest(BaseModel):
    """Request model for Jina Embeddings v4 API - Transcript focused"""
    
    input: List[Union[str, JinaInputItem]] = Field(..., description="List of texts to embed")
    model: str = Field(default="jina-embeddings-v4", description="Model name")
    task: str = Field(default="retrieval.passage", description="Task type for transcript content")
    dimensions: Optional[int] = Field(default=1024, description="Embedding dimensions (128-2048)")
    late_chunking: Optional[bool] = Field(default=False, description="Enable late chunking for long transcripts")
    truncate_at_maximum_length: Optional[bool] = Field(default=True, description="Truncate instead of error")
    output_multi_vector_embeddings: Optional[bool] = Field(default=False, description="Multi-vector for late interaction")
    output_data_type: Optional[str] = Field(default="float", description="Output format: float, binary, base64")
    
    class Config:
        schema_extra = {
            "example": {
                "input": [
                    {"text": "Video transcript content here..."},
                    {"text": "Another video transcript..."}
                ],
                "model": "jina-embeddings-v4",
                "task": "retrieval.passage",
                "dimensions": 1024,
                "late_chunking": True,
                "truncate_at_maximum_length": True
            }
        }

class JinaEmbeddingData(BaseModel):
    """Individual embedding data from JINA V4"""
    
    object: str = Field(..., description="Object type (embedding)")
    embedding: List[float] = Field(..., description="Embedding vector")
    index: int = Field(..., description="Index in request")

class JinaEmbeddingUsage(BaseModel):
    """Token usage statistics from JINA V4"""
    
    total_tokens: int = Field(..., description="Total tokens processed")

class JinaEmbeddingResponse(BaseModel):
    """Response model for Jina Embeddings v4 API"""
    
    object: str = Field(..., description="Response object type (list)")
    data: List[JinaEmbeddingData] = Field(..., description="Embedding data")
    model: str = Field(..., description="Model used (jina-embeddings-v4)")
    usage: JinaEmbeddingUsage = Field(..., description="Token usage statistics")
    
    class Config:
        schema_extra = {
            "example": {
                "object": "list",
                "data": [
                    {
                        "object": "embedding",
                        "embedding": [0.1, 0.2, 0.3],
                        "index": 0
                    }
                ],
                "model": "jina-embeddings-v4",
                "usage": {
                    "total_tokens": 100
                }
            }
        }

class JinaModelInfo(BaseModel):
    """JINA v4 Model information and capabilities for transcript processing"""
    
    id: str = Field(..., description="Model ID")
    name: str = Field(..., description="Model display name")
    description: str = Field(..., description="Model description")
    dimensions: int = Field(..., description="Default embedding dimensions")
    max_tokens: int = Field(..., description="Maximum context length")
    parameters: str = Field(..., description="Model parameters (3.8B)")
    github_repo: str = Field(..., description="GitHub repository")
    github_stars: int = Field(..., description="GitHub stars")
    mteb_score: float = Field(..., description="MTEB average score")
    retrieval_score: float = Field(..., description="Retrieval task score")
    
    # V4 specific capabilities
    supports_late_chunking: bool = Field(default=True, description="Supports late chunking")
    supports_multi_vector: bool = Field(default=True, description="Supports multi-vector embeddings")
    dimension_range: List[int] = Field(default=[128, 2048], description="Supported dimension range")
    tasks: List[str] = Field(default=["retrieval.passage", "retrieval.query", "text-matching", "code.query", "code.passage"], description="Supported tasks")
    optimal_for_transcripts: bool = Field(default=True, description="Optimized for video transcript processing")
    multilingual: bool = Field(default=True, description="Supports 100+ languages")
    context_length: int = Field(default=32768, description="Maximum context length (32K tokens)")
    
    class Config:
        schema_extra = {
            "example": {
                "id": "jina-embeddings-v4",
                "name": "Jina Embeddings v4",
                "description": "3.8B parameter universal embedding model optimized for transcript processing",
                "dimensions": 1024,
                "max_tokens": 32768,
                "parameters": "3.8B",
                "github_repo": "jina-ai/jina-embeddings-v4",
                "github_stars": 2847,
                "mteb_score": 64.41,
                "retrieval_score": 50.87,
                "supports_late_chunking": True,
                "optimal_for_transcripts": True
            }
        }

class TranscriptEmbeddingConfig(BaseModel):
    """Configuration for transcript-specific embedding with JINA V4"""
    
    task: str = Field(default="retrieval.passage", description="Task type for transcripts")
    dimensions: int = Field(default=1024, description="Embedding dimensions")
    late_chunking: bool = Field(default=True, description="Enable late chunking for long transcripts")
    chunk_size: Optional[int] = Field(default=None, description="Chunk size (auto if None)")
    chunk_overlap: Optional[int] = Field(default=None, description="Chunk overlap (auto if None)")
    multi_vector: bool = Field(default=False, description="Output multi-vector embeddings")
    optimize_for_rag: bool = Field(default=True, description="Optimize settings for RAG systems")
    
    class Config:
        schema_extra = {
            "example": {
                "task": "retrieval.passage",
                "dimensions": 1024,
                "late_chunking": True,
                "multi_vector": False,
                "optimize_for_rag": True
            }
        }

